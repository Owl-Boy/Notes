{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Obsidian Notes","text":"<p>Publish your public notes with MkDocs</p>"},{"location":"#hello-world","title":"Hello World!","text":"<p>The <code>index.md</code> in the <code>/docs</code> folder is the homepage you see here.</p> <p>The folders in <code>/docs</code> appear as the main sections on the navigation bar.</p> <p>The notes appear as pages within these sections. For example, Note 1 in <code>Topic 1</code></p>"},{"location":"%28Weighted%29%20Vertex%20Cover%20using%20LP/","title":"(Weighted) Vertex Cover using LP","text":"<p>202309211209</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note"]},{"location":"%28Weighted%29%20Vertex%20Cover%20using%20LP/#weighted-vertex-cover-using-lp","title":"(Weighted) Vertex Cover using LP","text":"","tags":["Note"]},{"location":"%28Weighted%29%20Vertex%20Cover%20using%20LP/#vertex-cover","title":"Vertex Cover","text":"<p>\\(|V| = n\\) \\(|E| = m\\)</p> <p>Variables: \\(x_1, \\dots , x_n\\) $$ x_{i}=\\left{ \\begin{align*} 1 &amp;&amp;\\text{iff } v_{i}\\in S\\ 0 &amp;&amp;\\text{otherwise.}</p> <p>\\end{align*} \\right. $$ \\(\\sum_{i=1}^n x_i = |S|\\).</p> <p>Minimise \\(|S|\\), wrt constraints: - \\(x_i + x_j \\geq 1 \\quad\\forall (v_i, v_j) \\in E\\) - \\(\\begin{align*}x_i \\in \\{0,1\\} \\quad\\forall v_i \\in E&amp;&amp;(@)\\end{align*}\\) @ makes the problem NP hard, so we sacrifice the integrality constraint, and just require the variables to be non negative. (We don't need \\(x_i \\leq 1\\) constraint because the program will anyway minimise \\(|S|\\).) We then use rounding techniques to get an actual solution.</p>","tags":["Note"]},{"location":"%28Weighted%29%20Vertex%20Cover%20using%20LP/#weighted-vertex-cover","title":"Weighted Vertex Cover","text":"<p>Graph \\(G\\), each vertex \\(v_i \\in V\\) has a weight \\(w_i \\geq 0\\). Find a vertex cover of minimum weight.</p> <pre><code>title: Check\nThe approx algo based on matchings will not give a 2-approx.\n</code></pre> <p>Minimise \\(\\sum_{i=1}^n w_ix_i\\) (weight of the vertex cover) wrt: - \\(x_i + x_j \\geq 1 \\quad\\forall (v_i, v_j) \\in E\\) - \\(x_i \\geq 0 \\quad\\forall v_i \\in E.\\)</p> <p>Opt solution of LP: \\(x^* = {x^*_1, x^*_2, \\cdots , x^*_n}\\) For any edge (\\(v_i,v_j\\)), \\(x_i^* + x_j^* \\geq 1\\) At least one of \\(x_i^*, x_j^* \\geq 1/2\\)</p>","tags":["Note"]},{"location":"%28Weighted%29%20Vertex%20Cover%20using%20LP/#rounding","title":"Rounding","text":"<p>\\(\\{x_i^*\\}_{i = 1, \\cdots , n}\\), fractional - Construct integral solution \\(x'\\). - Set \\(x'_i = 1\\) iff \\(x_i^* \\geq 1/2\\), otherwise set \\(x'_i = 0\\).</p> <p>Rounded integral solution \\(x' = {x'_i}\\).</p> <pre><code>title:\n**Claim:** $x'$ is a valid vertex cover.\n</code></pre> <p>For each \\(i\\), \\(x'_i \\leq 2x_i^*\\). \\(\\sum_{i=1}^n w_i x'_i \\leq 2\\sum_{i=1}^n w_i x_i^* = 2.OPT(LP).\\)</p> <p>\\(OPT(LP) \\leq OPT(ILP).\\)</p>","tags":["Note"]},{"location":"%28Weighted%29%20Vertex%20Cover%20using%20LP/#references","title":"References","text":"<p>Integrality Gap Linear Programming</p>","tags":["Note"]},{"location":"1-Forms/","title":"1 Forms","text":"<p>202210310910</p> <p>Type : #Note Tags : [[Calc]]</p>"},{"location":"1-Forms/#1-forms","title":"1-Forms","text":"<p>A k-form is a map \\(w: U \\to \\Lambda^k V\\) , the degree of such a \\(w\\) is \\(k\\), one forms are particularly important.</p> <p>Given a k-form \\(\\omega\\) and a l-form \\(\\eta\\) , they can be multiplied to get the k+l-form \\(\\omega\\wedge\\eta\\) $$ \\omega\\wedge\\eta \\equiv_{def} \\omega(p)\\wedge\\eta(p) $$ Given a function \\(f\\) on \\(U\\), we define its exterior derivative \\(df\\) to be the 1-form defined by \\(df (p) [u] = D f(p)[u]\\)  which is the directional derivative of \\(f\\) at \\(p\\) along the vector \\(u\\).</p> <p>Claim: $$ df = \\sum_i{\\partial f \\over \\partial x<sup>i}dx</sup>i $$ Proof: $$ Df(p)[u] = \\sum_i{\\partial f\\over\\partial x<sup>i}dx</sup>i(p)[u]=\\sum_i{\\partial f\\over\\partial x^i}(p)\\phi_i(u) $$</p> <p>This also gives $$ Df(p)[e_i]={\\partial f\\over \\partial x^i}(p)[e_i] $$ This also shows that it is \\(C^\\infty\\) as it is a sum of the form in which all the terms are \\(C^\\infty\\), which are a function multiplied by a constant 1-form. $$ df =\\sum_i{\\partial f \\over\\partial x^i}\\phi_i  $$</p>"},{"location":"1-Forms/#related-problems","title":"Related Problems","text":""},{"location":"1-Forms/#references","title":"References","text":""},{"location":"2023-04-11/","title":"2023 04 11","text":"<p>202304111004</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"2023-04-11/#2023-04-11","title":"2023-04-11","text":""},{"location":"2023-04-11/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nSuppose that $X = U \\cup V$ is covered by two open, simply connected subspaces $U$ and $V$. Further assume that their intersection is path connected. Then $X$ is simply connected.\n</code></pre>"},{"location":"2023-04-11/#theorem-2","title":"Theorem 2:","text":"<pre><code>title:\nSuppose that $X = U \\cup V$ is a union of two open path connected subspaces of $X$. Suppose $U \\cap V$ is path connected.\nFix $x_0 \\in U \\cap V$. Let $i : U \\hookrightarrow X$ and $j : V \\hookrightarrow X$ then the images of the induced homomorphisms $i_*$ and $j_*$ generate $\\Pi_1(X,x_0)$ as a group.\n</code></pre>"},{"location":"2023-04-11/#note-theorem-1-follows-from-theorem-2","title":"NOTE: Theorem 1 follows from theorem 2.","text":"<ul> <li>The theorem says that: Given any loop \\(f\\), in \\(X\\), based at \\(x_{0}\\) it is path homotopic to a product of the form \\((g_{1}*(g_{2}*(g_{3}*(\\dots g_{n})))\\), where each \\(g_{i}\\) is a loop in \\(X\\) based at \\(x_{0}\\), that lies in either \\(U\\) or \\(V\\) (or both).</li> </ul>"},{"location":"2023-04-11/#proof","title":"Proof:","text":"<p>Note that \\(X\\) is path connected. We show that there is a finite subdivision \\(a_{0} &lt; a_{1} &lt; \\dots &lt; a_{n}\\) of \\([0,1]\\) s.t. \\(f(a_{i}) \\in U \\cap V\\)  and \\(f([a_{i},a_{i+1}]) \\subseteq U\\) or \\(V\\) or both. - For this, choose a subdivision \\(b_{0}&lt;b_{1}&lt;\\dots&lt;b_{m}\\) of \\([0,1]\\) s.t. for each \\(i\\), the set \\(f([b_{i-1},b_{i}])\\) is contained in either \\(U\\) or \\(V\\). - This is possible because \\(f ^{-1}(U)\\) and \\(f ^{-1}(V)\\) cover \\([0,1]\\). So by lebesgue number lemma, there is a partition \\(b_{0} &lt; b_{1} &lt; \\dots &lt; b_{m}\\) s.t. each \\([b_{j-1},b_{j}]\\) is contained in either \\(f ^{-1}(U)\\) or \\(f ^{-1}(V)\\) (or both). - If \\(f(b_{i}) \\in U \\cap V\\) for all \\(i\\), then done. Otherwise, let \\(i\\) be an index s.t. \\(f(b_{i}) \\notin U \\cap V\\).  - \\(f([b_{i-1},b_{i}])\\) and \\(f([b_{i},b_{i+1}])\\) lie in either \\(U\\) or \\(V\\). WLOG, \\(f(b_{i}) \\in U\\) and so \\(f([b_{i-1},b_{i}])\\) and \\(f([b_{i},b_{i+1}])\\) both lie in \\(U\\). So we can just delete \\(b_{i}\\).  - Then the new subdivision still has the same property.</p> <p>Given this subdivision, define \\(f_{i}\\) to be the path $$ [0,1] \\xrightarrow{} [a_{i-1},a_{i}]\\xrightarrow{f}X $$ By construction, \\([f] = [f_{1}]*[f_{2}]*\\dots*[f_{n}]\\). Since \\(U \\cap V\\) is path connected choose a path \\(\\alpha_{i} \\subseteq U \\cap V\\) from \\(x_{0}\\) to \\(f(a_{i})\\).  Then define \\(g_{i} := \\alpha_{i-1} * f*\\bar{\\alpha_{i}}\\) So, \\([f] = [g_{1}]*[g_{2}]*\\dots*[g_{n}]\\), which is what was desired.</p>"},{"location":"2023-04-11/#theorem-3","title":"Theorem 3:","text":"<pre><code>title:\n$\\Pi_1(S_n) \\cong \\{1\\}$ for $n \\ge 2$.\n</code></pre>"},{"location":"2023-04-11/#proof_1","title":"Proof:","text":"<ul> <li>Let \\(N = (0,0,\\dots,0,1)\\) and \\(S = (0,0,\\dots,0,-1)\\) be the north and south pole of \\(S ^{n}\\) respectively.</li> <li>Let \\(U = S ^{n}\\setminus \\{ S \\}\\) and \\(V = S ^{n}\\setminus \\{ N \\}\\)</li> <li>\\(h : U \\to \\mathbb{R}^{n}\\), \\(h(x_{1},x_{2},\\dots,x_{n+1}) = \\frac{1}{1+x_{n+1}}(x_{1},x_{2},\\dots,x_{n})\\)</li> <li>Similarly, \\(V \\cong \\mathbb{R} ^{n}\\) and \\(U \\cap V \\cong \\mathbb{R} ^{n} \\setminus\\{ 0 \\}\\) path connected.</li> <li>Then \\(\\Pi_{1}(S_{n})\\) is generated by \\(i_{*}(\\Pi_{1}(\\mathbb{R}^{n}))\\) and \\(j_{*}(\\Pi_{1}(\\mathbb{R}^{n})) \\implies \\Pi_{1}(S ^{n}) \\cong \\{ 1 \\}\\).</li> </ul>"},{"location":"2023-04-11/#theorem-4","title":"Theorem 4:","text":"<pre><code>title:\n$\\Pi_1(\\mathbb{R}\\mathbb{P}^n) \\cong \\mathbb{Z}/2$\n</code></pre>"},{"location":"2023-04-11/#proof_2","title":"Proof:","text":"<ul> <li>\\(p : S ^{n} \\to \\mathbb{RP}^{n}\\) is a covering map, given by identifying each point \\(x\\) to \\(-x\\).</li> <li>This is an open map.</li> <li>\\(S ^{n}\\) is simply connected. </li> <li>This implies, \\(\\Pi_{1}(\\mathbb{RP}^{n},y) \\xrightarrow{bijection}p ^{-1}(y)\\).</li> <li>So \\(\\Pi_{1}(\\mathbb{RP}^{n},y)\\) has only two elements, hence is isomorphic to \\(\\mathbb{Z}/2\\).</li> </ul>"},{"location":"2023-04-11/#example","title":"Example:","text":"<p>Let \\(X\\) be the figure 8 space. Let \\(U\\) and \\(V\\) be the subspaces of \\(X\\) that deformation retract to \\(S ^{1}\\). \\(U \\cap V\\) is path connected. \\(\\implies \\Pi_{1}(X)\\) is generated by the images of two maps from \\(\\Pi_{1}(S ^{1})\\cong \\mathbb{Z}\\), say generators are \\([a],[b]\\). So every element of \\(\\Pi_{1}(X)\\) is written as $$ [a]^{n_{1}}[b] ^{n_{2}}\\dots $$ - The following is a covering space of the figure 8:    Take the axes \\(x,y\\) in \\(\\mathbb{R}^{2}\\) attach a loop at each lattice point in the union of these axes.   This can be used to show that \\([a]*[b] \\neq [b]*[a]\\)</p>"},{"location":"2023-04-11/#references","title":"References","text":""},{"location":"2023-04-13/","title":"2023 04 13","text":"<p>202304131004</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"2023-04-13/#2023-04-13","title":"2023-04-13","text":""},{"location":"2023-04-13/#2-torus","title":"2-torus","text":"<p>Take two torii, paste them along a circle. Denote it by \\(T \\# T\\).</p>"},{"location":"2023-04-13/#theorem","title":"Theorem:","text":"<pre><code>title:\n$\\Pi_1(T\\# T)$ is not abelian.\n</code></pre>"},{"location":"2023-04-13/#proof","title":"Proof:","text":"<p>\\(A \\subset T \\# T\\) given by the edges labelled \\(a\\) and \\(c\\). This is the figure 8 subspace embedded in \\(T \\# T\\). That has fundamental group non abelian.</p>"},{"location":"2023-04-13/#claim-exists-t-t-to-a-a-retraction","title":"Claim: \\(\\exists \\ T\\# T \\to A\\) a retraction.","text":"<ul> <li>Collapse along the circle used to glue the two torii. To get two torii glued at a single point and then retract each \\(T\\) to \\(S ^{1}\\). Hence, \\(\\Pi_{1}(A) \\subseteq \\Pi_{1}(T \\# T)\\) and so, the required group is non abelian.</li> </ul> <p>Take \\((X,x_{0})\\) and \\((Y,y_{0})\\) be two pointed spaces, then \\((X \\times Y,(x_{0},y_{0}))\\) is a pointed space. <pre><code>\\usepackage{tikz-cd}\n\\begin{document}\n\\begin{tikzcd}\n%&amp;{(X \\times Y,(x_0,y_0))} \\arrow[ld, \"p = \\pi_X\"] \\arrow[rd, \"q = \\pi_Y\"] &amp; \\\\\n{(X,x_0)} &amp; &amp; {(Y,y_0)}\n\\end{tikzcd}\n\\end{document}\n</code></pre> The maps $$ p_{} : \\Pi_{1}(X\\times Y,(x_{0},y_{0})) \\to \\Pi_{1}(X,x_{0}) $$ $$ q_{}:\\Pi_{1}(X \\times Y,(x_{0},y_{0})) \\to \\Pi_{1}(Y,y_{0}) $$</p> <p>Give an isomorphism:$$ \\Phi : (p_{},q_{}) : \\Pi_{1}(X\\times Y,(x_{0},y_{0})) \\to \\Pi_{1}(X,x_{0}) \\times \\Pi_{1}(Y,y_{0}) $$</p>"},{"location":"2023-04-13/#proof_1","title":"Proof:","text":"<p>Any element of RHS: \\(([f],[g])\\) Then \\(h : I \\to X \\times Y\\) : \\(h(s) = (f(s),g(s))\\). Then \\(p \\circ h = f\\), \\(q \\circ h = g \\implies \\Phi([h]) = ([f],[g])\\) surjective.</p> <p>If \\(h : I \\to X \\times Y\\), s.t. \\(\\Phi([h]) = 0\\) then \\(p \\circ h : I \\to X\\) is path homotopic to the constant loop \\(e_{x_{0}}\\) at \\(x_{0}\\). Similarly \\(q \\circ h\\).</p> <p>Let \\(F : I \\times I \\to X\\) be the first homotopy and \\(G : I \\times I \\to Y\\) be the second homotopy. Then \\(H : I \\times I \\to X \\times Y\\)$$ H(s,t) := (F(s,t),G(s,t)) $$ \\(\\implies [h] = e_{(x_{0},y_{0})} \\implies \\Pi_{1}(T) \\cong \\mathbb{Z} \\times \\mathbb{Z}\\).</p>"},{"location":"2023-04-13/#corollary","title":"Corollary:","text":"<p>Compact, connected surfaces \\(S^{2},T,T \\# T\\) are all topologically distinct.</p>"},{"location":"2023-04-13/#references","title":"References","text":""},{"location":"24th%20October%20Classwork/","title":"24th October Classwork","text":""},{"location":"24th%20October%20Classwork/#ifpl","title":"IFPL","text":"<ul> <li>Zipwith example with pattern matching definition </li> <li>Zipwith example with case expressions</li> <li>nub (nodubs) example with pattern matching and case expressions</li> <li></li> </ul>"},{"location":"30%20Oct%20Work/","title":"30 Oct Work","text":"<p>L- language D be a non empty set A is the universe A D-structure would be First Order Structure but relations are take stuff to D</p> <p>A valuation takes stuff to the universe like it normally does D will be given structure like Heyting Algebra or Boolean Algebra, but the structure needs to complete</p> <p>\\(r:A^x\\to D\\) \\([[\\bot]]=0\\) \\([[\\phi \\land \\psi]]=[[\\phi]]\\sqcap [[\\psi]]\\) \\([[\\phi \\lor\\psi]]=[[\\phi]]\\sqcup[[\\psi]]\\) \\([[\\psi\\to\\psi]]=[[\\phi]]\\implies[[\\psi]]\\) \\([[\\exists x \\phi]]_{\\rho}= \\sup\\{[[\\phi]]_{\\rho'}\\}\\) where rho rewrites x. for all rewriting of x \\([[\\forall x \\phi]]_{\\rho}= \\inf\\{[[\\phi]]_{\\rho'}\\}\\)</p> <p>Then we proved some of the formulas are not valid in first order intuitionistic logic, examples were given in the book</p> <p>Then we showed soundness</p>"},{"location":"A%20Proof%20System%20for%20FOL/","title":"A Proof System for FOL","text":"<p>202310101410</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"A%20Proof%20System%20for%20FOL/#a-proof-system-for-fol","title":"A Proof System for FOL","text":"","tags":["Note","Incomplete"]},{"location":"A%20Proof%20System%20for%20FOL/#axioms","title":"Axioms","text":"<ul> <li>All propositional axioms</li> <li>\\(A_{2a}:x\\equiv x\\) for all variables \\(x\\)</li> <li>\\(A_{2b}: t\\equiv u \\implies \\varphi(t)\\iff\\varphi(u)\\), where \\(\\varphi\\) is any atomic formula</li> <li>\\(A_{3}:\\varphi(t)\\implies\\exists x\\ \\varphi(x)\\)</li> </ul>","tags":["Note","Incomplete"]},{"location":"A%20Proof%20System%20for%20FOL/#derivation-rules","title":"Derivation rules","text":"<ul> <li>Modus Ponens: \\(\\dfrac{\\varphi\\ \\varphi\\implies\\psi}{\\psi}\\)</li> <li>Generalisation: \\(\\dfrac{\\varphi(x)\\implies\\psi}{\\exists x\\ \\varphi(x)\\implies\\psi}x\\not\\in Fv(\\psi)\\)</li> </ul> <p>You can take a look at this example. </p>","tags":["Note","Incomplete"]},{"location":"A%20Proof%20System%20for%20FOL/#soundness","title":"Soundness","text":"<p>If \\(X\\vdash\\varphi\\) then \\(X\\vDash\\varphi\\).</p> <pre><code>title: Motivation\nWe just need to prove that\n- the axioms are true\n- the derivation rules preserve validity (i.e. if a formula is valid, MP or Gen on it will be valid)\n\nThen we look at the derivation of $\\varphi$ from $X$ and be like every step in the derivation is either an axiom, which we have shown is sound, or uses previous stuff and a derivation rule, which we have also shown is sound.\n</code></pre> <p>Proof: By induction on the length of the derivation \\(X\\vdash\\varphi\\). - Propositional axioms:   - \\(a\\implies(b\\implies a)\\)   For every interpretation \\(\\mathcal{I}\\), we want to show that either \\(\\mathcal{I}\\vDash\\lnot a\\) or \\(\\mathcal{I}\\vDash\\lnot b \\lor a\\).   If \\(\\mathcal{I}\\vDash\\lnot a\\), we are done.   Otherwise \\(\\mathcal{I}\\vDash a\\) which implies \\(\\mathcal{I}\\vDash\\lnot b\\lor a\\), and we are done.   - \\((a\\implies(b\\implies c))\\implies((a\\implies b)\\implies(a\\implies c))\\)   For every interpretation \\(\\mathcal{I}\\), we want to show that either \\(\\mathcal{I}\\vDash a\\land(b\\land \\lnot c)\\) or \\(\\mathcal{I}\\vDash(a\\land \\lnot b)\\lor(\\lnot a \\lor c)\\).   If \\(\\mathcal{I}\\vDash\\lnot a\\) or if \\(\\mathcal{I}\\vDash c\\) we are done.   So \\(\\mathcal{I}\\vDash a\\) and \\(\\mathcal{I}\\vDash\\lnot c\\).   If \\(\\mathcal{I}\\vDash b\\), we are done, and so are we if \\(\\mathcal{I}\\vDash\\lnot b\\).   - \\((a\\implies b)\\implies((\\lnot a\\implies b)\\implies b)\\)   For every interpretation \\(\\mathcal{I}\\), we want to show that either \\(\\mathcal{I}\\vDash a\\land \\lnot b\\) or \\(\\mathcal{I}\\vDash(\\lnot a\\land \\lnot b)\\lor b\\).   If \\(\\mathcal{I}\\vDash b\\), then we are done.   Otherwise, \\(\\mathcal{I}\\vDash\\lnot b\\). Now if \\(\\mathcal{I}\\vDash a\\), we are done and so are we if \\(\\mathcal{I}\\vDash\\lnot a\\). - \\(A_{2a}\\) Given an interpretation \\(\\mathcal{I}\\), \\(\\sigma(x)=e\\) for some element \\(e\\) in the universe, which is literally the same as saying \\(\\sigma(x)=e\\). - \\(A_{2b}\\) Suppose \\(\\sigma(t)=e\\), \\(e\\) element in the universe. This means \\(\\sigma(u)=e\\). \\(\\varphi^{\\mathcal{I}}(t)=\\varphi^{\\mathcal{I}}(u)\\). - \\(A_{3}\\)</p>","tags":["Note","Incomplete"]},{"location":"A%20Proof%20System%20for%20FOL/#completeness","title":"Completeness","text":"<p>If \\(X\\vDash\\varphi\\), then \\(X\\vdash\\varphi\\).</p> <p>Proof: \\(X\\cup\\{\\lnot\\varphi\\}\\) is not satisfiable. So \\(X\\cup\\{\\lnot\\varphi\\}\\cup\\phi_{Q}\\cup\\phi_{\\text{Eq}}\\cup\\phi_H\\) is not propositionally satisfiable. So there is a finite subset \\(Y\\subset X\\cup\\{\\lnot\\varphi\\}\\cup\\phi_{Q}\\cup\\phi_{\\text{Eq}}\\cup\\phi_H\\) that is not satisfiable.</p> <p>List the formulas in \\(Y\\) as \\(\\alpha_{1},\\dots,\\alpha_{n},\\beta_{1},\\dots,\\beta_{m}\\) as follows. - \\(\\alpha_{1},\\dots,\\alpha_{n}\\) are formulas on \\(X\\cup\\phi_{Q}\\cup\\phi_{Eq}\\), listed in any arbitrary order. - \\(\\beta_{1},\\dots,\\beta_{m}\\) are from \\(Y\\cup\\phi_{H}\\), listed carefully: \\(L_H\\) is the limit of \\(L_{0}\\subset L_{1}\\subset L_{2}\\subset\\dots\\) For a formula \\(\\psi\\) over \\(L_{H}\\), let the rank of \\(\\psi\\) be the least \\(i\\) s.t. \\(\\psi\\) is a formula over \\(L_{i}\\). The list \\(\\beta_{1},\\dots,\\beta_{k}\\) is s.t. \\(\\text{rank}(\\beta_{i})\\ge\\text{rank}(\\beta_{i+1})\\), for all \\(i\\in\\{1,\\dots,m-1\\}\\).</p> <p>A formula in \\(\\phi_H\\) is of the form \\(\\exists x\\ \\psi(x)\\implies\\psi(c_{\\psi(x)})\\). Let's call \\(c_{\\psi(x)}\\) the witnessing constant of \\(\\beta_{l}\\). The witnessing constant of \\(\\beta_{i}\\) does not appear in \\(\\beta_{i+1},\\dots,\\beta_{m}\\). \\(Y\\cup\\{\\lnot\\varphi\\}\\) is not propositionally satisfiable. So the following formula is valid, hence derivable. \\(X\\vdash\\alpha_{1}\\implies(\\alpha_{2}\\implies\\dots\\implies\\alpha_{n}\\implies\\beta_{1}\\implies\\beta_{2}\\implies\\dots\\implies(\\beta_{m}\\implies\\varphi))\\) We replace witnessing constants by variables: \\(X\\vdash\\alpha'_{1}\\implies(\\alpha'_{2}\\implies\\dots\\implies\\alpha'_{n}\\implies\\beta'_{1}\\implies\\beta'_{2}\\implies\\dots\\implies(\\beta'_{m}\\implies\\varphi))\\)</p> <p>\\(X\\vdash\\beta_{1}\\implies(\\beta'_{2}\\implies\\dots\\beta'_{m})\\)</p>","tags":["Note","Incomplete"]},{"location":"A%20Proof%20System%20for%20FOL/#references","title":"References","text":"<p>First Order Logic Satisfiability in First Order Logic</p>","tags":["Note","Incomplete"]},{"location":"A%20Simple%20Example%20-%20cat%20theory%201/","title":"A Simple Example   cat theory 1","text":"<p>202301270201</p> <p>Type : #Example  Tags : [[Category Theory]]</p>"},{"location":"A%20Simple%20Example%20-%20cat%20theory%201/#a-simple-example-cat-theory-1","title":"A Simple Example - cat theory 1","text":"<p>Consider three points; call them \u25cf, \u25cb and \u2217. Here the system:\\(S\\) will simply be a way of connecting these points together. This connection can represent points as sites on a power grid, or a system describing connection by power lines. or as people susceptible to some disease, with a system describing interactions that lead to contagion.</p> <p>An example of such a system would be like: { width=\"150\" } The connections are symmetric and transitive so friendship cannot be represented by the system as a it is not transitive( A friends' friend is not necessaritly a friend) but a disease or a possible communication can be. Two more systems can be </p> <p>![[20230127_03h10m05s_grim.png| center | 420]] Now we define an observation \\(\\Phi:S\\to Boolean\\), which is if \u25cf is connected to \u2217. There are a total of \\(5\\) systems and \\(\\Phi\\) assigns <code>true</code> to the following systems</p> <p>![[20230127_03h31m37s_grim.png| center | 420]] and it assigns <code>false</code> to the remaining systems ![[20230127_03h33m25s_grim.png| center | 550]]</p> <p>The last part of the setup is the sort of operations that we want to perform on the system, one common example is called join. The idea being that the connectivity observations will not be compositional with respect to the join operator, which will be the  generative effect. </p> <p>The Join of two systems \\(A\\) and \\(B\\) is performed by combining their connection, denoted by \\(A\\lor B\\)  ![[20230127_03h45m38s_grim.png| center | 500]] The last example joins two systems that are assigned <code>false</code> by the \\(\\Phi\\) but their join is assigned <code>true</code>. Hence the observation is lossy under \\(\\Phi\\) and a more detailed observation is needed which also inclued \u25cb.  </p>"},{"location":"A%20Simple%20Example%20-%20cat%20theory%201/#references","title":"References","text":"<p>Generating Effects</p>"},{"location":"Abelian%20Groups/","title":"Abelian Groups","text":"<p>202304151104</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Abelian%20Groups/#abelian-groups","title":"Abelian Groups","text":"<pre><code>title:\nGroups in which the binary operation is commutative are called _abelian groups_.\n</code></pre>"},{"location":"Abelian%20Groups/#lemma-1","title":"Lemma 1:","text":"<pre><code>title:\nIn a finite abelian group $G$, $\\mathrm{ord}(b) | \\max \\{\\mathrm{ord}(a) | a \\in G\\}$ for all $b \\in G$.\n</code></pre>"},{"location":"Abelian%20Groups/#proof","title":"Proof:","text":"<p>Let \\(a\\) be the element with maximum order and let \\(b\\) be any other element.  If \\(\\mathrm{ord}(b) \\nmid \\mathrm{ord}(a)\\) then there is a prime \\(p\\) such that \\(\\mathrm{ord}(b) = p^jn\\) and \\(\\mathrm{ord}(a) = p ^{i}m\\) such that \\(j &gt; i\\), and \\(m,n\\) are coprime to \\(p\\). Then \\(\\mathrm{ord}(b^n a ^{p ^{i}}) = p ^{j} m &gt; \\mathrm{ord}(a)\\). This is a contradiction. Hence \\(\\mathrm{ord}(b) | \\mathrm{ord}(a)\\).</p> <p>We are using the fact that if \\(x,y\\) have coprime orders then their product has order equal to the product of their orders. </p>"},{"location":"Abelian%20Groups/#references","title":"References","text":"<p>[[Groups]]</p>"},{"location":"Adal%20week%203/","title":"Adal week 3","text":"<p>Name: Aditi Muthkhod Roll Number: BMC202107</p>","tags":["Assignment"]},{"location":"Adal%20week%203/#nearest-neighbour-algo-for-tsp","title":"Nearest neighbour algo for TSP","text":"<p>Run the nearest neighbour algorithm. Arrange the edges in non increasing order of their weights. \\(E = \\{l_{1},\\dots,l_{n}\\}.\\)</p> <pre><code>title:  Abuse of notation\nWe use $l_{i}$ to denote both the edge and the weight of that edge.\n</code></pre> <p>Claim: \\(l_{1}\\le \\frac{OPT}{2}\\). Proof: Let \\(a,b\\) be the endpoints of \\(l_{1}\\). \\(OPT\\) contains two paths from \\(a\\) to \\(b\\) (one of which might be \\(l_{1}\\) itself). By metric property each of these paths has total weight at least as much as \\(l_{1}\\). So we get \\(OPT \\ge 2l_{1}\\). It follows that \\(l_{2}\\le \\frac{OPT}{2}\\).</p> <p>Lemma: For any edge \\(l_{i}\\), let \\(a_{i}\\) be the endpoint that was chosen before the other endpoint. For any pair of edges \\(l_{i},l_{j}\\), \\(wt(a_{i},a_{j})\\ge\\min\\{l_{i},l_{j}\\}\\). Proof: 1. Case I: In our algorithm, \\(l_{i}\\) is chosen before \\(l_{j}\\). Then \\(wt(a_{i},a_{j}) \\ge l_{i}\\). 2. Case I: In our algorithm, \\(l_{j}\\) is chosen before \\(l_{i}\\). Then \\(wt(a_{i},a_{j}) \\ge l_{j}\\).</p> <p>Claim: \\(l_{2^{k}+1}+\\dots +l_{2^{k+1}}\\le \\frac{OPT}{2}\\), \\((0\\le k\\le \\lceil\\log n\\rceil-1)\\). Proof: Let \\(S\\) be the weight of the tour \\(\\mathcal T\\) obtained by shortcutting the optimal solution by eliminating all vertices other than \\(a_{i},\\dots,a_{2^{k+1}}\\). Every edge in this tour is of the form \\((a_{i},a_{j})\\) for some \\(i,j \\le 2^{k+1}\\). From the previous Lemma, we know that the weight of this edge is at least \\(\\min\\{l_{i},l_{j}\\}\\). So, \\(\\(OPT \\ge S \\ge\\sum\\limits_{(a_{i},a_{j})\\in\\mathcal T}\\min\\{l_{i},l_{j}\\}.\\)\\) Observe that, on the RHS, each term \\(l_{i}\\), specifically for \\(2^{k}+1\\le i\\le 2^{k+1}\\), can appear at most twice (because \\(a_{i}\\) has exactly two edges of \\(\\mathcal T\\) incident on it). We get \\(\\(OPT \\ge S \\ge\\sum\\limits_{(a_{i},a_{j})\\in\\mathcal T}\\min\\{l_{i},l_{j}\\}\\ge 2(l_{2^{k}+1}+\\dots+l_{2^{k+1}}),\\)\\) as required.</p> <p>Thus we get that weight of our solution \\(= l_{1}+\\dots+l_{n}\\le \\frac{\\log n}{2}.OPT\\), an \\(O(\\log n)\\) approximation!</p>","tags":["Assignment"]},{"location":"Adjacency%20Algebra%20of%20a%20Graph/","title":"Adjacency Algebra of a Graph","text":"<p>202308091108</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Adjacency%20Algebra%20of%20a%20Graph/#adjacency-algebra-of-a-graph","title":"Adjacency Algebra of a Graph","text":"<pre><code>title:\nLet $A$ be the adjacency matric os $G$\nThen the adjacency algebra of $G$ is\n$$\n\\mathcal A(G) = \\sum\\limits_{t} r_{t}A^{t} : r_{t}\\in\\mathbb R\n$$\n</code></pre>"},{"location":"Adjacency%20Algebra%20of%20a%20Graph/#theorem","title":"Theorem","text":"<p>Let \\(d=diam(G)\\), then \\(\\dim(\\mathcal A(G))&gt;d\\)</p>"},{"location":"Adjacency%20Algebra%20of%20a%20Graph/#proof","title":"Proof","text":"<p>\\(\\exists x,y\\in V\\) and \\(d(x,y)=d\\) then \\((A^{t})_{xy}=0\\) for \\(t&lt;d\\) but it is \\(1\\) for \\(t=d\\) then  \\(I, A, A^{2},\\dots, A^{d}\\) are linearly independent, Assume  \\(I, A, A^{2,\\dots,}A^{d-1}\\) are linearly independent but \\(A^d\\) is the only matrix with a non zero value at position (x,y). Hence to add them to \\(0\\) the coefficient of \\(A^{d}\\) should be \\(0\\). To justify the assumption, given the above \\(x, y\\) there exists \\(y'\\) such that \\(d(x, y')=d-1\\), which means the sum would be zero if coefficient of \\(A^{d-1}\\) is \\(0\\). Then induct down to nothingness.</p>"},{"location":"Adjacency%20Algebra%20of%20a%20Graph/#theorem_1","title":"Theorem","text":"<p>Let \\(G\\) have edges. then the following are equivalent: 1. \\(G\\) is connected and \\(k-\\)regular for some \\(k\\ge 0\\)  2. \\(J\\in \\mathcal A(G)\\)</p>"},{"location":"Adjacency%20Algebra%20of%20a%20Graph/#proof_1","title":"Proof","text":"<p>\\((1\\to 2)\\) Let \\(m(x)=(x-k)q(x)\\) be the minimal polynomial of \\(A\\) we have \\(Aq(A)=kq(A)\\) every column of \\(q(A)\\) is an eigenvector corresponding to \\(k\\) so every column is a multiple of \\(\\hat j\\)  But since \\(kq(A)\\) belongs to the Adjacency Algebra its symmetric. hence the the matrix will be \\(xJ\\).</p> <p>\\((2\\to 1)\\)  given that \\(J\\) is a polynomial in \\(\\mathcal A\\) we have that \\(J\\) and \\(A\\) commute.</p> <p>Hence matrix containing column sums along a column is the same as the matrix containing row sums along the rows.</p> <p>Thus \\(AJ=JA=kJ\\)  Existence of \\(J\\) in the algebra also gives that the graph is connected. As otherwise, relabel the vertices of \\(A\\) such that vertices of connected components are together, then the adjacency matrix would look like blocks along diagonal and everything else is \\(0\\), which means \\(J\\) could not be in the algebra.</p>"},{"location":"Adjacency%20Algebra%20of%20a%20Graph/#references","title":"References","text":"<p>Distances in Graphs</p>"},{"location":"Alg3%20class/","title":"Alg3 class","text":"<p>16-11-2022 09:27 am</p> <p>Type : #Note Tags : [[Algebra]]</p> <pre><code>title: Theorem\n$\\exists$ a finite field of order $q = p^n$, and all fields of order $q$ are isomorphic. \n</code></pre>"},{"location":"Alg3%20class/#proof","title":"Proof:","text":"<ul> <li>Let \\(K\\) be the field of roots of the polynomial \\(x^q-x\\) over \\(\\mathbb{F}_p\\). This shows existence.</li> <li>For uniqueness, let \\(K,K'\\) be two fields of order \\(q\\), they are vector spaces over \\(\\mathbb{F}_p\\). Elements of \\(K,K'\\) satisfy \\(x^q-x\\) (why? because the non zero elements of \\(K'\\) forms a group of order \\(q-1\\), hence \\(x^{q-1}=1 \\ \\forall \\ x \\in K'\\)). Hence they are isomorphic.</li> </ul> <pre><code>title: Theorem\nA field of order $p^r$ contains a field of order $p^k$ iff $k | r$\n</code></pre> <pre><code>title: Lemma\nLet $X,Y$ be variables and $q = p^r$, then \n$$(X+Y)^q = X^q + Y^q$$\n</code></pre>"},{"location":"Alg3%20class/#proof_1","title":"Proof:","text":"<ul> <li>Binomial expansion</li> </ul> <pre><code>title:Lemma \nLet $K$ be a field and let $L = \\{ \\alpha \\in K \\ | \\ \\alpha^q-\\alpha = 0\\}$\nThen $L$ is a subfield of $K$.\n</code></pre>"},{"location":"Alg3%20class/#proof_2","title":"Proof:","text":"<ul> <li>Just check closure under addition, multiplication, and inverses.</li> </ul> <pre><code>title: Claim\n$-1$ is a root of $x^q-x$.\n</code></pre>"},{"location":"Alg3%20class/#proof_3","title":"Proof:","text":"<ul> <li>\\(q\\) is odd, if \\(p\\) is odd.</li> <li>if \\(p = 2\\), then \\((-1)^q+1 = 1 + 1 = 2 = 0 \\ ( \\text{mod} \\ 2)\\)</li> </ul>"},{"location":"Alg3%20class/#proof-of-theorem","title":"Proof of Theorem:","text":"<ul> <li>Let \\(q = p^r, q' = p^k\\)</li> <li>Then, \\([\\mathbb{F}_q : \\mathbb{F}_p] = r\\) and \\([\\mathbb{F}_{q'} : \\mathbb{F}_p] = k\\)</li> <li>If \\(\\mathbb{F}_q\\) contains an isomorphic copy of \\(\\mathbb{F}_{q'}\\), then \\(k | r\\).</li> <li>Conversely, if \\(k | r\\), we need to show that \\(\\mathbb{F}_{q'}\\) is a subfield of \\(\\mathbb{F}_q\\).</li> <li>By our lemma, it is enough to show that elements of \\(\\mathbb{F}_{q'}\\) satisfy \\(x^q-x\\).</li> <li>enough to show that \\(x^{q'}-x | x^q-x\\).</li> </ul> <pre><code>title: Primitive Element\nLet $K/F$ be a field extension. An element $\\alpha \\in K$ is called a primitive element if $K = F(\\alpha)$\n</code></pre> <pre><code>title: Primitive Element Theorem\n1) Let $K/F$ be a finite extension then there is a primitive element for K/F iff the number of intermediate fields is finite.\n2) Let $K/F$ be a finite and separable extension, then $K$ has a primitive element.\n</code></pre>"},{"location":"Alg3%20class/#proof_4","title":"Proof:","text":"<ul> <li>If \\(K/F\\) is a finite field, then \\(K^{\\times}\\) is a cyclic group.</li> </ul>"},{"location":"Alg3%20class/#references","title":"References:","text":"<p>Algebraic Extension Extension Field Fields</p>"},{"location":"Algebraic%20Closure/","title":"Algebraic Closure","text":"<p>202305250905</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Algebraic%20Closure/#algebraic-closure","title":"Algebraic Closure","text":"<pre><code>title:\nA field $\\Omega$ is called **algebraically closed** if it satisfies one of the following equivalent conditions:\n1. All nonconstant polynomials in $\\Omega[X]$ have a root in $\\Omega$.\n2. The irreducibles in $\\Omega[X]$ are the linear polynomials.\n3. All nonconstant polynomials in $\\Omega[X]$ split in $\\Omega$.\n4. Every field of finite degree over $\\Omega$ equals $\\Omega$.\n</code></pre>"},{"location":"Algebraic%20Closure/#proof-that-the-statements-are-equivalent","title":"Proof that the statements are equivalent:","text":"<p>\\((1) \\implies (2) \\implies (3) \\implies (1)\\) is trivial. \\((1) \\implies (4)\\) : A field of finite degree over \\(\\Omega\\) contains elements of only finite degree over \\(\\Omega\\). Take such an element \\(\\alpha\\), and its minimal poly over \\(\\Omega\\), call it \\(f\\). \\(f\\) has a root in \\(\\Omega\\) by (1), but \\(f\\) is irreducible hence it must be linear, and so \\(\\alpha \\in \\Omega\\).</p> <p>\\((4) \\implies (1)\\) Take a non constant poly \\(f\\), take the extension \\(\\Omega/(f) = \\Omega\\) by (4). Then \\(deg(f) = [\\Omega /(f) : \\Omega] = 1\\) And so \\(f\\) has a root in \\(\\Omega\\).</p>"},{"location":"Algebraic%20Closure/#theorem","title":"Theorem:","text":"<pre><code>title:\nIf $\\Omega$ is algebraic over $F$ and every polynomial $f \\in F[X]$ splits in $\\Omega[X]$, then $\\Omega$ is algebraically closed. \n</code></pre>"},{"location":"Algebraic%20Closure/#proof","title":"Proof:","text":"<p>Take a polynomial \\(f = a_{0} + a_{1}x + \\dots a_{n}x ^{n} \\in \\Omega[X]\\). Take a root \\(\\alpha\\) of \\(f\\) in an extension \\(\\Omega'\\) of \\(\\Omega\\). $$ F \\subset F[a_{0},a_{1},\\dots a_{n}] \\subset F[a_{0},a_{1},\\dots a_{n}][\\alpha] $$ Since each extension is finite, each extension is algebraic and hence \\(\\alpha\\) is algebraic over \\(F\\). So there is a poly \\(g \\in F[X]\\) with \\(g(\\alpha) = 0\\). But \\(g\\) splits in \\(\\Omega[X]\\) hence \\(\\alpha \\in \\Omega\\). Thus, all roots of \\(f\\) are in \\(\\Omega\\), hence \\(f\\) splits in \\(\\Omega\\), so \\(\\Omega\\) is algebraically closed.</p>"},{"location":"Algebraic%20Closure/#definition","title":"Definition:","text":"<pre><code>title: Algebraic closure\n1. Let $F$ be a field and $\\Omega$ an integral domain containing $F$ as a subring.\nThen $$ \\bar{F} := \\{\\alpha \\in \\Omega \\ |\\  \\alpha \\text{ algebraic over } F\\} $$\nis a field called the _algebraic closure of $F$ in $\\Omega$_.\n\n2. A field $\\Omega$ is an _algebraic closure_ of a subfield $F$ if it is algebraic over $F$ and is algebraically closed.\n</code></pre>"},{"location":"Algebraic%20Closure/#theorem_1","title":"Theorem:","text":"<pre><code>title:\nLet $\\Omega$ be an algebraically closed field. For any subfield $F$ of $\\Omega$, the algebraic closure $E$ of $F$ in $\\Omega$ is an algebraic closure of $F$.\n</code></pre>"},{"location":"Algebraic%20Closure/#proof_1","title":"Proof:","text":"<p>\\(E\\) is algebraic over \\(F\\) by definition. Now take a poly over \\(F\\), we want to show it splits in \\(E\\). This poly obviously splits in \\(\\Omega\\), since it is algebraically closed. But the roots all belong to \\(E\\) by definition of \\(E\\), hence it splits in \\(E\\).</p>"},{"location":"Algebraic%20Closure/#references","title":"References","text":"<p>Algebraic Extension</p>"},{"location":"Algebraic%20Extension/","title":"Algebraic Extension","text":"<p>202210200910</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Algebraic%20Extension/#algebraic-extension","title":"Algebraic Extension","text":"<p>\\(\\alpha \\in K\\) is said to be algebraic over \\(F\\) if it is the root of some non-zero polynomial \\(f(x)\\in F[x]\\). If not, \\(\\alpha\\) is called Transcendental over \\(F\\). If \\(\\forall\\alpha\\in K\\) is algebraic over \\(F\\), \\(K\\) is called an Algebraic Extension of \\(F\\). </p> <pre><code>If $\\alpha$ is algebraic over some field $F$ then it is algebraic over all field extensions $L$ of $F$.\n</code></pre> <p>Proposition: Let \\(\\alpha\\) be algebraic over \\(F\\), then there exists a unique irreducible monic polynomial \\(m_{\\alpha, F}(x)\\in F(x)\\) which has \\(\\alpha\\) as its root. Any \\(f(x)\\) has \\(\\alpha\\) as a root iff \\(m_{\\alpha, F}(x) \\nmid f(x)\\) Proof: Let \\(g(x)\\) be the polynomial of least degree with \\(\\alpha\\) as its root. \\(g(x)\\) cannot be written as \\(a(x)\\cdot b(x)\\) where \\(a(x)\\) and \\(b(x)\\) are polynomials of smaller degrees, becuase that would imply \\(\\alpha\\) is a root of either \\(a(x)\\) or \\(b(x)\\), which contradicts the minimality of \\(g(x)\\). Suppose there exits another monic polynomial \\(f(x)\\) which is irreducible and \\(\\alpha\\) is its root. Then using the euclidian algorithm, we can find  $$ f(x) = q(x)\\cdot g(x) + r(x) $$ which implies \\(\\alpha\\) is a root of \\(r(x)\\) which has degree less than \\(g(x)\\) which also contradicts the minimality of \\(g(x)\\), Hence it is unique.</p> <p>Proposition: If \\(\\alpha\\) is algebraic over \\(F\\) and \\(F(\\alpha)\\) is the field generated by \\(\\alpha\\) over \\(F\\) then  $$ F(\\alpha)\\cong F[x]/\\big(m_{\\alpha}(x)\\big) $$ and  $$ [F(\\alpha):F]=\\deg m_\\alpha(x) = \\deg \\alpha $$ i.e the degree of \\(\\alpha\\) over \\(F\\) is the degree of the extension it generates over \\(F\\) </p> <p>Proposition: If \\(\\alpha\\) is algebraic over \\(F\\) and \\(F(\\alpha)\\) is the smallest field extension of \\(F\\) containing \\(\\alpha\\),then \\([F(\\alpha):F]\\) will be finite Proof: Let \\([F(\\alpha):F] = n\\), take the \\(n+1\\) elements  $$ 1, \\alpha, \\alpha^2\\dots \\alpha^n $$ of \\(F(\\alpha)\\) are linearly dependent. Then $$ b_0 + b_1\\alpha+\\dots+ b_n\\alpha^n = 0 $$ for not all \\(b_i=0\\), hence, there exits a polynomial of degree atmost \\(n\\) such that \\(\\alpha\\) is a root of it.</p> <p>This implies  Proposition: If the extension \\(K/F\\) is finite, then it is algebraic. and Theorem: The extension \\(K/F\\) is finite \\(\\iff\\) \\(K\\) is generated by a finite number of algebraic elements over \\(F\\). More precisely, a field generated by algebraic elements of degrees \\(n_1,n_2, n_3\\dots n_k\\) is algebraic of degree \\(&lt;n_1n_2n_3\\dots n_k\\) Corollary: if \\(\\alpha\\) and \\(\\beta\\) are algebraic over \\(F\\), then \\(\\alpha \\pm \\beta, \\alpha\\beta,\\frac\\alpha\\beta\\) are also algebraic over \\(F\\).</p>"},{"location":"Algebraic%20Extension/#related-problems","title":"Related Problems","text":""},{"location":"Algebraic%20Extension/#references","title":"References","text":"<p>Fields</p>"},{"location":"Algebraic%20Integers/","title":"Algebraic Integers","text":"<p>202305311805</p> <p>Type : #Note Tags : [[Number Theory]] [[Algebra]]</p>"},{"location":"Algebraic%20Integers/#algebraic-integers","title":"Algebraic Integers","text":"<pre><code>title:\nA complex number is called an _algebraic integer_ if it satisfies some monic integer polynomial. (Denote the set of alg ints by $\\bar{\\mathbb{Z}}$)\n</code></pre>"},{"location":"Algebraic%20Integers/#lemma-1","title":"Lemma 1:","text":"<pre><code>title: \nLet $f$ be a monic polynomial with coefficients in $\\mathbb{Z}$, and suppose $f = gh$ with $g,h$ monic in $\\mathbb{Q}$.\nThen $g,h \\in \\mathbb{Z}[x]$.\n</code></pre>"},{"location":"Algebraic%20Integers/#proof","title":"Proof:","text":"<p>Let \\(m,n\\) be the smallest integers such that \\(mg,nh \\in \\mathbb{Z}[x]\\), then the coefficients of \\(mg\\) are coprime, and those of \\(nh\\) are also coprime.</p> <p>Then \\(mnf = mg \\cdot nh \\in mn\\mathbb{Z}[x]\\). If \\(mn &gt; 1\\), there is a prime \\(p\\) such that \\(p \\mid mn\\), then modding out by \\(p\\), we get \\(\\bar{0} = \\overline{mg}\\cdot \\overline{nh}\\) in \\(\\mathbb{Z} /p\\mathbb{Z}[x]\\). But since \\(\\mathbb{Z} /p\\mathbb{Z}[x]\\) is an integral domain, one of \\(mg\\) and \\(nh\\) belong to \\(p\\mathbb{Z}[x]\\) this contradicts that both of them have gcd 1 among their coefficients. This gives \\(mn = 1 = m = n\\), hence \\(g,h \\in \\mathbb{Z}[x]\\).</p>"},{"location":"Algebraic%20Integers/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nLet $\\alpha$ be an algebraic integer, and let $f$ be a monic polynomial over $\\mathbb{Z}$ of least degree having $\\alpha$ as a root. Then $f$ is irreducible over $\\mathbb{Q}$. (Equivalently, the monic irreducible polynomial over $\\mathbb{Q}$, having $\\alpha$ as a root has coefficients in $\\mathbb{Z}$)\n</code></pre>"},{"location":"Algebraic%20Integers/#proof_1","title":"Proof:","text":"<p>If \\(f\\) is reducible, then \\(f = gh\\) where \\(g,h \\in \\mathbb{Q}[x]\\) and are monic(WLOG). Then by the lemma, \\(g,h \\in \\mathbb{Z}[x]\\). This gives either \\(g\\) or \\(h\\) has \\(\\alpha\\) as root, contradiction since they have smaller degrees.</p>"},{"location":"Algebraic%20Integers/#corollary-1","title":"Corollary 1:","text":"<p>```ad-note: title: \\(\\mathbb{Q} \\cap \\bar{\\mathbb{Z}} = \\mathbb{Z}\\). <pre><code>### Corollary 2:\n```ad-note\ntitle:\nLet $m$ be a squarefree integer. The set of algebraic integers in the quadratic field $\\mathbb{Q}(\\sqrt[]{ m })$ is \n$$\n\\begin{align}\n\\{ a+b \\sqrt[]{ m } : a,b \\in \\mathbb{Z} \\} \\ \\text{if}\\ m\\equiv 2,3 \\ (\\mathrm{mo d} \\ 4) \\\\ \\\\\n\\left\\{  \\frac{a+b \\sqrt[]{ m }}{2} : a,b \\in \\mathbb{Z}  \\right\\} \\ \\text{if} \\ m \\equiv 1 \\ (\\mathrm{mo d} \\ 4)\n\\end{align}\n$$\n</code></pre></p> <p>This shows that the set of algebraic integers in \\(\\mathbb{Q}[\\sqrt[]{ m }]\\) form a ring. The same is true for any number field.</p>"},{"location":"Algebraic%20Integers/#theorem-2","title":"Theorem 2:","text":"<pre><code>title:\nTFAE for $\\alpha \\in \\mathbb{C}$:\n1. $\\alpha$ is an algebraic integer.\n2. The additive group of the ring $\\mathbb{Z}[\\alpha]$ is finitely generated.\n3. $\\alpha$ is a member some subring of $\\mathbb{C}$ having a finitely generated additive group.\n4. $\\alpha A \\subset A$ for some finitely generated additive subgroup $A \\subset \\mathbb{C}$.\n</code></pre>"},{"location":"Algebraic%20Integers/#proof_2","title":"Proof:","text":"<p>\\((1) \\implies (2)\\) If \\(\\alpha\\) is an algebraic integer, then there is some \\(n \\in \\mathbb{N}\\) such that \\(1,\\alpha, \\dots, \\alpha^n\\) are linearly dependent. Hence \\(\\mathbb{Z}[\\alpha]\\) is finitely generated as an additive group.</p> <p>\\((2) \\implies (3) \\implies (4)\\) Trivial</p> <p>\\((4) \\implies (1)\\) Let \\(a_{1},a_{2},\\dots,a_{n}\\) generate \\(A\\). Then there is an \\(n \\times n\\) matrix \\(M\\) with integer entries such that: $$ \\begin{pmatrix} \\alpha a_{1}  \\ \\alpha a_{2} \\ \\vdots \\ \\alpha a_{n} \\end{pmatrix} = M \\begin{pmatrix} a_{1} \\ a_{2} \\ \\vdots \\ a_{n} \\end{pmatrix} $$ This means \\(M-\\alpha I\\) has zero determinant. This gives a monic polynomial in \\(\\alpha\\) evaluating to \\(0\\).</p>"},{"location":"Algebraic%20Integers/#corollary-1_1","title":"Corollary 1:","text":"<pre><code>title:\nIf $\\alpha,\\beta$ are algebraic integers, then so are $\\alpha+\\beta$ and $\\alpha\\beta$.\n</code></pre> <p>This shows that the set of algebraic integers in \\(\\mathbb{C}\\) form a ring, denoted \\(\\bar{\\mathbb{Z}}\\).</p>"},{"location":"Algebraic%20Integers/#note","title":"NOTE:","text":"<p>Given a number field \\(K\\), the subring \\(\\bar{\\mathbb{Z}} \\cap K\\) is the number ring corresponding to \\(K\\).</p>"},{"location":"Algebraic%20Integers/#theorem-3","title":"Theorem 3:","text":"<pre><code>title:\nLet $\\alpha$ be a root of a monic polynomial over $\\overline{\\mathbb{Z}}$ then $\\alpha \\in \\overline{\\mathbb{Z}}$.\n</code></pre>"},{"location":"Algebraic%20Integers/#proof_3","title":"Proof:","text":"<p>Let the poly be \\(f(x) = x ^{n} + a_{n-1}x ^{n-1} + \\dots +a_{0}\\) Note that \\(\\mathbb{Z}[\\alpha_{1},\\dots\\alpha_{n-1},\\alpha]\\) is finitely generated, then by Theorem 2 part (3) we are done.</p>"},{"location":"Algebraic%20Integers/#references","title":"References","text":"<p>Number Field</p>"},{"location":"Algorithmic%20Coding%20Theory%20Lec%201/","title":"Algorithmic Coding Theory Lec 1","text":"<p>202308101508</p> <p>Type : #Note #Rename Tags : Algorithmic Coding Theory</p>"},{"location":"Algorithmic%20Coding%20Theory%20Lec%201/#algorithmic-coding-theory-lec-1","title":"Algorithmic Coding Theory Lec 1","text":"<pre><code>title: Redundancies\nWe want to either transmit a message through a noisy channel or read some stored data which might have been corrupted due to hardware issues . So we add redundancies\n</code></pre> <p>\\(\\Sigma\\) is a finite alphabet. Code of blocks of length \\(n\\in\\Sigma^{n}\\) \\(|\\Sigma|=q\\) and it might grow with \\(n\\) </p> <p>The two ideas that are used to measure the amount of redundancies in a code are Dimension and Rate of a code where higher rate implies code with less redundancies.</p>"},{"location":"Algorithmic%20Coding%20Theory%20Lec%201/#some-basic-codes","title":"Some Basic Codes","text":""},{"location":"Algorithmic%20Coding%20Theory%20Lec%201/#repetition","title":"Repetition","text":"<p>For every single character in the code, repeat it twice \\(\\(010\\longmapsto001100\\)\\) Here if one of the digit gets changed in the noisy channel we get \\(001101\\). Here we can see that an error is detected at the last digit.</p> <p>We can replace every single digit with 3 digits. Then a single error can be both detected and corrected</p>"},{"location":"Algorithmic%20Coding%20Theory%20Lec%201/#parity-code","title":"Parity Code","text":"<p>$$ x_{1}, x_{2}, x_{3}, x_{4} \\longmapsto x_{1}, x_{2}, x_{3}, x_{4},x_{1}\\oplus x_{2}\\oplus x_{3}\\oplus x_{4} $$ this adds the parity of \\(1\\) in the code, changing any one digit makes the parity not match, hence it detects a single error.</p>"},{"location":"Algorithmic%20Coding%20Theory%20Lec%201/#references","title":"References","text":"<p>Measurement of Redundancy in a code Encoding and Decoding Functions Distance of a Code</p>"},{"location":"Almost%20All%20Automorphism%20Groups%20are%20Trivial/","title":"Almost All Automorphism Groups are Trivial","text":"<p>202308141108</p> <p>Type : #Note  Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Almost%20All%20Automorphism%20Groups%20are%20Trivial/#almost-all-automorphism-groups-are-trivial","title":"Almost All Automorphism Groups are Trivial","text":""},{"location":"Almost%20All%20Automorphism%20Groups%20are%20Trivial/#theorem","title":"Theorem","text":"\\[ \\lim\\limits_{n\\to\\infty} \\frac{\\zeta(n)}{\\frac{2^{n\\choose 2}}{n!}}=1 \\]"},{"location":"Almost%20All%20Automorphism%20Groups%20are%20Trivial/#references","title":"References","text":""},{"location":"Alt/","title":"Alt","text":"<p>202210110910</p> <p>Type : #Note Tags : [[Calc]]</p>"},{"location":"Alt/#alt","title":"Alt","text":"<p>The \\(\\text{Alt}\\) function is of the type \\(\\mathcal T^k(V)\\to \\Lambda^k(V)\\) where \\(\\mathcal T^k(V)\\) is the set of all multilinear maps from \\(V^k \\to \\mathbb R\\) \\(\\Lambda^k(V)\\) is the set of all skew-symmetric multilinear maps from \\(V^k\\to\\mathbb R\\)</p>"},{"location":"Alt/#alt2mathcal-t2vto-lambda2v","title":"\\(Alt^2:\\mathcal T^2(V)\\to \\Lambda^2(V)\\)","text":"<p>let \\(B\\in \\mathcal T^2(V)\\)</p> <p>$$ \\text{Alt}^2(B)(v_1, v_2) = \\frac12\\big(B(v_1,v_2)-B(v_2,v_1)\\big) $$ If \\(B\\in \\Lambda^2(V)\\), Then \\(\\text{Alt}^2(B) = B\\)</p> <p>Similarly we can define:</p>"},{"location":"Alt/#textaltk-mathcal-tk-to-lambdak","title":"\\(\\text{Alt}^k : \\mathcal T^k \\to \\Lambda^k\\)","text":"<p>let \\(B\\in\\mathcal T^k(V)\\) $$ \\text{Alt}^k(B)(v_1,v_2\\dots v_n) =  \\frac1{k!}\\sum \\text{sgn}(\\sigma)B(v_{\\sigma(1)},v_{\\sigma(2)}\\dots v_{\\sigma(n)}) $$</p>"},{"location":"Alt/#related-problems","title":"Related Problems","text":""},{"location":"Alt/#references","title":"References","text":"<p>Exterior Algebra</p>"},{"location":"Alternating%20Finite%20Automaton/","title":"Alternating Finite Automaton","text":"<p>202311031511</p> <p>Tags : [[Theory of Computation]]</p>","tags":["Note"]},{"location":"Alternating%20Finite%20Automaton/#alternating-finite-automaton","title":"Alternating Finite Automaton","text":"<p>[!important] Motivation Some of the basic operations that [[regular languages]] are closed under are really simple to perform on a Deterministic Finite Automaton, but not as easy as a Non-Deterministic Finite Automaton. Specifically complementation. NFAs cannot be trivially complemented because a word can have multiple runs that may or may not reach a final state. Dealing with all of them together makes the process easier. The problem is created because there is a tool to deal with disjunction of runs, namely non-determinism. But there isn't a tool to deal with conjunctions. Alternating Finite Automata offer us that tool.</p> <p>The idea is to give transitions that allow a letter to go to multiple states, but with an and condition. That is, all runs that diverge at this set of transitions much reach accepting states.</p> <p>An Alternative Finite Automaton is an NFA where the following changes are made to transitions. $$ \\delta: Q\\times \\Sigma\\to 2<sup>{2</sup>Q} $$ Intuitively, the transitions in the above set represent disjunctions of transitions that, from a state after accepting a letter, goes to a set of states and all runs from that state should go to final states.</p>","tags":["Note"]},{"location":"Alternating%20Finite%20Automaton/#expressive-power","title":"Expressive Power","text":"<p>Alternating Finite Automata can accept regular langauges. This is because  there is a direct construction of an AFA to an NFA, that is the subset construction, where the final states of the NFA \\(2^F\\) if \\(F\\) is the set of final states in the AFA. </p>","tags":["Note"]},{"location":"Alternating%20Finite%20Automaton/#closure-properties","title":"Closure Properties","text":"<p>Alternating Finite Automata are closed under - Union and Intersection (use the new transition to directly get a new construction) - Complementation (Replace all the accepting and non accepting states, and switch all \\(\\land\\) and \\(\\lor\\) in the transition)</p>","tags":["Note"]},{"location":"Alternating%20Finite%20Automaton/#references","title":"References","text":"<p>Wiki Alternating Timed Automata</p>","tags":["Note"]},{"location":"Alternating%20Timed%20Automata/","title":"Alternating Timed Automata","text":"<p>202311031611</p> <p>Tags : Timed Automata</p>","tags":["Note"]},{"location":"Alternating%20Timed%20Automata/#alternating-timed-automata","title":"Alternating Timed Automata","text":"","tags":["Note"]},{"location":"Alternating%20Timed%20Automata/#definition","title":"Definition","text":"<p>An Alternating Timed Automaton is a tuple \\(\\langle Q,q_{0},\\Sigma,X,T,F\\rangle\\) where - \\(Q\\) is a set of states - \\(q_0\\in Q\\) is the starting state - \\(X\\) is a set of clock - \\(\\Sigma\\) is the alphabet - \\(F\\) is the set of final states. - \\(T:Q\\times \\Sigma \\times \\Phi(X)\\to \\mathcal P(Q\\times \\mathcal P(X))\\) is the set of transitions or - \\(T:Q\\times \\Sigma \\times \\Phi(X)\\to \\mathcal B^+(Q\\times \\mathcal P(X))\\)</p> <p>Intuitively, given a state and a transition that can satisfied you return a set of states and resets for each of the states, such that from each of those states, there should be an accepting run. Or it goes to a boolean formula of states that represents both disjunctions and conjunctions of runs.</p>","tags":["Note"]},{"location":"Alternating%20Timed%20Automata/#accepting-runs","title":"Accepting runs","text":"<p>Given a transition in an Alternating Timed Automaton </p> <p>There is an accepting run from \\(q\\) if  - There is an accepting run from each of\\(q_1\\) and \\(q_2\\) after the transition. or - There is an accepting run from \\(q_3\\) after the transition, or - There is an accepting run from each of \\(q_1\\), \\(q_2\\) and \\(q_3\\) after the transition.</p>","tags":["Note"]},{"location":"Alternating%20Timed%20Automata/#acceptance-game","title":"Acceptance Game","text":"<p>There is a game corresponding to the acceptance of a given word ina timed automata that can be played between \\(2\\) players such that if one the players has a winning strategy then the word is accepted.</p>","tags":["Note"]},{"location":"Alternating%20Timed%20Automata/#construction-and-gameplay","title":"Construction and Gameplay","text":"<p>Given a timed word \\(w\\in (\\Sigma,T)^*\\) we construct a graph such that we construct a tree for a timed automata, and we start at the starting state as the root. Then for each transition to an element of \\(b\\in B^+(Q\\times \\mathcal P(X))\\) If \\(b\\) if of the from - \\(b_1\\land b_2\\dots b_n\\) then Player 1 chooses a sub-formula - \\(b_1\\lor b_2\\dots b_n\\) then Player 2 chooses a sub-formula After we reach an atomic formula \\(p\\) we pick the corresponding transition in the graph keep repeating this until the word is fully red</p>","tags":["Note"]},{"location":"Alternating%20Timed%20Automata/#winning-scenarios","title":"Winning Scenarios","text":"<p>We say that Player 2 wins, if the final state that the game ends on is a final state, otherwise Player 1 wins</p> <p>We say that a word is accepted iff Player 2 has a winning strategy</p> <p>[!example] Running the alternating timed automaton given in the Alternating Timed Automata#Example section, we can construct the following graph for its runs on the word \\(\\{aaa , (0.3,0.8,1.5)\\}\\) ![[Alternating Timed Automata Game Example.excalidraw]] Here, Player 1 has a choice to make on the first step, but whatever they do, they eventually reach a final state. Hence Player 2 will always win. Hence the word will be accepted.</p> <p>The correctness of the game is trivial.</p>","tags":["Note"]},{"location":"Alternating%20Timed%20Automata/#closure-properties","title":"Closure Properties","text":"<p>Alternating Timed Automata are closed under - Union and Intersection (use the new transition to directly get a new construction) - Complementation (Replace all the accepting and non accepting states, and switch all \\(\\land\\) and \\(\\lor\\) in the transition)</p> <p>This shows that Alternating Timed Automata are strictly more powerful than timed automata as the languages accepted by these are closed under complementation.</p>","tags":["Note"]},{"location":"Alternating%20Timed%20Automata/#example","title":"Example","text":"<p>![[Timed Automata Example.excalidraw|700]]</p>","tags":["Note"]},{"location":"Alternating%20Timed%20Automata/#references","title":"References","text":"<p>Expressability of One-Clock Alternating Timed Automata Alternating Finite Automaton</p>","tags":["Note"]},{"location":"Analytic%20Function/","title":"Analytic Function","text":"<p>202301041401</p> <p>Type : #Note Tags : [[Complex Analysis]]</p>"},{"location":"Analytic%20Function/#analytic-function","title":"Analytic Function","text":""},{"location":"Analytic%20Function/#complex-derivative","title":"Complex Derivative","text":"<pre><code>title: \nlet $f:U\\to\\mathbb C$ where $U$ is an open subset of $\\mathbb C$, then the derivative of $f$ exists at $a\\in U$ if the following limit exists\n$$\n\\lim_{z\\to a}\\frac{f(z)-f(a)}{z-a}\n$$\n</code></pre> <p>If \\(f, g\\) are differentiable at \\(a\\) then \\(f\\pm g, fg\\) are differentiable; \\(\\frac fg\\) is differentiable at \\(a\\) if \\(g(a)\\ne 0\\)</p>"},{"location":"Analytic%20Function/#analytic-function_1","title":"Analytic Function","text":"<p><pre><code>title:\nA function $f:U\\to \\mathbb C$ is said to be **Analytic**(or **Holomorphic**) if $f'(z)$ exists $\\forall z\\in U$\n</code></pre> Some examples are - Identity Function - Constant Functions - Polynomials - Rational Functions (away from zeros of divisors) - Power Series</p> <p>Proposition: If \\(u(x, y), v(x, y)\\) have first order partial deriavtives which satisfy the  CR Equations, then \\(f=u+\\iota v\\) is analytic with continuous partial derivative Proof: We may write \\(u(x+h, y+k) - u(x, y) = {\\partial u \\over \\partial x}h+{\\partial u \\over \\partial x}k + \\epsilon_1\\) and \\(v(x+h, y+k) - v(x, y) = {\\partial b \\over \\partial x}h+{\\partial v \\over \\partial x}k + \\epsilon_2\\) such that \\(\\epsilon_i \\over n + \\iota k\\) \\(\\to 0\\) as \\(n+\\iota k \\to 0\\)  $$ \\begin{aligned} f(z+h + ik) - f(z) &amp;= \\left(\\frac{\\partial u}{\\partial x} + \\iota\\frac{\\partial v}{\\partial x}\\right)\\cdot(h+\\iota k)+\\epsilon_1 + \\epsilon_2\\ \\frac{f(z+h+\\iota k) - f(z)}{h + \\iota k} &amp;= \\left(\\frac{\\partial u}{\\partial x} + \\iota\\frac{\\partial v}{\\partial x}\\right) + \\frac{\\epsilon_1 + \\epsilon_2}{h+\\iota k}\\ \\implies f'(z) &amp;= \\frac{\\partial u}{\\partial x} + \\iota\\frac{\\partial v}{\\partial x} \\end{aligned} $$ \\({\\partial u \\over \\partial x}, {\\partial v \\over \\partial x}\\) are continuous, Hence \\(f'(x)\\) is continuous.</p>"},{"location":"Analytic%20Function/#related-problems","title":"Related Problems","text":""},{"location":"Analytic%20Function/#exercise","title":"exercise:","text":"<p>If \\(f:\\mathbb{C} \\to \\mathbb{R}\\) is a function such that \\(f'(a)\\) exists then show that \\(f'(a) = 0\\).</p>"},{"location":"Analytic%20Function/#references","title":"References","text":""},{"location":"Applications%20of%20Error%20Correcting%20Codes/","title":"Applications of Error Correcting Codes","text":"<p>202311141511</p> <p>Tags : Algorithmic Coding Theory</p>","tags":["Note","Incomplete"]},{"location":"Applications%20of%20Error%20Correcting%20Codes/#applications-of-error-correcting-codes","title":"Applications of Error Correcting Codes","text":"","tags":["Note","Incomplete"]},{"location":"Applications%20of%20Error%20Correcting%20Codes/#group-testing-problem","title":"Group Testing problem","text":"<p>There are \\(N\\) people, \\(n\\) have some disease, say CoViD. We want to test the people for the disease, and the tests are costly. \\(n\\ll N\\). The naive way would be to draw a sample from each of them, and test each sample separately, which is costly. So another way to approach this is 'pooled testing', i.e. you pool samples together, and test them in groups.</p>","tags":["Note","Incomplete"]},{"location":"Applications%20of%20Error%20Correcting%20Codes/#sparse-recovery-compressed-sensing","title":"Sparse recovery-compressed sensing","text":"<p>\\(x \\in\\mathbb{C}^{n}\\) vector is \\(L-\\)sparse (# non zero entries \\(=L\\)) The goal is to design a measurement matrix \\(M_{(m\\times n)}\\) s.t. given \\(y=Mx\\), we can uniquely reconstruct \\(x\\). \\(m\\) should be as small as possible.</p>","tags":["Note","Incomplete"]},{"location":"Applications%20of%20Error%20Correcting%20Codes/#streaming-algorithms","title":"Streaming algorithms","text":"<p>Data is coming in a stream: \\(x_{1},x_{2},\\dots,x_{t}\\in U\\) (universe), \\(|U|=n\\). We want to do frequency counts \\(f_{i}=\\#i\\) appears in the stream We don't want to store the frequency vector, because it can be huuuugeeeeeeee, if \\(n\\) is large, although that would solve the problem. Here it would be easier to multiply the frequency vector, \\(f_{v}\\) by a 'short, fat matrix', \\(\\phi\\) and store the small vector instead.</p>","tags":["Note","Incomplete"]},{"location":"Applications%20of%20Error%20Correcting%20Codes/#references","title":"References","text":"<p>Error Correcting Codes</p>","tags":["Note","Incomplete"]},{"location":"Approximation%20Algorithms/","title":"Approximation Algorithms","text":"<p>202309281809</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note"]},{"location":"Approximation%20Algorithms/#approximation-algorithms","title":"Approximation Algorithms","text":"<pre><code>title: Motivation\n\nIn life we are faced with a lot of problems. Some of them are *Decision Problems*: (Yes/No)\n- Is there a path of weight $\\le w$ from $s$ to $t$ in $G$?\n\nSome are *Optimization Problems*: (Each solution has a cost.)\n- Find the min cost/weight path from $s$ to $t$ in $G$.\n- Find the min cost brain we can replace my friend's with.\nIn this course, we will mostly concern ourselves with the latter.\n\nBecause we believe that we can't find optimal solutions to all of these fast $(P\\neq NP)$, we try coming up with *Approximation Algorithms*, which we then prove to not be &lt;i&gt;that&lt;/i&gt; far from optimal.\n</code></pre> <pre><code>*$NP$ complete problems* are decision problems in $NP$ s.t. every problem in $NP$ reduces to them in polytime.\nPolytime algo for one $NP$ complete problem $\\implies$ polytime algo for all $NP$ problems.\n</code></pre>","tags":["Note"]},{"location":"Approximation%20Algorithms/#approximation","title":"Approximation","text":"<p>Definition: An algorithm \\(A\\) is said to be an \\(\\alpha\\)-approximation for an optimisation problem if for every instance \\(I\\), $$ \\begin{align} \\frac{A(I)}{OPT(I)}&amp;\\le \\alpha &amp;&amp;\\text{Minimisation}\\\\ \\frac{OPT(I)}{A(I)}&amp;\\le \\alpha &amp;&amp;\\text{Maximisation} \\end{align} $$</p> <pre><code>Approximation is good as a ratio and not as an additive factor because we can scale and ignore the additive factor.\n</code></pre>","tags":["Note"]},{"location":"Approximation%20Algorithms/#references","title":"References","text":"<p>Vertex Cover Problem</p>","tags":["Note"]},{"location":"Arc-Transitivity%20and%20Girth%20Relation/","title":"Arc Transitivity and Girth Relation","text":"<p>202308251128</p> <p>type :  #Example tags : [[Algebraic Graph Theory]]</p>"},{"location":"Arc-Transitivity%20and%20Girth%20Relation/#arc-transitivity-and-girth-relation","title":"Arc-Transitivity and Girth Relation","text":"<p>Statement: Let minimum degree \\(\\delta(G)\\ge 3\\). Let \\(G\\) be \\(s\\)-transitive. Then the girth \\(g\\) satisfies $$ g\\ge 2(s-1) $$</p> <p>Proof: If \\(\\not\\exists\\) a cycle then \\(g=\\infty\\), nothing to prove. So let \\(g\\) be a positive integer. If \\(s\\le 2\\) then nothing to prove as minimum value for \\(g\\) is \\(3\\) If \\(s\\ge 3\\) then we claim that \\(g\\ge s\\) Suppose not. Then there exists an arc of length \\(g\\) with its starting and ending vertices the same, and an arc of length \\(g\\) with different starting and ending points. Hence \\(g\\ge s\\) ![[Drawing 2023-08-25 19.47.12.excalidraw|200]] Now consider a graph with girth \\(g\\).  And consider the path \\(g_{0},g_{1},\\dots g_{s}\\). Since minimum degree is \\(3\\), consider \\(z\\sim g_{s-1}\\). Then the path \\(g_{0},g_{1}\\dots g_{s-1},z\\) will also be a path of a cycle because this is an automorphism.</p> <p>![[Drawing 2023-08-25 19.52.20.excalidraw|200]]</p> <p>Now we have two cycles that contain \\(g_{1}\\) and \\(g_{s-1}\\), each of which is \\(g\\) long. and the common path is \\(g-s\\) long. If we remove the common part, then we get another cycle, whose length has to be at least \\(g\\) as the graph has girth \\(g\\) hence \\(2(g-s+1)\\ge g\\) which proves the theorem.</p>"},{"location":"Arc-Transitivity%20and%20Girth%20Relation/#related","title":"Related","text":"<p>Arc-Transitivity of a Graph Arc-Transitivity and diameter</p>"},{"location":"Arc-Transitivity%20and%20diameter/","title":"Arc Transitivity and diameter","text":"<p>202308281040</p> <p>type : #Example tags : [[Algebraic Graph Theory]]</p>"},{"location":"Arc-Transitivity%20and%20diameter/#arc-transitivity-and-diameter","title":"Arc-Transitivity and diameter","text":""},{"location":"Arc-Transitivity%20and%20diameter/#theorem","title":"Theorem:","text":"<p>Let \\(G\\) be an \\(s-\\)arc transitive graph with \\(s\\ge 4\\) and \\(g=2s-s\\). Then the diameter of a graph is \\(s-1\\) and the graph is bipartite.</p>"},{"location":"Arc-Transitivity%20and%20diameter/#answer","title":"Answer:","text":"<p>The Girth of the graph is \\(2s-2\\). Hence there is a cycle of length \\(2s-2\\) and the diametrically opposite points have distance \\(s-1\\) between them.</p> <p>Suppose there exists points with distance more than \\(s-1\\). There there are \\(x, y\\) such that the distance between them is exactly \\(s\\). Then let \\(\\alpha=x_{0},x_{1},\\dots,x_{s}\\). Then map an arc in a cycle of length \\(2s-2\\) onto the \\(\\alpha\\). Then the the other arc in the cycle gives a path of length \\(s-2\\) from \\(x_{0}\\) to \\(x_{s}\\) of length \\(s-2\\) which is a contradiction. Hence the diameter of the graph is \\(s-1\\).</p> <p>To show that it is bipartite we need to show that \\(G\\) has no cycles of length \\(2t-1\\) where \\(t\\) is at least \\(s\\). Consider the smallest such \\(t\\). Such a cycle will be induced. As if there are chords there. Then the cycle will be divided into two parts where one of them would be a smaller odd cycle.</p> <p>Since the diameter of the graph is \\(s-1\\) then \\(t\\not\\ge s\\) otherwise the induced odd cycle would have diameter more than \\(s-1\\).</p> <p>Consider an odd cycle of size \\(2s-1\\) then consider \"diametrically opposite points\" \\(z,y\\) of \\(x\\).</p> <p>Consider the path along the cycle \\(x\\rightsquigarrow y\\rightsquigarrow z\\) which is a path of length \\(s\\). Map an arc of the same length onto it from a cycle of length \\(2s-2\\). This gives another path of length \\(s-2\\) from \\(x\\) to \\(z\\). We also have a path of length \\(s-1\\) from \\(x\\) to \\(z\\). then combining them gives a cycle of length \\(s-3\\) which is a contradiction.</p>"},{"location":"Arc-Transitivity%20and%20diameter/#related","title":"Related","text":"<p>Arc-Transitivity and Girth Relation Arc-Transitivity of a Graph</p>"},{"location":"Arc-Transitivity%20of%20a%20Graph/","title":"Arc Transitivity of a Graph","text":"<p>202308251108</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Arc-Transitivity%20of%20a%20Graph/#arc-transitivity-of-a-graph","title":"Arc-Transitivity of a Graph","text":"<p>An \\(s\\)-arc is a sequence \\(v_{0}v_{1}\\dots v_{s-1}v_{s}\\) such that \\(v_{i}\\sim v_{i+1}\\)</p> <p>Let \\(\\Gamma\\) be an Automorphism Group of \\(G\\) such that \\(\\Gamma\\) is \\(s\\)-transitive \\(\\forall\\) s-arcs \\(\\alpha,\\beta\\in G\\exists g\\in\\Gamma\\) such that \\(g(\\alpha)=\\beta\\)</p>"},{"location":"Arc-Transitivity%20of%20a%20Graph/#references","title":"References","text":"<p>Caley Graphs T-Transitiviy of Graphs [[Sharp Transitively]] Arc-Transitivity and Girth Relation</p>"},{"location":"Arrow%20Type/","title":"Arrow Type","text":"<p>202307250007</p> <p>Type : #Note Tags : [[Type Theory]]</p>"},{"location":"Arrow%20Type/#arrow-type","title":"Arrow Type","text":"<p>A term of arrow type is reducible iff all its applications to reducible terms are reducible. 1. CR1 If \\(t\\) is reducible of type \\(U\\to V\\), let \\(x\\) be a variable of type \\(U\\); the induction hypothesis(CR3) for \\(U\\) says that the term \\(x\\), which is neutral and normal, is reducible. So \\(t\\ x\\) is reducible. Just as in the case of product type, we remark that \\(\\nu(t)\\le \\nu(t\\ x)\\). The induction hypothesis(CR1) for \\(V\\) guarantees that \\(\\nu(t\\ x)\\) is finite so \\(\\nu(t)\\) is finite, and the \\(t\\) is strongly normalizable. 2. CR2 If \\(t\\rightsquigarrow t'\\) and \\(t\\) is reducible, take \\(u\\) reducible of type \\(U\\); then \\(t\\ u\\) is reducible and \\(t'\\ u\\) is reducible hence \\(t'\\) is reducible 3. CR3 Let \\(t\\) be neutral and suppose all the \\(t'\\) one strop from \\(t\\) are reducible. Let \\(u\\) be a reducible term of type \\(U\\); we want to show that \\(t\\) is reducible. By induction hypothesis(CR1) for \\(U\\), we know that \\(u\\) is strongly normalizable. So we can reason by induction of \\(\\nu(u)\\).</p> <p>In one step, \\(t\\ u\\) converts to    - \\(t'\\ u\\) with \\(t'\\) one step from \\(t\\); but \\(t'\\) is reducible, so is \\(t'\\ u\\)    - \\(t\\ u'\\) with \\(u'\\) one step from \\(u\\), since \\(u'\\) is reducible, by induction hypothesis(CR2) for \\(U\\), and \\(\\nu(u')&lt;\\nu(u)\\); so the induction hypothesis for \\(u'\\) tells us that \\(t\\ u'\\) is reducible    - since \\(t\\ u\\) cannot be a redex, it is reducible, hence \\(t\\) is reducible</p>"},{"location":"Arrow%20Type/#references","title":"References","text":"<p>Product Type Atomic Types</p>"},{"location":"Asymmetric%20Graphs/","title":"Asymmetric Graphs","text":"<p>202308202308</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Asymmetric%20Graphs/#asymmetric-graphs","title":"Asymmetric Graphs","text":"<p>A graph is called asymmetric if its automorphism group is the identity group.</p>"},{"location":"Asymmetric%20Graphs/#theorem","title":"Theorem","text":"<p>Almost all graphs are Asymmetrical</p>"},{"location":"Asymmetric%20Graphs/#proof","title":"Proof","text":"<p>Given \\(n\\) vertices on a graph. The number of possible edges is \\(n\\choose 2\\), hence the total number of possible graphs with \\(n\\) vertices are \\(2^{n\\choose 2}\\). </p> <p>Given a graph \\(X\\), the set of isomorphic graphs to \\(X\\) is called the isomorphism class of \\(X\\). And the size of the isomorphism class of \\(X\\) is  $$ \\frac{n!}{|\\text{Aut}(X)|} $$ Proof from Orbit Stabilizer Theorem .</p> <p>If a permutation of vertices has \\(r\\) orbits, then it fixes at least \\(2^{r}\\) sets of edges(r sets directly and union of those sets of edges). Then Burnside's Lemma states that the number of isomorphism classes on a vertex set \\(V\\) is equal to  $$ \\frac{1}{n!}\\sum\\limits_{g\\in\\text{Sym}(V)}2^{orb_{2}(g)} $$And if all graphs were asymmetrical then every isomorphism class would contain \\(n!\\) elments and the number of isomorphism class would be  $$ \\frac{2^{n\\choose 2}}{n!} $$ We use the following lemma </p> <p>Suppose the proportion of isomorphic classes that are asymmetric is \\(\\mu\\) and each isomorphic class that is not asymmetric contains atmost \\(\\frac{n!}{2}\\) elements. Then the average size of an isomorphism class is atmost  $$ n!\\left(\\mu + \\frac{1-\\mu}{2}\\right)=\\frac{n!}{2}(1+\\mu) $$ Hence  $$ \\frac{n!}{2}(1+\\mu)(1+o(1)) \\frac{2^{n\\choose 2}}{n!}&gt;2 $$ which implies \\(\\mu = 1\\). Since the proportion of Asymmetric graphs is atleast as much as the proportion of ismorphic classes as they have the largest ismorphism groups, it follows that almost all graphs as asymmetric.</p>"},{"location":"Asymmetric%20Graphs/#references","title":"References","text":"<p>Lemma-Number of isomorphism classes on graphs</p>"},{"location":"Atomic%20Types/","title":"Atomic Types","text":"<p>202307242307</p> <p>Type : #Note Tags : [[Type Theory]]</p>"},{"location":"Atomic%20Types/#atomic-types","title":"Atomic Types","text":"<p>A term of an atomic type is reducible iff it is strongly normalizable so every term must satisfy the following conditions - CR1 is a Tautology  - CR2 If \\(t\\) is strongly normalizable then every term \\(t'\\)  which reduces \\(t\\) to is also strongly normalizable. - CR3 A reduction path leaving \\(t\\) must be through one of the \\(t'\\), which are all strongly normalizable, and so if finite, and \\(v(t)\\) is maximum of \\(v(t')+1\\) where \\(t'\\) vary over one step conversions from \\(t\\)</p>"},{"location":"Atomic%20Types/#references","title":"References","text":"<p>Product Type Arrow Type</p>"},{"location":"Automorphism%20Group%20of%20a%20Graph/","title":"Automorphism Group of a Graph","text":"<p>202308121608</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Automorphism%20Group%20of%20a%20Graph/#automorphism-group-of-a-graph","title":"Automorphism Group of a Graph","text":"<p>The Automorphism Group of a graph is the Group of Automorphisms that can be applied on a graph with the group operation being composition.</p> <p>For example, the Automorphism of a graph \\(C_{4}\\) is \\(D_{4}\\)</p> <p>Graphs that have a trivial Automorphism group are called asymmetric, and if they have a non trivial Automorphism group, they are called symmetric</p> <p>let \\(\\sigma\\in S_n\\) be the action on the set of vertices, then \\(\\sigma'\\) denotes the action of the same automorphism on the edges. (It's a bijection on the powerset of edges)</p> <p>Let \\(\\sigma'\\) have \\(t\\) orbits on \\(E[n]\\), then \\(\\sigma'\\) has \\(2^{t}\\) fixed points on \\(\\mathcal PE[n]\\) Let \\(\\text{orb}(\\sigma')\\) be the number of orbits of \\(\\sigma'\\) on \\(E[n]\\)</p>"},{"location":"Automorphism%20Group%20of%20a%20Graph/#references","title":"References","text":"<p>Peterson Graph Hoffmann-singleton Graph Permutation Matrix Theorem Edge-Transitive, Non Transitive is Bipartite Support of an Automorphism on a Graph ^8a19cf</p>"},{"location":"Autonomous%20System/","title":"Autonomous System","text":"<p>202301111201</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"Autonomous%20System/#autonomous-system","title":"Autonomous System","text":"<p>\\(\\overline x = \\overline v(t, \\overline x)\\) is dependent on \\(t\\), hence it is called a Non Autonomous system whereas the equation \\(\\overline x = \\overline v(\\overline x)\\) is independent of \\(t\\), hence it is called an Autonomous system.</p> <p>Every Non Autonomous system can be converted into an Autonomous System in the following way  $$ \\begin{aligned} {\\partial x \\over \\partial t} &amp;= t\\ {\\partial x\\over \\partial u} &amp;= y \\text{ and } {\\partial y\\over \\partial u}=1 \\end{aligned} $$</p>"},{"location":"Autonomous%20System/#related-problems","title":"Related Problems","text":""},{"location":"Autonomous%20System/#references","title":"References","text":"<p>System of First Order Differential Equations</p>"},{"location":"BPP%20Complexity%20Class/","title":"BPP Complexity Class","text":"<p>202301022001</p> <p>Type : #Note Tags : [[Complexity Theory]]</p>"},{"location":"BPP%20Complexity%20Class/#bpp-complexity-class","title":"BPP Complexity Class","text":""},{"location":"BPP%20Complexity%20Class/#bpp","title":"BPP","text":"<pre><code>title:\nClass of decision problems which can be solved in polynomial time with an  error probability bouneded by $\\frac13$.\n</code></pre>"},{"location":"BPP%20Complexity%20Class/#examples","title":"Examples","text":""},{"location":"BPP%20Complexity%20Class/#related-problems","title":"Related Problems","text":""},{"location":"BPP%20Complexity%20Class/#references","title":"References","text":"<p>Complexity Classes</p>"},{"location":"Banach-Mazur%20Games/","title":"Banach Mazur Games","text":"<p>202311172211</p> <p>Tags : [[Topology]]</p>","tags":["Note","Incomplete"]},{"location":"Banach-Mazur%20Games/#banach-mazur-games","title":"Banach-Mazur Games","text":"<p>[!info]  A Banach-Mazur game is a 2-player game where the players try to pin down points in a space. The concept is closely related to [[Baire Spaces]]. It was the first infinite game positional game with perfect information to be studied. It was introduced by Stanis\u0142aw Mazur as the 43rd problem in the [[Scottish Book]] and the problem was solved by Banach.</p>","tags":["Note","Incomplete"]},{"location":"Banach-Mazur%20Games/#setup","title":"Setup","text":"<p>The game requires 3 things to be played - A topological space \\(Y\\) - A subset \\(X\\subseteq Y\\) - A family of subsets of \\(Y\\) called \\(\\mathcal W\\) which has the following property     - Each member of \\(\\mathcal W\\) has a non-empty interior     - Every non-empty open set of \\(Y\\) contains a member of \\(W\\)</p> <p>A Banach-Mazur game is denoted by \\(MB(X,Y,\\mathcal W)\\)</p>","tags":["Note","Incomplete"]},{"location":"Banach-Mazur%20Games/#gameplay","title":"Gameplay","text":"<p>Starting with player 1, each player a sequence of members of \\(\\mathcal W\\) that are contained in the previous one to form a chain of sets like  $$ W_{0}\\subseteq W_{1} \\subseteq\\dots $$ Given any \\(W_i\\), we know that it has a non-empty interior. So the next player can find an open set \\(O\\subseteq W_i\\). Then she can find \\(W_{i+1}\\subseteq O\\), hence the game will not terminate in a finite number of steps.</p>","tags":["Note","Incomplete"]},{"location":"Banach-Mazur%20Games/#winning-conditions","title":"Winning Conditions","text":"<p>Player 1 wins if \\(W' \\cap X\\ne\\emptyset\\) where \\(W' = \\bigcap_{i\\in\\mathbb N}W_i\\)  Otherwise, Player 2 wins.</p> <ul> <li>There exists a Winning Strategy for Player 2 in Banach-Mazur games iff \\(X\\) is meager. ^972715</li> <li>If \\(Y\\) is a complete metric space, then there exists a Winning Strategy for Player 1 in Banach-Mazur games iff \\(X\\) is large in some non empty open subset of \\(Y\\).</li> </ul> <p>[!info]  The question that Mazur added to the book was that the above conditions are the only conditions where the game is determinable and a winning strategy exists.</p>","tags":["Note","Incomplete"]},{"location":"Banach-Mazur%20Games/#references","title":"References","text":"<p>Wikipedia </p>","tags":["Note","Incomplete"]},{"location":"Bases%20for%20a%20topology/","title":"Bases for a topology","text":"<p>202301051001</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Bases%20for%20a%20topology/#bases-for-a-topology","title":"Bases for a topology","text":"<ul> <li>Topology of \\(\\mathbb{R}\\) \\(\\(\\tau_{\\mathbb{R}} = \\set{U \\subset \\mathbb{R} : \\forall \\ x \\in U, \\exists \\epsilon&gt;0 s.t. \\ (x-\\epsilon,x+\\epsilon) \\subset U}\\)\\)</li> <li>Discrete topology on a set \\(X\\) can be described simply as union of all singleton sets.</li> <li>This motivates the definition for a Bases for a topology. <pre><code>title: Bases for a set\n### Definition: \n$X$ is a set, a collection of subsets of $X$ is called a basis on $X$ if:\n\n(1) Covering property: For all $x\\in X$, $\\exists$ at least one $B\\in \\mathcal{B}$ s.t. $x\\in B$. This is the same as saying that $X = \\bigcup_{B\\in \\mathcal{B}}B$.\n\n(2) Gluing property: For all $B_1,B_2 \\in \\mathcal{B}$, $x\\in B_1\\cap B_2$, $\\exists B_3 \\in \\mathcal{B}$ s.t. $x \\in B_3 \\subset B_1\\cap B_2$.\n</code></pre></li> <li>Note that this definition of \\(\\mathcal{B}\\) is entirely self contained, it does not refer to the topology on \\(X\\) whatsoever.</li> <li>The gluing property's importance is shown below.</li> </ul> <pre><code>title: Topology generated by a basis\n\n### Definition:\nLet $X$ be a set, and $\\mathcal{B}$ be a basis on $X$. Then define:\n$$\\tau_{\\mathcal{B}} = \\left\\{\\bigcup_{c\\in \\mathcal{C}}c :  \\mathcal{C} \\subset \\mathcal{B}\\right\\}$$\nThis is $\\textbf{the}$ topology generated by $\\mathcal{B}$.\n</code></pre> <ul> <li>Check that this forms a topology on \\(X\\).</li> <li>Let \\(\\tau'\\) be a topology on \\(X\\) which contains \\(\\mathcal{B}\\), then \\(\\tau_{\\mathcal{B}} \\subset \\tau'\\) .</li> <li>Question: Could we define \\(\\tau_{\\mathcal{B}}\\) as simple the intersection of all topologies containing \\(\\mathcal{B}\\)?</li> <li>Answer: The intersection definitely forms a topology (Check), but why is it equal to \\(\\tau_{\\mathcal{B}}\\)?</li> </ul>"},{"location":"Bases%20for%20a%20topology/#proof-for-tau_mathcalb-being-a-topology-on-x","title":"Proof for \\(\\tau_{\\mathcal{B}}\\) being a topology on \\(X\\):","text":"<ul> <li>\\(\\phi \\in \\tau_{\\mathcal{B}}\\), since we can take \\(\\mathcal{C}\\) to be the empty set. \\(X \\in \\tau_{\\mathcal{B}}\\), by taking \\(\\mathcal{C} = \\mathcal{B}\\).</li> <li>Union of two elements of \\(\\tau_{\\mathcal{B}}\\): Since these elements are unions of elements of \\(\\mathcal{B}\\), their union is a union of elements of \\(\\mathcal{B}\\), hence it is in \\(\\tau_{\\mathcal{B}}\\). </li> <li>In the proof for showing that \\(\\tau_{\\mathcal{B}}\\) forms a topology, showing that finite intersection of elements of \\(\\tau_{\\mathcal{B}}\\) is in \\(\\tau_{\\mathcal{B}}\\), it reduces to showing that the intersection of two elements of \\(\\mathcal{B}\\) is in \\(\\tau_{\\mathcal{B}}\\), but this follows from the \\(\\textbf{gluing property}\\).</li> </ul>"},{"location":"Bases%20for%20a%20topology/#examples","title":"Examples:","text":"<p>(1) \\(X\\) set, \\(\\mathcal{B} = \\{\\{x\\}\\}_{x \\in X}\\) (2) \\(X\\) topo space, \\(\\tau_X\\) forms a basis on \\(X\\) (3) \\(X = \\mathbb{R}\\), \\(\\{ (a,b) \\in \\mathbb{R}: a&lt;b\\}\\) (4) \\(X = \\mathbb{R}^2\\), \\(\\{B_\\epsilon(x): x \\in \\mathbb{R}^2, \\epsilon&gt;0\\}\\) (5) \\(X = \\mathbb{R}, \\{[a,b) \\subset \\mathbb{R}: a&lt;b\\}\\)</p> <p><pre><code>title: Another definition of $\\tau_{\\mathcal{B}}$\n### Definition':\n$X$ is a set, $\\mathcal{B}$ basis on $X$, $\\tau_{\\mathcal{B}}' = \\{U \\subset X \\ | \\ \\forall \\ x \\in U, \\exists \\ B \\in \\mathcal{B}\\ s.t. \\ x \\in B \\subset U\\}$\n</code></pre> - Show that \\(\\tau_{\\mathcal{B}}\\) and \\(\\tau_{\\mathcal{B}}'\\) are the same topology.</p> <pre><code>title: Lemma\nLet $(X,\\tau_X)$ be a topo space. Suppose $\\mathcal{C}$ is a collection of open sets in $X$, such that, for each open set $U$ in $X$, and each $x \\in U$, $\\exists\\ C\\in \\mathcal{C}$ s.t. $x \\in C \\subset U$. Then $\\mathcal{C}$ is a basis on $X$, that generates $\\tau_X$.\n\n#### Proof: \n(1) $\\mathcal{C}$ is a basis on X:\n- Covering property holds, take $U=X$.\n- Gluing property, $x \\in C_1\\cap C_2 \\implies C_1 \\cap C_2$ is an open set $\\implies C_3 \\in \\mathcal{C}$ s.t. $x \\in C_3 \\subset C_1 \\cap C_2$.\n- Check that $\\mathcal{C}$ generates $\\tau_X$.\n</code></pre>"},{"location":"Bases%20for%20a%20topology/#question","title":"Question:","text":"<p>When can we say that the bases \\(\\mathcal{B}_1, \\mathcal{B}_2\\) generate the same topology?</p> <pre><code>title: Lemma \nLet $\\mathcal{B}_1,\\mathcal{B}_2$ be two bases on $X$. Then $\\tau_{\\mathcal{B}_1} \\subset \\tau_{\\mathcal{B}_2}$ iff for every $x \\in X$ and for $x \\in B_1 \\in \\mathcal{B}_1, \\exists \\ B_2 \\in \\mathcal{B}_2$ s.t. $x \\in B_2 \\subset B_1$.\n\n#### Proof:\n($\\Leftarrow$)\nGiven $U \\in \\tau_{\\mathcal{B}_1}$, need to show that $U \\in \\tau_{\\mathcal{B}_2}$. $x \\in U \\implies x \\in B_1 \\subset U \\implies \\exists \\ B_2 \\in \\mathcal{B}_2$ s.t. $x \\in B_2 \\subset B_1 \\subset U$, hence $U \\in \\tau_{\\mathcal{B}_2}$.\n\n($\\Rightarrow$)\nAssume $\\tau_{\\mathcal{B}_1} \\subset \\tau_{\\mathcal{B}_2}$. Now we are given $x \\in X, B_1 \\in \\mathcal{B}_1$ s.t. $x \\in B_1 \\implies B_1$ is open in $\\tau_{\\mathcal{B}_1}$ hence in $\\tau_{\\mathcal{B}_2} \\implies \\exists B_2 \\in \\mathcal{B}_2$ s.t. $x \\in B_2 \\subset B_1$.\n</code></pre>"},{"location":"Bases%20for%20a%20topology/#related-problems","title":"Related Problems","text":""},{"location":"Bases%20for%20a%20topology/#references","title":"References","text":"<p>Sub-basis</p>"},{"location":"Block-Graph%20Theory/","title":"Block Graph Theory","text":"<p>202308181008</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Block-Graph%20Theory/#block","title":"Block","text":"<p>Let \\((\\Gamma,X)\\) be a Transitive Permutation Group. A block \\(B\\) is a subset of \\(X\\) such that \\(\\forall \\sigma \\in\\Gamma,\\sigma(B)=B\\) or \\(\\sigma(B)\\cap B=\\emptyset\\). - Trivial Block: \\(B=\\emptyset, B=X, B\\) is a singleton - Non Trivial Block:     - \\(\\Gamma =\\mathbb Z_{n}\\) in its action on \\(X=[n-1]\\), \\(\\Gamma=\\langle\\sigma\\rangle\\) where \\(\\sigma(i)=i+1\\mod n\\)      - Let \\(n=10\\) then \\(\\{0,5\\}\\) and \\(\\{0,2,4,6,8\\}\\) are blocks.</p> <p>If \\(B\\) is a block and let \\(B'=\\sigma(B),\\sigma\\in\\Gamma\\) then \\(B'\\) is also a block. And The transitivity property ensure that given a block, all the other blocks  that can be derived from it, have the same size and form a partition of \\(X\\).</p>"},{"location":"Block-Graph%20Theory/#references","title":"References","text":"<p>Primitive Transitive Permutation Group</p>"},{"location":"Boolean%20Algebra/","title":"Boolean Algebra","text":"<p>202308161608</p> <p>Type : #Note Tags : [[Logic]]</p>"},{"location":"Boolean%20Algebra/#boolean-algebra","title":"Boolean Algebra","text":"<p>A Boolean Algebra is a A distributive lattice where very element\u00a0\u00a0has a complement (denoted by\u00a0\\(-a\\)) ^89f0ee</p> <p>Which is represented as \\(\\mathcal B=\\langle B, \\sqcup,\\sqcap,-,0,1\\rangle\\)</p> <p>A Valuation in a Boolean Algebra is a function \\(\\nu\\) from the set \\(\\text{PV}\\) of Propositional Variables to the set \\(B\\) and the value of a formula is defined as  { width=\"300\" } One write \\(\\mathcal B,\\nu\\models\\varphi\\) when \\(\\textlbrackdbl\\varphi\\textrbrackdbl_{\\nu}=1\\) and \\(\\mathcal B\\models \\varphi\\) if \\(\\mathcal B,\\nu\\models\\varphi\\) for all \\(\\nu\\).</p>"},{"location":"Boolean%20Algebra/#examples","title":"Examples:","text":"<ul> <li>\\(\\mathbb B=(\\{0,1\\}, \\le)\\)</li> <li>\\((\\mathbb P(X),\\subseteq)\\)</li> <li>\\(X\\) is an infinite set then (\\(\\{A:A\\text{ is finite or } X\\setminus A \\text{ is finite}\\}\\))</li> <li>Any Field of sets (over a base set \\(X\\))</li> </ul>"},{"location":"Boolean%20Algebra/#references","title":"References","text":"<p>Stone's Representation Theorem</p>"},{"location":"Borsuk%20Ulam%20Theorem/","title":"Borsuk Ulam Theorem","text":"<p>202304041504</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Borsuk%20Ulam%20Theorem/#borsuk-ulam-theorem","title":"Borsuk Ulam Theorem","text":"<p><pre><code>title:\n</code></pre> <pre><code>title:\n$f : S ^{2} \\to \\mathbb{R}^{2}$ is a continuous map, $\\implies$ there is a pair of antipodal points $\\pm x \\in S ^{2}$ such that $f(x) = f(-x)$.\n</code></pre> For the case when \\(S^{2}\\) is replaced by \\(S ^{1}\\), we can just take \\(g(x) = f(x)-f(-x)\\). </p>"},{"location":"Borsuk%20Ulam%20Theorem/#definition","title":"Definition:","text":"<p><pre><code>title:\n$x \\in S^n$ then a continuous map $h : S^n \\to S^m$ is _antipode preserving_ if $h(-x) = -h(x)$\n</code></pre> Example: \\(r_{\\theta} : S ^{1} \\to S ^{1}, \\ \\ z \\mapsto e ^{i\\theta}z\\)</p>"},{"location":"Borsuk%20Ulam%20Theorem/#lemma","title":"Lemma:","text":"<pre><code>title:\n\n(1) Composition of two antipode preserving maps is antipode preserving.\n\n(2) $h : S^1 \\to S^1$ is null homotopic, then $r_\\theta \\circ h$ is also null homotopic for any $\\theta$. \n</code></pre>"},{"location":"Borsuk%20Ulam%20Theorem/#theorem","title":"Theorem:","text":"<pre><code>title:\nIf $h : S^1 \\to S^1$ is continuous and antipode preserving, then $h$ is not null homotopic.\n</code></pre>"},{"location":"Borsuk%20Ulam%20Theorem/#proof","title":"Proof:","text":"<p>Refer to S^1.</p>"},{"location":"Borsuk%20Ulam%20Theorem/#corollary","title":"Corollary:","text":"<p><pre><code>title:\nThere is no antipode preserving map $g : S^2 \\to S^1$\n</code></pre> Suppose there is such a \\(g\\). \\(S ^{1}\\) is an equator of \\(S ^{2}\\), then \\(h := g|_{S ^{1}}\\) is antipode preserving map from \\(S ^{1}\\) to \\(S ^{1}\\), \\(h\\) is not null homotopic, but has \\(g|_{\\mathrm{upper\\ hemisphere}}\\) is an extension to \\(B ^{2}\\). That is a contradiction to Lemma 2 in S^1.</p>"},{"location":"Borsuk%20Ulam%20Theorem/#corollary_1","title":"Corollary:","text":"<pre><code>title:\n$f: S ^{2}\\to \\mathbb{R}^{2}$, $\\exists \\ x$ s.t. $f(x) = f(-x)$.\n</code></pre>"},{"location":"Borsuk%20Ulam%20Theorem/#proof_1","title":"Proof:","text":"<p>Assume not then \\(g(x) := \\dfrac{f(x)-f(-x)}{\\mid\\mid f(x)-f(-x)\\mid\\mid}\\) \\(g : S ^{2} \\to S ^{1}\\) s.t. \\(g\\) is antipode preserving. Contradiction.</p>"},{"location":"Borsuk%20Ulam%20Theorem/#applications","title":"Applications","text":"<ol> <li>An open set in \\(\\mathbb{R}^{2}\\) cannot be homeomorphic to an open set in \\(\\mathbb{R}^{n}\\) for \\(n \\ge 3\\).    ##### Proof:    Assume they were a homeomorphism \\(f : U \\to V\\) with \\(U\\) an open subset of \\(\\mathbb{R}^{n}\\), and \\(V\\) an open subset of \\(\\mathbb{R}^{2}\\).    Then there exists an \\(x\\) and an \\(r &gt; 0\\) such that \\(Cl(B_{r}(x)) \\subset U\\), which is homeomorphic to \\(B ^{n} \\supseteq B^3 \\supset S^2\\). So by restriction, we get a cts and injective map from \\(S^{2} \\to \\mathbb{R}^{2}\\). This contradicts Borsuk Ulam.</li> </ol>"},{"location":"Borsuk%20Ulam%20Theorem/#references","title":"References","text":"<p>S^1 Fundamental Group Homotopy of paths Retractions</p>"},{"location":"Brouwer%27s%20Fixed%20Point%20Theorem/","title":"Brouwer's Fixed Point Theorem","text":"<p>202304041204</p> <p>Type : #Note Tags :[[Topology]]</p>"},{"location":"Brouwer%27s%20Fixed%20Point%20Theorem/#brouwers-fixed-point-theorem","title":"Brouwer's Fixed Point Theorem","text":"<pre><code>title:\nIf $f : B ^{2} \\to B ^{2}$ is continuous where $B ^{2} = \\{ (x,y) \\in \\mathbb{R}^{2} : x ^{2} + y ^{2} \\le 1 \\}$, then $\\exists \\ x \\in B ^{2}$ s.t. $f(x) = x$.\n</code></pre>"},{"location":"Brouwer%27s%20Fixed%20Point%20Theorem/#proof-1","title":"Proof 1:","text":"<p>There is no retraction from \\(B ^{2} \\to S ^{1}\\) (Retraction: \\(A \\subseteq X\\), \\(r : X \\to A\\) cts. s.t. \\(r |_{A} = id_{A}\\)) Assume that for some \\(f : B^{2} \\to B^{2}\\), \\(f(x) \\neq x\\) for all \\(x \\in B^{2}\\). Then we can define a retraction \\(h : B^{2} \\to S ^{1}\\) as follows: Take a point \\(b \\in B ^{2}\\) and draw that ray origination from the \\(f(b)\\) and passing through \\(b\\), define \\(h(b)\\) to be the point where this ray hits \\(\\partial B^{2}\\). $$ h(x) = x + t(x-f(x))  $$ where \\(t &gt; 0\\), such that \\(|h(t)| = 1\\). We can solve for \\(t\\) using the quadratic formula, so \\(t\\) depends continuously on \\(x\\)  and so \\(h\\) is continuous.</p> <p>This is a retraction from \\(B^{2}\\) to \\(S ^{1}\\), contrary to the fact that there is no retraction between these spaces. (Refer to Retractions)</p>"},{"location":"Brouwer%27s%20Fixed%20Point%20Theorem/#proof-2","title":"Proof 2:","text":""},{"location":"Brouwer%27s%20Fixed%20Point%20Theorem/#lemma","title":"Lemma:","text":"<pre><code>title:\nLet $h : S ^{1} \\to X$ cts. Then the following are equivalent.\n\n(1) $h$ is null homotopic.\n\n(2) $h$ extends to a cts map $k: B ^{2} \\to X$\n\n(3) $h_{*} : \\Pi_{1}(S ^{1}) \\to \\Pi_{1}(X)$ is the trivial homomorphism.\n</code></pre>"},{"location":"Brouwer%27s%20Fixed%20Point%20Theorem/#proof","title":"Proof:","text":"<p>Refer to S^1.</p>"},{"location":"Brouwer%27s%20Fixed%20Point%20Theorem/#corollary","title":"Corollary","text":"<pre><code>title:\n$f: S ^{1} \\to \\mathbb{R}^{2}\\setminus \\{ (0,0) \\}$ is not null homotopic.\n</code></pre>"},{"location":"Brouwer%27s%20Fixed%20Point%20Theorem/#proof_1","title":"Proof:","text":"<p>Refer to S^1.</p>"},{"location":"Brouwer%27s%20Fixed%20Point%20Theorem/#corollary_1","title":"Corollary","text":"<pre><code>title:\n$id: S ^{1} \\to S ^{1}$ is not null homotopic\n</code></pre>"},{"location":"Brouwer%27s%20Fixed%20Point%20Theorem/#proof_2","title":"Proof:","text":"<p>Refer to S^1.</p>"},{"location":"Brouwer%27s%20Fixed%20Point%20Theorem/#proof-of-brouwers-fixed-point-theorem","title":"Proof of Brouwer's fixed point theorem:","text":"<p>Assume \\(f: B ^{2} \\to B ^{2}\\) cts s.t. \\(f\\) has no fixed point. Then define \\(g : B ^{2} \\to \\mathbb{R}^{2} \\setminus \\{ (0,0) \\}\\), \\(g(p) = p-f(p)\\) For a fixed \\(p \\in S ^{1}\\), the points \\(p, g(p)\\) lie in a convex subset of \\(\\mathbb{R}^{2} \\setminus \\{ (0,0) \\}\\). This means there is a straight line homotopy between \\(g(p)\\) and \\(p\\). \\(H(p,t) := (1-t)g(p) + tp\\) \\(\\implies\\) Homotopy of maps : \\(g|_{S ^{1}} \\to \\mathbb{R}^{2} \\setminus \\{ (0,0) \\}\\) This is \\(H\\) at \\(t = 0\\). and \\(i : S ^{1} \\to \\mathbb{R}^{2}\\setminus \\{ (0,0) \\}\\) This is \\(H\\) at \\(t=1\\).</p> <p>So, \\(g|_{S ^{1}}\\) is not null homotopic by corollary. But it has an extension to \\(B ^{2}\\) namely \\(g.\\) This is contradictory to our lemma.</p>"},{"location":"Brouwer%27s%20Fixed%20Point%20Theorem/#references","title":"References","text":"<p>Retractions Fundamental Group Homotopy of paths</p>"},{"location":"Burnside%27s%20Lemma/","title":"Burnside's Lemma","text":"<p>202308091108</p> <p>Type : #Note Tags : [[Groups]], [[Algebraic Graph Theory]]</p>"},{"location":"Burnside%27s%20Lemma/#burnsides-lemma","title":"Burnside's Lemma","text":"<p><pre><code>title:\nNumber of orbits = $\\frac{1}{|G|}\\sum\\limits_{r}fix(r)$\n</code></pre> where \\(fix(r)\\) is the number of elements fixed by \\(r\\)</p> <p>This can also be thought as the size of the automorphism class.</p> <p>The possible sizes of autmorphism classes are  \\(\\frac{n!}{i}\\) where \\(i\\) can be a natural number.</p>"},{"location":"Burnside%27s%20Lemma/#references","title":"References","text":""},{"location":"CANA-HW-4/","title":"CANA HW 4","text":"<p>Roll Number: BMC202144</p>"},{"location":"CANA-HW-4/#1-pg-130-problem-5","title":"1. Pg 130, Problem 5","text":"<p>Suppose \\(f\\) has an isolated singularity, if this were an essential singularity, then \\(f\\) should come arbitrarily close to any complex value in every nbhd of the essential singularity. But this would mean that the \\(\\mathrm{Im}(f)\\) and \\(\\mathrm{Re}(f)\\) are both unbounded, so \\(f\\) doesn't have an essential singularity.</p> <p>If \\(f\\) has a pole at say, \\(z_{0}\\) then \\(\\frac{1}{f}\\) has a zero at \\(z_{0}\\). So let \\(U\\) be an open nbhd around \\(z_{0}\\), it is mapped onto an open nbhd \\(V\\) around \\(0\\) by \\(\\frac{1}{f}\\) (open mapping theorem), and so \\(f\\) maps \\(U\\) onto an open nbhd around \\(\\infty\\). But an open nbhd around \\(\\infty\\) has \\(\\mathrm{Im}\\) and \\(\\mathrm{Re}\\) parts unbounded. That means \\(\\mathrm{Re}(f(U))\\) and \\(\\mathrm{Im}(f(U))\\) are both unbdd, which is contrary to the hypothesis.</p>"},{"location":"CANA-HW-4/#2-pg-130-problem-6","title":"2. Pg 130, Problem 6","text":"<p>Suppose \\(f(z)\\) has a singularity at \\(z_{0}\\), and let \\(z_{0}\\) be a pole of \\(e ^{f(z)}\\). But this implies that \\(e ^{u(z)+iv(z)}\\) has a pole at \\(z_{0}\\), where \\(f(z) = u(z) + iv(z)\\). This means that \\(u(z) \\to \\infty\\) as \\(z \\to z_{0}\\). Which implies that \\(f(z)\\) has a pole at \\(z_{0}\\). Now we claim that \\(f(z)\\) having a pole at \\(z_{0}\\) implies \\(e ^{f(z)}\\) has an essential singularity at \\(z_{0}\\), this will give us the desired contradiction.</p> <p>Having a pole at \\(z_{0}\\) means that given any \\(M &gt; 0\\), there is an \\(\\epsilon &gt; 0\\) such that \\(\\forall \\ z \\in B_{\\epsilon}(z_{0})\\), \\(|f(z)| &gt; M\\). Now take any point \\(z\\) in \\(B_{\\epsilon}(z_{0})\\), its image is something finite in size, but then \\(f(z) + 2\\pi in\\) forms a sequence converging to \\(\\infty\\), hence their preimages form a sequence converging to \\(z_{0}\\), but \\(e ^{f(z) + 2\\pi in}\\) forms a constant sequence, hence we found a sequence \\(z_{n} \\to z_{0}\\) with \\(e ^{f(z_{n})} \\to e ^{f(z)}\\) for an arbitrary \\(z \\in B_{\\epsilon}(z_{0})\\). Hence \\(z_{0}\\) is neither a pole nor a removable singularity of \\(e^f\\), it is an essential singularity.</p> <p>This completes the proof.</p>"},{"location":"CANA-HW-4/#3-pg-133-problem-3","title":"3. Pg 133, Problem 3","text":"<p>Let \\(f(z) = \\cos z\\), now \\(z_{0} = 0 \\implies w_{0} = f(0) = 1\\), and so \\(f(z) - 1\\) has 0 as a root. The derivative of this function \\(= f'(z) = \\sin z\\), this is 0 at \\(z = 0\\).  The second derivative \\(=f''(z) = -\\cos z\\), this is 1 at \\(z = 0\\). So, 0 is a zero of order 2 of the function \\(f(z)-1\\). This gives \\(\\cos z-1 = z^{2}g(z) = \\zeta(z)^{2}\\) where \\(\\zeta(z) = z\\sqrt{g(z)}\\) in some nbhd around 0. But since we have the taylor series for \\(\\cos z\\), we can write  $$ \\zeta(z)^{2} = -\\frac{1}{2!}z^{2}+\\frac{1}{4}z ^{4} - \\dots $$ which gives \\(\\zeta(z) = \\sqrt{-\\frac{1}{2}z^{2} + \\frac{1}{4}z ^{4} - \\dots}\\)</p>"},{"location":"CANA-HW-4/#4-pg-133-problem-4","title":"4. Pg 133, Problem 4","text":"<p>We know that \\(f(z ^{n}) - f(0)\\) has all the \\(i\\)th order derivatives = 0, for all \\(i = 1,\\dots n-1\\), hence 0 is a root of order \\(n\\) of \\(f(z ^{n}) - f(0)\\). Hence, \\(f(z ^{n}) -f(0) = z ^{n}h(z)\\) where \\(h(0) \\neq 0\\). But then we can choose a nbhd of 0 s.t. \\(|h(z)-h(0)| &lt; |h(0)|\\) for all \\(z\\) in that nbhd, and so we can define a single valued analytic branch of \\(\\sqrt[n]{h(z)}\\) in this nbhd of \\(0\\). Thus we get:  $$ f(z ^{n})-f(0) = \\left(z \\cdot \\sqrt[n]{h(z)}\\right)^{n} $$ Which gives \\(g(z) := z \\sqrt[n]{h(z)}\\), and we are done.</p>"},{"location":"CANA-HW-4/#5-pg-136-problem-1","title":"5. Pg 136, Problem 1","text":"<p>The equation \\((36)\\) states that, given a function \\(f\\) analytic on \\(|z| &lt; R\\), s.t. \\(|f(z)| \\le M, f(z_{0}) = w_{0}\\), then we have  $$ \\left| \\frac{M(f(z)-w_{0})}{M^{2}-\\bar{w}{0}f(z)}\\right| \\le \\left| \\frac{R(z-z{0})}{R^{2}-\\bar{z}{0}z} \\right| $$ Putting \\(R = 1\\), \\(M = 1\\), we get  $$ \\left| \\frac{f(z)-w{0}}{1-\\bar{w}{0}f(z)} \\right| \\le \\left| \\frac{z-z{0}}{1-\\bar{z}{0}z} \\right| $$ Now let \\(z \\to z_{0}\\), then \\(f(z) \\to w_{0}\\), which gives: $$ \\lim{ z \\to z_{0} }  \\left| \\frac{f(z)-w_{0}}{(z-z_{0})(1-\\bar{w}{0}f(z))} \\right| \\le \\lim{ z \\to z_{0} } \\left| \\frac{1}{1-\\bar{z}{0}z} \\right| $$ This gives  $$ \\left| \\frac{f'(z{0})}{1-|f(z_{0})^{2}|} \\right| \\le \\left| \\frac{1}{1-|z_{0}|^{2}} \\right| $$ But here, \\(z_{0}\\) is any chosen point, hence  $$ \\frac{|f'(z)|}{1-|f(z)|^{2}} \\le \\frac{1}{1-|z|^{2}} $$</p>"},{"location":"CANA-HW-4/#6-pg-136-problem-2","title":"6. Pg 136, Problem 2","text":"<p>Consider the map \\(g : \\mathcal{H} \\to D\\), where \\(D\\) is the open unit disk and \\(\\mathcal{H}\\) is the upper half plane, given by \\(\\displaystyle g(z) = \\frac{z-z_{0}}{z-\\bar{z}_{0}}\\), where \\(z_{0} \\in \\mathcal{H}\\) is a fixed point. Also consider the map \\(h : \\mathcal{H} \\to D\\) given by \\(\\displaystyle h(z) = \\frac{z-f(z_{0})}{z-\\overline{f(z_{0})}}\\) Consider the map \\(h \\circ f \\circ g ^{-1} : D \\to \\bar{D}\\) This satisfies the conditions that the function is analytic on \\(D\\), and \\(|h(f(g ^{-1}(z)))| \\le 1\\) for all \\(z \\in D\\).  Also that \\(h(f(g ^{-1}(0))) = 0\\).  This gives \\(\\displaystyle |h(f(g ^{-1}(z)))| \\le |z| \\implies |h(f(z))| \\le |g(z)| \\implies \\frac{|f(z) - f(z_{0})|}{|f(z)-\\overline{f(z_{0})}|} \\le \\frac{z-z_{0}}{z-\\bar{z}_{0}}\\).</p> <p>Now take the limit as \\(z \\to z_{0}\\), which gives  $$ \\frac{|f'(z_{0})|}{\\mathrm{Im}(f(z_{0}))} \\le \\frac{1}{\\mathrm{Im}(z_{0})} $$ Thus we are done.</p>"},{"location":"CANA-HW-4/#7-pg-136-problem-3","title":"7. Pg 136, Problem 3","text":"<p>In Problem 1, equality holds iff  \\(\\(\\left| \\frac{f(z)-w_{0}}{1-\\bar{w}_{0}f(z)} \\right| = \\left| \\frac{z-z_{0}}{1-\\bar{z}_{0}z} \\right|\\)\\) which holds iff equality holds in (36) with \\(M = R = 1\\), which implies that \\(|Sf(T ^{-1}\\zeta)| = |\\zeta|\\) where \\(S\\) is an LFT mapping \\(|w| &lt; M\\) onto \\(|Sw| &lt; 1\\) with \\(Sw_{0} = 0\\), and T is an LFT mapping \\(|z|&lt;R\\) onto \\(|\\zeta| &lt; 1\\) with \\(T(z_{0}) = 0\\), as in the discussion in the proof of \\((36)\\) in ahlfors.</p> <p>This gives \\(SfT ^{-1}(\\zeta) = c \\zeta\\) for some constant c. This gives \\(f = S ^{-1}(cT(\\zeta))\\) which is a composition of 3 LFTs and hence is an LFT.</p> <p>In problem 2, equality holds iff \\(|h(f(g ^{-1}(z)))| = |z|\\) which gives that \\(h(f(g ^{-1}(z))) = cz\\) for some constant \\(c\\), and all \\(z\\). This gives \\(f(z) = h ^{-1}(c g(z))\\) which is again a composition of 3 LFTs and hence is an LFT.</p>"},{"location":"CANA-HW-4/#8-pg-148-problem-2","title":"8. Pg 148, Problem 2","text":"<p>Let \\(S\\) be the simply connected space. Let \\(P = S\\) minus the \\(m\\) points. The \\(P ^{c} = S ^{c} + m\\) points which are the \\(m+1\\) connected components, hence \\(P\\) has connectivity \\(m+1\\).</p> <p>The homology basis consists of \\(m\\) loops each centered around the \\(m\\) points. So for each of the \\(m\\) points \\(\\{a_{i}\\}_{i=1}^m\\), we can find a circle \\(\\gamma_{i}\\) around the \\(a_{i}\\) and we will have \\(n(\\gamma_{i}, a_{j}) = \\delta_{i,j}\\) hence it forms a basis.  </p>"},{"location":"CANA-HW-4/#9-pg-148-problem-5","title":"9. Pg 148, Problem 5","text":"<p>Suppose \\(S = \\mathbb{R}^{2}\\setminus [-1,1]\\). Suppose \\(\\Omega \\subseteq S\\) is the region on which we wish to define the analytic branch. Take any closed curve \\(\\gamma\\) in \\(\\Omega\\), if it contains \\(\\pm 1\\) in its interior, then  $$ \\begin{align} \\int {\\gamma} \\frac{1}{1-z^{2}} dz &amp;= \\frac{1}{2} \\int {\\gamma} \\left(\\frac{1}{1-z} + \\frac{1}{1+z} \\right) dz \\  &amp;= \\frac{1}{2}(-2\\pi i + 2\\pi i) = 0 \\end{align} $$ And if it doesn't contain \\(\\pm 1\\) in its interior, then the integral is obviously 0 since then \\(\\displaystyle\\int _{\\gamma} \\frac{1}{1-z} \\, dz = 0 = \\int _{\\gamma} \\frac{1}{1+z} \\, dz\\). Hence, \\(\\frac{1}{1-z^{2}}\\) is the derivative of an analytic function on \\(\\Omega\\). The analytic function is just \\(\\frac{1}{2}\\log \\left( \\frac{1-z}{1+z} \\right)\\), and hence \\(e^{\\frac{1}{2} \\log((1-z)/(1+z))} = \\sqrt{\\frac{1-z}{1+z}}\\) has a single valued analytic branch on \\(\\Omega\\). And therefore, multiplying by \\(z+1\\), we get that \\(\\sqrt{1-z^{2}}\\) has a single valued analytic branch.</p> <p>Now for the second part,  If the closed curve is homologous to 0, then the integral is just 0. If the closed curve is not homologous to 0, then it either contains \\([-1,1]\\) in its interior or it doesn't. If it doesn't, then looking at it as a closed curve in \\(S\\), it is homologous to 0 mod \\(S\\) and hence the integral is 0.</p> <p>Otherwise, it is homologous to a multiple of \\(|z| = 2\\) mod \\(S\\). And so, the integral is just an integer multiple of \\(\\displaystyle\\int _{|z| = 2}  \\frac{dz}{\\sqrt{1-z^{2}}}\\). $$ \\begin{align} \\int {|z|=2} \\frac{dz}{\\sqrt{1-z^{2}}} &amp;= \\frac{1}{2}\\int {|z| =2} \\left(\\sqrt{\\frac{1+z}{1-z}} - \\sqrt{\\frac{1-z}{1+z}}\\right) dz \\ &amp;=\\frac{1}{2} \\int _{|z| = 2} \\left( -\\frac{\\sqrt{1-z^{2}}}{z-1} - \\frac{\\sqrt{1-z^{2}}}{1+z} \\right) dz \\  \\end{align} $$ By cauchy's formula, we have  $$ \\frac{1}{2\\pi i}\\int_{|z| = 2} \\frac{\\sqrt{1-z^{2}}}{z-1} = \\sqrt{1-1^{2}} = 0  $$ and  $$ \\frac{1}{2\\pi i} \\int _{{|z|=2}} \\frac{\\sqrt{1-z^{2}}}{1+z} = \\sqrt{1-(-1)^{2}} = 0  $$ This gives the integral  = 0. Hence the only possible value of the integral is 0.</p>"},{"location":"CANA-HW-5/","title":"CANA-HW-5","text":"<p>Name: Rishabh Sharma  Roll Number: BMC202144</p> <p>### Pg. 154, P1  Take \\(f(z) = z ^{7}-2 z^{5}+6z^{3}-z+1\\) and \\(g(z) = 6z^{3}\\), then \\(|f(z)-g(z)| = |z ^{7}- 2 z ^{5} -z +1| \\le |z| ^{7} + 2|z|^{5}+|z|+1 = 5 &lt; 6 = |6z^{3}| = |g(z)|\\) on \\(|z| = 1\\).  Hence, \\(f(z)\\) and \\(g(z)\\) have the same number of roots inside \\(|z| = 1\\) by Rouche's theorem, and \\(g(z)\\) has 3 roots in that region, hence \\(f(z)\\) has three roots in that region.</p> <p>### Pg. 154, P2  Take \\(f(z) = z ^{4}-6z+3\\) and \\(g(z) = -6z\\), then \\(|f(z)-g(z)| = |z ^{4}+3| \\le 4 &lt; 6 = |g(z)|\\) on \\(|z|=1\\).  Hence, \\(g\\) and \\(f\\) have same number of roots inside \\(|z| = 1\\) by rouche's. Hence \\(f\\) has 1 root inside \\(|z|=1\\).</p> <p>We can see that there are no roots of \\(f\\) on the circle \\(|z| = 1\\), by taking \\(f\\) and \\(g\\) same as above and looking at the circle \\(|z| = 1 + \\epsilon\\) for small \\(\\epsilon &gt; 0\\), since the inequalities above will not change with small change with small change in \\(\\epsilon\\).</p> <p>Now take \\(f(z) = z ^{4}- 6z+3\\) and \\(g(z) = z ^{4}\\), then \\(|f(z)-g(z)| = |-6z+3| \\le 6|z|+3 = 15 &lt; 16 = |z ^{4}| = |g(z)|\\) on \\(|z| = 2\\). Hence \\(f,g\\) have same number of roots inside \\(|z|=2\\), \\(g\\) has 4 roots inside \\(|z|=2\\), hence \\(f\\) does too.</p> <p>So, there are \\(4-1 = 3\\) roots of \\(f\\) between \\(|z| = 2\\) and \\(|z|=1\\).</p>"},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/","title":"Calc   Problem Session   7 Oct","text":"<p>202210071156</p> <p>type : #Example tags : </p>"},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/#calc-problem-session-7-oct","title":"Calc - Problem Session - 7 Oct","text":"<p>### Question:  Let \\(V\\) be a real Vector space of dimension \\(n\\)</p>"},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/#1","title":"1","text":"<p>Prove that \\(\\dim V^* = n\\) </p>"},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/#2","title":"2","text":"<p>Prove that \\(\\dim \\mathcal{T}^2(V) = n^2\\)</p>"},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/#3","title":"3","text":"<p>Let \\(\\Lambda^2(V)\\) denote the Vector space of all bilinear antisymmetric maps, what is the dimension of \\(\\Lambda^2(V)\\)?</p>"},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/#4","title":"4","text":"<p>Define \\(\\Lambda^k(V) \\subset \\mathcal{T}^k(V)\\) suitably and find its dimension</p>"},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/#5","title":"5","text":"<p>What about \\(S^k(V) \\subset \\mathcal{T}^k(V)\\)? where \\(S^k(V)\\) is the space of all symmetic maps</p>"},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/#answer","title":"Answer:","text":""},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/#1_1","title":"1","text":"<p>Let \\((e_1,e_2\\dots e_n)\\) be a bases for \\(V\\) The linear form defined by  $$   \\phi_i(e_j) =     \\begin{cases}       1 &amp; i = j \\       0 &amp; i\\ne j      \\end{cases}  $$ Given \\(\\phi \\in V^*,\\ v\\in \\sum\\limits_{j=1}^na_je_j\\)  $$ \\phi\\left(\\sum_{j=1}^na_je_j\\right) = \\sum_{j=1}^na_j\\phi\\left(e_j\\right) =  \\sum_{j=1}^n\\phi_j(v)\\phi(e_j)=  \\left(\\sum_{j=1}^nb_j\\phi_j\\right)(v)  $$ where \\(b_j = \\phi(e_j)\\) Taking \\(\\phi\\) to be the zero map we get  $$ \\left(\\sum_{i=1}^nb_i\\phi_i\\right)(e_j) = 0\\implies b_j = 0 \\forall j  $$ Thus the set of vectors spans \\(V*\\) and is linearly independent. Hence \\(\\dim V = n\\)</p>"},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/#2_1","title":"2","text":"<p>For \\(1 \\le i, j \\le n\\), define \\(B_{ij}\\in\\mathcal{T}^2(V)\\) by $$ B_ij\\left(\\sum a_ie_i, \\sum b_ie_i\\right) = a_ib_j $$ Given \\(B\\in \\mathcal{T}^2(V)\\) define \\(b_{ij} = B(e_i, e_j)\\)</p> <p>Claim: B = \\(\\sum_{i,j} b_{ij}B_{ij}\\)  $$ B\\left(\\sum a_ie_i, \\sum b_ie_i\\right) =  \\sum a_i \\sum b_i B(e_i, e_j) =  \\sum B_{ij}(u,v)B(e_i,e_j) = \\sum b_{ij}B_{ij}(u,v) $$ Claim: \\(B_{ij}\\)  are linearly independent Taking \\(B\\) to be the zero map we get  $$ \\sum b_{ij}B_{ij}(e_l,e_m) = 0 = b_{lm} \\forall l,m  $$ \\(\\therefore B_{ij}\\) gvies a basis for \\(\\mathcal{T}^2(V)\\) \\(\\therefore \\dim \\mathcal{T}^2(V) = n^2\\)</p>"},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/#3_1","title":"3","text":"<p>Start with an antisymmetric bilinear map \\(A\\) let \\(a_{ij} = A(e_i, e_j)\\implies -a_{ij}\\) Define a matrix \\(\\tilde A\\) to be a matrix with \\(\\tilde a_{ij} = a_{ij}\\) thus \\(\\tilde A\\) is antisymmetric Hence it is determined by \\(\\frac{n(n-1)}2\\) Therefore the span of \\(\\Lambda^2(V) = \\frac{n(n-2)}2\\)</p>"},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/#4_1","title":"4","text":""},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/#todo","title":"TODO","text":""},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/#5_1","title":"5","text":"<p>By similar construction as part 3, we get that \\(\\dim S^2(V) =\\frac{n(n+1)}2\\) <pre><code>$$\n\\begin{aligned}\n\\dim S^2(V) + \\dim \\Lambda^2(V)&amp;= dim \\mathcal T^2{V} \\\\\nS^2(V) \\cap \\Lambda^2(V) &amp;= \\emptyset\\\\\n\\implies S^2(V) \\oplus \\Lambda^2(V) &amp;= \\mathcal T^2(V)\n\\end{aligned}$$\nThis does not work for $k \\ge 3$\n$$\\Lambda^k(V) \\oplus S^k(V) \\ne \\mathcal T^k(V)\\text( for )k\\ge 3$$\n</code></pre></p>"},{"location":"Calc%20-%20Problem%20Session%20-%207%20Oct/#related","title":"Related","text":"<p>Tensors</p>"},{"location":"Calculus%20Assignment/","title":"Calculus Assignment","text":"<ul> <li>If \\(|x - y| &lt; \\delta\\) such that \\(x,y \\in [1,\\infty)\\)</li> <li> \\[\\begin{aligned}   |\\log(x) - \\log(y)| &amp;= \\left|\\log\\left(\\frac{x}y\\right)\\right|\\\\   &amp;\\le \\left|\\log\\left(\\frac{y+\\delta}y\\right)\\right|\\\\   &amp;\\le \\left|\\log\\left(1+\\frac{\\delta}{y}\\right)\\right|\\\\   &amp;\\le \\left|\\frac\\delta{y}\\right|\\\\    &amp;\\le \\delta   \\end{aligned}\\] </li> <li>Therefore, \\(\\forall \\epsilon&gt;0, \\exists \\delta=\\epsilon\\) such that \\(|x-y|&lt;\\delta \\implies |\\log(x)-\\log(y)| &lt; \\epsilon\\)</li> <li>Hence \\(\\log\\) is uniformly continuous in \\([1,\\infty)\\)</li> </ul> <ul> <li> </li> <li>If \\(\\phi\\) is a continuous function and \\(f\\)is Riemann integrable, then \\(\\phi\\circ f\\) is Riemann integrable</li> <li>let \\(g(x)= x^2,\\ g\\) is continuous</li> <li>then \\(g\\circ f_r + g\\circ f_i\\) is riemann integrable</li> <li>let \\(h(x)=\\sqrt{x}\\) which is continuous</li> <li>hence, \\(\\sqrt{f_r^2+f_i^2}\\) is Riemann integrable</li> <li>let \\(f\\) be a Riemann integrable complex function, \\(c=\\alpha+\\iota\\beta\\)</li> <li> \\[   \\begin{aligned}   c\\int\\limits_a^b f(x)dx &amp;=(\\alpha+\\iota\\beta)\\left(\\int\\limits_a^bf_r(x)dx+\\iota\\int\\limits_a^bf_i(x)dx\\right)\\\\   &amp;=\\left(\\alpha\\int\\limits_a^bf_r(x)dx-\\beta\\int\\limits_a^bf_i(x)dx\\right) +\\iota\\left(\\alpha\\int\\limits_a^b f_r(x)dx-\\beta\\int\\limits_a^b f_i(x)dx\\right)\\\\   &amp;=\\int\\limits_a^bcg(x)dx   \\end{aligned}   \\] </li> <li>Let \\(z=\\int\\limits_0^1f(x)dx\\). Define \\(O=\\frac{z}{|z|}\\)</li> <li> \\[   \\begin{aligned}   \\left|\\int\\limits_0^1f(x)dx\\right| = |z| &amp;= O\\int\\limits_0^1f(x)dx\\\\   &amp;= \\int\\limits_0^1Of(x)dx\\\\   &amp;= \\int\\limits_0^1Re(Of(x))dx + \\iota\\int\\limits_0^1Im(Of(x))dx   \\end{aligned}   \\] </li> <li>As LHS is a real number,</li> <li>\\(\\left|\\int\\limits_0^1f(x)dx\\right| = \\left|\\int\\limits_0^1Re(Of(x))dx\\right| \\le \\int\\limits_0^1\\left|Of(x)\\right|dx = \\int\\limits_0^1|O||f(x)|dx\\)</li> <li>Therefore \\(|\\int_0^1f(x)dx| \\le \\int_0^1|f(x)|dx\\)</li> </ul> <ul> <li> </li> </ul> <ul> <li> </li> <li> </li> <li> </li> </ul> <ul> <li> </li> <li> </li> <li> </li> </ul> <ul> <li> </li> <li>Since \\(f\\) is continuous, it is integrable in a closed interval, let \\(\\eta\\) be the closed figure which is the union of \\(\\Gamma_f\\), and the area enclosed in the curve.</li> <li>since \\(f\\) is integrable, we know that \\(\\chi_\\eta\\) is integrable \\(\\implies\\Gamma_f\\) has content \\(0\\), as it is a subset of the boundary of \\(\\eta\\)</li> <li>If \\(f\\) is only integrable, the above argument still holds as the only Information used in the above proof is that \\(f\\) is integrable.</li> </ul> <ul> <li> </li> <li>\\(\\int_\\mathbb{R}\\tilde{D} = \\int_ID\\)</li> <li> \\[   \\begin{aligned}   \\int\\limits_0^1 (1-x) dx &amp;= x - \\frac{x^2}2 |_{[0, 1]}\\\\   &amp;= \\frac12   \\end{aligned}   \\] </li> <li>Using Fubini's theorem</li> <li> \\[\\begin{aligned}   \\int_{I\\times I} D(x)D(xy) dx dy &amp;= \\int_{I\\times I} (1-x)(1-xy)dxdy\\\\   &amp;= \\int_0^1(\\int_0^1 (1-x)(1-y)dy)dx\\\\   &amp;= \\int_0^1(1-x)*(1-\\frac{x}2)dx\\\\   &amp;= \\frac52   \\end{aligned}\\] </li> </ul>"},{"location":"Calculus%20Assignment/#question-1","title":"Question 1","text":""},{"location":"Calculus%20Assignment/#question-2","title":"Question 2","text":""},{"location":"Calculus%20Assignment/#question-3","title":"Question 3","text":""},{"location":"Calculus%20Assignment/#a","title":"a)","text":"<ul> <li>\\(\\frac1p+\\frac1q =1\\)</li> <li>let \\(f(u) = \\frac{u^p}p+\\frac{v^q}q -uv\\)</li> <li>\\(f'(u) = u^{p-1} - v\\)</li> <li>\\(u^{p-1} = v\\) for critical points</li> <li>\\(f''(u) = (p-1)u^{p-2} = (p-1)\\frac{v}u\\)</li> <li>\\(\\frac1p &lt;1 \\implies p &gt; 1\\implies f''(u) &gt; 0\\)</li> <li>hence \\(u_0^{p-1} = v, u_0\\) is a minimum</li> <li> \\[   \\begin{aligned}   \\frac{u^p}p+\\frac{v^q}q -uv &amp;\\ge \\frac{u_0^p}p + \\frac{u_0^{(p-1)q}}q - u_0^p\\\\   &amp;\\ge \\frac{u_0^p}p+\\frac{u_0^p}q - u_0^p\\\\   &amp;\\ge 0   \\end{aligned}   \\] </li> </ul>"},{"location":"Calculus%20Assignment/#b","title":"b)","text":"<ul> <li>\\(u=\\frac{f(x)}{(\\int_0^1f^p(x))^{\\frac1p}}, v=\\frac{g(x)}{(\\int_0^1g^q(x))^{\\frac1q}}\\)</li> <li>applying part a</li> <li> \\[\\begin{aligned}   \\frac{fg}{(\\int_0^1f^pdx)^{\\frac1q}(\\int_0^1g^qdx)^{\\frac1q}} &amp;\\le \\frac1p\\left(\\frac{f}{\\left(\\int_0^1f^pdx\\right)^{\\frac1p}}\\right)^p + \\frac1q\\left(\\frac{g}{\\left(\\int_0^1g^qdx\\right)^{\\frac1q}}\\right)^q\\\\   &amp;\\le \\frac1p\\frac{f^p}{\\int_0^1f^pdc}+ \\frac1q\\frac{g^q}{\\int_0^1g^qdc}   \\end{aligned}\\] </li> <li>Integrating on both sides from \\(0\\) to \\(1\\)</li> <li> \\[\\frac{\\int_0^1fgdx}{(\\int_0^1f^pdx)^{\\frac1q}(\\int_0^1g^qdx)^{\\frac1q}}\\le\\frac1p+\\frac1q=1\\] </li> <li>therfore \\(\\(\\int_0^1fgdx \\le \\left(\\int_0^1f^pdx\\right)^\\frac1p + \\left(\\int_0^1g^qdx\\right)^\\frac1q\\)\\)</li> </ul>"},{"location":"Calculus%20Assignment/#c","title":"c)","text":"<ul> <li>as proved in problem 2</li> <li>\\(|\\int_0^1fgdx| \\le \\int_0^1|fg|dx = \\int_0^1|f||g|dx\\)</li> <li>taking \\(|f|\\) and \\(|g|\\) be function for path (b)</li> <li>\\(|\\int_0^1fgdx| \\le (\\int_0^1|f|^p)^\\frac1q + (\\int_0^1|g|^q)^\\frac1q\\)</li> </ul>"},{"location":"Calculus%20Assignment/#question-4","title":"Question 4","text":""},{"location":"Calculus%20Assignment/#a_1","title":"a)","text":"<ul> <li>\\(S\\) is not Jordan Measurable</li> <li>Consider a partition \\(P_1\\times P_2 = P\\)</li> <li>Since \\(S\\) is dense in \\(I^2,\\ L(P, \\chi_S) = 0,\\ U(P, \\chi_S)=1\\) for any partition \\(P\\)</li> <li>\\(\\inf L(P, \\chi_S)  = 0\\)</li> <li>\\(\\sup U(P, \\chi_S) = 1\\)</li> <li>hence \\(X_S\\) is not Integrable</li> </ul>"},{"location":"Calculus%20Assignment/#b_1","title":"b)","text":"<ul> <li>Since no open balls can be made around any point in \\(S\\) which are a subset of \\(S\\), \\(S^o = \\emptyset\\)</li> <li>\\(\\overline{S} = I^2\\)</li> <li>\\(\\delta S = \\overline{S}\\setminus S^o = I^2\\)</li> <li>\\(I^2\\) is a closed and bounded subset of \\(\\mathbb{R}^2\\), hence \\(I^2\\) is compact</li> <li>Since \\(I^2\\) is compact, there exits an open cover with finite subcover ${R_n}</li> <li>\\(I^2\\sub \\bigcup\\limits_{1\\le i\\le n} R_i\\)</li> <li>\\(Area(I^2) \\le Area\\left(\\bigcup\\limits_{1\\le i \\le n} R_i\\right)\\)</li> <li>Therefore area of any union of sets that make a cover of \\(I^2\\), hence The jordan measure of \\(\\delta S\\) is non zero</li> </ul>"},{"location":"Calculus%20Assignment/#question-5","title":"Question 5","text":""},{"location":"Calculus%20Assignment/#a_2","title":"a)","text":"<ul> <li>\\(\\delta S = \\left\\{\\left(\\frac1n, y\\right)|n\\in \\mathbb{N}, y\\in I\\right\\}\\)</li> <li>The set \\(\\left(\\frac1n, y\\right)\\) for a particulat \\(n\\) can be covered  by the rectangle \\(I\\times \\left[\\frac1n-\\frac\\epsilon3,\\frac1n+\\frac\\epsilon3\\right]\\)</li> <li>The area of the cover of the set is \\(\\frac{2\\epsilon}3&lt;\\epsilon\\), hence the has measure \\(0\\)</li> <li>countable union of zero measure sets has measure \\(0\\), hence \\(\\delta S\\) has measure 0, hence \\(S\\) is Integrable</li> </ul>"},{"location":"Calculus%20Assignment/#b_2","title":"b)","text":"<ul> <li>\\(Area(S) = Area(I^2) - \\sum\\limits_{i=1}^\\infty Area\\left(\\left[\\frac1n+\\frac{\\epsilon}{2\\cdot2^i},\\frac1n- \\frac{\\epsilon}{2\\cdot2^i} \\right]\\right)\\)</li> <li>\\(Area(S) = 1 - \\epsilon\\)</li> <li>\\(Area(S) = 1\\)</li> </ul>"},{"location":"Calculus%20Assignment/#question-6","title":"Question 6","text":""},{"location":"Calculus%20Assignment/#question-7","title":"Question 7","text":""},{"location":"Caley%20Graphs/","title":"Caley Graphs","text":"<p>202308231008</p> <p>Type : #Note  Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Caley%20Graphs/#cayley-graphs","title":"Cayley Graphs","text":"<p>Let \\(\\Gamma\\) be a finite group and \\(S\\subset\\Gamma\\) such that  - \\(S\\ne \\emptyset\\) - \\(1\\notin S\\) - \\(s\\in S\\iff s^{-1}\\in S\\) Then the Cayley Graph \\(G=\\text{Cay}(\\Gamma, S)\\) has \\(V=V(G)=\\Gamma\\) and \\(x\\sim y\\) if \\(x^{-1}y\\in S\\)</p> <p>Let \\(G=\\text{Cay}({\\Gamma, S})\\), then - \\(E=\\{(\\alpha, \\alpha s):\\alpha\\in\\Gamma,s\\in S\\}\\) - \\(G\\) is semiregular - Let \\(\\beta\\in\\Gamma\\). Then \\(\\rho_{\\beta}:V(G)\\to V(G)\\) given by \\(\\rho_{\\beta}(x)=x\\beta\\) is an automorphism - \\(\\rho_{\\gamma}\\circ\\rho_{\\beta}=\\rho_{\\gamma\\beta},\\gamma\\to\\rho_{\\gamma}\\) is an isomorphism</p>"},{"location":"Caley%20Graphs/#theorem","title":"Theorem","text":"<p>Statement: Let \\(G=\\text{Cay}(\\Gamma, S)\\) connected iff \\(S\\) is a generator set for \\(\\Omega\\). proof: Let \\(\\langle S\\rangle = \\Gamma\\). Then \\(\\forall r\\in\\Gamma,r=s_{1}s_{2}\\dots s_{r},s_{i}\\in S\\forall i\\) $$ 1 \\xrightarrow{s_{1}}s_{1}\\xrightarrow{s_{2}}s_{1}s_{2}\\xrightarrow{s_{3}}\\;\\cdots\\xrightarrow{s_{r}}\\; s_{1}s_{2}\\dots s_r $$ Conversely, suppose \\(\\langle S\\rangle =\\Gamma'\\subsetneq\\Gamma\\) then \\(r\\in\\Gamma\\) and \\(r\\notin\\Gamma'\\). Then \\(\\not\\exists\\) a path with the edge with labels in \\(S\\) such that the path goes from \\(1\\)  to \\(r\\). which means \\(G\\) is not connected.</p>"},{"location":"Caley%20Graphs/#theorem_1","title":"Theorem","text":"<p>Statement: Let \\(G\\) be the Peterson Graph. Then \\(G\\) is not a Cayley Graph Proof: Consider \\(\\Gamma=\\mathbb Z_{10}\\) Since the degree of graph is \\(3\\) \\(S=\\{\\alpha,\\beta,\\beta'\\}\\) and the only element that can be alpha is \\(+5\\)  Then there exists that path  \\(1\\sim\\beta\\sim\\beta\\alpha\\sim\\beta\\alpha\\beta^{-1}\\sim\\beta\\alpha\\beta^{-1}\\alpha^{-1}=1\\) which makes a cycle of \\(4\\) which is a contradiction. Hence \\(\\Gamma=D_{5}\\). \\(|S|\\) has to be 3 so we have \\(S=\\{\\alpha, \\beta, \\beta^{-1}\\}\\) Then consider \\(1\\sim\\beta\\sim\\beta\\alpha\\sim\\beta\\alpha\\beta\\sim\\beta\\alpha\\beta\\alpha=1\\) which also creates a cycle of \\(4\\) which is also a contradiction. Hence the Peterson Graph cannot be a Cayley Graph</p>"},{"location":"Caley%20Graphs/#theorem_2","title":"Theorem","text":"<p>Statement: Given \\((\\Gamma, S)\\), let \\(G=\\text{Cay}(\\Gamma, S)\\). Then \\(\\Gamma\\) acts [[Sharp Transitively]] on \\(G\\). Proof: Consider \\((\\beta, \\gamma)\\in E\\) $$ \\begin{align} (\\beta, \\gamma)\\in E &amp;\\implies \\beta^{-1}\\gamma\\in S\\ &amp;\\implies (\\alpha\\beta)^{-1}(\\alpha\\gamma)\\in S\\ &amp;\\implies \\tilde \\alpha(\\beta)\\tilde\\alpha(\\gamma)\\in S \\end{align} $$ So \\(\\tilde\\alpha\\) is an automorphism of \\(G\\)</p>"},{"location":"Caley%20Graphs/#theorem_3","title":"Theorem","text":"<p>Statement: Let \\(G\\) be a connected graph and \\(\\Gamma\\subseteq \\text{Aut}(G)\\). Then the following are equivalent: - \\(\\Gamma\\) acts [[Sharp Transitively]] upon \\(G\\) - \\(G=\\text{Cay}(\\Gamma, S)\\) for a suitable symmetric \\(S\\subset T\\) such that \\(S\\) generates \\(\\Gamma\\) Proof: (\\(1\\implies2\\)) Let \\(\\gamma_{i}\\in\\Gamma\\) (unique) such that \\(\\gamma_{i}(v_{1})=v_{i}\\)  $$ \\begin{align} \\text{Let } S^{+}&amp;= {s\\in\\Gamma : s(v_{1})\\sim v_{1} }\\ S^{-}&amp;= {s^{-1} : s\\in S^{+}}\\ S&amp;= S^{+}\\cup S^{-} \\end{align} $$ on vertex set \\(\\Gamma\\), define a graph where \\(\\gamma_{i}\\sim\\gamma_{j}\\) if \\(\\gamma_{j}=\\gamma_{i}s, s\\in S\\).</p> <p>Then  $$ \\begin{align} \\gamma_{i}\\sim \\gamma_{j}&amp;\\iff \\gamma_{i}^{-1}\\gamma_{j}\\in S \\ &amp;\\iff v_{i}\\sim (\\gamma_{i}^{-1}\\gamma_{j})v_{1}\\ &amp;\\iff \\gamma_{i}(v_{1})=\\gamma_{j}\\ &amp;\\iff v_{i}\\sim v_{j}  \\end{align} $$</p> <pre><code>break apart into different files\n</code></pre>"},{"location":"Caley%20Graphs/#references","title":"References","text":"<p>Cayley-Graph-Examples</p>"},{"location":"Cana%20Assignment%204/","title":"Cana Assignment 4","text":"<p>Name: Shubh Sharma Roll Number: BMC202170</p>"},{"location":"Cana%20Assignment%204/#1-pg-130-problem-5","title":"1. PG 130, Problem 5","text":"<p>If \\(f\\) has an essential singularity, it would come arbitrarily close to every point, then \\(f\\) is would not be bounded, hence \\(f\\) does not have an essential singularity.</p> <p>Let \\(z_0\\) be a pole of \\(f\\), then by the open mapping theorem, an open set around \\(z_0\\) would be mapped to an open neighbourhood of \\(\\infty\\), which would mean, either the \\(Re\\) or the \\(Im\\) parts are unbounded, Hence \\(f\\) does not have a pole either.</p> <p>Hence an isolated singularity of \\(f(z)\\) is removable.</p>"},{"location":"Cana%20Assignment%204/#2-pg-130-problem-6","title":"2. PG 130, Problem 6","text":"<p>If \\(e^{f(z)}\\) has a pole at \\(z_{0}\\) then the real part of \\(f(z)\\) goes to \\(\\infty\\) as \\(z\\) goes to \\(z_{0}\\). Hence \\(f(z)\\) would also have a pole at \\(z_0\\).</p> <p>Having a pole at \\(z_{0}\\) implies, given any \\(M&gt;0,\\;\\exists\\epsilon&gt;0\\) Such that \\(z\\in B_{\\epsilon}(z_{0})\\implies |f(z_{0})|&gt;M\\). Pick \\(z\\in B_\\epsilon(z_{0})\\ne z_{0}\\), its image is finite and the sequence \\(f(z)+2n\\pi i\\) converges to \\(\\infty\\)., take the inverse images of each of the points of the sequence. which form a sequence \\(z_{n}\\to z_{0}\\). \\(e^{f(z_{n})}\\) is a constant sequence which is finite, hence \\(z_{0}\\) is not a pole of \\(e^{f(z)}\\) which is a contradiction.</p>"},{"location":"Cana%20Assignment%204/#3-pg-133-problem-3","title":"3. PG 133, Problem 3","text":"<p>Let \\(f(z)= \\cos z\\) and let \\(z_{0}=0\\), \\(f(z)-1\\) has a root at \\(z_{0}\\)  The derivative of \\(f'(0)=\\sin 0=0\\),  second derivative is \\(-\\cos 0=-1\\). Hence the root has degree \\(2\\). Hence \\(\\cos z -1 = z^{2}g(z)=\\zeta^{2}(z)\\) , hence \\(\\zeta(z) = z\\sqrt {g(z)}\\) and we get $$ \\begin{align} f(z)-1 &amp;= \\cos z -1\\ &amp;= -2\\sin^{2}\\left(\\frac{z}{2}\\right)\\ &amp;= \\zeta^{2}(z)\\ \\therefore \\zeta(z) &amp;= \\sqrt 2 i \\sin\\left(\\frac{z}{2}\\right) \\end{align} $$</p>"},{"location":"Cana%20Assignment%204/#4-pg-133-problem-4","title":"4. PG 133, Problem 4","text":"<p>\\(f(z^{n})-f(0)\\) has all \\(i\\) order derivatives for \\(i\\in\\{1\\dots n-1\\}\\) and \\(0\\) is a root  hence \\(f(z^n)-f(0)=z^{n}h(z)\\) where \\(h(0)\\ne 0\\) Hence we can choose a neighborhood of \\(0\\) such that \\(|h(z)-h(0)|\\le|h(0)|\\) for all \\(z\\) in the neighborhood, so we can define a single valued branch of \\(\\sqrt[n]{h(z)}\\) in the neighbourhood of \\(0\\) $$ f(z^{n})-f(0) = \\left(z\\sqrt[n]{h(z)}\\right)^n $$ and have \\(g(z)=z\\sqrt[n]{h(z)}\\) </p>"},{"location":"Cana%20Assignment%204/#5-pg-136-problem-1","title":"5. PG 136, Problem 1","text":"<p>The equation \\(36\\) in ahlfors states that, given a function \\(f\\) analytic on \\(|z|&lt; R\\), such taht \\(|f(z)|&lt;M,f(z_0)=w_0\\), then we have  $$ \\left |     \\frac     {M(f(z)-w_0)}     {M^{2}-\\bar w_{0}f(z)} \\right| \\le \\left |     \\frac     {R(z-z_{0})}     {R^{2}-\\bar z_{0}z} \\right| $$ putting \\(R=M=1\\) we get $$ \\left |     \\frac         {f(z)-w_{0}}         {1-\\bar w_{0}f(z)} \\right| \\le \\left |     \\frac         {z-z_{0}}         {1-\\bar z_{0}z} \\right| $$ as \\(z\\to z_0\\) we have \\(f(z)\\to w_{0}\\)  $$ \\left |     \\frac         {f'(z_{0})}         {1-|f^{2}(z_{0})} \\right| \\le \\left |     \\frac         {1}         {1-|z_{0}|^{2}} \\right| $$ but \\(z_0\\) can be any point $$ \\left |     \\frac         {f'(z)}         {1-|f^{2}(z)} \\right| \\le \\left |     \\frac         {1}         {1-|z|^{2}} \\right| $$</p>"},{"location":"Cana%20Assignment%204/#6-pg-136-problem-2","title":"6. PG 136, Problem 2","text":"<p>Consider the map \\(g:\\mathcal H\\to\\mathcal D\\), where \\(\\mathcal D\\) is an open unit disk and \\(\\mathcal H\\) is the upper half plane, given by \\(g(z)= \\frac{z-z_{0}}{z-\\bar{z_{0}}}\\), where \\(z_0\\in\\mathcal H\\) is a fixed point. Consider the map \\(h(z)= \\frac{z-f(z_{0})}{z-\\overline{f(z_{0})}}\\) </p> <p>And consider the map \\(h\\circ f \\circ g^{-1}: \\mathcal D\\to \\bar{\\mathcal D}\\)  This satisfies the conditions that the function is analytic on \\(\\mathcal D\\), and \\(|h(f(g^{-1}(z)))|\\le 1\\) for all \\(z\\in \\mathcal D\\)  also \\(h(f(g^{-1}(0)))=0\\).</p> <p>This gives \\(|h(f(g^{-1}(z)))|\\le|z|\\implies |h(f(z))|\\le |g(z)|\\implies \\left| \\frac{f(z)-f(z_{0})}{f(z)-\\overline{f(z_{0})}} \\right|\\le \\frac{z-z_{0}}{z-\\bar{z}_{0}}\\)  taking \\(z\\to z_{0}\\) we get $$ \\frac{|f'(z_{0})|}{Im(f(z_{0}))}\\le \\frac{1}{Im(z_{0})} $$</p>"},{"location":"Cana%20Assignment%204/#7-pg-136-problem-3","title":"7. PG 136, Problem 3","text":"<p>In problem 5, equality holds iff $$ \\left|     \\frac         {f(z)-w_{0}}         {1-\\overline w_{0}f(z)} \\right| = \\left|      \\frac         {z-z_{0}}         {1-\\overline z_{0}z} \\right| $$ which holds iff equality holds in \\((36)\\) with \\(M=R=1\\) hence \\(|Sf(T^{-1}\\zeta)|=|\\zeta|\\) where \\(S\\) adn \\(T\\) are linear fractional transformations.</p> <p>This gives \\(SfT^{-1}(\\zeta)=c\\zeta\\) for some \\(c\\). This gives \\(f=S^{-1}(cT(\\zeta))\\) which is a compostition of \\(3\\) linear fractional transformationsm, and hence is a Linear fractional transformation.</p> <p>In Problem 6, equality holds iff \\(|h(f(g^{-1}(z)))|=|z|\\) which gives \\(h(f(g^{-1}(z)))=cz\\) for some constant \\(c\\). That gives \\(f\\) as \\(f(z)=h^{-1}(cg(z))\\) which is also a linear fractional transformation.</p>"},{"location":"Cana%20Assignment%204/#8-pg-148-problem-2","title":"8. PG 148, Problem 2","text":"<p>Let \\(S\\) be a simply connected space. Let \\(P=S\\setminus M\\) where \\(M\\) is a set with \\(m\\) points The \\(P^{c}= S^{C}\\cup M\\) points which are \\(m+1\\) connected components, hence \\(p\\) has connectivity  \\(m+1\\)</p> <p>The homology basis consists of \\(m\\) loops each centered around the \\(m\\) points. So for each of the \\(m\\) points, we can find a circle \\(\\gamma_{i}\\) around the points \\(a_{i}\\in M\\)  and we will have \\(n(\\gamma_{i}, a_{j})= \\delta_{i, j}\\) which forms a basis</p>"},{"location":"Cana%20Assignment%204/#9-pg-148-problem-5","title":"9. PG 148, Problem 5","text":"<p>Any closed curve \\(\\gamma\\) in the domain \\(\\Omega\\) that winds around \\(1\\) also winds around \\(-1\\). consider some point \\(z_{0}\\in\\gamma\\) and choose some value of \\(\\theta_{1}=arg(1-z)\\) such that \\((1-z_{0})=r_{1} e^{i\\theta_{1}}\\) . and take a branch of \\(\\sqrt{1-z_{0}}\\). Now as we travel around \\(\\gamma\\), as we come back to \\(z_0\\) we add \\(2\\pi\\) to the argument of \\((1-z)\\) . The same works for the argument of \\((1+z)\\) and so the argument of the product changes by \\(4\\pi\\). Now when we take the square root, we divide the argument by \\(2\\) which means that as we move around \\(\\gamma\\)  and come back to \\(z_0\\), the argument of \\(\\sqrt {1-z^{2}}\\) changes by \\(2\\pi\\).</p> <p>Now we compute  $$ \\int_\\gamma\\frac{1}{\\sqrt{1-z^{2}}}dz. $$ And by Cauchy's Theorem, we can assume \\(\\gamma\\) is a very large cirle and so acts as a small disc about infinity. we apply the change of variables \\(z=\\frac{1}{w}\\) and we get the integral $$ -\\int_{|w|=\\delta} \\frac{dw}{w\\sqrt{w^{2}-1}}  $$ \\(\\sqrt{w^{2}-1}\\) is analytic in a neighbourhood of \\(0\\) so we can compute the integral by cauchy's formula to be \\(2\\pi\\)</p>"},{"location":"Category%20Theory%20Introduction%20Page/","title":"Category Theory Introduction Page","text":"<p>202301270201</p> <p>Type : #Note Tags : [[Category Theory]]</p>"},{"location":"Category%20Theory%20Introduction%20Page/#category-theory-introduction-page","title":"Category Theory Introduction Page","text":"<pre><code>\\usepackage{tikz-cd} \n\\begin{document} \n\\begin{tikzcd} \u00a0 \u00a0 T \u00a0 \u00a0 \\arrow[drr, bend left, \"x\"] \u00a0 \u00a0 \\arrow[ddr, bend right, \"y\"] \u00a0 \u00a0 \\arrow[dr, dotted, \"{(x,y)}\" description] &amp; &amp; \\\\\nK &amp; X \\times_Z Y \\arrow[r, \"p\"] \\arrow[d, \"q\"] \u00a0 \u00a0 &amp; X \\arrow[d, \"f\"] \\\\\n&amp; Y \\arrow[r, \"g\"] \u00a0 \u00a0 &amp; Z \n\\end{tikzcd}\n\\end{document} \n</code></pre> <pre><code>\\usepackage{tikz-cd}\n\\begin{document}\n\\begin{tikzcd}\n&amp; X\\times Y \\arrow[dl, \"\\pi_1\"'] \\arrow[dr, \"\\pi_2\"]  &amp; \\\\\nX &amp;  &amp;  Y \\\\\n&amp; A \\arrow[ul, \"f\"] \\arrow[ur, \"g\"'] \\arrow[uu, \"\\langle{f,g}\\rangle\", dashed]&amp;\n\\end{tikzcd}\n\\end{document}\n</code></pre> <pre><code>\\usepackage{tikz-cd}\n\\begin{document}\n\\begin{tikzcd}\nX\\cap Y \n\\arrow[d, \"j_2\"'] \n\\arrow[r, \"j_1\"]     \n&amp; \nX \\arrow[d, \"i_1\"'] \\arrow[rdd, \"f\", bend left] \n&amp;   \n\\\\\nY \\arrow[r, \"i_2\"] \\arrow[rrd, \"g\", bend right] \n&amp;\nX\\sqcup_{X\\cap Y}Y \\arrow[rd, \"!h\", dashed]     \n&amp;\n\\\\\n&amp;\n&amp; \nA\n\\end{tikzcd}\n\\end{document}\n</code></pre>"},{"location":"Category%20Theory%20Introduction%20Page/#references","title":"References","text":"<p>Resources Used for making these notes: - Books:   - An Invitation to Applied Category Theory: Seven Sketches in Composibilty - Websites:   - A collection of resources on Github</p>"},{"location":"Cauchy-Reimann%20Equation/","title":"Cauchy Reimann Equation","text":"<p>202301041401</p> <p>Type : #Note Tags : [[Complex Analysis]]</p>"},{"location":"Cauchy-Reimann%20Equation/#cauchy-reimann-equation","title":"Cauchy-Reimann Equation","text":"<p>let \\(f: U\\to \\mathbb C\\) be an analytic function, \\(f(z) = u(z) + iv(z)\\), where \\(u\\) and \\(v\\) are real functions. On computing \\(f'\\) in \\(2\\) different ways 1. Along x-axis    $$    \\begin{aligned}    f'(z)&amp;=\\lim\\limits_{h\\to 0, h\\in \\mathbb R} \\frac{f(z+h)-f(z)}h\\    &amp;= \\lim\\limits_{h\\to 0}\\frac{u(z+h)-u(z)}h + \\iota\\lim\\limits_{h\\to 0}\\frac{v(z+h)-v(z)}h\\    &amp;=\\frac{\\partial u}{\\partial x}(z) + \\iota\\frac{\\partial v}{\\partial x}(z)    \\end{aligned}    $$ 2. Along y-axis     $$    \\begin{aligned}    f'(z)&amp;=\\lim\\limits_{h\\to 0, h\\in \\mathbb R} \\frac{f(z+\\iota h)-f(z)}{\\iota h}\\    &amp;= -\\iota\\lim\\limits_{h\\to 0}\\frac{u(z+\\iota h)-u(z)}h + \\lim\\limits_{h\\to 0}\\frac{v(z+\\iota h)-v(z)}h\\    &amp;=-\\iota\\frac{\\partial u}{\\partial x}(z) + \\frac{\\partial v}{\\partial x}(z)    \\end{aligned}    $$ By comparing the two we have  $$ {\\partial u \\over \\partial x} = {\\partial v \\over \\partial y}, {\\partial u \\over \\partial y} = -{\\partial v \\over \\partial x} $$</p>"},{"location":"Cauchy-Reimann%20Equation/#cauchy-riemann-real-differentiablity-implies-complex-differentiability","title":"Cauchy riemann + real differentiablity implies complex differentiability","text":"<pre><code>title: Theorem\nGiven a function $f : \\mathbb{R}^2 \\to \\mathbb{R}^2$, $f = (u,v)$ where $u,v : \\mathbb{R} \\to \\mathbb{R}$ which is differentiable as a real function at the point $p$, such that $\\dfrac{\\partial u}{\\partial x} = \\dfrac{\\partial v}{\\partial y}$ and $\\dfrac{\\partial v}{\\partial x} = -\\dfrac{\\partial u}{\\partial y}$, then $f$ is differentiable at $p$ when thought of as a complex function.\n</code></pre>"},{"location":"Cauchy-Reimann%20Equation/#proof","title":"Proof:","text":"<p>By real differentiability we get \\(\\(\\lim\\limits_{h \\to 0} \\dfrac{\\|f(p+h)-f(p) - Ah\\|}{\\|h\\|} = 0\\)\\) For some real matrix \\(A\\) which is a matrix whose corresponding linear transformation is just a rotation along with a scaling (look at how a rotation matrix looks like and how A looks, use cauchy riemann here). Since \\(A\\) takes in a vector in \\(\\mathbb{R}^2\\), and spits out a rotated and scaled version of the same, we can emulate this with a complex number. Suppose \\(A\\) looks like \\(\\begin{pmatrix} a &amp; -b \\\\ b &amp; a \\end{pmatrix}\\), then the corresponding complex number will be \\(a+bi\\), we can check that \\(A \\begin{bmatrix} h_1 \\\\ h2 \\end{bmatrix}\\) and \\((a+bi)(h1+ih2)\\) give the same vector in the plane, just in different ways of writing. And so, we can rewrite the limit as  \\(\\(\\lim\\limits_{h \\to 0} \\dfrac{\\|f(z+h)-f(z) - (a+bi)h\\|}{\\|h\\|} = 0\\)\\) Here, \\(z\\) denotes the complex number associated to \\(p\\) and \\(h\\) is a compex number as well. Also notice that we can now drop the modulus signs since if a complex number's modulus approaches 0, the complex number itself approaches 0. So this gives that \\(f'(z) = a+bi\\) and \\(f\\) is complex differentiable there.</p>"},{"location":"Cauchy-Reimann%20Equation/#related-problems","title":"Related Problems","text":""},{"location":"Cauchy-Reimann%20Equation/#references","title":"References","text":"<p>Analytic Function</p>"},{"location":"Cayley-Graph-Examples/","title":"Cayley Graph Examples","text":"<p>202308231114</p> <p>type : #Example  tags : [[Algebraic Graph Theory]]</p>"},{"location":"Cayley-Graph-Examples/#cayley-graph-examples","title":"Cayley-Graph-Examples","text":"<p>Example 1:  \\(\\Gamma=S_{3}, S={(1,2), (2,3), (3, 1)}\\) This makes \\(K_{3,3}\\) ![[Drawing 2023-08-24 22.00.20.excalidraw]]</p> <p>Example 2:  \\(\\Gamma=(\\mathbb Z_{2})^{3}=\\mathbb Z_{2}\\oplus\\mathbb Z_{2}\\oplus\\mathbb Z_{2}\\) where \\(S={a,b,c}\\) where \\(a=(100),b=(010),c=(001)\\) ![[Drawing 2023-08-25 02.03.42.excalidraw]]</p> <p>Example 3: \\(\\Gamma=A_{5}\\) where \\(S=\\{\\alpha=(1,2)(3,4)\\;,\\;\\beta=(1,2,3,4,5)\\;,\\;\\beta^{-1}\\}\\) This makes Fullerene. { width=\"400\" }</p>"},{"location":"Cayley-Graph-Examples/#related","title":"Related","text":"<p>Caley Graphs</p>"},{"location":"Cesaro%20Summability/","title":"Cesaro Summability","text":"<p>2022-11-13 03:11 pm</p> <p>Type : #Note Tags : [[Analysis]]</p>"},{"location":"Cesaro%20Summability/#cesaro-summability","title":"Cesaro Summability","text":"<pre><code>title:Motivation \nSometimes convergence of a series is too much to ask for, so you look at convergence of an average series.\n\nGiven a series $\\sum c_n$, let $s_n$ be the partial sums of the series. Then,\n$$\\sigma_N \\colon= \\dfrac{s_0 + s_1 + \\cdots s_{N-1}}{N}$$\n$\\sigma_N$ is called the $N$th Cesaro mean of the sequence $\\{s_n\\}$ or the $N$th Cesaro sum of the series $\\sum c_n$.\n\nIf $\\sigma_N$ converges to $\\sigma$ as $N$ tends to infinity, then the series $\\sum s_n$ is said to be *Cesaro summable* to $\\sigma$.\n\nFor example the sequence, $0,1,0,1,\\cdots$ is not convergent.\nBut it's cesaro sums converge to $1/2$. \n</code></pre> <ul> <li>Cesaro sums of a convergent series converge to the series limit.   ### Proof:   Let \\(\\displaystyle\\sum\\limits_{k=0}^{\\infty} c_k = c\\).   Let \\(\\sigma_N = \\dfrac{s_0 + s_1 + \\cdots s_{N-1}}{N}\\)   Look at \\(|c-\\sigma_N| = \\left|\\dfrac{(c-s_0)+(c-s_1)+\\cdots(c-s_{N-1})}{N}\\right|\\).   \\(\\implies |c-\\sigma_N| \\le \\dfrac{|c-s_0| + \\cdots |c-s_M|}{N} + \\dfrac{|c-s_{M+1}|+\\cdots+|c-s_{N-1}|}{N}\\)   where \\(m\\) is such that \\(|s_k-c| &lt; \\epsilon\\) for all \\(k &gt; m\\).   \\(\\implies |c-\\sigma_N| \\le (N-M-1)\\epsilon/N + F/N \\le 2\\epsilon\\)   for large N.</li> </ul>"},{"location":"Cesaro%20Summability/#-","title":"---","text":""},{"location":"Cesaro%20Summability/#related-problems","title":"Related Problems","text":"<p>Fejer Kernel</p>"},{"location":"Cesaro%20Summability/#references","title":"References","text":"<p>Fourier Series</p>"},{"location":"Channel%20Systems/","title":"Channel Systems","text":"<p>202310241410</p> <p>Tags : [[Theory of Computation]]</p>","tags":["Note"]},{"location":"Channel%20Systems/#channel-systems","title":"Channel Systems","text":"<p>A Channel System is a [[Finite State Automaton|Finite State Automata]] which is similar to a Push Down Automata which uses a Queue(called Channels) instead of a Stack. Channel Systems are Turing Complete as reading and inputting the entire stack can be used to emulate a transition of a Turing Machine.</p>","tags":["Note"]},{"location":"Channel%20Systems/#formal-definition","title":"Formal Definition","text":"<p>A Channel System \\(S\\) is defined by the tuple \\(\\langle Q, C,\\Sigma, \\Delta\\rangle\\) where - \\(Q=\\{q_{1}\\dots q_{n}\\}\\) is a set of control states - \\(q_{0}\\in Q\\) is an initial state - \\(A\\) is a finite set of alphabet and \\(\\epsilon\\in A\\)  - \\(C\\) is a finite set of channels - \\(\\Delta:Q\\times C\\times\\{?,!\\}\\times\\Sigma^{*}\\times A\\times Q\\) is a finite set of transitions</p> <p></p>","tags":["Note"]},{"location":"Channel%20Systems/#configuration","title":"Configuration","text":"<p>A configuration or a global state of a channel system with \\(n\\) channels is an \\(n+1\\) tuple \\(\\in\\) \\(Q\\times \\prod\\limits_{i=1}^{n}(\\Sigma^{*})\\). Intuitively, a configuration \\((q, w_{1},w_{2}\\dots w_{n})\\) represents the current state and contents of each channel where \\(w_{i}\\) represents the contents of the \\(i^{th}\\) channel.</p> <p>The initial configuration is \\((q_{0},\\epsilon,\\epsilon\\dots \\epsilon)\\).</p>","tags":["Note"]},{"location":"Channel%20Systems/#execution","title":"Execution","text":"<p>A transition \\((q,c_{i},!,u,a,q')\\) takes the control from state \\(q\\) to \\(q'\\) while writing \\(u\\) to the channel \\(c_{i}\\) after reading \\(a\\). A transition \\((q, c_{i},?, u, a, q')\\) takes the control from state \\(q\\) to \\(q'\\) while popping \\(u\\) from the channel \\(c_{i}\\) after reading \\(a\\). A transitions with \\(\\epsilon\\) neither writes, not removes from the channels.</p> <p>A sequence of Configurations received by successive transitions is called a Run.</p>","tags":["Note"]},{"location":"Channel%20Systems/#references","title":"References","text":"<p>Reachability in Lossy Channel Systems</p>","tags":["Note"]},{"location":"Characterisic%20of%20a%20field/","title":"Characterisic of a field","text":"<p>202210120910</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Characterisic%20of%20a%20field/#characterisic-of-a-field","title":"Characterisic of a field","text":"<p>The Characteristic of a field \\(F\\), denoted \\(\\text{ch}(F)\\) is the smallest positive integer \\(p\\) such that \\(p\\cdot1_F=0\\), if such \\(p\\) exists, otherwise its defined to be \\(0\\). $$ \\begin{aligned} n\\times1_F+m\\times1_F &amp;= (n+m)\\times1_F\\ (n\\times1_F)\\cdot(m\\times1_F) &amp;= mn\\cdot1_F  \\end{aligned} $$ from this we get that \\(\\text{ch}(F)\\) is either a prime \\(p\\) or \\(0\\). Also, \\(p\\cdot\\alpha = 0\\forall\\alpha\\in F\\)</p> <p>If we define \\((-n)\\cdot1_F=-(n\\cdot1_F)\\), we can define a homomorphism \\(\\phi:\\mathbb Z\\to F\\) such that \\(n\\mapsto n\\cdot1_F\\) such that \\(\\ker(\\phi) = \\text{ch}(F)\\mathbb Z\\)  Since \\(F\\) is a field, \\(F\\) contains a subfield that is either isomorphic to \\(\\mathbb Z/p\\mathbb Z\\) or \\(\\mathbb Q\\) which in either case is the smallest subfield containing \\(1_F\\).</p> <p>A Prime Subfield of a field \\(F\\) is the subfield generated by the element \\(1_F\\) of \\(F\\). It is isomorphic to either \\(\\mathbb Z/p\\mathbb Z\\) where \\(p\\ne0\\) is \\(\\text{ch}(F)\\) or \\(\\mathbb Q\\) if \\(\\text{ch}(F) = 0\\).</p>"},{"location":"Characterisic%20of%20a%20field/#related-problems","title":"Related Problems","text":""},{"location":"Characterisic%20of%20a%20field/#references","title":"References","text":"<p>Fields</p>"},{"location":"Church%20Booleans/","title":"Church Booleans","text":"<p>202303150403</p> <p>Type : #Note Tags : [[Lambda Calculus]]</p>"},{"location":"Church%20Booleans/#church-booleans","title":"Church Booleans","text":"<p>By convention, the defintions of <code>TRUE</code> and <code>FALSE</code> used (also called Church Booleans) is  $$ \\begin{align} \\text{TRUE}&amp;:= \\lambda xy.x\\ \\text{FALSE}&amp;:= \\lambda xy.y \\end{align} $$ and with these two defintions, we can definte the following logical opeators easily $$ \\begin{align} \\text{AND} &amp;:= \\lambda ab.aba\\ \\text{OR} &amp;:= \\lambda ab.aab\\ \\text{NOT} &amp;:= \\lambda p. p \\text{ (FALSE)  (TRUE)} \\end{align} $$</p> <p>This can now be used in defining predicates, which are functions that return a boolean value. For example the \\(\\text{ISZERO}\\) predicate, which returns \\(\\text{TRUE}\\) if an input number is then Church Numerals \\(0\\) and returns \\(\\text{FALSE}\\) otherwise, can be written as $$ \\text{ISZERO} := \\lambda n.n\\; (\\lambda x.\\text{ FALSE })\\; \\text{ TRUE}  $$ and the predicate \\(\\text{LEQ}\\)  $$ \\text{LEQ} := \\lambda mn. \\text{ ISZERO } (\\text{ SUB }m\\;n) $$ where \\(\\text{SUB}\\) is defined in Church Numerals</p>"},{"location":"Church%20Booleans/#references","title":"References","text":"<p>Church Numerals</p>"},{"location":"Church%20Numerals/","title":"Church Numerals","text":"<p>202303150503</p> <p>Type : #Note Tags : [[Lambda Calculus]]</p>"},{"location":"Church%20Numerals/#church-numerals","title":"Church Numerals","text":""},{"location":"Church%20Numerals/#numerals","title":"Numerals","text":"<p>The most common way of defining numerals in lambda calculus is the definition given by Alonzo Church, which is also called the Church Numerals, which are defined as repeated application of a function of a value.</p> <p>The Church Numerals are defined as  $$ \\begin{aligned} 0&amp;= \\lambda fx.x\\ 1&amp;= \\lambda fx.f x\\ 2&amp;= \\lambda fx.f (f x)\\ 3&amp;= \\lambda fx.f (f (f x))\\ &amp;\\vdots \\end{aligned} $$</p>"},{"location":"Church%20Numerals/#successor-function","title":"Successor Function","text":"<p>We can define a \\(\\text{SUCC}\\) function which finds the successor of a given church numeral by  \\(\\(\\text{SUCC}:=\\lambda\\;n.(\\lambda\\; fx.\\;f(n\\;f\\;x))\\)\\)</p> <p>which add one \\(f\\) behind the chain of applied \\(f\\)s <pre><code>title:Infix Notation\nwe can write $SUCC(n)$ as $n++$\n</code></pre></p>"},{"location":"Church%20Numerals/#addition","title":"Addition","text":"<p>Adding a number by \\(n\\) can be thought of repeatedly applying the successor function to it \\(n\\) times. Hence an addition function can be written as  \\(\\(\\text{ADD}:=\\lambda\\;mn.\\;m\\;\\text{SUCC}\\;n\\)\\) which captures the above procedure.</p> <p>Given the way that the church numerals are define, we can directly append the parameter \\(f\\) a certain number of times before the defintion, so another way to write the add function would be  $$ ADD := \\lambda\\; mnfx.\\;\\underbrace{m\\;f}{(i)}\\;\\underbrace{(n\\;f\\;x)}{(ii)} $$   (i)  applying \\(f\\) \\(m\\) many times to    (ii) \\(f\\) applied to \\(x\\) \\(n\\) times. <pre><code>title: Infix Notation\nwe can write $ADD(m,n)$ as $m+n$\n</code></pre></p>"},{"location":"Church%20Numerals/#multiplication","title":"Multiplication","text":"<p>We can define Multiplication as repeated addition as \\(\\(\\text{MULT}:=\\lambda\\;mn.\\;m\\;(\\text{PLUS }n)\\;0\\)\\) A smaller version to do would be  $$ MULT := \\lambda\\;mn.(\\lambda fx.\\;\\underbrace{n\\;(\\lambda x.mfx)}_{(i)}\\;x) $$  (i) Here the term inside the bracket is a function which applies \\(f\\;m\\) times on an input, and that is applied on \\(x\\) \\(n-\\)many times, which is the same as applied the function \\(f\\) on the input \\(x\\) \\(n*m\\) times.  Applying eta reduction on the above function to make a bit cleanere gives us</p> <p>$$ MULT:= \\lambda\\; mn.(\\lambda f. n (mf)) $$ <pre><code>title: Infix Notation\nwe can write $MULT(m,n)$ as $m\\times n$\n</code></pre></p>"},{"location":"Church%20Numerals/#exponentiation","title":"Exponentiation","text":"<p>Exponentiation can be defined as repeated multiplication, and that definition can be written as  $$ EXP := \\lambda mn. n (MULT\\; m) 1 $$</p> <p>The shorter way to write exponentiation would be the following. The procedure would be to apply \\(f\\) repeadly \\(m\\) times, then apply that \\(m\\) times, and that \\(m\\) times and so on again and again, repeating this process \\(n\\) many times. A numeral can be thought of taking a single function as an input and returning that applied repeatedly as an output. $$ EXP := \\lambda mnfx.\\; (n\\; \\underbrace{m (\\lambda x.fx)}{(i)})\\; x $$ The part representing _doing \\(f\\) m many times is represented by (i), and repeating that process \\(n\\) times can be thought of as the entire expression</p> <p>and applying eta reduction would simply give $$ \\text{EXP}:=\\lambda mn.\\; n\\;m $$ <pre><code>title: Infix Notation\nwe can write $EXP(m,n)$ as $m^n$\n</code></pre></p>"},{"location":"Church%20Numerals/#predecessor","title":"Predecessor","text":"<p>The predecessor function returns the precessor of the number in the normal sense except at \\(0\\) where it returns \\(0\\). The definition of the predecessor uses pairs of consecutive numbers. The procedure takes a number \\(n\\) as an input and works with the following sequence of pair $$ \\begin{align} (0&amp;,0)\\ (0&amp;,1)\\ (1&amp;,2)\\ &amp;\\vdots\\ (i&amp;,i+1)\\ (i+1&amp;,i+2)\\ &amp;\\vdots\\ (n-2&amp;,n-1) \\end{align} $$ at which point the second elment of the tuple is returned we know when to stop because there are exactly \\(n\\) steps taken, so we can write the precessor function as  \\(\\(PRED:=\\lambda n.(n\\; \\underbrace{\\lambda p.(p.2, p.2++)}_\\text{next step function}\\; (0,0)).2\\)\\) <pre><code>title: Infix Notation\n we can write $PRED(n)$ as $n--$\n</code></pre></p>"},{"location":"Church%20Numerals/#subtraction","title":"Subtraction","text":"<p>Subtraction can be defined as repeatedly applying the precessor function. \\(\\(SUB:=\\lambda \\;mn.\\;n\\text{ PRED }m\\)\\) <pre><code>title: Infix notation\nwe can write $SUB(m,n)$ as $m-n$\n</code></pre></p>"},{"location":"Church%20Numerals/#references","title":"References","text":"<p>Church Booleans</p>"},{"location":"Church-Style%20typing/","title":"Church Style typing","text":"<p>202308282208</p> <p>Type : #Note Tags : [[Lambda Calculus]], [[Type Theory]]</p>"},{"location":"Church-Style%20typing/#church-style-typing","title":"Church-Style typing","text":""},{"location":"Church-Style%20typing/#orthodox-church-style","title":"Orthodox Church Style","text":"<p>In Church's original system. Typing rules were built into the term formulation rules.</p> <p>{ width=\"500\" }</p>"},{"location":"Church-Style%20typing/#modern-church-style","title":"Modern Church Style","text":"<p>The set \\(\\Lambda_{\\Pi}\\) of all typed lambda calculus terms of Curry style are given by the following grammar: $$ \\begin{matrix}\\Lambda_{\\Pi} &amp; ::= &amp; V &amp; | &amp; (\\lambda x:\\Pi .\\Lambda_{\\Pi}) &amp; | &amp; (\\Lambda_{\\Pi} \\Lambda_{\\Pi})\\end{matrix} $$</p> <p>^111a8d</p> <p>where \\(V\\) is the set of all Variables and \\(\\Pi\\) is the set of all input types.</p> <p>The set of typability relation \\(\\Vdash\\) on \\(C\\times\\Lambda_{\\Pi}\\times\\Pi\\) are </p> <p>Then the triple \\((\\Lambda_{\\Pi},\\Pi,\\Vdash)\\) is the lambda calculus</p> <p>Examples: $$ \\begin{align} \\mathbf{I} &amp;:= \\lambda x:\\sigma.x &amp;&amp;:\\sigma\\to\\sigma\\ \\mathbf{K} &amp;:= \\lambda x:\\sigma.\\lambda y:\\tau.x &amp;&amp;:\\sigma\\to\\tau\\to\\sigma\\ \\mathbf{S} &amp;:= \\lambda x:\\sigma\\to\\tau\\to\\theta.\\lambda y:\\sigma\\to\\tau.\\lambda z:\\sigma.xz(yz) &amp;&amp;:(\\sigma\\to\\tau\\to\\theta)\\to(\\sigma\\to\\tau)\\to\\sigma\\to\\theta \\end{align} $$</p> <p>^acb5f4</p>"},{"location":"Church-Style%20typing/#references","title":"References","text":""},{"location":"Circulant%20Graphs/","title":"Circulant Graphs","text":"<p>202308201208</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Circulant%20Graphs/#circulant-graphs","title":"Circulant Graphs","text":"<p>A Circulant Graph is a Graph whose Permutation group contains the cyclic Groups. Some examples of Circulant Graphs are - All Cyclic Graphs - </p>"},{"location":"Circulant%20Graphs/#references","title":"References","text":""},{"location":"Closed%20sets/","title":"Closed sets","text":"<p>202301161301</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Closed%20sets/#closed-sets","title":"Closed sets","text":""},{"location":"Closed%20sets/#definition","title":"Definition","text":"<p><pre><code>title:\nA subset A of a T.S. X is called closed if its complement in X is open.\n</code></pre> 1. \\(\\phi\\) and X are closed 2. finite union of closed sets is closed 3. arbitrary intersection of closed sets is closed.</p>"},{"location":"Closed%20sets/#note","title":"Note:","text":"<p>Y is a subspace of X and A is a subset of Y. A being closed in Y does not imply A being closed in X. example: Y = (2,3), X = \\(\\mathbb{R}\\), A = (2,2,5].</p>"},{"location":"Closed%20sets/#lemma","title":"Lemma","text":"<pre><code>title:\nLet Y be a subspace of X,A is a subset of Y. A is closed in Y iff it is the intersection of a closed set in X with Y.\n</code></pre>"},{"location":"Closed%20sets/#lemma_1","title":"Lemma","text":"<pre><code>title:\nLet Y be a subspace of X. If A is closed in Y and Y is closed in X then A is closed in X.\n</code></pre>"},{"location":"Closed%20sets/#related-problems","title":"Related Problems","text":"<p>Closure and Interior and Limit Points</p>"},{"location":"Closed%20sets/#references","title":"References","text":""},{"location":"Closure%20and%20Interior%20and%20Limit%20Points/","title":"Closure and Interior and Limit Points","text":"<p>202301161301</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Closure%20and%20Interior%20and%20Limit%20Points/#closure","title":"Closure","text":""},{"location":"Closure%20and%20Interior%20and%20Limit%20Points/#definition","title":"Definition","text":"<pre><code>title:\nLet $A \\subset X$, then $Cl_X(A)$, closure of A in X, is the intersection of all closed sets of X containing A.\n</code></pre> <ul> <li>Closure can't be empty unless A = \\(\\phi\\).</li> <li>We say that \\(A\\) is dense in \\(X\\) is \\(Cl_X(A) = X\\).</li> </ul>"},{"location":"Closure%20and%20Interior%20and%20Limit%20Points/#definition_1","title":"Definition","text":"<p>A topo X is called separable if it has a countable dense subset.</p>"},{"location":"Closure%20and%20Interior%20and%20Limit%20Points/#theorem","title":"Theorem","text":"<pre><code>title:\nX is a T.S., Y subspace of X, A subset of Y. Then $Cl_Y(A) = Cl_X(A) \\cap Y$.\n</code></pre>"},{"location":"Closure%20and%20Interior%20and%20Limit%20Points/#interior","title":"Interior","text":""},{"location":"Closure%20and%20Interior%20and%20Limit%20Points/#definition_2","title":"Definition","text":"<p><pre><code>title:\nLet $A \\subset X$, then $Int_X(A)$, interior of A in X, is the union of all open sets of X contained in A.\n</code></pre> - Int A can be empty.</p>"},{"location":"Closure%20and%20Interior%20and%20Limit%20Points/#lemma","title":"Lemma:","text":"<ol> <li>\\(Int (X-A) = X - Cl(A)\\)</li> <li>\\(X - Int(A) = Cl(X-A)\\)</li> </ol>"},{"location":"Closure%20and%20Interior%20and%20Limit%20Points/#theorem_1","title":"Theorem","text":"<pre><code>title:\n$x \\in Cl(A) \\iff$ every open nbhd of x intersects A.\n</code></pre>"},{"location":"Closure%20and%20Interior%20and%20Limit%20Points/#limit-points","title":"Limit Points","text":""},{"location":"Closure%20and%20Interior%20and%20Limit%20Points/#definition_3","title":"Definition","text":"<pre><code>title: Limit points\n$x$ is a limit point of $A$ if every open nbhd of $x$ contains a point of $A$ other than $x$.\n\n$A'$ := set of limit points of $A$.\n</code></pre>"},{"location":"Closure%20and%20Interior%20and%20Limit%20Points/#theorem_2","title":"Theorem","text":"<pre><code>title:\n$Cl(A) = A \\cup A'$\n</code></pre>"},{"location":"Closure%20and%20Interior%20and%20Limit%20Points/#related-problems","title":"Related Problems","text":""},{"location":"Closure%20and%20Interior%20and%20Limit%20Points/#references","title":"References","text":"<p>Closed sets</p>"},{"location":"Coherence%20Spaces/","title":"Coherence Spaces","text":"<p>202307281707</p> <p>Type : #Note Tags : [[Type Theory]], [[Category Theory]]</p>"},{"location":"Coherence%20Spaces/#coherence-spaces","title":"Coherence Spaces","text":"<pre><code>A **Coherence Space** is a set (of sets) $\\mathcal A$ which satisfies:\n- _Down Closure_: If $a\\in\\mathcal A$ and $a'\\subset a$ then $a'\\in\\mathcal A$\n- _Binary Completeness_: If $M\\subset \\mathcal A$ and $\\forall a_{1},a_{2}\\in M$ we have $a_{1}\\cup a_{2}\\in\\mathcal A$, then $\\bigcup M\\in\\mathcal A$\n- $\\emptyset\\in\\mathcal A$\n</code></pre> <p>for example </p>"},{"location":"Coherence%20Spaces/#web","title":"Web","text":"<p>We define \\(|\\mathcal A| = \\bigcup\\mathcal A=\\{\\alpha : \\{\\alpha\\}\\in\\mathcal A\\}\\) and elements of \\(|\\mathcal A|\\) are called Tokens of \\(\\mathcal A\\). We then define a Coherence Relation Module \\(\\mathcal A\\) as follows $$ \\alpha\\sim\\alpha'\\iff{\\alpha,\\alpha'}\\in\\mathcal A $$ Which is an equivalence relation. \\(|\\mathcal A|\\) along with the relation \\(\\sim\\) is called the Web of \\(\\mathcal A\\)</p> <p>Webs of \\(\\mathcal {Bool}\\) and \\(\\mathcal{Int}\\) are discrete graphs whose points are Booleans and Natural number respectively.</p> <p>Construction of the web of a coherence space is a bijection to a graph, which is symmetric and transitive.</p> <p>We can reconstruct the space by  $$ a\\in\\mathcal A \\iff  a\\subseteq |\\mathcal A|,  \\forall {\\alpha,\\alpha'}\\subset a,\\alpha\\sim\\alpha' $$</p>"},{"location":"Coherence%20Spaces/#references","title":"References","text":""},{"location":"Combinatorial%20%20Proofs/","title":"Combinatorial  Proofs","text":"<p>202305281305</p> <p>Type : #Note Tags : [[Enumerative Combinatorics]]</p>"},{"location":"Combinatorial%20%20Proofs/#combinatorial-proofs","title":"Combinatorial  Proofs","text":"<p>A proof that is done by creating a bijection from a set of a set of known size. These kinds of proof can be extremely elegant and easy to understand and one of my favorite ways to prove an identity.</p> <p>The precise border between Combinatorial Proofs and Non-Combinatorial Proofs is hazy, because A more formal looking proof might have combinatorial ideas behind it which might not be noticed by someone who is aware of standard techniques of converting apparently Non Combinatorial arguements to Combinatorial ones. In such a case a more knowledgeable mathematician would find the argument combinatorial while a novice would not.</p>"},{"location":"Combinatorial%20%20Proofs/#example","title":"Example","text":"<p>Let \\(n\\) and \\(k\\) be positive integers. How many ways are there to pick \\(k\\) subsets of \\([n]\\) \\((X_{1},X_{2}\\dots X_{k})\\) such that \\(X_{1}\\cap X_{2}\\dots X_{k}=\\emptyset\\) </p> <p>The Following is a non combinatorial proof. Say \\(\\bigcap\\limits_{i=1}^{k-1} X_i=T\\) let \\(Y_{j}= X_{j}\\setminus T\\) hence \\(\\bigcap Y_j=\\emptyset\\) Hence there are \\(f(k-1,n-1)\\) such collectoins for \\(X_i\\) and \\(X_k\\) can be any subset of \\([n]\\setminus T\\) . if \\(\\#T=m\\), then there are \\(n\\choose m\\) possible \\(T's\\), thus we get $$ \\begin{align} f(k,n) &amp;= \\sum\\limits_{i=0}^{n} {n\\choose i} 2^{n-i}f(k-1,n-i) \\end{align} $$ Let \\(F_k(x)=\\sum\\limits_{n\\ge0}f(k,n) \\frac {x^{n}}{n!}\\) . Then from the above equation we get $$ F_{k}(x)= e^{x}F_{k-1}(2x) $$ where \\(F_{1}(x)=e^{x}\\) so we get that $$ \\begin{align} F_{k}(x)&amp;= exp(x+2x+4x+\\dots+2^{k-1}x)\\ &amp;= exp(2^{k}x-x)\\ &amp;= \\sum\\limits_{n\\ge0}(2<sup>{k}-1)</sup>{n}\\frac{x^{n}}{n!} \\end{align} $$ So \\(f(k,n)=(2^{k}-1)^{n}\\)</p> <p>The previous proof is rather long and non elegant, and solution is surprisingly simple. \\((2^{k}-1)^{n}\\) also happens to be the number of \\(n\\) tuples of stict subsets of \\([k]\\). The easy to understand bijection between the two sets gives a very elegant combinatorial proof.</p> <p>Given an element of \\((Z_{1},Z_{2},\\dots, Z_{n})\\) which are all strict subsets of \\([k]\\) we say that an element \\(i\\in X_{j}\\iff j\\in Z_{i}\\). Which means that the set \\(Z_{i}\\) denotes all of the \\(k\\) sets that contain \\(i\\), which are not all sets, which confirms that the intersection of all \\(X_{i}\\) will be empty. The exact proof is to show two trival injections between the sets of tuples.</p>"},{"location":"Combinatorial%20%20Proofs/#references","title":"References","text":"<p>How to count</p>"},{"location":"Combinatory%20Logic/","title":"Combinatory Logic","text":"<p>202309210909</p> <p>Tags : [[Logic]], [[Lambda Calculus]]</p>","tags":["Note"]},{"location":"Combinatory%20Logic/#combinatory-logic","title":"Combinatory Logic","text":"<pre><code>title:History\nThis section ties more stuff in *Logic* and *Lambda Calculus*\nNamely representing [Hilbert Style Proofs](&lt;./Hilbert Style Proofs.md&gt;) with **Combinators**.\nAnd the emulation of *Lambda Calculus* with **Combinators** directly translates to Deduction Theorem for *Hilbert's Proof System*\n</code></pre>","tags":["Note"]},{"location":"Combinatory%20Logic/#syntax","title":"Syntax","text":"<p>The Context Free Grammar for Combinatory Logic $$ \\begin{matrix}T &amp; := &amp; x &amp; | &amp; S &amp; | &amp; K &amp; | &amp; T T\\end{matrix} $$ such that  $$ \\begin{align} K M N &amp;\\to_{\\omega} M\\ S M N P &amp;\\to_{\\omega} M P (N P) \\end{align} $$ and \\(x\\) is a variable, while \\(T\\ T\\) represents application.</p> <p>We can also build  - \\(I\\) as \\(SKK\\) - \\(B = S(KS)K\\) - \\(W = SS(KI)\\)</p> <p>Combinators can also be defined as all Closed Lambda Terms. </p> <p>Lambda Term In Combinatory Logic</p>","tags":["Note"]},{"location":"Combinatory%20Logic/#references","title":"References","text":"","tags":["Note"]},{"location":"Compactness/","title":"Compactness","text":"<p>202302131502</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Compactness/#compactness","title":"Compactness","text":"<pre><code>title: Open Cover\n$X$ space, let $\\mathcal{U}$ be a collection of open subsets of $X$. We say that $\\mathcal{U}$ is an open cover of $X$ if $\\bigcup\\limits_{U \\in \\mathcal{U}} U = X$ \n</code></pre> <p><pre><code>title: Compact space\n$X$ is said to be _compact_ if every open cover of $X$ has a finite subcover.\n</code></pre> Example: 1) \\(X  = \\mathbb{R}\\) is not compact. 2) \\(X = (0,1]\\) not compact. 3) \\(X = \\{0\\} \\cup \\{1/n: n \\in \\mathbb{N}\\}\\) is compact. (Prove it)</p> <pre><code>title: Lemma\nLet $A \\subset X$ then $A$ is compact(Lindelof) iff each cover of $A$ by open subsets of $X$ contains a finite(countable) subcover of $A$.\n</code></pre> <pre><code>title: Proposition\nEvery closed subspace of a compact space is compact.\n</code></pre>"},{"location":"Compactness/#proof","title":"Proof:","text":"<p>Take an open cover of \\(Y\\) (closed subspace of compact space \\(X\\)), let it be \\(\\{V = U \\cap Y\\}\\). Then extend this cover to a cover of \\(X\\), \\(\\{U\\}\\cup X\\backslash Y\\). Then take a finite subcover and intersect this with \\(Y\\).</p> <pre><code>title: Proposition\nCompact subspace of a Hausdorff space is closed.\n</code></pre>"},{"location":"Compactness/#proof_1","title":"Proof:","text":"<p>Take a point in the complement of the compact subspace \\(Y\\), call it \\(x_0\\). For any point \\(y \\in Y\\), we can separate \\(x_0\\) and \\(y\\) using open sets \\(y \\in U_y\\) and \\(x_0 \\in V_y\\) because \\(X\\) is hausdorff. So we get a family of open sets \\(\\{V_y\\}\\) around \\(x_0\\), one open set for each \\(y \\in Y\\). Take a finite subcover of \\(\\{U_y\\}\\), and take the corresponding \\(V_y\\)'s. Then their intersection is an open set containing \\(x_0\\) but not intersecting \\(Y\\). Hence \\(X \\backslash Y\\) is open. \\(\\square\\) </p> <pre><code>title:\nImage of a compact set under a cts function is compact.\n</code></pre> <pre><code>title:Theorem\nLet $f: X \\to Y$ be a bijective cts map. If $X$ is cpt and $Y$ is hausdorff then $f$ is a homeomorphism.\n</code></pre>"},{"location":"Compactness/#proof_2","title":"Proof:","text":"<p>Let \\(A\\) be closed in \\(X\\), then \\(A\\) is compact, hence \\(f(A)\\) is cpt, and so it's closed by the previous proposition. Hence \\(f\\) is a closed map, this makes it a homeomorphism. \\(\\square\\)</p> <pre><code>title:Definition\n$f:X \\to Y$ is called proper if for all compact subspaces $L$ of Y, $f^{-1}(L)$ is compact.\n</code></pre> <pre><code>title:Theorem\nLet $f :X \\to Y$ be a cts map from compact $X$ to hausdorff $Y$. Then\n1) f is a proper and closed map.\n2) If f is surjective then f is a quotient map.\n3) If f is injective then it is an embedding.\n4) If f is bijective then it is a homeomorphism.\n</code></pre> <pre><code>title: Theorem\nIf 2 spaces $X,Y$ are compact then their product is also compact.\n</code></pre>"},{"location":"Compactness/#proof_3","title":"Proof:","text":"<p>Let \\(\\mathcal{C} = \\{W_\\alpha\\}_{\\alpha\\in J}\\) be a cover of \\(X \\times Y\\). For each point \\(p \\in X\\), \\(\\{p\\} \\times Y\\) is compact and hence is covered by \\(\\{W_{\\alpha_1,p} \\cdots W_{\\alpha_n,p}\\}\\). Let \\(N_p = W_{\\alpha_1,p} \\cup \\cdots \\cup W_{\\alpha_n,p}\\). By Tube Lemma, there is a nbhd \\(U_p\\) s.t. \\(U_p \\times Y \\subset N_p\\). Therefore, \\(\\{U_p\\}_{x \\in X}\\) is a cover of \\(X\\) hence \\(\\{U_{p_i}\\}\\) is a cover of \\(X\\). Hence \\(U_{p_i} \\times Y\\) covers \\(X\\times Y\\), and so \\(N_{p_i}\\) covers \\(X\\times Y\\). \\(\\square\\)</p> <pre><code>title: Theorem\nArbitrary product of compact spaces is again compact.\nThis is called [Tychonoff's Theorem](&lt;./Tychonoff's Theorem.md&gt;).\n</code></pre> <pre><code>title: Proposition\nX is compact iff for all collections $\\mathcal{C}$ of closed subsets of X having [Finite Intersection Property](&lt;./Finite Intersection Property.md&gt;), $\\bigcap\\limits_{C\\in \\mathcal{C}} C$ is non empty.\n</code></pre>"},{"location":"Compactness/#proof_4","title":"Proof:","text":"<p>If X is compact: If the intersection of all subsets in \\(\\mathcal{C}\\) is empty, then the collection \\(\\{U | U^c \\in \\mathcal{C}\\}\\) is an open cover of X, take a finite subcover then \\(U_1^c, U_2^c \\cdots U_n^c\\) is a finite subcollection of \\(\\mathcal{C}\\) whose intersection is empty, contradicting FIP.</p> <p>Converse: Take a cover of \\(X\\) which does not have a finite subcover. Now take the complement of each element in this cover, we get a family of closed subsets of \\(X\\) having the FIP. But then the intersection of all elements of the family is non empty and so the cover does not contain some point of \\(X\\) which is a contradiction.</p>"},{"location":"Compactness/#theorem","title":"Theorem","text":"<pre><code>title:\nEach closed interval $[a,b]$ in $\\mathbb{R}$ is compact.\n</code></pre>"},{"location":"Compactness/#corollary","title":"Corollary","text":"<pre><code>title:\nAny finite product of closed intervals $\\displaystyle \\prod \\limits_{i=1}^{n} [a_{i},b_{i}]$ is compact.\n</code></pre>"},{"location":"Compactness/#theorem_1","title":"Theorem","text":"<pre><code>title: Heine Borel\nA subspace $A$ of $\\mathbb{R}^{n}$ is compact iff it is closed and bounded.\n</code></pre>"},{"location":"Compactness/#theorem_2","title":"Theorem","text":"<pre><code>title:\nLet $f : (X,d) \\to (Y,d')$ be a continuous map on metric spaces. If $X$ is compact, then $f$ is uniformly continuous.\n</code></pre>"},{"location":"Compactness/#related-results","title":"Related Results","text":"<p>1) Compact + Hausdorff \\(\\implies\\) Normal</p>"},{"location":"Compactness/#related-problems","title":"Related Problems","text":"<ol> <li>\\(X\\) is non empty Hausdorff compact space. If \\(X\\) has no isolated points, i.e. no point \\(x\\) such that \\(\\{ x \\}\\) is open, then \\(X\\) is uncountable. (Theorem 27.7 Munkres)</li> </ol>"},{"location":"Compactness/#references","title":"References","text":"<p>Lindelof Space Finite Intersection Property Tube Lemma Homeomorphisms Hausdorff Property Quotient Topology Tychonoff's Theorem</p>"},{"location":"Complex%20Analysis%20Assignment%203/","title":"Complex Analysis Assignment 3","text":"<p>Name: Shubh Sharma Roll Number: BMC202170</p> <p>### 1. Map the common part of the disks \\(|z| &lt; 1\\) and \\(|z-1| &lt; 1\\) on the inside of the unit circle. Choose the mapping so that the two symmetries are preserved.</p> <p>The two circles intersect at points \\(a= \\frac{1}{2}+ \\frac{\\sqrt3i}{2}\\) and \\(b= \\frac{1}{2}- \\frac{\\sqrt3i}{2}\\) , hence we can map these two points at \\(0\\) and \\(\\infty\\) respectively by  $$ f(z) = -\\frac{z-a}{z-b} $$ This makes a sector of angle \\(\\frac{\\pi}{3}\\) passing through \\(f(0)=b\\), \\(f(1)=a\\) and \\(f\\left(\\frac{1}{2}\\right) = 1\\) applying \\(\\(g(z) = z^\\frac{3}{2}\\)\\) considering a branch cut which makes it the half plane \\(Re(z) \\ge 0\\) and the map \\(\\(h(z) = \\frac{z-1}{z+1}\\)\\) sends it to the unit circle The map \\(h\\circ g\\circ f\\) takes the intersection of the two discs to the unit circle</p> <p>### 2. Map the reigon between \\(|z|=1\\) and \\(|z- \\frac{1}{2}|=\\frac{1}{2}\\) on a half plane.  The map \\(\\(f(z) = \\frac{-i\\pi z}{z-1}\\)\\) to the strip \\(0\\le Im(z)\\le \\pi\\)  The map \\(\\(g(z)=e^{z}\\)\\) sends the strip to the half plane \\(Im(z)&gt;0\\)  the map \\(g\\circ f\\) takes the reigon between the two circles to a half plane</p> <p>### 3. map the complement of the arc \\(|z|=1, y\\ge 0\\) on the outside of the unit circle so that the points at infinity correspont to each other. The map \\(\\(a(z) = -i\\frac{z+1}{z-1}\\)\\) this maps the the arc to the line \\(x\\le0, y=0\\), sending \\(\\infty\\) to \\(-i\\) \\(\\(b(z)=\\sqrt z\\)\\) with the branch cut being the negative number real line, maps the points complement to the line on the half plane \\(Re(z)&gt;0\\) , sending \\(-i\\) to \\(\\frac{1}{\\sqrt2}-\\frac{i}{\\sqrt2}\\) The function $$ c(z) = i\\frac{z-1}{z+1} $$ maps the half plane onto the unit circle. sending \\(\\frac{1}{\\sqrt2}- \\frac{i}{\\sqrt2}\\) to \\(\\sqrt 2 -1\\)  The function  $$ d(z) = \\left(\\sqrt 2 - 1\\right)\\cdot \\frac{z+1}{1-z}  $$ sends the circle to the right plane and send \\(\\sqrt 2 -1\\) to \\(1\\) The function  $$ e(x) = \\frac{z+1}{z-1} $$ Send the half plane to the complement of the circle and sends \\(1\\) to \\(\\infty\\) Hence the function \\(\\(e\\circ d\\circ c\\circ b\\circ a\\)\\) sends the complement of the arc to outside the unit circle and sends \\(\\infty\\) to itself</p> <p>### 4. Compute \\(\\(\\int_{\\gamma}xdz\\)\\) where \\(\\gamma\\) is the directed line segment from \\(0\\) to \\(i+1\\)  Let \\(z(t)=t+it, t\\in[0,1]\\)  Then   $$  \\int_{\\gamma}xdz = \\int\\limits_{0}^{1}x(z(t))\\cdot z'(t)dt = \\int\\limits_{0}^{1}t(1+i)dt = \\frac{1+i}{2} $$</p> <p>### 5. Compute \\(\\(\\int_{z=|r|}xdz\\)\\) for the positive sense in two ways, first by the use of parameters and second, by observing that \\(x = \\frac{z+\\overline z}{2}=\\frac{1}{2}\\left(z+ r^{2}{z}\\right)\\)  on the circle 1. Parameters:    \\(z(t) = re^{it},0\\le t\\le 2\\pi\\) \\(z'(t)=ire^{it}\\)     Hence    $$    \\begin{align}    \\int_{|z|=r}xdz &amp;= \\int\\limits_{0}^{2\\pi}x(z(t))\\cdot z'(t)dt\\    &amp;= \\int\\limits_{0}^{2\\pi}(r\\cos t)(ir)(\\cos t+i\\sin t)dt\\    &amp;= r<sup>{2}i\\left(\\int\\limits_{0}</sup>{2\\pi}cos^{2}t dt + \\int\\limits_{0}^{2\\pi}\\sin t\\cos t dt\\right)\\    &amp;= \\pi r^2i    \\end{align} $$ 2. Observation: \\(f(z) = z\\) is an analytic function, Hence the integral in a closed loop is \\(0\\) thus    $$    \\begin{align}    \\int_{|z|=r}xdz &amp;= \\int_{|z|=r} \\frac{r^{2}dz}{2z}\\ &amp;= \\frac{r^{2}}{2}\\cdot 2\\pi i = \\pi i r^{2} \\end{align} $$</p> <p>### 6. Compute \\(\\(\\int_{|z|=2} \\frac{dz}{z^{2}-1}\\)\\) in the positive sense of the circle</p> <p>This is equal to    $$   \\begin{align} \\frac{1}{2}\\left(\\int_{|z|=2} \\frac{dz}{z-1}-\\int_{|z|=2} \\frac{dz}{z+1}\\right) \\end{align} $$ for both the integral, the roots of the follwoing the linear polynimals, lie inside the loop, hence both of them evaluate to \\(2\\pi i\\) Hence the total integral evaluates to \\(0\\).</p> <p>### 7.  ##### a)  \\(n(\\sigma_{1},x_{1})=0\\), hence \\(n(\\sigma_{1}, z)=0\\) for \\(z\\in\\gamma_2\\)  We know that \\(\\sigma_1\\) intersects the X-axis. Let \\(\\sigma_1\\) intersect the X-axis at \\(x'\\) where \\(x'\\ne 0\\). Such that \\(x'\\) is the rightmost intersection of \\(\\sigma_{1}\\) and X-axis.  If \\(n(\\sigma_{1}, x_{2})\\ne 0, x_2\\) lies in the interior of \\(\\sigma_1\\), hence \\(x_2&lt;x'\\) But this contradicts the fact that \\(x_2\\) is the rightmost intersction of \\(\\gamma_1\\) with the X-axis.  \\(\\therefore n(\\sigma_{1}, x_{2})=0\\)  as \\(x_2\\in\\gamma_2\\), for \\(z\\in \\gamma_2,n(\\sigma_{1}, z)=0\\) as \\(\\gamma_2\\) is a connected curve and \\(n\\) is a continuous function, connected component get mapped to connected components. Image of \\(\\mathbb N\\) is singleton</p> <p>##### b)  \\(n(\\sigma_{1},x)=n(\\sigma_{2}, x)=1\\) for small \\(x&gt;0\\).  Let \\(x'\\) be the point where \\(\\gamma\\) intersects X-axis for the first time. As \\(Im\\ z_1&lt;0\\) and \\(Im\\ z_2&gt;0\\) and subarc \\(z_{1}\\to z_{2}\\) passing through the origin. If \\(\\sigma_1\\) and \\(\\sigma_{2}\\) does not meet the X-axis at any point greater than \\(\\frac{x'}{2}\\) and the subarc \\(z_{1}\\to z_{2}\\) passing through \\(x'\\) of \\(\\gamma\\) does not meet the X-axis at any point less than \\(\\frac{x'}{2}, n(\\sigma_{1}, \\frac{x'}{2})=n(\\sigma_{2}, \\frac{x'}{2})=1\\). As \\(x'&gt;0, \\frac{x'}{2}&gt;0\\)  ##### c)  The first intersection of \\(x_1\\) of the positive real axis with \\(\\gamma\\) lies on \\(\\gamma_1\\).  If not, the first intersection \\(x_1\\) of the positive real axis with \\(\\gamma\\) lies on \\(\\gamma_2\\).  \\(Im\\ z_1&lt;0\\) and \\(Im\\ z_2&gt;0\\), the subarc \\(z_{1}\\to z_{2}\\) whcih \\(\\gamma_1\\) does not meet the X-axis at any point less than \\(x_1\\) and the subserc \\(z_{1}\\to z_{2}\\) of \\(\\sigma_1\\)passing through the origin does not meet the X-axis at any point greater than \\(x_{1}\\) \\(\\therefore n(\\sigma_{1},x_1)=1\\)  But if \\(x_1\\in\\gamma_2,n(\\sigma_1,x_1)=0\\) which is a contradiction  \\(\\therefore\\) the first intersection \\(x_1\\) of the positive real axis  with \\(\\gamma\\) lies on \\(\\gamma_2\\)  ##### d)  \\(n(\\sigma_{2},x_2)=1\\), hence \\(n(\\sigma_2,z)=1\\) for \\(z\\in\\gamma_1\\) \\(Im\\ z_1&lt;0\\) and \\(Im\\ z_2&gt;0\\), the subarc \\(z_{1}\\to z_{2}\\) of \\(\\sigma_2\\) which \\(\\gamma_{2}\\) does not meet the X-axis at any point less than \\(x_1\\) and the subarc \\(z_{1}\\to z_{2}\\) of \\(\\sigma_2\\) which passes through the origin does not meet the \\(x\\) axis at any point greater than \\(x_1\\), \\(\\therefore n(\\sigma_{2},x_{1})=1\\) \\(\\therefore n(\\sigma_{2},z)=1\\), for \\(z\\in \\gamma_{1}\\) as it is connected and \\(n\\) is a continuous function, \\(n(\\sigma_2,z)\\) is connected \\(\\forall z\\in\\gamma_{1}\\) and hence a singleton image of \\(\\mathbb N\\)</p> <p>### 8. Compute \\(\\(\\int_{|z|=1} \\frac{e^{z}}{z}dz\\)\\) \\(e^z\\) is an analytic function, and by cauchy's integral formula $$ e^{0}= 1=\\frac{1}{2i\\pi}\\int_{|z|=1} \\frac{e^{z}}{z-0}dz $$ Thus the integral computes to \\(2i\\pi\\)</p> <p>### 9. Compute \\(\\(\\int_{|z|=2} \\frac{dz}{z^{2}+1}\\)\\) by decomposing the integrand in partial fraction $$ \\begin{align} \\int_{|z|=2} \\frac{dz}{z^{2}+1} &amp;= \\frac{1}{2i}\\left(\\int_{|z|=2} \\frac{dz}{z-i}-\\int_{|z|=2} \\frac{dz}{z+i}\\right)\\ \\end{align} $$ as \\(i\\) and \\(-i\\) both lie in the interior of \\(|z|=2\\) \\(\\(\\int_{|z|=2} \\frac{dz}{z+i} = \\int_{|z|=2} \\frac{dz}{z-i}\\)\\) Hence  \\(\\(\\int_{|z|=2} \\frac{dz}{z^{2}+1}=0\\)\\)</p> <p>### 10 Compute \\(\\(\\int_{|z|=\\rho} \\frac{|dz|}{|z-a|^{2}}\\)\\) under the condition that \\(|a| \\ne \\rho\\)  $$  \\begin{align} \\int_{|z|=\\rho} \\frac{|dz|}{|z-a|^{2}}&amp;= -i\\rho\\int_{|z|=\\rho} \\frac{dz}{z(z-a)(\\bar z - \\bar a)}\\ &amp;= -i\\rho\\int_{|z|=\\rho} \\frac{dz}{(z-a)(\\rho^{2}-\\bar a z)}\\ &amp;= \\frac{i\\rho}{\\bar a}\\int_{|z|=\\rho} \\frac{dz}{(z-a)\\left(z-\\frac{\\rho^{2}}{\\bar{a}}\\right)}\\ &amp;= \\frac{i\\rho}{|a|<sup>{2}-\\rho</sup>{2}}\\left(\\int_{|z|=\\rho} \\frac{dz}{z-a}-\\int_{|z|=\\rho} \\frac{dz}{z-\\frac{\\rho^{2}}{\\bar{a}}}\\right)\\   a&lt;\\rho&amp;\\implies \\int_{|z|=\\rho} \\frac{dz}{z-a}=2\\pi i, \\int_{|z|=\\rho} \\frac{dz}{z- \\frac{\\rho^{2}}{\\bar{a}}}=0\\ a&gt;\\rho&amp;\\implies \\int_{|z|=\\rho} \\frac{dz}{z-a}=0, \\int_{|z|=\\rho} \\frac{dz}{z- \\frac{\\rho^{2}}{\\bar{a}}}=2\\pi i\\ \\text{Thus }\\int_{|z|=\\rho} \\frac{|dz|}{|z-a|^{2}} &amp;= \\frac{2\\pi\\rho}{\\rho<sup>{2}-|a|</sup>{2}}\\ &amp;= \\frac{2\\pi\\rho}{|a|<sup>{2}-\\rho</sup>{2}} \\end{align} $$</p> <p>### 11. Compute \\(\\(\\begin{align*}\\int_{|z|=1} e^{z}z^{-n}dz,\\ \\ \\ \\ \\ \\ \\int_{|z|=2}z^{n}(1-z)^{m}dz,\\ \\ \\ \\ \\ \\ \\int_{|z|=\\rho} |z-a|^{-4}|dz|\\ (|a|\\ne\\rho)\\end{align*}\\)\\) Part 1: \\(\\(\\int_{|z|=1} e^{z}z^{n}dz= \\frac{2i\\pi}{(n-1)!}e^{z}\\Bigg|_{z=0} = \\frac{2i\\pi}{(n-1)!}\\)\\)</p> <p>Part 2: If \\(n, m&gt;0\\) since \\(z^n\\) and \\((1-z)^{m}\\) are analytic, the integral evalutes to \\(0\\) If \\(n&lt;0\\) and \\(m&gt;0\\) $$ \\begin{align} \\int_{|z|=2} \\frac{(1-z)<sup>{m}}{z</sup>{|n|}}dz &amp;= \\frac{2i\\pi}{(n-1)!} \\frac{d<sup>{|n|-1}}{dz</sup>{|n|-1}}(1-z)<sup>{m}\\Bigg|_{z=0}=2i\\pi(-1)</sup>{|n|-1}{m \\choose |n|-1}  \\end{align} $$  If \\(n &gt; 0\\) and \\(m&lt;0\\), similarly we get $$ \\int_{|z|=2} \\frac{z<sup>{n}}{(1-z)</sup>{|m|}}dz = 2i\\pi(-1)^{m}{n\\choose |m| -1} $$ Part 3: $$I=\\int_{|z|=\\rho} \\frac{|dz|}{|z-a|^{4}}= \\frac{-i\\rho}{a^{2}}\\int_{|z|=\\rho} \\frac{zdz}{(z-a)<sup>{2}(z-\\rho</sup>{2}{a<sup>{2})</sup>{2}}} $$ if \\(|a|&lt;\\rho\\) then \\(1/(z- \\frac{\\rho^{2}}{a^{2}})\\) is analytic and $$ I=\\frac{-i\\rho}{a^{2}} \\frac{2i\\pi}{1!} \\frac{d}{dz}\\left(\\frac{z}{z-\\frac{\\rho<sup>{2}}{a</sup>{2}}}\\right)\\Bigg|_{z=a} $$ The derivative evalutates to  $$ \\frac{d}{dz}\\left(\\frac{z}{\\left(z-\\rho^{2} /a^{2}\\right)}\\right)= a<sup>{2}\\frac{\\rho</sup>{4}-a<sup>{4}}{(az-\\rho</sup>{2})^{4}} $$ hence the integral evalueates to \\(\\(I=2\\pi\\rho\\frac{a^{2}+\\rho^{2}}{(\\rho^{2}-a^{2})^{3}}\\)\\) Similarly if \\(|a|&gt;\\rho\\) \\(\\(I=2\\pi\\rho\\frac{a^{2}+\\rho^{2}}{(a^{2}-\\rho^{2})^{3}}\\)\\)</p> <p>### 12. Prove that a function which is analytic in the whole plane and satisfies the inequality \\(|f(z)|&lt;|z|\\) for some \\(n\\) and all sufficiently large \\(|z|\\) reduces to a polynomial  $$  \\begin{align} |f^{(n+1)}(a)| &amp;\\le \\left|\\frac{(n+1)!}{2i\\pi}\\int_{|z|=r} \\frac{f(z)dz}{(z-a)^{n+2}}\\right| &amp;\\le \\frac{(n+1)!}{2\\pi}\\int_{|z|=r} \\frac{|f(z)|dz}{|z-a|^{n+2}}\\ &amp;&lt; \\frac{(n+1)!}{2\\pi}\\int_{|z|=r} \\frac{|z|<sup>{n}dz}{|z-a|</sup>{n+2}} &amp;&lt;   \\frac{(n+1)!}{2\\pi}\\int_{|z|=r} \\frac{r<sup>{n}dz}{|r-a|</sup>{n+2}}\\ &amp;\\le \\frac{(n+1)!r<sup>{n}}{2\\pi(r-a)</sup>{n+2}}\\int_{|z|=r}|dz| &amp;= \\frac{(n+1)!r<sup>{n}}{(r-a)</sup>{n+2}}\\ \\text{As }r\\to\\infty&amp;, \\frac{(n+1)!r<sup>{n}}{(r-a)</sup>{n+2}}\\to0 \\end{align} $$ Hence \\(f^{(n+1)}=0\\), thus \\(f\\) is a polynomial of degree upto \\(n\\).</p> <p>### 13. If \\(f(z)\\) is analytic and \\(|f(z)|\\le M\\) for \\(|z|\\le R\\), find and upper bound for \\(f^{(n)}(z)\\) in \\(|z|\\le\\rho&lt; R\\) $$ \\begin{align} |f^{(n)}(a)|&amp;= \\left|\\frac{n!}{2i\\pi}\\int_{|z|=R} \\frac{f(z)dz}{(z-a)^{n+1}}\\right|\\ &amp;\\le \\frac{n!}{2\\pi}\\int_{|z|=R} \\frac{|f(z)||dz|}{|z-a|^{n+1}}\\ &amp;\\le \\frac{n!M}{2\\pi}\\int_{|z|=R} \\frac{|dz|}{|z-a|^{n+1}}  \\ &amp;\\le \\frac{n!MR}{(R-a)^{n}} \\end{align} $$</p> <p>### 14. If \\(f(z)\\) is analytic for \\(|z|&lt;1\\) and \\(|f(z)|\\le \\frac{1}{1-|z|}\\), find the best estimate of \\(|f^{(n)}(0)|\\) that Cauchy Inequality will yield.  $$  \\begin{align} |f^{(n)}(0)| &amp;= \\left|\\frac{n!}{2i\\pi}\\int_{|z|=R} \\frac{f(z)dz}{z^{n+1}} \\right|\\ &amp;\\le \\frac{n!}{2\\pi} \\int_{|z|=R} \\frac{|f(z)||dz|}{|z|^{n+1}}\\ &amp;= \\frac{n!}{R^{n}(1-R)}\\ \\text{On minimizing by setting } R&amp;= \\frac{n}{n+1}\\ |f^{(n)}(0)| &amp;\\le (n+1)!\\left(1+ \\frac{1}{n}\\right)^{n} \\end{align} $$</p>"},{"location":"Complex%20Analysis%20Assignment%203/#15-show-that-successive-derivatives-of-an-analytic-function-at-a-point-can-never-satisfy-fnznnn-fromulate-a-sharper-theorem-of-the-same-kind","title":"15. Show that successive derivatives of an analytic function at a point can never satisfy \\(|f^{(n)}(z)|&gt;n!n^{n}\\). Fromulate a sharper theorem of the same kind","text":"<p>$$ \\begin{align} |f^{(n)}(z)| &amp;= \\frac{n!}{2\\pi} \\int_{|z|=R} \\frac{|f(z)||dz|}{|z-a|^{n+1}} &amp;\\le \\frac{n!M}{R} \\text{ where }M=\\sup\\limits_{|z|=R}|f(z)|\\ \\end{align} $$ \\(M\\) exists as \\(f\\) is continuous adn \\(|z|=R\\) is compact if \\(f^{(n)}(z) &gt;n!n^n\\) then \\(\\frac{n!M}{R}&gt; n!n^n\\) hence \\(\\frac{M}{R}&gt;n^{n}\\) which is a contradictoin. </p>"},{"location":"Complex%20Convergence%20Results/","title":"Complex Convergence Results","text":"<p>202301161101</p> <p>Type : #Note Tags : [[Complex Analysis]]</p>"},{"location":"Complex%20Convergence%20Results/#complex-convergence-results","title":"Complex Convergence Results","text":""},{"location":"Complex%20Convergence%20Results/#series","title":"Series","text":"<p><pre><code>title: \nA formal sum $\\sum\\limits_{n=0}^{\\infty} a_n$;  $a_n \\in \\mathbb{C} \\ \\forall \\ n$.\n</code></pre> We say that a series is convergent if its sequence of partial sums is convergent.</p>"},{"location":"Complex%20Convergence%20Results/#geometric-series","title":"Geometric series","text":"<p>\\(\\displaystyle \\sum \\limits_{n=1}^{\\infty} z^n\\) , \\(z \\in \\mathbb{C}\\).  The partial sums \\(S_n\\) are given by \\(S_n = \\dfrac{1-z^n}{1-z}\\).  The series converges iff \\(|z| &lt; 1\\), and limit \\(S = \\dfrac{1}{1-z}\\).</p>"},{"location":"Complex%20Convergence%20Results/#absolutely-convergent","title":"Absolutely convergent","text":"<pre><code>title: \nA sequence $\\{a_n\\}$ is called absolutely convergent if $\\{|a_n|\\}$ converges.\n\n$\\sum a_n$ is absolutely convergent if $\\sum |a_n|$ is convergent.\n</code></pre>"},{"location":"Complex%20Convergence%20Results/#remarks","title":"Remarks:","text":"<ol> <li>Absolute convergence does not imply convergence. For example, \\(|z_n| = 1 \\ \\forall \\  n\\) does not guarantee that \\(z_n\\) will converge.</li> <li>Convergence \\(\\implies\\) absolute convergence (for sequences).</li> <li>Convergence does not imply absolute convergence (for series). Example \\(a_j = (-1)^j/j\\)</li> <li>Abs convergence \\(\\implies\\) convergence for series. (proof exercise)</li> </ol>"},{"location":"Complex%20Convergence%20Results/#cauchy-sequence","title":"Cauchy sequence","text":"<pre><code>title: Theorem\nA sequence $\\{a_n\\}$ is convergent iff given $\\epsilon &gt; 0, \\exists \\ N \\in \\mathbb{N}$ s.t. $|a_n-a_m| &lt; \\epsilon$ for all $m,n \\ge N$.\n</code></pre>"},{"location":"Complex%20Convergence%20Results/#pointwise-convergence","title":"Pointwise convergence","text":"<pre><code>title:\nLet $F_n : E \\to \\mathbb{C}$, $n \\in \\mathbb{N}$, be a sequence of functions defined on $E\\subset \\mathbb{C}$.\n$\\{F_n\\}$ is said to **pointwise converge** to a function $f:E \\to \\mathbb{C}$ if \n$$\\lim\\limits_{n\\to \\infty} f_n(z) = f(z) \\ \\forall \\ z \\in E$$\n</code></pre>"},{"location":"Complex%20Convergence%20Results/#uniform-convergence","title":"Uniform convergence","text":"<p><pre><code>title:\nLet $F_n : E \\to \\mathbb{C}$, $n \\in \\mathbb{N}$, be a sequence of functions defined on $E\\subset \\mathbb{C}$.\n$\\{F_n\\}$ is said to **uniformly converge** to a function $f:E \\to \\mathbb{C}$ if given $\\epsilon &gt; 0$, there exists $N \\in \\mathbb{N}$ such that $|f_m(z) - f(z)| &lt; \\epsilon \\ \\forall \\ m \\ge N\\ \\forall \\ z \\in E$. \n</code></pre> The limit function of a sequence of uniformly converging continuous functions is continuous.</p>"},{"location":"Complex%20Convergence%20Results/#corollary","title":"Corollary:","text":"<p>Suppose \\(\\{a_n\\}\\) is a convergent sequence s.t. \\(|f_n(z) - f_m(z)| \\le |a_m-a_n|\\) on \\(E\\), then \\(\\{f_n\\}\\) is uniformly convergent.</p>"},{"location":"Complex%20Convergence%20Results/#corollary_1","title":"Corollary:","text":"<p>(Weierstrass M test) Suppose \\(\\sum f_n\\)  is a series of functions s.t. \\(|f_n(z)| \\le M a_n\\) for a sequence \\(\\{a_n\\}\\), if \\(\\{a_n\\}\\) converges then the series \\(\\sum f_n\\) converges.</p>"},{"location":"Complex%20Convergence%20Results/#related-problems","title":"Related Problems","text":""},{"location":"Complex%20Convergence%20Results/#references","title":"References","text":""},{"location":"Complex%20Numbers/","title":"Complex Numbers","text":"<p>202301041401</p> <p>Type : #Note Tags : [[Complex Analysis]]</p>"},{"location":"Complex%20Numbers/#complex-numbers","title":"Complex Numbers","text":"<p>$$ \\mathbb C=\\frac {\\mathbb{R}[x]}{(1+x^2)} $$ where \\(\\mathbb C\\) is the field of Complex Numbers, and is a field extension of Real Numbers, and the basis for it are written as \\(\\{1, \\iota\\}\\) Hence if \\(z\\in \\mathbb C,\\exists a, b\\in\\mathbb R\\) such that \\(! z=a+b\\iota\\) Also we have an automorphism \\(c: a+b\\iota \\mapsto a-b\\iota\\) which is called conjugation.</p> <p>Since \\(\\mathbb C\\) is a \\(2\\) dimensional vector space, it is isomorphic to \\(\\mathbb R^2\\) Hence we can pull back the euclidian norm of \\(\\mathbb R^2\\) onto \\(\\mathbb C\\). This induces the norm \\(|a+bi| = \\sqrt{a^2+b^2}\\) </p>"},{"location":"Complex%20Numbers/#related-problems","title":"Related Problems","text":""},{"location":"Complex%20Numbers/#references","title":"References","text":"<p>Analytic Function</p>"},{"location":"Complex%20Polynomials/","title":"Complex Polynomials","text":"<p>202301111401</p> <p>Type : #Note Tags : [[Complex Analysis]]</p>"},{"location":"Complex%20Polynomials/#complex-polynomials","title":"Complex Polynomials","text":""},{"location":"Complex%20Polynomials/#fundamental-theorem-of-algebra","title":"Fundamental Theorem of Algebra","text":"<p><pre><code>title:\nEvery polynomial $p(z)\\in \\mathbb C[z]$ has a root.\nHence, every polynomial factors into a product of linear polynomials.\n</code></pre> A polynomial has a Zero of order \\(m\\)  at \\(a\\in \\mathbb C\\) if \\(p(z) = (z-a)^m\\cdot q(z)\\) where \\(q(a) \\ne 0\\).</p>"},{"location":"Complex%20Polynomials/#lucas-theorem","title":"Lucas Theorem","text":"<p>Proposition: If all zeroes of a polynomial lie in a half plane in \\(\\mathbb C\\), Then all zeroes of \\(p'(z)\\) in it too. Which implies that if you take the convex hull of the zeroes of \\(p(z)\\), inclusive of the boundary line, then all the roots of \\(p'(z)\\) will all lie in size that. Proof: Suppose \\(p(z)= (z-\\alpha_1)\\cdots(z-\\alpha_n)\\)  Then  $$ {p'(z)\\over p(z)} = \\frac1{z-\\alpha_1}+\\cdots+\\frac1{z-\\alpha_n} $$ Suppose the half plane which contains the zeroes of \\(p(z)\\) is  $$ H:\\frac{Im(z-a)}{b} &lt; 0 $$ We have \\(\\alpha_k\\in H\\forall k\\), but suppose \\(z_0\\in \\mathbb C \\setminus H\\) $$ \\begin{aligned} {Im(z_0-\\alpha_k)\\over b}&amp;= {Im(z_0-a)\\over b}-{Im(z_0 - \\alpha_k)\\over b}\\ &amp;\\implies b(z_0-\\alpha_k)^{-1}&lt;0&amp;\\forall k\\ \\implies Im(b)\\frac{p'(z_0)}{p(z_0)} &amp;= Im\\frac{b}{z-\\alpha_1}+\\cdots+Im\\frac{b}{z-\\alpha_n}&lt;0\\ &amp;\\implies p'(z_0)\\ne 0 \\end{aligned} $$</p>"},{"location":"Complex%20Polynomials/#related-problems","title":"Related Problems","text":""},{"location":"Complex%20Polynomials/#references","title":"References","text":"<p>Analytic Function</p>"},{"location":"Complex%20Rational%20Functions/","title":"Complex Rational Functions","text":"<p>202301111401</p> <p>Type : #Note Tags : </p>"},{"location":"Complex%20Rational%20Functions/#complex-rational-functions","title":"Complex Rational Functions","text":"<p>Let  $$ R(z)=\\frac {P(z)} {Q(z)} $$ be in its lowest term where \\(P,Q\\) are Complex Polynomials. We may give \\(R(z)\\) the value \\(\\infty\\) whenever \\(Q(z)=0\\). Then \\(R(z):\\mathbb C\\to S\\) is continuous.</p> <p>A point \\(a\\) where \\(Q(z)\\) has a zero is called a pole of if \\(R(z)\\). Its order is the order of \\(a\\) as a root of \\(Q(z)\\)</p> <p>For greater unity, consider a map  \\(R: S\\to S\\) and let \\(R_1(z)=R(\\frac1z)\\)  then \\(R(\\infty) = R_1(0)\\) If \\(R_1(0) = 0\\) or \\(\\infty\\) , then the order of the zero or pole of \\(R(z)\\) at \\(\\infty\\) is defined as the order of the zero or pole of \\(R_1(z)\\) at \\(0\\) $$ \\begin{aligned} \\text{Say } R(z) &amp;= {a_0 + a_1z+\\cdots+a_nz^n\\over b_0+b_1z+\\cdots+b_mz^m}\\ R_1(z) &amp;= z<sup>{m-n}{a_0z</sup>n + a_1z^{n-1}+\\cdots+a_n\\over b_0z<sup>m+b_1z</sup>{m-1}+\\cdots+b_m}\\ \\end{aligned} $$ If \\(m&gt;n\\),  \\(R(z)\\) has a zero of order \\(m-n\\) at \\(\\infty\\); if \\(m&lt;n\\), \\(R(z)\\) has a pole of order \\(n-m\\) at \\(\\infty\\).</p> <p><pre><code>title: Lemma\norder($R$) = order($R-a$), $a \\in \\mathbb{C}$\n</code></pre> This gives that order(\\(R\\)) = \\(1 \\implies R\\) is injective. (Since if not then order(\\(R-a) \\ge 2\\) for some \\(a\\), but that's a contradiction to the lemma). We will see that \\(R\\) is surjective too.</p> <p>This would imply that any \\(R(z) = \\dfrac{az+b}{cz+d}\\) , \\(ad-bc \\neq 0\\) (why? \\(R(z)\\) becomes a constant otherwise) gives a bijective (analytic) function of \\(S \\to S\\). \\(R^{-1}(z) = \\dfrac{dz-b}{-cz+a}\\) </p>"},{"location":"Complex%20Rational%20Functions/#related-problems","title":"Related Problems","text":"<p>Number Of Roots and Poles of a Rational Function Find the group of 1-1 analytic self-homeomorphisms of \\(\\mathbb{C}\\). Assume that it is a subgroup of \\(SL_2(\\mathbb{C})/\\{\\pm 1\\}\\).</p>"},{"location":"Complex%20Rational%20Functions/#references","title":"References","text":"<p>Analytic Function</p>"},{"location":"Complexity%20Assignment%201/","title":"Complexity Assignment 1","text":"<p>Name: Shubh Sharma Roll Number: BMC202170</p> <p>### 1  A Language \\(L\\) is in \\(DTIME(T(n))\\), if it can be solved by a \\(k\\)-tape Deterministic Turing Machine with the language \\(\\Sigma\\) in time \\(O(T(n))\\). For any such turing machine we can construct and equivalent Single Tape Turing Machine with the language \\((\\Sigma\\cup\\{\\sqcup\\}\\times\\{\\sqcup,\\uparrow\\})^k\\) which can be thought of as a Single tape turing machine with \\(2k\\) tracks(numbered 1,2,3 .. and so on) such that the \\(2i-1th\\) track represets the \\(i^{th}\\) tape of the \\(k\\) tape turing machine and in the \\(2i^{th}\\) track all places except for the corresponding pointer location are \\(\\sqcup\\) where the location where the pointer is pointing to has \\(\\uparrow\\).  One Step of the \\(k\\) tape turing machine can be emulated on the one tape turing machine by, starting to read from the left, until the location of all the pointers is read, then starting from the left again, making changes on all \\(k\\) tracks, one cell at a time, then going back to the left most tape and restarting the process.</p> <p>If the Machine takes \\(c=O(T(n))\\) steps to execute then it reads atmost \\(c\\) cells in the tape. Hence to emulate on step of the \\(k\\) tape turing machine on a single tape turing Machine it would take \\(4c\\) steps which would be-</p> <ul> <li>Reading all cells to find where the pointers would be - \\(c\\) step_NFA Universality is PSPACE-complete_s.</li> <li>Going back to the beginning - \\(c\\) steps</li> <li>Making changes in the tapes - \\(c\\) steps</li> <li>Going back to the beginning - \\(c\\) steps</li> </ul> <p>and these are repeated exactly \\(c\\) times hence it takes \\(c^2\\) steps at most which makes it \\(O(T(n)^2)\\).  ### 2.  Consider one of the tapes of the 2-tape turing machine as the main tape and the other one as the support tap.  Initially keep the support tape empty.  Both tapes have \\(k\\) many tracks, same as the number of tapes in the initial Turing Machine. Both the tapes are two way infinite tapes.  Idea: consider the position of the head to be fixed and imagine sliding the tracks of the tape instead.  The storage in the main tape is divided into multiple sections as follows.  -  The cell where the head is pointing to is called the Home column.  -  To its right and left, the sections are lablelled \\(R_{1},R_{2},\\dots\\) and \\(L_{1}, L_{2},\\dots\\) respective  -  The size of section labelled \\(R_i\\) or \\(L_i\\) is \\(2^{i}\\) cells</p> <p>The support tape is used to move a string of length \\(n\\) on the main tape by \\(m\\) positions in \\(O(n+m)\\) time in the following way.  The header in the main tape goes to the start of the string, then it reads the string, and while doing that it deletes it from the main tape and copies it to the support tape, then the header moves back to the position of the start of the string, and then moves \\(m\\) positions, then it deletes the work from the support tape and copies it onto the main tape in the new location.  This process took \\(3n+m\\) steps.  From now on, the support tape will not be mentioned and shifting a string will be considered to take linear time.</p> <p>In the initial setup, the letter where the header is at, is placed in the Home column, while the rest are placed to the left and right such that, half of each section is filled with letters (include blank spaces from the \\(k\\) tape turing machine as characters if there aren't enough to fill)</p> <p>The invariant to be maintained - number of filled cells in \\(R_i\\) + \\(L_i\\) are \\(2^i\\)</p> <p>To simulate a step, if you want to change a letter and move the header to the right(move the tape to the left.)  -  scan to the right to find the first empty or half empty section. keep moving the previous sections to fill up half of the sections after them until there is space in \\(R_{1}\\) then move the element in the Home column to \\(R_{1}\\)  - Scan to the right to find the first non empty section. If it is full, copy half of it onto the different sections, such that the last element is now in the Home column, and if the first non empty section is half full, copy all its contents to the right filling half of each of the following section, putting the required element in the Home column.</p> <p>Moving in the other direction in symmetric.</p> <p>To prove this takes \\(\\log(T(n))\\) steps  Consider the section \\(L_{i}\\)  Moving \\(2^{i-1}\\) elements out of \\(L_{i}\\) takes \\(2^{i+1}\\) steps atmost, after that \\(L_j\\) are all half full \\(\\forall j&lt;i\\). To access \\(L_i\\) we either have to empty all \\(L_{j},\\forall j&lt;i\\) , which takes \\(2^{j+1}\\) steps for each, givinf a total of \\(2^{i+1}\\)$ steps.  Moving elments into \\(L_i\\) takes atmost \\(2^{i+1}\\) steps, trying to add more elements in \\(L_{i}\\) would require filling all \\(L_{j}, \\forall j&lt; i\\) which would take \\(2^j+1\\) steps each, which gives a total of \\(2^{i+1}\\) steps</p> <p>Hence \\(L_i\\) can be access atmost twice in \\(2^{i+2}\\) steps, hence after \\(c\\cdot f(n)\\) steps, \\(L_i\\) can be accessed atmost \\(\\frac{c\\cdot f(n)}{2^{i}}\\) times.  This process in repeated for every track, hence emulating one step of the \\(k\\) tape turing machien takes atmost \\(k\\log_{2}(c\\cdot f(n))\\) steps.  Doing that for \\(c\\cdot (f(n))\\) steps gives \\(kc\\cdot f(n)\\log(f(n))\\) </p> <p>To prove: one side infintie tape turing machine and two way infinte tape can exectute with same time complexity.</p> <p>To emulate a two way infinite turing machine, we can divide the tape into two tracks, which will be equivalent to folding the two way tape at an arbitrary point,  keep track of which track the header is in (this can be done by copying the states where one of two copies handes the case where head is on the upper track and the other handles the case where the head is on the lower track), while moving between the letters in the left most elment, we can consider switching the tracks, hence one step in the two  way infinite tape corresponst to one step in the one way infintie tape.</p> <p>Hence if something can be done in a \\(k\\) tape turing machine in \\(T(n)\\), it can be done in a 2 tape turing machine in \\(T(n)\\log(T(n))\\) time.  ### 3  We first show that if \\(P=NP\\) the search version on \\(SAT\\) is in \\(P\\). we cam show this by first making \\(2\\) by setting the first variable to <code>true</code> and <code>false</code>, and check for the decision version of sat in both cases. if any of the cases is true, we pick owe, permanently set the variable that value and continue the process with other variables. If the number of variables in \\(n\\) then we only call the decidability version \\(n\\) times, and we make multiple cases upto \\(n\\) times.</p> <p>By Cook-Levin theorem, we can convert every problem (Say \\(L\\)) in \\(NP\\) to \\(SAT\\), also the conversion is a Levin Reuction, hence an assignment to \\(SAT\\) can be mapped back a to the language \\(L\\) in polynomial time, hence \\(L\\in P\\).  ### 4.  FTSOC: \\(P=SPACE(n)\\)  Then, there exists a polynomial time solution for every language in \\(SPACE(n)\\). Consider \\(L\\in SPACE(n^2)\\), and \\(w\\in L\\). There exists a Turing machine such that it accepts \\(w\\) on space \\(O(|w|^2)\\), now we can pad it to get \\(w'=w10^{|w|^{2}-|w|-1}\\), which makes it \\(|w|^2\\) long. That means, there is a machine which takes in \\(w'\\) and tells if \\(w\\) is accepted by a turing machine in \\(O(n)\\) space. Thus we construced \\(w'\\) in polynomial time and checked if \\(w\\) is in \\(L\\) in polynmial time. which gives us  \\(SPACE(n^{2})\\subseteq P \\subseteq SPACE(n)\\) which violates the Space Hierarchy Theorem.  ### 5. Not to be done  ### 6.   If \\(P = NP\\), then any language \\(L\\in NP\\) can be solved in polynomial time. For any language \\(A\\in P\\) such that \\(A\\ne\\emptyset, A\\ne\\Sigma^*\\), we can find words \\(w\\in A\\) and \\(w'\\not\\in A\\). For any \\(x\\in L\\), we can reduce it to \\(A\\) as follows.  We compute if \\(x\\in L\\) in polynomial time, If it does, we map it to \\(w\\) otherwise, we map it to \\(w'\\) then \\(x\\) is accepted by \\(L\\) if and only if \\(w\\) is accepted by \\(A\\), and the solution was computed in polynomial time, hence this is a polynomial time reduction. Thus, we can get that \\(A\\) is \\(NP\\) complete.  ### 7.  The first step is to verify if the given input is a valid finite state automata. this can be done in \\(O(n)\\) as we only need to check as per the given description  \\(ALL_{DFA}\\in P\\) . To show that, consider a turing machine, that takes in a \\(DFA\\) as an input, the \\(DFA\\) can be encoded in the following way-  -  The encoding consists of multiple sections separated by \\(\\$\\)   -  The first section tells the number of states, assuming states are now identified using numbers.  -  The next section's first letter represents the starting state and the rest represent the accepting state  -  The following sections are of the form <code>State| Letter | State</code> which represents the transition from the first state to the second one written using the letter.  We can replace each of the <code>State | Letter | State</code> transition to <code>State | State</code>, and now the \\(DFA\\) representation becomes the representation of the graph, now we can check for reachability from teh start state to all non accepting states, if any of them is true, then we can say the \\(DFA\\) does not belong to our language. The conversion of the \\(DFA\\) transitions to graph edges is Polynomial time, and we are using reachability (which is polynomial time) polynomial. Hence the claim is proved.</p> <p>\\(ALL_{NFA}\\in PSPACE\\). To show that, consider a turing machine that takes in a description of an \\(NFA\\) similiar to the description of the \\(DFA\\) in the previous section, but if a transition goes from \\(1\\) state to \\(k\\) states using a single letter, we accept it as \\(k\\) distinct transitions.  For every \\(NFA\\) with \\(q\\) states we can make an equivalent \\(DFA\\) with \\(2^{q}\\) states.  We can then check words with length upto \\(2^q\\) length non deterministically, along with that we need to keep a counter that will check if the length of the word exceeds \\(2^q\\) and one tape that keeps track of the set of states that the word can be in.  If at any point one of the states in the set are the non accpeting states we can conclude that the \\(NFA\\) does not belong to \\(ALL_{NFA}\\) During this process, one counter was used which took \\(n\\) cells and the set of states that the run can go to, which also took upto \\(n\\) states, Hence \\(ALL_{NFA}\\in NSPACE=PSPACE\\) </p>"},{"location":"Complexity%20Assignment%202/","title":"Complexity Assignment 2","text":"<p>Name: Shubh Sharma, Satya Sreevani Bh Roll Number: BMC202170, BMC202162</p>"},{"location":"Complexity%20Assignment%202/#3","title":"3.","text":"<p>We need to show that by fixing \\(x, y\\in \\mathbb F_{2}^{n}, u, v\\in\\mathbb F_{2}^{k}\\) we get  \\(\\(Pr(Ax+b=u \\land Ay+b=v)= \\frac{1}{2^{2k}}\\)\\) Multiplication by a matrix \\(A\\) is the same as taking dot product with single rows of the matrix, that are picked indpendently from each other.  Probability that \\(x\\cdot u=1\\) where \\(x\\) is a non zero vector is \\(1/2\\), to show that, consider the case where \\(x\\) has exactly \\(l\\) 1s and the rest \\(n-l\\) are 0s. Then let the probablilty that \\(x.u=1\\) be \\(p_l\\), let \\(p_{l-m}\\) be the probablility that the vector which is \\(x\\) with first \\(m\\) \\(1\\) fillped times \\(u\\) is \\(1\\), then we have the recurrence relation \\(p_{l}= \\frac{1}{2}(1-p_{l-m}) + \\frac{1}{2}p_{l-m}=p_{l-m}\\)  This recurrence relation holds because the dot product is defined simply by counting the number of positions in which \\(u\\) has \\(1\\)(that correspont to locations on \\(x\\)) hence the recurrence says the total number of matching positions is odd, if first one matches and even many of the following match, or the first one does not match and odd many of the following match also \\(p_1=\\frac12\\) hence \\(p_{l}=\\frac{1}{2}\\) This means that the \\(i^{th}\\) index of \\(Ax\\) match the \\(i^{th}\\) index of \\(u-b\\) is \\(1/2\\) and same for \\(y\\) and \\(v\\) hence probability that  \\(i^{th}\\) index of \\(Ax\\) matches with \\(u-b\\) and \\(i^{th}\\) index of \\(Ay\\) matches \\(v-b\\) is \\(1/4\\)  Doing the same for all \\(k\\) components gives that  \\(\\(Pr(Ax+b=u \\land Ay+b=v)= \\frac{1}{2^{2k}}\\)\\) hence the familty \\(\\mathcal H_{n,k}\\) is a set of pairwise independent hash functions.</p>"},{"location":"Complexity%20Assignment%202/#4-prove-that-if-pnp-then-exp-nexp","title":"4. Prove that if \\(P=NP\\) then \\(EXP = NEXP\\)","text":"<p>Given any language in \\(EXP\\), we can accept in by a Non Deterministic Turing Machine in the same time, by not using non determinism in the turing machien, hence \\(EXP\\subseteq NEXP\\)</p> <p>If \\(P=NP\\) Consider a language \\(L\\in NEXP\\), then there is a Non-Deterministic Turing Machine that accepts or rejects any word from \\(L\\) in time \\(O\\left(2^{n^{c}}\\right)\\). Given any word, \\(l\\in L\\) we can construct the word \\(l\\#1^{n^{c}}\\) by appending \\(n^{c}\\;\\; 1s\\)   to the word, this new language \\(L'=\\{\\;l\\#1^{n^{c}}:l\\in L, n=|l|\\;\\}\\) is accepted by a Non-Deterministic Turing Machine in time \\(O(n^{c})\\). Since \\(NP=P\\) There exists a deterministic Turing Macine \\(M\\) that has the language \\(L'\\) in time \\(O(n^{c'})\\). Now for any given \\(l\\in L\\), follow these steps on a Deterministic Turing Machine + Given a word of length \\(n\\), appened \\(O(n^{c})\\) 1s after the word. + Run the machine \\(M\\) on the input These steps would accept the word in time \\(O\\left(2^{n^{c'}}\\right)\\) hence \\(L\\) is accepted by a Deterministic Time Turing machien in exponential time, this \\(NEXP\\subseteq EXP\\) </p> <p>Thus \\(EXP = NEXP\\)</p>"},{"location":"Complexity%20Assignment%202/#6","title":"6.","text":"<p>\\(MA\\subseteq PSPACE\\) because \\(MA\\subseteq AM\\subseteq IP= PSPACE\\)</p> <p>To show that if \\(PSPACE\\subseteq P/Poly\\)  then \\(PSPACE\\subseteq MA\\) We use the result that a problem in PSPACE can be simulated using an IP protocol where the computation done by the prover is in PSPACE let \\(L\\in PSPACE\\), Then there exits a polynomial size circuit family \\(C\\) that computes the provers message in the \\(IP\\) protocol for \\(L\\) We use the following \\(MA\\) protocol for \\(L\\) Merlin sends Arthur the circuit family \\(C\\) bounded by some polynomial of the input length. Arthur then simulates the circuit family on the input to get the prover's message at each step. Arthur then accepts iff the verifier it is simluating accpets.</p> <p>This computation takes polynomial time because Arthur has to check polynomially many circuits atmost polynomially many times.(is IP protocol has polynomie)</p> <p>If \\(x\\in L\\) then Merlin can start with a message that will cause Arthur to accept with probability \\(\\frac{3}{4}\\) and if \\(x\\notin L\\) then never accpets with probability more than \\(\\frac{1}{4}\\) (by completeness and soundness of \\(IP\\) protocol) hence \\(PSPACE=MA\\)</p>"},{"location":"Complexity%20Classes/","title":"Complexity Classes","text":"<p>202301021901</p> <p>Type : #Note Tags : [[Complexity Theory]]</p>"},{"location":"Complexity%20Classes/#complexity-classes","title":"Complexity Classes","text":"<p>The goal of complexity theory is to classify problems by their difficulty, we can classify problems into multiple complexity classes.</p> <p></p> <p> - \\(P\\subseteq NP\\) </p> <p> - \\(P\\subseteq BPP\\)</p> <p>The equality of P and NP and the equality of P and BPP, are two big unsolved problems in complexity theory. </p> <p></p> <p></p>"},{"location":"Complexity%20Classes/#related-problems","title":"Related Problems","text":""},{"location":"Complexity%20Classes/#references","title":"References","text":""},{"location":"Complexity%20of%20an%20FO%20formula/","title":"Complexity of an FO formula","text":"<p>202310171410</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"Complexity%20of%20an%20FO%20formula/#complexity-of-an-fo-formula","title":"Complexity of an FO formula","text":"<p>We will call this the Quantifier rank of the FO formula.</p> <p>\\(qr(\\varphi)\\)</p> <p>\\(qr(t_{1}\\equiv t_{2})=qr(r(t_{1},\\dots,t_{n}))=0\\) \\(qr(\\lnot\\varphi)=qr(\\varphi)\\) \\(qr(\\varphi_{1}\\lor\\varphi_{2})=qr(\\varphi_{1}\\land\\varphi_{2})=\\max(qr(\\varphi_{1}),qr(\\varphi_{2}))\\) \\(qr(\\exists x\\ \\varphi)=qr(\\forall x\\ \\varphi)=qr(\\varphi)+1\\)</p>","tags":["Note","Incomplete"]},{"location":"Complexity%20of%20an%20FO%20formula/#references","title":"References","text":"<p>First Order Logic FOL Inexpressibility</p>","tags":["Note","Incomplete"]},{"location":"Composite%20Field/","title":"Composite Field","text":"<p>202210261010</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Composite%20Field/#composite-field","title":"Composite Field","text":"<pre><code>title: Composite Fields\nIf $K_1$ and $K_2$ are subfields of $K$, then the [ composite field](&lt;./Composite Field.md&gt;) of $K_1$ and $K_2$ denoted by $K_1K_2$, is the smallest subfield of $K$ which contains both $K_1$ and $K_2$. Similarly, the composite field of any collection of subfields of $K$ is the smallest subfield of $K$ which contains all the other ones.\n</code></pre> <p>If \\(K_1\\) and \\(K_2\\) are finite extensions of a field \\(F\\), let \\(\\alpha_1,\\alpha_2\\dots\\alpha_n\\) are and \\(F\\)-basis for \\(K_1\\), let \\(\\beta_1,\\beta_2\\dots\\beta_m\\) are and \\(F\\)-basis for \\(K_2\\). The the composite field \\(K_1K_2\\) over \\(F\\) is: $$ K_1K_2 = F(\\alpha_1,\\alpha_2\\dots\\alpha_n,\\beta_1,\\beta_2\\dots\\beta_m) $$ Proposition: \\([K_1K_2:F] \\le [K_1:F][K_2:F]\\) with equality if and only the basis of one fields remains linearly independent over the other. Proof: \\(K_1K_2 = K_2(\\beta_1,\\beta_2\\dots\\beta_m)\\)  Hence \\([K_1K_2:K_1] \\le m = [K_2:F]\\)  Since \\([K_1K_2:F] = [K_1K_2:K_1][K_1:F]\\)  hence proved we also have the diagram. $$ \\begin{matrix} &amp; &amp; K_1K_2 &amp; &amp; \\ \\le m &amp; \\huge/ &amp; &amp; \\huge \\backslash &amp; \\le n \\ K_1 &amp; &amp; &amp; &amp; K_2\\  n &amp; \\huge\\backslash &amp; &amp; \\huge / &amp; \\le n \\ &amp; &amp; F &amp; &amp; \\end{matrix} $$</p>"},{"location":"Composite%20Field/#related-problems","title":"Related Problems","text":""},{"location":"Composite%20Field/#references","title":"References","text":""},{"location":"Computable%20and%20Uncomputable%20Functions/","title":"Computable and Uncomputable Functions","text":"<p>202212152212</p> <p>Type : #Note Tags :</p>"},{"location":"Computable%20and%20Uncomputable%20Functions/#computable-and-uncomputable-functions","title":"Computable and Uncomputable Functions","text":"<p>[[Theory of Computation]] gives a precise categorization of functions that are computible in principle.</p> <pre><code>title:\nA function is _Computable_ if there is some program which computes it.\n</code></pre> <p>A</p>"},{"location":"Computable%20and%20Uncomputable%20Functions/#related-problems","title":"Related Problems","text":""},{"location":"Computable%20and%20Uncomputable%20Functions/#references","title":"References","text":"<p>Partial Functions</p>"},{"location":"Computational%20History/","title":"Computational History","text":"<p>202301090201</p> <p>Type : #Note Tags : [[Theory of Computation]]</p>"},{"location":"Computational%20History/#computational-history","title":"Computational History","text":""},{"location":"Computational%20History/#related-problems","title":"Related Problems","text":""},{"location":"Computational%20History/#references","title":"References","text":"<p>Turing Machines</p>"},{"location":"Cones%20and%20Suspensions/","title":"Cones and Suspensions","text":"<p>202304121804</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Cones%20and%20Suspensions/#cone","title":"Cone","text":""},{"location":"Cones%20and%20Suspensions/#definition","title":"Definition:","text":"<p><pre><code>title:\nThe (unreduced) cone is defined to be the quotient space $$\n\\tilde{C}X := (X \\times I)/(X \\times \\{1\\})\n$$\n</code></pre> Let \\(j_{X} : X \\to \\widetilde{C}X\\) by sending \\(x \\to [(x,0)]\\) where we use [] to denote equivalence classes in the quotient space \\(\\widetilde{C}X\\).</p>"},{"location":"Cones%20and%20Suspensions/#lemma-1","title":"Lemma 1:","text":"<pre><code>title:\n$\\widetilde{C}X$ is a contractible space.\n</code></pre>"},{"location":"Cones%20and%20Suspensions/#proof","title":"Proof:","text":"<p>We can contract \\(\\widetilde{C}X\\) to its vertex via the straight line homotopy $$ F_{s}([x,t]) := [x,(1-s)t+s] $$</p>"},{"location":"Cones%20and%20Suspensions/#lemma-2","title":"Lemma 2:","text":"<pre><code>title:\n$j_X$ is a closed embedding\n</code></pre>"},{"location":"Cones%20and%20Suspensions/#proof_1","title":"Proof:","text":"<p>Take a closed set \\(U\\) in \\(X\\), \\(j_{X}(U)\\) is closed in \\(j_{X}(X)\\) but \\(j_{X}(X)\\) is closed in \\(\\widetilde{C}X\\), hence \\(j_{X}(U)\\) is closed in \\(\\widetilde{C}X\\).</p>"},{"location":"Cones%20and%20Suspensions/#lemma-3","title":"Lemma 3:","text":"<pre><code>title:\nA map $f : X \\to Y$ is null homotopic iff it extends over $\\widetilde{C}X$.\n</code></pre>"},{"location":"Cones%20and%20Suspensions/#proof_2","title":"Proof:","text":"<p>If \\(f\\) is null homotopic, then there is a homotopy \\(H : X \\times I \\to Y\\) to a constant map \\(c(x) := y_{0}\\). Then \\(H_{1}(X) \\subseteq \\{ y_{0} \\}\\), so that the homotopy factors through the quotient \\(X\\times I /X \\times \\{ 1 \\}\\) to give a map \\(\\widetilde{f} : \\widetilde{C}X \\to Y\\) that satisfies \\(\\widetilde{f}\\circ j_{X} = H_{0} = f\\). The </p>"},{"location":"Cones%20and%20Suspensions/#references","title":"References","text":""},{"location":"Conformality%20Transformation%20on%20let%28rec%29%20expressions/","title":"Conformality Transformation on let(rec) expressions","text":"<p>202311071911</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note","Incomplete"]},{"location":"Conformality%20Transformation%20on%20let%28rec%29%20expressions/#conformality-transformation-on-letrec-expression","title":"Conformality Transformation on let(rec) expression","text":"<p>Consider the example <pre><code>head xs = y\n  where (y:ys) = xs\n</code></pre></p> <p>There is an obvious way in which the above function fails, i.e if the function is given an empty list as in input.</p> <p>This can happen in case of sum patterns and constant patterns and converting it to Enriched Lambda Calculus does not fix the problem</p> <pre><code>head = \\xs.(let (CONS y ys) = xs in y)\n</code></pre> <p>we convert <code>CONS</code> to <code>PAIR</code> in the following way</p> <p><pre><code>head := \n    \\xs.((let (PAIR y ys) \n        = (\\(CONS y ys).PAIR y ys) xs) |&gt; ERROR in y)\n</code></pre> in general for  <pre><code>p = B\n</code></pre> we get</p> <pre><code>(t v1 ... vn) = ((\\p.(t v1 ... vn)) B) |&gt; ERROR\n</code></pre> <p>Here <code>t</code> is a Product constructor so the pattern is irrefutable.</p>","tags":["Note","Incomplete"]},{"location":"Conformality%20Transformation%20on%20let%28rec%29%20expressions/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Connectedness/","title":"Connectedness","text":"<p>202302101202</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Connectedness/#connectedness","title":"Connectedness","text":"<pre><code>title:\n$X$ is non empty, topological space.\n$A$ _separation_ of $X$ is a pair of subsets $U,V$ which are disjoint, open and non empty.\n</code></pre> <pre><code>title:\n$X$ is _connected_ if $\\nexists$ a separation of $X$. Otherwise it's called _disconnected_.\n</code></pre> <pre><code>title:\n$X$ is called _totally disconnected_ if only singleton subsets of $X$ are connected.\n</code></pre> <pre><code>title: Proposition\nThe following are equivalent:\n1) $X$ is disconnected.\n2) $\\exists$ nonempty, disjoint, closed subsets $A, B$ s.t. $X = A \\sqcup B$.\n3) $\\exists$ a subset $A \\subset X$ both open and closed s.t. $A \\neq \\phi$ or $X$.\n4) $\\exists$ a non trivial clopen set.\n</code></pre> <pre><code>title: Lemma\nIf $T \\subset X$ is connected, then $Cl(T)$ is connected too. And if $T \\subset T' \\subset Cl(T)$ then T' is also connected.\n</code></pre>"},{"location":"Connectedness/#proof","title":"Proof:","text":"<p>Assume to the contrary that it is not connected. Then \\(Cl(T) = T_1 \\sqcup T_2\\), Then \\(T = (T_1 \\cap T) \\sqcup (T_2 \\cap T) \\implies\\) T is disconnected. \\(T_i \\cap T \\neq \\phi\\) because they are open sets of \\(Cl(T)\\) and so intersect T non trivially.</p> <p>Suppose T' = \\(T_1 \\sqcup T_2\\), Then \\(T' = (C_1 \\cap T') \\sqcup (C_2 \\cap T')\\) where \\(C_1,C_2\\) are open subsets of \\(Cl(T)\\). Hence \\(T = T' \\cap T = (T_1 \\cap T) \\sqcup (T_2 \\cap T) = (C_1 \\cap T' \\cap T) \\sqcup (C_2 \\cap T' \\cap T) = (C_1 \\cap T) \\sqcup (C_2 \\cap T)\\) which is a union of open subsets of T.  Contradiction. </p> <pre><code>title: Theorem\n$\\mathbb{R}$ is connected.\n</code></pre>"},{"location":"Connectedness/#proof_1","title":"Proof:","text":"<p>Suppose not, then \\(\\mathbb{R} = X \\sqcup Y\\) where X and Y are open. Let \\(a \\in X, b \\in Y\\) such that \\(a &lt; b\\). Let \\(A = X \\cap (-\\infty,b]\\), Let \\(p = \\sup A\\).  If p is in X, then \\((p-\\epsilon,p+\\epsilon) \\subset X\\) and since \\(p \\neq b\\), then this interval is also contained in \\((-\\infty,b]\\) and hence \\((p-\\epsilon,p+\\epsilon) \\subset A\\), contradiction to sup$ A = p$. If p is not in \\(X\\), then since \\(Y\\) is open,  \\((p-\\epsilon,p+\\epsilon) \\subset Y\\), then \\(p-\\epsilon\\) is also an upperbound to \\(A\\). Contradiction.</p> <pre><code>title: Definition\nA linearly ordered set $L$ having more than one element is called a _linear continuum_ if\n1) $L$ has the least upper bound property\n2) If $x &lt; y$, $\\exists z$ s.t. $x &lt; z &lt; y$.\n</code></pre> <pre><code>title: Theorem\nIf $L$ is a linear continuum then it is connected in the order topology and so are the rays and intervals of $L$.\n</code></pre>"},{"location":"Connectedness/#proof_2","title":"proof:","text":"<p>Same as the proof for connectedness of \\(\\mathbb{R}\\).</p> <pre><code>title: Intermediate Value Theorem\nLet $f : X \\to \\mathbb{R}$ be a continuous map where $X$ is connected. If $a,b\\in X$ and $r \\in \\mathbb{R}$ such that $f(a) &lt; r &lt; f(b)$ then $\\exists c \\in X$ with $f(c) = r$.\n</code></pre>"},{"location":"Connectedness/#proof_3","title":"Proof:","text":"<p>Assume to the contrary that there was no c s.t. \\(f(c) = r\\). Then \\(X = f^{-1}((-\\infty,r) \\sqcup f^{-1}((r,\\infty))\\), contradiction.</p> <pre><code>title: Theorem\nThe continuous image of a connected space is connected.\n</code></pre> <pre><code>title: Theorem\nA topological space X is connected iff every [continuous function](&lt;./Continuous Functions.md&gt;) \n$f: X \\to \\{0,1\\}$ is constant.\n</code></pre>"},{"location":"Connectedness/#constructions","title":"Constructions","text":"<p>1) The union or intersection of connected sets are not connected.</p> <p>2) Union of a collection of connected subspaces of X that all have a point in common is connected.    #### Proof:    A cts function f on the union takes a fixed value at the common point, and hence the same value over all of the union. </p> <p>3) Finite product of connected spaces is connected.    #### Proof:    Let \\(f: X\\times Y \\to \\{0,1\\}\\) then \\(f((x_1,y_1)) = f((x_1,y_2)) = f((x_2,y_2))\\).  You need to show that the restriction of \\(f\\) to X is continuous.</p> <p>4) Arbitrary product of connected spaces is connected.    #### Proof:     Let X = \\(A \\sqcup B\\), then there is some coordinate where the projection of A and B is disjoint and open and covers that component, and so that component is disconnected. Contradiction.    Alternate: Fix a point \\((a_{\\alpha}) \\in X\\).     Given any finite subset K of J where \\(X  = \\prod \\limits_{\\alpha \\in J} X_{\\alpha}\\) let \\(X_K\\) denote the subspace containing all points \\(x\\) such that \\(x_\\alpha = a_\\alpha \\ \\forall \\alpha \\notin K\\). Show that \\(X_K\\) is connected.    The union Y of all X_K's is connected by construction (2).    Hence the closure of Y = X is also connected.</p> <p>5) Quotient of a connected space is connected.    #### Proof:    Quotient map is continuous and surjective, cts maps take connected spaces to connected spaces.</p> <pre><code>title: Proposition\nThe connected components of a space X are connected disjoint spaces of X, whose union is X, s.t. each non empty connected subspace of X intersects only one of them.\n</code></pre>"},{"location":"Connectedness/#related-problems","title":"Related Problems","text":"<p>Local Connectedness Path Connectedness Local Path Connectedness</p>"},{"location":"Connectedness/#references","title":"References","text":"<p>Intermediate Value Theorem Product topology Quotient Topology Continuous Functions</p>"},{"location":"Constant%20Pattern%20to%20Lambda%20Calculus/","title":"Constant Pattern to Lambda Calculus","text":"<p>202311041811</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note","Incomplete"]},{"location":"Constant%20Pattern%20to%20Lambda%20Calculus/#constant-pattern-to-lambda-calculus","title":"Constant Pattern to Lambda Calculus","text":"<p>This pattern checks if a given argument matches a particular constant or a lambda expression. written as  $$ (\\lambda k.E) $$ The semantics of the Constant pattern give </p> <p>Hence we can simply replace these expressions with $$ (\\lambda k.E) \\quad\\equiv\\quad (\\lambda x.\\text{IF}\\;(=\\; k\\; v)\\;\\; E\\;\\; \\text{FAIL}) $$</p> <p>[!example] Consider the following Haskell Program <pre><code>flip 0 = 1\nflip 1 = 0\n</code></pre> which gets converted to <code>haskell flip = \\x.( ((\\0.1) x)          |&gt; ((\\1.0) x)          |&gt; ERROR )</code>  Applying the above rule converts it to the following lambda expression  <code>haskell flip = \\x.( ((\\v. IF (= v 0) 1 FAIL) x)           |&gt; ((\\v. IF (= v 1) 0 FAIL) x)           |&gt; ERROR )</code></p>","tags":["Note","Incomplete"]},{"location":"Constant%20Pattern%20to%20Lambda%20Calculus/#references","title":"References","text":"<p>Patterns</p>","tags":["Note","Incomplete"]},{"location":"Constructible%20Numbers/","title":"Constructible Numbers","text":"<p>202305242105</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Constructible%20Numbers/#constructible-numbers","title":"Constructible Numbers","text":"<pre><code>title:\nGiven a length, which we call 1, a straight-edge and a compass, a real number (a length) is _constructible_ it can be constructed by forming successive intersections of \n- lines drawn through pairs of already constructed points\n- circles drawn with centre a constructed point and radius a constructed length\n</code></pre>"},{"location":"Constructible%20Numbers/#lemma","title":"Lemma:","text":"<pre><code>title:\nConstrucible numbers form a field.\n</code></pre>"},{"location":"Constructible%20Numbers/#proof","title":"Proof:","text":"<p>Given \\(a,b\\) constructible, obviously we can construct \\(a+b,-a\\). Note that given a point and a line, we can construct a line parallel to the given line and passing through the given point, and a line perpendicular to the given line and passing through the point.</p> <p>Make a right triangle \\(ABC\\) of perpendicular sides \\(AB = 1, BC = a\\), mark a point \\(P\\) along the side \\(BA\\), such that \\(BP = b\\), then make a line parallel to \\(AC\\) passing through \\(P\\), it cuts \\(BC\\) at \\(Q\\), \\(BQ = ab\\).</p> <p>Similarly, taking \\(BP = 1\\) along \\(BC\\), we get \\(BQ = \\frac{1}{a}\\) with \\(Q\\) on \\(BA\\).</p>"},{"location":"Constructible%20Numbers/#lemma_1","title":"Lemma:","text":"<pre><code>title:\nIf $c &gt; 0$ is constructible, then so is $\\sqrt{c}$.\n</code></pre>"},{"location":"Constructible%20Numbers/#proof_1","title":"Proof:","text":"<p>Draw a circle of radius \\(\\frac{c+1}{2}\\) with centre \\(\\left( \\frac{c+1}{2},0 \\right)\\), and a line passing through \\((1,0)\\) perpendicular to the \\(x-\\)axis. It intersects the circle at length \\(\\sqrt[]{ c }\\).</p>"},{"location":"Constructible%20Numbers/#theorem","title":"Theorem:","text":"<pre><code>title:\nA number $\\alpha$ is constructible iff it is contained in a field of the form $\\mathbb{Q}[\\sqrt{\\alpha_1},\\sqrt{\\alpha_2},\\dots,\\sqrt{\\alpha_n}]$ such that $\\alpha_i \\in \\mathbb{Q}[\\sqrt{\\alpha_1},\\sqrt{\\alpha_2},\\dots,\\sqrt{\\alpha_{i-1}}]$ and $a_i &gt; 0$, for all $i$.\n</code></pre>"},{"location":"Constructible%20Numbers/#proof_2","title":"Proof:","text":"<p>All rationals are constructible, so by a simple induction, one direction of the theorem is clear.</p> <p>Now suppose \\(\\alpha\\) is constructible. Note that any construction using a given set of numbers \\(\\{ \\alpha_{1},\\alpha_{2},\\dots\\alpha_{n} \\}\\) known to be already constructed, can be shown to belong to the field \\(\\mathbb{Q}[{\\alpha_1},{\\alpha_2},\\dots,{\\alpha_n}][\\sqrt[]{ e }]\\) for some \\(e \\in \\mathbb{Q}[{\\alpha_1},{\\alpha_2},\\dots,{\\alpha_n}]\\) (Use coordinate geometry and get that the intersection points of circles and lines come from quadratics.)</p> <p>So we get a constructible number in this field extension, and hence the whole field extension consists of constructible numbers. This shows that any constructible \\(\\alpha\\) is contained in a field of the required form.</p>"},{"location":"Constructible%20Numbers/#corollary","title":"Corollary:","text":"<pre><code>title:\nIf $\\alpha$ is constructible, then $\\mathbb{Q}(\\alpha)$ is algebraic over $\\mathbb{Q}$ and $[\\mathbb{Q}(\\alpha):\\mathbb{Q}]$ is a power of 2.\n</code></pre>"},{"location":"Constructible%20Numbers/#proof_3","title":"Proof:","text":"<p>Since \\(\\alpha \\in \\mathbb{Q}[\\sqrt[]{ \\alpha_{1} },\\dots,\\sqrt[]{ \\alpha_{n} }]\\), we get that degree of \\(\\alpha\\) divides \\([\\mathbb{Q}(\\sqrt[]{ \\alpha_{1} },\\dots,\\sqrt[]{ \\alpha_{n} }) : \\mathbb{Q}]\\) which is a power of 2.</p>"},{"location":"Constructible%20Numbers/#corollary_1","title":"Corollary:","text":"<pre><code>title:\nIt is impossible to duplicate the cube or square the circle.\n</code></pre>"},{"location":"Constructible%20Numbers/#proof_4","title":"Proof:","text":"<p>Duplicating the cube is equivalent to constructing \\(\\sqrt[3]{ 2 }\\), but \\([\\mathbb{Q}(\\sqrt[3]{ 2 }) : \\mathbb{Q}] = 3\\) is not a power of 2. Squaring the circle is equivalent to constructing \\(\\sqrt[]{ \\pi }\\). But \\(\\sqrt[]{ \\pi }\\) is transcendental since \\(\\pi\\) is, hence \\(\\mathbb{Q}(\\pi)\\) is not algebraic over \\(\\mathbb{Q}\\).</p>"},{"location":"Constructible%20Numbers/#corollary_2","title":"Corollary:","text":"<pre><code>title:\nIt is impossible to trisect an angle by ruler and compass constructions.\n</code></pre>"},{"location":"Constructible%20Numbers/#proof_5","title":"Proof:","text":"<p>Constructiblity of the angle is equivalent to that of its cosine. Therefore to trisect \\(3\\theta\\), we need to know the solution to the equation, $$ \\cos(3\\theta) = 4 \\cos ^{3}(\\theta) - 3 \\cos (\\theta) $$ For example, to construct \\(\\cos(20^{\\circ})\\), we need a solution to $$ \\frac{1}{2} = 4 x^{3}-3x $$ Or equivalently, \\(8x^{3}-6x-1\\). This is irreducible (check roots). Hence \\(\\cos(20 ^{\\circ})\\) is not constructible.</p>"},{"location":"Constructible%20Numbers/#theorem_1","title":"Theorem:","text":"<p><code>ad-note: title: If the regular p-gon is constructible, then $p = 2^{2^k} + 1$, where $p$ is a prime.</code></p>"},{"location":"Constructible%20Numbers/#proof_6","title":"Proof:","text":"<p>To construct a regular \\(p\\)-gon, it is necessary to construct the angle \\(\\frac{2\\pi}{p}\\) and hence \\(\\cos\\left( \\frac{2\\pi}{p} \\right)\\). Notice that $$ \\cos\\left( \\frac{2\\pi}{p} \\right) = \\frac{e ^{2\\pi i/p} + e ^{-2\\pi i/p}}{2} $$ Hence, \\(\\mathbb{Q} \\subset \\mathbb{Q}\\left[ \\cos\\left( \\frac{2\\pi}{p} \\right) \\right] \\subset \\mathbb{Q}[e ^{2\\pi i/p}]\\).</p> <p>The degree of the latter extension is 2 since, $$ t^{2} - 2\\cos\\left( \\frac{2\\pi}{p} \\right) t + 1 = 0 $$ is the minimal poly of \\(e ^{2\\pi i/p}\\) over \\(\\mathbb{Q}\\left[ \\cos\\left( \\frac{2\\pi}{p} \\right) \\right]\\). Thus, \\(\\left[ \\mathbb{Q}\\left( \\cos\\left( \\frac{2\\pi}{p} \\right) \\right) : \\mathbb{Q} \\right] = \\frac{p-1}{2}\\). (See Cyclotomic Fields)</p> <p>Thus \\(\\frac{p-1}{2} = 2^k\\) with \\(k \\ge 0 \\implies p = 2^{k+1}+1 \\implies 2^{2^n}+1\\) (Fermat primes). </p>"},{"location":"Constructible%20Numbers/#references","title":"References","text":"<p>Fields [[Field Extensions]] Algebraic Extension Cyclotomic Fields</p>"},{"location":"Constructor%20Rule%20for%20Match%20Function/","title":"Constructor Rule for Match Function","text":"<p>202310251710</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note"]},{"location":"Constructor%20Rule%20for%20Match%20Function/#constructor-rule-for-match-function","title":"Constructor Rule for Match Function","text":"","tags":["Note"]},{"location":"Constructor%20Rule%20for%20Match%20Function/#example","title":"Example","text":"<p>After evaluating the example in the main note using the Variable Rule once, we get</p> <pre><code>demo =\n  \\u1. \\u2. \\u3. \n    match [u2, u3]\n          [ ( [[],     ys],     (A u1 ys)  ),\n            ( [(x:xs), []],     (B u1 x xs)),\n            ( [(x:xs), (y:ys)], (C u1 x xs y ys)) ]\n          ERROR\n</code></pre> <p>This this an obvious situation where we should involve \\(\\text{case-}\\)expressions, on solving this using the Constructor rule we get</p> <p><pre><code>demo =\n  \\u1. \\u2. \\u3. \n    case u2 of\n      []      =&gt; \n          match [u3]\n                [( [ys], (A u1 ys) )]\n                ERROR\n      (u4:u5) =&gt; \n          match [u4, u5, u3]\n                [( [x, xs, NIL], (B u1 u4 u5)),\n                [( [x, xs ,(y:ys)], C u1 u4 u5 y ys)]\n                ERROR\n</code></pre> which on applying variable rule in both places becomes.</p> <pre><code>demo =\n  \\u1. \\u2. \\u3. \n    case u2 of\n      []      =&gt; \n          match []\n                [( [], (A u1 u3) )]\n                ERROR\n      (u4:u5) =&gt; \n          match [u3]\n                [( [NIL], (B u1 u4 u5)),\n                [( [(y:ys)], C u1 u4 u5 y ys)]\n                ERROR\n</code></pre>","tags":["Note"]},{"location":"Constructor%20Rule%20for%20Match%20Function/#constructor-rule","title":"Constructor Rule","text":"<p>The Constructor Rule deal with the situation where given sum type, all options are constructors of the type, we can use \\(\\text{case-}\\)expressions to write them while eliminating one of the variables used.</p> <p>The call of Match for constructor rule will have the form <pre><code>match (u:us) (qs1 ++ ... ++ qsk) E\n</code></pre> where each <code>qsi</code> has the form  <pre><code>[ (((ci psi1'):psi1), Ei1),\n  ...\n  (((ci psim'):psim), Eim)] \n</code></pre> that is, its list of constructor pattern matches for the first argument, rest of the pattern and the expressions, for the constructor \\(c_{i}\\).</p> <p>Then the above expressions is evaluated to <pre><code>case u of\n  c1 us1' =&gt; match (us1' ++ us) qs1' E\n  ...\n  ck usk' =&gt; match (usk' ++ us) qsk' E \n</code></pre> where each <code>qsi'</code> is of the from  <pre><code>[ ((psi1' ++ psi1), Ei1),\n  ...\n  ((psim' ++ psim), Eim) ]\n</code></pre> that is, for each branch of the case, the new variables used in the constructor are also added , but only via the variable rule.</p>","tags":["Note"]},{"location":"Constructor%20Rule%20for%20Match%20Function/#optimisation","title":"Optimisation","text":"<p>As seen in the above section. The expression \\(E\\) is repeated several times, this can cause a huge blow up of the compiled file size if \\(E\\) is huge. To deal with that we use \\(\\text{FAIL}\\) and \\(\\triangleright\\) and rewrite the above definition in the following way</p> <p></p> <p>discussed with and example in Optimisations for Overlapping Patterns.</p>","tags":["Note"]},{"location":"Constructor%20Rule%20for%20Match%20Function/#termination","title":"Termination","text":"<p>The Variable and Empty Rules terminate in a single step, the termination of this rule is not obvious, although not too hard.</p> <p>Say a type has \\(n\\) constructors and the maximum number of parameters for a given constructor is \\(m\\). Then after applying the rule, the patterns(say we have \\(x\\) patterns) are split into \\(n\\) branches of the cases statement, so the number of patterns does not increase.  For each pattern the number of variables used in them increase at most by \\(m\\), but all increase can be dealt with using the variable rule. We now still have \\(x\\) patterns, but after applying the variable rules to remove the new variables at most \\(m\\) times for each pattern, we are now left with \\(x\\) patterns with one less variable in them.</p> <p>This entire process took 1(making the case conditions) + mx(getting rid of new variables) steps to complete the procedure and we end up with \\(n\\) branches. </p> <p>Hence the pattern match will terminate.</p>","tags":["Note"]},{"location":"Constructor%20Rule%20for%20Match%20Function/#references","title":"References","text":"<ul> <li>Empty Rule for Match Function</li> <li>Variable Rule for Match Function</li> <li>Match Function for Enriched Lambda Calculus</li> <li>Mixture Rule for Match Expression</li> </ul>","tags":["Note"]},{"location":"Continuous%20Functions/","title":"Continuous Functions","text":"<p>202301181801</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Continuous%20Functions/#continuous-functions","title":"Continuous Functions","text":"<p><pre><code>title: Idea\nThe idea is to convert the eqsilon delta definitions for metric spaces to topology, with closeness between points is represented by being in a common open set\n</code></pre> Let \\((X, \\mathcal T_X)\\) and \\((Y, \\mathcal T_Y)\\) be \\(2\\) Topological Spaces and let \\(f:X\\to Y\\) be a Topological Space be a function between them. We say \\(f\\) is continuous if \\(f^{-1}(V)\\in \\mathcal T_X\\) for every \\(V\\in \\mathcal T_Y\\).</p> <p>This is a Global Definition, a Local Definition will be given later Examples: - If a function from \\(\\mathbb R\\to \\mathbb R\\) is continuous in the Calculus Sense, then it is continouous in the Topological Sense. - The projection functions from \\(\\mathbb R^{2}\\to \\mathbb R\\) are continuous - Any function from a discrete space to a topological space is continouous - Any function from any topological space to the trivial topology is continuous - Any Constant function from any topological space to any other topological space is continuous - If \\(f:X\\to Y\\) and \\(g:Y\\to Z\\) are continuous then \\(g\\circ f:X\\to Z\\) is continuous - The identitiy function from \\(\\mathbb R\\to\\mathbb R_l\\) is not continuous, but the identitiy function from \\(\\mathbb R_{l}\\to\\mathbb R\\) is continuous</p> <p>Equivalent statements for continuous functions:  - Pre images of open sets are open, i.e \\(f^{-1}(V)\\in \\mathcal T_X\\) for all \\(V\\in\\mathcal T_Y\\)  - Pre images of basic open sets are open, i.e \\(f^{-1}(V)\\in \\mathcal T_X\\) for all \\(V\\in\\mathcal B\\)  - Pre images of sub-basic open sets are open, i.e \\(f^{-1}(V)\\in \\mathcal T_X\\) for all \\(V\\in\\mathcal S\\) - Pre images of closed sets are closed. - \\(f\\) is continuous at each point of the topological space. (Local Definition) - For every subset \\(A\\subset X\\), \\(f(Cl \\ A) \\subset Cl \\ f(A)\\). </p> <p>Continuity at a point: <pre><code>title: \n$f: X \\to Y$ is continuous at a point $x \\in X$ if for every open set $V \\in \\mathscr{T}_Y$ containing $f(x)$, there exists an open set $U \\in \\mathscr{T}_X$ containing $x$ such that $f(U) \\subset V$.\n</code></pre></p> <p>Local formulation of continuity: <pre><code>title:\nThe map $f : X \\to Y$ is continuous iff $X = \\bigcup\\limits_{\\alpha} U_{\\alpha}$ for open sets $U_{\\alpha}$ such that $f|_{U_{\\alpha}}$ is continuous for all $\\alpha$.\n</code></pre> - Apparantly, this is useful in Algebraic geometry. (Grothendieck topology)</p> <p>Pasting Lemma: <pre><code>title:\nLet $X = A \\cup B$ where A and B are closed (or open) in X. Let $f : A \\to Y$\nand $g : B \\to Y$ be continuous functions. If $f(x) = g(x)$ for all $x \\in A \\cap B$, then f and g \u2018glue together\u2019 to\ngive a continuous function $h : X \\to Y$ obtained by setting $h(x) = f(x)$ if $x \\in A$ and h(x) = g(x) if $x \\in B$.\n</code></pre> - Basically this says that if \\(X\\) is a disjoint union of two clopen sets, then I can define a continuous function on \\(X\\) by defining it independently on A and B. - The infinite analogue of this for the closed case is not necessarily true, for example: just take the closed sets to be singletons in \\(\\mathbb{R}\\), standard topology. Being continuous on these closed sets tells us nothing about the continuity of the function as a whole.</p>"},{"location":"Continuous%20Functions/#related-problems","title":"Related Problems","text":""},{"location":"Continuous%20Functions/#references","title":"References","text":"<p>Homeomorphisms</p>"},{"location":"Convergence%20of%20Fourier%20Series/","title":"Convergence of Fourier Series","text":"<p>2022-10-27 10:10 pm</p> <p>Type : #Note Tags : [[Analysis]]</p>"},{"location":"Convergence%20of%20Fourier%20Series/#convergence-of-fourier-series","title":"Convergence of Fourier Series","text":"<pre><code>title: Theorem \nSuppose that $f$ is integrable on the circle, with $\\hat{f}(n) = 0 \\ \\forall \\ n \\in \\mathbb{Z}$. Then $f(\\theta_0) = 0$ for all points of continuity $\\theta_0$ of $f$.\n</code></pre>"},{"location":"Convergence%20of%20Fourier%20Series/#proof","title":"Proof:","text":""},{"location":"Convergence%20of%20Fourier%20Series/#corollary","title":"Corollary:","text":"<p>Suppose \\(f\\) is continuous on the circle, and the fourier series of \\(f\\) converges absolutely i.e., \\(\\sum\\limits_{n=-\\infty}^{\\infty}|\\hat{f}(n)|\\), then the fourier series converges uniformly to \\(f\\), that is, $$\\lim_{N\\to\\infty}S_N(f)(\\theta) = f(\\theta) $$ for all \\(\\theta\\).</p>"},{"location":"Convergence%20of%20Fourier%20Series/#related-problems","title":"Related Problems","text":"<p>Uniqueness of fourier series Fourier series is the best approximation</p>"},{"location":"Convergence%20of%20Fourier%20Series/#references","title":"References","text":"<p>Fourier Series</p>"},{"location":"Converting%20Case%20Expressions%20to%20Ordinary%20Lambda%20Calculus/","title":"Converting Case Expressions to Ordinary Lambda Calculus","text":"<p>202311051411</p> <p>Tags : [[Programming Languages]], [[Lambda Calculus]]</p>","tags":["Note","Incomplete"]},{"location":"Converting%20Case%20Expressions%20to%20Ordinary%20Lambda%20Calculus/#converting-case-expressions-to-ordinary-lambda-calculus","title":"Converting Case Expressions to Ordinary Lambda Calculus","text":"<p>Translating Haskell Programs to Lambda Calculus introduces a lot of case expressions. Here we give a conversion from Case Expressions to Ordinary Lambda Calculus.</p> <p>As with Constructor Patterns there are two types of Case Expressions</p>","tags":["Note","Incomplete"]},{"location":"Converting%20Case%20Expressions%20to%20Ordinary%20Lambda%20Calculus/#case-expressions-involving-product-type","title":"Case-expressions involving Product Type","text":"<p>The general case expression of product type is of the form <pre><code>case v of\n    t v1 v2 ... vr = E1\n</code></pre></p> <p>Since the constructor is degenerate, there is no need to check for nested patters so we can directly convert it to</p> <pre><code>case v of            === UNPACK-PRODUCT-t (\\v1 ... vr.E1) v\n    t v1 ... vr =&gt; E1\n</code></pre>","tags":["Note","Incomplete"]},{"location":"Converting%20Case%20Expressions%20to%20Ordinary%20Lambda%20Calculus/#case-expressions-involving-sum-type","title":"Case-expressions involving Sum Type","text":"<p>If the constructors are of sum-type then the case-expression would look like <pre><code>case v of\n    s1 v1 ... vr1 =&gt; E1\n    ...\n    sn v1 ... vrn =&gt; En\n</code></pre> which can be converted in the following way <pre><code>case v of\n    s1 v1 ... vr1 =&gt; E1\n    ...\n    sn v1 ... vrn =&gt; En\n===\nCASE-T v (UNPACK-SUM-s1 (\\v1...vr1.E1) v)\n         ...\n         (UNPACK-SUM-sn (\\v1...vrn.En) v)\n</code></pre> where <code>CASE-T</code>selects one of its \\(n\\) arguments based in which constructor matches.</p>","tags":["Note","Incomplete"]},{"location":"Converting%20Case%20Expressions%20to%20Ordinary%20Lambda%20Calculus/#converting-case-expressions-to-let-expressions","title":"Converting Case-expressions to let-expressions","text":"<p>let-expressions can be implemented much more efficiently than lambda abstractions</p>","tags":["Note","Incomplete"]},{"location":"Converting%20Case%20Expressions%20to%20Ordinary%20Lambda%20Calculus/#product-type","title":"Product Type","text":"<p>The following transformation can be used in case of product type <pre><code>case v of           ===  let v1 = SEL-t-1 v\n    t v1...vr =&gt; E1          v2 = SEL-t-2 v\n                             ...\n                             vr = SEL-t-r v\n                         in E1\n</code></pre> which is an equivalent translation as the previous one which can be confirmed by simplifying the let expressions as discussed in [[let(rec)-expressions to Ordinary Lambda Calculus]].</p>","tags":["Note","Incomplete"]},{"location":"Converting%20Case%20Expressions%20to%20Ordinary%20Lambda%20Calculus/#sum-type","title":"Sum Type","text":"<p>A similar idea as the previous one can be used to convert sum types <pre><code>case v of\n    s1 v1 ... vr1 =&gt; E1\n    ...\n    sn v1 ... vrn =&gt; En\n===\nCASE-T v (let v1 = SEL-SUM-s1-1 v\n              ...\n              vr1 = SEL-SUM-s1-r1 v\n          in E1)\n         ...\n         (let v1 = SEL-SUM-sn-1 v\n              ...\n              vrn = SEL-SUM-sn-rn v\n          in En)\n</code></pre> This does increase the complexity of the expression, but it achieves the important objective of eliminating lambda abstractions, which means that this will run significantly faster on all but the simplest scenarios</p>","tags":["Note","Incomplete"]},{"location":"Converting%20Case%20Expressions%20to%20Ordinary%20Lambda%20Calculus/#references","title":"References","text":"<p>Converting Enriched Lambda Calculus to Ordinary Lambda Calculus</p>","tags":["Note","Incomplete"]},{"location":"Converting%20Enriched%20Lambda%20Calculus%20to%20Ordinary%20Lambda%20Calculus/","title":"Converting Enriched Lambda Calculus to Ordinary Lambda Calculus","text":"<p>202311051311</p> <p>Tags : [[Programming Languages]], [[Lambda Calculus]]</p>","tags":["Note","Incomplete"]},{"location":"Converting%20Enriched%20Lambda%20Calculus%20to%20Ordinary%20Lambda%20Calculus/#converting-enriched-lambda-calculus-to-ordinary-lambda-calculus","title":"Converting Enriched Lambda Calculus to Ordinary Lambda Calculus","text":"<p>After Translating Haskell Programs to Lambda Calculus with features like pattern matching lambda abstractions, let/letrec-expressions, case statements and then \\(\\triangleright\\) operator, for each situation we can give a conversion from Enriched Lambda Calculus to Lambda Calculus.</p>","tags":["Note","Incomplete"]},{"location":"Converting%20Enriched%20Lambda%20Calculus%20to%20Ordinary%20Lambda%20Calculus/#pattern-matching","title":"Pattern Matching","text":"<p>Converting Pattern matching to Ordinary lambda Calculus can be summed up as  </p>","tags":["Note","Incomplete"]},{"location":"Converting%20Enriched%20Lambda%20Calculus%20to%20Ordinary%20Lambda%20Calculus/#letletrec-expressions","title":"let/letrec-expressions","text":"<p>Converting Let/Letrec-expressions is bit more involved than the others, and is discussed in detail in [[let(rec)-expressions to Ordinary Lambda Calculus]]</p> <p>It is a multi step process whose plan can be given by ![[let(rec)-expressions to Ordinary Lambda Calculus#^aa141c]]</p>","tags":["Note","Incomplete"]},{"location":"Converting%20Enriched%20Lambda%20Calculus%20to%20Ordinary%20Lambda%20Calculus/#case-statements","title":"Case Statements","text":"<p>Converting Case-expressions to lambda calculus can be summaries in the following way  for product type and   for sum-types. This is discussed in more depth in Converting Case Expressions to Ordinary Lambda Calculus</p>","tags":["Note","Incomplete"]},{"location":"Converting%20Enriched%20Lambda%20Calculus%20to%20Ordinary%20Lambda%20Calculus/#triangleright-and-textfail","title":"\\(\\triangleright\\) and \\(\\text{FAIL}\\)","text":"<p>The above two expressions can be directly encoded in Lambda Calculus by adding them as Built-in Constants and having </p> <p>$$ E_{1}\\; \\triangleright\\;E_{2}\\quad\\equiv\\quad\\text{FATBAR}\\;\\;E_{1}\\;\\;E_{2} $$ Although we can also get rid of most of them as discussed in Optimisations for Expressions containing FAIL and I&gt;.</p>","tags":["Note","Incomplete"]},{"location":"Converting%20Enriched%20Lambda%20Calculus%20to%20Ordinary%20Lambda%20Calculus/#references","title":"References","text":"<ul> <li>Pattern Matching to Ordinary Lambda Calculus</li> <li>Converting Case Expressions to Ordinary Lambda Calculus</li> <li>[[let(rec)-expressions to Ordinary Lambda Calculus]]</li> </ul>","tags":["Note","Incomplete"]},{"location":"Convolutions/","title":"Convolutions","text":"<p>2022-11-16 00:26 am</p> <p>Type : #Note Tags : [[Analysis]]</p>"},{"location":"Convolutions/#convolutions","title":"Convolutions","text":"<pre><code>title: Definition\nGiven two $2\\pi$-periodic integrable functions $f,g$ on $\\mathbb{R}$, we define their convolution as follows:\n$$(f*g)(x) = \\dfrac{1}{2\\pi} \\int \\limits_{-\\pi}^{\\pi}f(y)g(x-y)dy$$\n</code></pre> <ul> <li>Partial sums of fourier series can be written as convolutions.       \\(\\(S_N(f)(x) \\colon = \\sum \\limits_{-N}^{N} \\hat{f}(n)e^{inx} =(f*D_N)(x)\\)\\)</li> <li> </li> </ul>"},{"location":"Convolutions/#properties","title":"Properties:","text":"1) \\(f*(g+h) = f*g + f*h\\) 2) \\((cf)*g = c (f*g) = f*(cg) \\ \\forall c \\in \\mathbb{C}\\) 3) \\(f*g = g*f\\) 4) \\((f*g)*h = f*(g*h)\\) 5) \\(f*g\\) is continuous 6) \\(\\widehat{(f*g)}(n) = \\hat{f}(n)\\hat{g}(n)\\)"},{"location":"Convolutions/#related-problems","title":"Related Problems","text":"<p>Good Kernels</p>"},{"location":"Convolutions/#references","title":"References","text":""},{"location":"Coproducts%20in%20Category%20Theory/","title":"Coproducts in Category Theory","text":"<p>202304020404</p> <p>Type : #Note Tags : [[Category Theory]]</p>"},{"location":"Coproducts%20in%20Category%20Theory/#coproducts-in-category-theory","title":"Coproducts in Category Theory","text":"<p>The Universal Property for Products in Category Theory  is </p> <p>Let \\(X\\) and \\(Y\\) be sets, then for any set \\(A\\) and functions \\(f:X\\to A\\) and \\(g:Y\\to A\\), there exists a unique function \\(h:X\\sqcup Y\\to A\\) such that the following diagram commutes. <pre><code>\\usepackage{tikz-cd}\n\\begin{document}\n\\begin{tikzcd}[column sep=small]\n&amp; \nX\\sqcup Y \n\\arrow[dd, \"h\", dashed] \n&amp; \n\\\\\nX \n\\arrow[ur, \"i_1\"] \n&amp;  \n&amp;\nY \n\\arrow[ul, \"i_2\"']  \n\\\\\n&amp; \nA \\arrow[ul, \"f\"] \\arrow[ur, \"g\"']\n&amp;\n\\end{tikzcd}\n\\end{document}\n</code></pre> where  $$ \\begin{equation} h(a)= \\begin{cases} f(a)&amp;a\\in X\\ g(a) &amp; a\\in Y  \\end{cases} \\end{equation} $$ and \\(h:X\\sqcup Y\\to A\\) Since this is a universal property, Any object \\(B\\) such that given two functions from a set \\(X\\) to \\(A\\) and \\(Y\\) to \\(A\\) gives a unique map from \\(B\\) to \\(A\\) then \\(B\\) is isomorphic to \\(X\\sqcup Y\\).</p>"},{"location":"Coproducts%20in%20Category%20Theory/#references","title":"References","text":"<p>Product topology</p>"},{"location":"Correspondence%20of%20Topological%20and%20Probabilistic%20Semantics%20for%20LTL/","title":"Correspondence of Topological and Probabilistic Semantics for LTL","text":"<p>202311181611</p> <p>Tags : [[Topology]], [[Probability]], Timed Automata</p>","tags":["Note","Incomplete"]},{"location":"Correspondence%20of%20Topological%20and%20Probabilistic%20Semantics%20for%20LTL/#correspondence-of-topological-and-probabilistic-semantics-for-ltl","title":"Correspondence of Topological and Probabilistic Semantics for LTL","text":"<p>[!info] Proposition Let \\(\\mathcal A\\) be a Non-blocking Timed Automata and \\(\\pi\\) be an unconstrained symbolic path in \\(\\text{R}(\\mathcal A)\\).  Then \\(\\mathbb P_{\\text{R}(\\mathcal A)}(\\pi) &gt;0\\) iff dimension of the path is defined</p> <p>[!note] Theorem Let \\(\\mathcal A\\) be a Non-blocking Timed Automata and \\(s\\) be a state in \\(\\mathcal A\\), and \\(\\varphi\\) be an LTL formula, then  $$ \\mathcal A, s\\mid!\\approx_{\\mathbb P} \\varphi \\iff \\mathcal A, s\\mid!\\approx_{\\mathcal T} \\varphi $$</p> <p>We prove the above theorem by proving the equivalence between the semantics on region automata, more over \\(\\text{R}(\\mathcal A),\\iota(s)\\mid\\!\\approx_\\mathbb P \\varphi\\) iff \\(\\mathbb P_\\mathcal A(\\iota(s),\\lnot\\varphi)=0\\), this applying the above proposition gives us that \\(\\text{R}(\\mathcal A),\\iota(s)\\mid\\!\\approx_\\mathbb P \\varphi\\) iff every symbolic path \\(\\pi\\) in \\(\\text{R}_\\mathcal A\\) starting at \\(\\iota(s)\\) and satisfying \\(\\iota\\) has undefined dimension, and that is equivalent to  \\(\\text{R}(\\mathcal A),\\iota(s)\\mid\\!\\approx_\\mathcal L \\varphi\\) or \\(\\textlbrackdbl \\lnot\\varphi \\textrbrackdbl=\\{ \\varrho \\in \\text{Runs}(\\text{R}(\\mathcal A), \\iota(s))\\;:\\;\\varrho \\not\\models\\varphi \\}\\) is topologically meager.</p> <p>So we prove the contrapositive version of the statement.</p> <p>For the first implication we use Banach-Mazur Games and show that Player 2 has a winning strategy if the game is played on \\(\\textlbrackdbl \\lnot\\varphi \\textrbrackdbl\\)  Say Player 1 picked \\(\\pi_1\\). Since dimension of \\(\\pi_1\\) is defined so it satisfies \\(\\varphi\\). Then whatever is played afterwords, its intersection with the target will be empty so player 2 wins. So \\(\\textlbrackdbl \\lnot\\varphi \\textrbrackdbl\\) is meager.</p> <p>For the other way, assume \\(\\textlbrackdbl \\lnot\\varphi \\textrbrackdbl\\) is meager. Since the topological space is a [[Baire Spaces|Baire Space]] we have \\(\\text{Int}(\\textlbrackdbl \\lnot\\varphi \\textrbrackdbl)\\) is empty. So there does not exist a path with defined dimension that satisfies \\(\\lnot\\varphi\\). And by the above proposition we get \\(\\mathbb P_\\mathcal A(\\iota(s),\\lnot\\varphi)=0\\).</p> \\[ \\textlbrackdbl \\lnot\\varphi \\textrbrackdbl \\]","tags":["Note","Incomplete"]},{"location":"Correspondence%20of%20Topological%20and%20Probabilistic%20Semantics%20for%20LTL/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Countably%20Compact/","title":"Countably Compact","text":"<p>202303011803</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Countably%20Compact/#countably-compact","title":"Countably Compact","text":"<pre><code>title:\nA space $X$ is called _countably compact_ if every open cover of $X$ has a finite subcover.\n</code></pre>"},{"location":"Countably%20Compact/#proposition","title":"Proposition","text":"<pre><code>title:\nA first countable space is countably compact iff it is sequentially compact.\n</code></pre>"},{"location":"Countably%20Compact/#proof","title":"Proof:","text":"<p>If it is sequentially compact, then take a minimal countable cover, take points \\(x_{1},x_{2},\\dots\\) such that each \\(x_{i}\\) is in exactly one of the open sets of the cover, then there is a point \\(x\\) such that the element of the cover containing it has infinitely many of these \\(x_{i}\\)'s. Contradiction.</p> <p>If it is countably compact, then take a sequence \\(x_{1},x_{2},\\dots\\), assume this is a sequence without any convergent subsequence. Then for any \\(x_{i}\\), there is some \\(V_{i}\\) around it which contains only finitely many of the other \\(x_{j}'s\\). Then let \\(U = (\\bigcup V_{i})^{c}\\), so the open cover</p>"},{"location":"Countably%20Compact/#related-problems","title":"Related Problems","text":""},{"location":"Countably%20Compact/#references","title":"References","text":""},{"location":"Counter%20Automata/","title":"Counter Automata","text":"<p>202211171811</p> <p>Type : #Note Tags : [[Theory of Computation]]</p>"},{"location":"Counter%20Automata/#counter-automata","title":"Counter Automata","text":"<p>A \\(k\\)-counter Automata is a machine equipped with a two-way read-only input head and \\(k\\) integer counters that can store an arbitrary non-negative integer. In each step we can independently increment or decrement one cell in either direction, test them for \\(0\\) or move the head by one cell in either direction.</p>"},{"location":"Counter%20Automata/#using-2-counters-to-simulate-a-stack","title":"Using 2 counters to simulate a stack","text":"<p>A counter Automata with \\(2\\) counters can simulate a stack. The stack symbols can be written in binary and the numbers can be concatenated to form a huge binary strings which will be the number represented in the stack.</p> <p>After each step of the stack, one the counter will hold the contents of the stack and the other one will be kept empty so that the contents will be transferred to it in the next step.</p> <p>To simulate pushing a zero, we reduce the non empty stack by one and adding 2 to the empty one repeatedly till the first one reaches zero, to simulate adding a 1, its the same procedure but we increment a 1 in the end, to simluate popping we can decrement one of the counters and add one to the other counter every other turn, the parity of the decrement will tell if it popped a 1 or a 0</p> <p>Since a two- stack machine can simulate an arbitrary turing machine we can say that a 4-counter automata can simulate a turing machine. But we can do better </p>"},{"location":"Counter%20Automata/#simulating-a-turing-machine-with-2-counter-automata","title":"Simulating a turing machine with 2-counter automata","text":"<p>A Turing machine can be simulated by a 4-counter automata, if the values in the 4 counters are represented by \\(i,j,k,l\\) then the two counter automata can have the value \\(2^i3^j5^k7^l\\)  adding 1 one to the third counter can be simulated by multiplying the counter by \\(5\\), hence a 4 counter automata can be simluate by a 2 counter automata,</p> <p>Thus a turing machine can be simulated by a 2-counter automata. Even though it would be ridiculously inefficient and even simulating one step of a turing machine will take a lot of steps</p> <p>One counter automata are not as powerful as a turing macine but they can accept non-CFLs like \\(\\{a^nb^nc^n | n&gt;0\\}\\) </p>"},{"location":"Counter%20Automata/#related-problems","title":"Related Problems","text":"<p>Two Stacks</p>"},{"location":"Counter%20Automata/#references","title":"References","text":"<p>Turing Machines</p>"},{"location":"Covering%20Spaces/","title":"Covering Spaces","text":"<p>202304032204</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Covering%20Spaces/#covering-spaces","title":"Covering Spaces","text":"<pre><code>title:\nLet $p : E \\to B$ be a continuous surjective map. We say $p$ _evenly covers_ an open subset $U \\subseteq B$ if \n$$\np ^{-1}(U) = \\bigsqcup \\limits_{ \\alpha \\in I} V_{\\alpha}\n$$\nwhere $I$ is some indexing set, $V_{\\alpha} \\subseteq E$ are disjoint open subsets and for each $\\alpha \\in I$, $p|_{V_{\\alpha}} : V_{\\alpha} \\to U$ is a homeomorphism.\n</code></pre>"},{"location":"Covering%20Spaces/#note-the-sets-v_alpha-are-called-slices","title":"NOTE: The sets \\(V_{\\alpha}\\) are called slices.","text":"<pre><code>title:\nLet $p : E \\to B$ be a cts surjective map. If every point of $B$ has a nbhd that is evenly covered by $p$, then we say $E$ is a **covering space** of $B$ and $p$ is a **covering map**. B is called the _base_ of the covering.\n</code></pre>"},{"location":"Covering%20Spaces/#proposition","title":"Proposition:","text":"<pre><code>title:\nIf $p$ is a covering map, then it is an open map.\n</code></pre>"},{"location":"Covering%20Spaces/#proof","title":"Proof:","text":"<p>Let \\(U\\) be an open set in \\(E\\) and let \\(e \\in U\\). Let \\(p(e) =b\\) and let \\(V\\) be an evenly covered nbhd of \\(b\\). Then \\(p ^{-1}(V) = \\bigsqcup V_{\\alpha}\\) where one of the \\(V_{\\alpha}\\) contains \\(e\\). Then take \\(U \\cap V_{\\alpha}\\), this is a nbhd of \\(e\\) such that \\(p(U \\cap V_{\\alpha})\\) is an open subset of \\(V\\) and hence of \\(B\\), also \\(b \\in p(U  \\cap V_{\\alpha}) \\subseteq p(U)\\). Similarly, for all \\(x \\in U\\), there is a nbhd around \\(p(x)\\) in \\(p(U)\\), hence \\(p(U)\\) is open.</p>"},{"location":"Covering%20Spaces/#proposition_1","title":"Proposition:","text":"<pre><code>title:\nLet $p:E \\to B$ a covering map. If $B_0$ is a subspace of $B$, and if $E_0 = p^{-1}(B_0)$, then the map $p_0 := p|_{E_0} : E_0 \\to B_0$ is a covering map.\n</code></pre>"},{"location":"Covering%20Spaces/#proof_1","title":"Proof:","text":"<p>For any point \\(b \\in B_{0}\\), there is an evenly covered nbhd \\(U\\). Take \\(U \\cap B_{0}\\), \\(p ^{-1}(U \\cap B_{0}) = p ^{-1}(U) \\cap p ^{-1}(B_{0}) = \\bigsqcup V_{\\alpha} \\cap p ^{-1}(B_{0})\\). Each \\(V_{\\alpha} \\cap p ^{-1}(B_{0})\\) is open in \\(E_{0}\\) and is homeomorphic to \\(U \\cap B_{0}\\).</p>"},{"location":"Covering%20Spaces/#proposition_2","title":"Proposition:","text":"<pre><code>title:\nIf $p : E \\to B$ and $p' : E'\\to B'$ are covering maps then $p \\times p' : E \\times E' \\to B \\times B'$ is a covering map.\n</code></pre>"},{"location":"Covering%20Spaces/#proof_2","title":"Proof:","text":"<p>Given \\((b,b') \\in B \\times B'\\), there are open nbhds \\(U,U'\\) of \\(b,b'\\) respectively that are evenly covered. Let \\(p ^{-1}(U) = \\bigsqcup V_{\\alpha}\\) and \\(p' ^{-1}(U') = \\bigsqcup V'_{\\beta}\\). \\((p \\times p')^{-1}(U \\times U') = p ^{-1}(U)\\times p' ^{-1}(U') = \\bigsqcup V_{\\alpha} \\times V'_{\\beta}\\)</p> <ul> <li> <p>See example 2.</p> </li> <li> <p>Let \\(b_{0} = (1,0)\\) then \\(B_{0} := (S ^{1} \\times b_{0}) \\cup (b_{0} \\times S ^{1}) \\subset S ^{1} \\times S ^{1}\\). Then \\(B_{0}\\) is the figure 8 space. Then \\(E_{0} = p ^{-1}(B_{0})\\) is a covering space, \\(E_{0} = (\\mathbb{R} \\times \\mathbb{Z}) \\cup (\\mathbb{Z} \\times \\mathbb{R})\\).</p> </li> </ul>"},{"location":"Covering%20Spaces/#examples","title":"Examples","text":"<ol> <li>\\(p : \\mathbb{R} \\to S ^{1}\\) given by \\(p(t) = (\\cos(2\\pi t),\\sin(2\\pi t))\\) is a covering map.</li> <li>Given the above \\(p\\), we have \\(p \\times p : \\mathbb{R}^{2} \\to S ^{1} \\times S ^{1}\\) as a covering map.</li> </ol>"},{"location":"Covering%20Spaces/#references","title":"References","text":"<p>Homeomorphisms Continuous Functions</p>"},{"location":"Curry%20Howard%20Isomorphism/","title":"Curry Howard Isomorphism","text":"<p>202309041409</p> <p>Type : #Note #Incomplete  Tags : [[Lambda Calculus]]</p>"},{"location":"Curry%20Howard%20Isomorphism/#curry-howard-isomorphism","title":"Curry Howard Isomorphism","text":"<p><pre><code>Fix This\n</code></pre> 1. If \\(\\Gamma\\vdash M:\\varphi\\) in \\(\\lambda_{\\to}\\) then \\(\\text{rg}(\\Gamma)\\vdash\\varphi\\) in <code>LPC(-&gt;)</code> 2. if \\(\\Delta \\vdash\\varphi\\) in <code>LPC(-&gt;</code>, then there is \\(\\Gamma\\) and \\(M\\) such that \\(\\text{rg}(\\Gamma)=\\Delta\\) and \\(\\Gamma\\vdash M:\\varphi\\) in \\(\\lambda_\\to\\) \\(\\Gamma=\\{(x_\\sigma,\\sigma)\\ :\\ \\sigma\\in\\Delta\\}\\) </p>"},{"location":"Curry%20Howard%20Isomorphism/#references","title":"References","text":""},{"location":"Curry-Style%20typing/","title":"Curry Style typing","text":"<p>202308282108</p> <p>Type : #Note Tags : [[Lambda Calculus]], [[Type Theory]]</p>"},{"location":"Curry-Style%20typing/#curry-style-typing","title":"Curry-Style typing","text":"<p>Curry first introduced types for Combinatory Logic, and that formulation was later adopted to Lambda Calculus </p> <p>In Curry's typing system, the terms are those of untyped lambda calculus, and the rules of typing selects valid terms if they are typable.</p> <p>Since each term does not carry a type of itself, we need an environment to give types to terms. Then we use the typablility rules on those and the terms that can be typed are considered valid.</p> <p>Given a denumerably infinite set of alphabet \\(U\\) whose member are called type variable and the set of simple types is represented by the strings generated by the grammar: $$ \\begin{matrix}\\Pi  &amp; ::= &amp; U &amp; | &amp; (\\Pi\\to\\Pi)\\end{matrix} $$ And the lambda Terms themselves are generated by the grammar $$ \\begin{matrix}\\Lambda &amp; ::= &amp; V &amp; | &amp; \\lambda V.\\Lambda &amp; | &amp; \\Lambda\\; \\Lambda\\end{matrix} $$</p> <p>^f4b8ba</p> <p>Where \\(V\\) is the set of variables.</p> <p>Along with these, since typing is not built into the syntax of the terms, there is a context \\(C\\) of variables and types</p> <p>$$ {x_{1}:\\tau_{1},\\dots,x_{n}:\\tau_{n}} $$ where \\(x_{i}\\) are the variables and \\(\\tau_{i}\\in \\Pi\\) Along with these, since typing is not built into the syntax of the </p> <p>We define the domain function for the context \\(\\Gamma =  \\{x_{1}:\\tau_{1},\\dots,x_{n}:\\tau_{n}\\}\\) such that \\(\\text{dom}(\\Gamma)=\\{x_{1},\\dots,x_{n}\\}\\)</p> <p>We also define a range function for context \\(\\Gamma =  \\{x_{1}:\\tau_{1},\\dots,x_{n}:\\tau_{n}\\}\\) such that \\(|\\Gamma|=\\{\\tau_{1},\\dots,\\tau_{n}\\}\\)</p> <p>We use the following typability relation \\(\\vdash\\) on \\(C\\times\\Gamma\\times\\Pi\\) </p> <p>And the set \\((\\vdash, \\Gamma,\\Pi)\\) is the calculus.</p> <p>Examples: $$ \\begin{align} \\mathbf{I} &amp;:= \\lambda x.x &amp;&amp;:\\sigma\\to\\sigma\\ \\mathbf{K} &amp;:= \\lambda xy.x &amp;&amp;:\\sigma\\to\\tau\\to\\sigma\\ \\mathbf{S} &amp;:= \\lambda xyz.xz(yz) &amp;&amp;:(\\sigma\\to\\tau\\to\\theta)\\to(\\sigma\\to\\tau)\\to\\sigma\\to\\theta \\end{align} $$</p> <p>^84a06b</p>"},{"location":"Curry-Style%20typing/#references","title":"References","text":"<p>Simply Typed Lambda Calculus Syntax</p>"},{"location":"Cyclotomic%20Fields/","title":"Cyclotomic Fields","text":"<p>202306012006</p> <p>Type : #Note Tags : [[Number Theory]]</p>"},{"location":"Cyclotomic%20Fields/#cyclotomic-fields","title":"Cyclotomic Fields","text":"<p><pre><code>title:\nLet $\\omega = e ^{2\\pi i/m}$, where $m &gt; 1$ is an integer. Then the field $\\mathbb{Q}(\\omega)$ is called the $m ^{th}$ cyclotomic field.\n</code></pre> It can be seen that: - For odd \\(m\\), the \\(m ^{th}\\) cyclotomic field is equal to the \\(2m ^{th}\\) one. (\\(e ^{2\\pi i/2m} = -e ^{2\\pi i(m+1)/2m} \\in \\mathbb{Q}[e ^{2\\pi i/m}]\\)). - The field is a finite extension of \\(\\mathbb{Q}\\).</p>"},{"location":"Cyclotomic%20Fields/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nAll $\\omega^{k}$, $1 \\le k \\le m$, $(m,k) = 1$, are exactly the conjugates of $\\omega$.\n</code></pre>"},{"location":"Cyclotomic%20Fields/#proof","title":"Proof:","text":"<p>It is easy to see that every conjugate of \\(\\omega\\) is also a root of \\(x^{m}-1\\) but not of \\(x ^{n}-1\\) for all \\(n &lt; m\\). But these are the numbers \\(\\omega^{k}\\) with \\(1 \\le k \\le m, \\ (m,k) = 1\\). Thus the set of conjugates is contained in the set \\(S = \\{ \\omega^{k} : 1 \\le k \\le m, (m,k) = 1 \\}\\).</p> <p>Consider the field homomorphism \\(\\sigma : \\mathbb{Q}(\\omega) \\to \\mathbb{Q}(\\omega)\\) which takes \\(\\omega\\) to \\(\\omega^{k}\\) and fixes \\(\\mathbb{Q}\\) for some \\(1 \\le k \\le m\\), \\((k,m) = 1\\). Now, \\(\\omega \\in \\sigma(\\mathbb{Q}(\\omega))\\). Why? Because \\((k,m) = 1\\), and so \\(um+vk =1\\) for some \\(u,v \\in \\mathbb{Z}\\). Thus, \\(\\omega = \\omega^{um+vk} = (\\omega^{k})^{v} = \\sigma(\\omega^{v})\\). So \\(\\sigma\\) is a linear map from a finite dimensional \\(\\mathbb{Q}\\)-vector space to itself, which is injective (since it's a non zero field homomorphism).</p> <p>This makes it a vector space isomorphism, and hence a field automorphism fixing \\(\\mathbb{Q}\\). Note that \\(\\mathbb{Q}(\\omega)\\) is a galois extension of \\(\\mathbb{Q}\\), since it is the splitting field of \\(x ^{m}- 1\\) over \\(\\mathbb{Q}\\). This makes \\(\\sigma\\) a member of \\(\\mathrm{Gal}(\\mathbb{Q}(\\omega) /\\mathbb{Q})\\), implying \\(\\varphi(m) = |S| \\le |\\mathrm{Gal}(\\mathbb{Q}(\\omega) /\\mathbb{Q})| = deg(\\omega) \\le \\varphi(m)\\). This means \\(|\\mathrm{Gal}(\\mathbb{Q}(\\omega) / \\mathbb{Q})| = \\varphi(m) = deg(\\omega)\\), and so \\(S =\\) the set of conjugates of \\(\\omega\\).</p>"},{"location":"Cyclotomic%20Fields/#corollary","title":"Corollary:","text":"<pre><code>title:\n$\\mathbb{Q}(\\omega)$ has degree $\\varphi(m)$ over $\\mathbb{Q}$.\n</code></pre>"},{"location":"Cyclotomic%20Fields/#corollary_1","title":"Corollary:","text":"<pre><code>title:\n$\\mathrm{Gal}(\\mathbb{Q}(\\omega) / \\mathbb{Q})$ is isomorphic to the multiplicative group of integers mod $m$. For each $k$, the corresponding automorphism sends $\\omega$ to $\\omega^{k}$ as in the proof.\n</code></pre>"},{"location":"Cyclotomic%20Fields/#note","title":"Note:","text":"<p>By the fundamental theorem of galois theory, we find that subfields of \\(\\mathbb{Q}(\\omega)\\) and subgroups of \\(\\mathbb{Z}_{m}^{*}\\) are in correspondence with each other. In particular, for \\(p ^{th}\\) cyclotomic fields, with \\(p\\) an odd prime, there is a unique quadratic field (since \\(\\mathbb{Z}_{p}^{*}\\) is of order \\(p-1\\) and cyclic). It turns out that this field is in fact \\(\\mathbb{Q}[\\sqrt[]{ \\pm p }]\\). The proof follows from Theorem 1 in Gauss sums</p>"},{"location":"Cyclotomic%20Fields/#corollary_2","title":"Corollary:","text":"<pre><code>title:\nLet $\\omega = e ^{2\\pi i/m}$. If $m$ is even, the only roots of $1$ in $\\mathbb{Q}[\\omega]$ are the $m ^{th}$ roots of unity. If $m$ is odd, the only ones are the $2m ^{th}$ roots of unity.\n</code></pre>"},{"location":"Cyclotomic%20Fields/#proof_1","title":"Proof:","text":"<p>The second part follows from the first and the fact that \\(2m ^{th}\\) cyclotomic field is equal to the \\(m ^{th}\\) one, if \\(m\\) is odd.</p> <p>To prove the first part, suppose \\(\\theta\\) is a primitive \\(k ^{th}\\) root of unity in \\(\\mathbb{Q}(\\omega)\\). Then there is a primitive \\(r ^{th}\\) root of unity in \\(\\mathbb{Q}(\\omega)\\) where \\(r = lcm(k,m)\\). Implying \\(\\phi(r) \\le \\phi(m)\\), but since \\(r = \\frac{km}{gcd(k,m)} = k'm\\) where \\(gcd(k',m) = 1\\), we get \\(\\phi(r) = \\phi(k')\\phi(m) \\le \\phi(m) \\implies \\phi(k') \\le 1 \\implies k' = 1 \\ \\text{or} \\ 2\\). But \\(k'\\) can't be 2 since it is coprime to \\(m\\), thus \\(k' = 1\\) giving \\(r = m\\). Hence, \\(k \\mid m\\) and \\(\\theta\\) is an \\(m ^{th}\\) root of unity.</p>"},{"location":"Cyclotomic%20Fields/#corollary_3","title":"Corollary:","text":"<pre><code>title:\nThe $m ^{th}$ cyclotomic fields, for $m$ even, are all distinct, and pairwise non isomorphic.\n</code></pre>"},{"location":"Cyclotomic%20Fields/#ring-of-integers-of-cyclotomic-fields","title":"Ring of integers of Cyclotomic fields","text":""},{"location":"Cyclotomic%20Fields/#theorem-2","title":"Theorem 2:","text":"<pre><code>title:\nLet $\\omega = e ^{2\\pi i/m}$, where $m = p ^{r},p$ a prime. Then $\\overline{\\mathbb{Z}}\\cap \\mathbb{Q}(\\omega) = \\mathbb{Z}[\\omega]$.\n</code></pre>"},{"location":"Cyclotomic%20Fields/#lemma-1","title":"Lemma 1:","text":"<pre><code>title:\nFor $m \\ge 3$,\n$\\mathbb{Z}[\\omega] = \\mathbb{Z}[1-\\omega]$ and $\\mathrm{disc}(\\omega) = \\mathrm{disc}(1-\\omega)$.\n</code></pre>"},{"location":"Cyclotomic%20Fields/#proof_2","title":"Proof:","text":"<p>\\(\\mathbb{Z}[\\omega] = \\mathbb{Z}[1-\\omega]\\) is clear. $$ \\mathrm{disc}(\\omega) = \\prod_{1 \\le r &lt; s\\le n} (\\alpha_{r}-\\alpha_{s})^{2} =\\prod_{1 \\le r &lt; s\\le n} (1-\\alpha_{r}-(1-\\alpha_{s}))^{2} = \\mathrm{disc}(1-\\omega)  $$</p>"},{"location":"Cyclotomic%20Fields/#lemma-2","title":"Lemma 2:","text":"<pre><code>title:\nFor $m = p ^{r}$, \n$$\n\\prod_{k}(1-\\omega^{k}) = p\n$$\nwhere the product is over all $1\\le k \\le m$ such that $(k,p) = 1$\n</code></pre>"},{"location":"Cyclotomic%20Fields/#proof_3","title":"Proof:","text":"<p>Set \\(f(x) = \\frac{x ^{p ^{r}}-1}{x ^{p ^{r-1}}-1} = 1+ x ^{p ^{r-1}} + \\dots + x ^{(p-1)p ^{r-1}}\\). Then all \\(\\omega^{k}\\) (\\(k\\) as above) are roots of \\(f\\). Thus \\(f(x) = \\prod_{k}(x-\\omega^{k})\\) since there are exactly \\(\\phi(p ^{r})\\) values of \\(k\\). Now set \\(x = 1\\).</p>"},{"location":"Cyclotomic%20Fields/#proof-of-theorem","title":"Proof of Theorem:","text":"<p>By theorem 1 of Number Rings, every \\(\\alpha \\in R\\) can be expressed in the form $$ \\alpha = \\frac{m_{1} + m_{2}(1-\\omega) + \\dots m_{n}(1-\\omega)^{n-1}}{d} $$ where \\(n = \\phi(p ^{r})\\), and \\(d = \\mathrm{disc}(\\omega) = \\mathrm{disc}(1-\\omega)\\). We know that \\(\\mathrm{disc}(\\omega) \\mid m ^{\\phi(m)}\\) (Discriminant of an n-tuple/ related problems/ problem 2), thus \\(\\mathrm{disc}(1-\\omega)\\) is a power of \\(p\\). Now suppose there is an \\(\\alpha \\in R\\) such that some \\(m_{i}\\) is not divisible by \\(d\\), then \\(R\\) contains $$ \\beta = \\frac{m_{i}(1-\\omega)^{i-1} + \\dots + m_{n}(1-\\omega)^{n-1}}{p} $$ where \\(p \\nmid m_{i}\\) (because, we can take \\(m_{i}\\) to be the one with the least power of \\(p\\), then cancel out all prime powers with \\(d\\) until \\(p \\nmid m_{i}\\) and multiply everything by a suitable power of \\(p\\) until only \\(p\\) remains in the denominator, and then remove terms of the form \\(\\mathbb{Z}(1-\\omega)^{k}\\)).</p> <p>Lemma 2 gives \\(\\frac{p}{(1-\\omega)^{n}} \\in \\mathbb{Z}[\\omega]\\). Then \\(\\frac{p}{(1-\\omega)^{i}} \\in \\mathbb{Z}[\\omega]\\) and hence \\(\\frac{\\beta p}{(1-\\omega)^{i}} \\in R\\). Implying \\(\\frac{m_{i}}{(1-\\omega)} \\in R\\). It follows that \\(N(1-\\omega) | N(m_{i}) \\implies p | m_{i}^{\\phi(m)}\\) a contradiction.</p> <p>Thus \\(R = \\mathbb{Z}[1-\\omega] = \\mathbb{Z}[\\omega]\\).</p>"},{"location":"Cyclotomic%20Fields/#theorem-3","title":"Theorem 3:","text":"<pre><code>title:\nLet $K = \\mathbb{Q}(\\omega), \\omega = e ^{2\\pi i/m}, R = \\overline{\\mathbb{Z}}\\cap K$. Then $R = \\mathbb{Z}[\\omega]$.\n</code></pre>"},{"location":"Cyclotomic%20Fields/#proof_4","title":"Proof:","text":"<p>We know this holds for \\(m = p ^{r}\\). We induct on the number of distinct prime factors. Let \\(m = p_{1} ^{r_{1}}p_{2}^{r_{2}}\\). Let \\(\\omega_{1} = e ^{2\\pi i/p_{1}^{r_{1}}}\\) and \\(\\omega_{2} = e ^{2\\pi i/p_{2}^{r_{2}}}\\), \\(\\omega = e ^{2\\pi i/m}\\) Then \\(\\mathbb{Q}(\\omega) = \\mathbb{Q}(\\omega_{1})\\mathbb{Q}(\\omega_{2})\\) and \\(\\mathbb{Z}[\\omega] = \\mathbb{Z}[\\omega_{1}]\\mathbb{Z}[\\omega_{2}]\\). Since \\(d = \\mathrm{gcd}(\\mathrm{disc}(\\mathbb{Z}[\\omega_{1}]),\\mathrm{disc}(\\mathbb{Z}[\\omega_{2}])) = \\mathrm{gcd}(p_{1}^{t_{1}},p_{2}^{t_{2}}) = 1\\), therefore by corollary 1 to theorem 3 in Number Rings, we get \\(\\overline{\\mathbb{Z}} \\cap \\mathbb{Q}(\\omega) = \\mathbb{Z}[\\omega_{1}]\\mathbb{Z}[\\omega_{2}] = \\mathbb{Z}[\\omega]\\).</p> <p>We can write down a similar argument for the induction step.</p>"},{"location":"Cyclotomic%20Fields/#references","title":"References","text":"<p>Number Field Galois Extensions The fundamental theorem of Galois Theory Gauss sums Number Rings Discriminant of an n-tuple</p>"},{"location":"D-Timed%20Automata%20to%20Timed%20Automata%20Construction/","title":"D Timed Automata to Timed Automata Construction","text":"<p>202310282042</p> <p>tags : Timed Automata</p>","tags":["Example"]},{"location":"D-Timed%20Automata%20to%20Timed%20Automata%20Construction/#d-timed-automata-to-timed-automata-construction","title":"D-Timed Automata to Timed Automata Construction","text":"<p>[!hint] Idea We use extra states to represent if the diagonal constraints are true or false, in each transition we check if the value of the diagonal constraints change or not. </p> <p>Consider  a d-timed automaton with just \\(1\\) diagonal constraint. Let \\(\\mathcal A=(Q,\\Sigma,X,T,q_{0},F)\\) be that d-timed automaton  We construct \\(\\mathcal A'\\) where \\(Q' = Q \\times \\{\\bot,\\top\\}\\) where \\((q,\\top)\\) represents the automata is in state \\(q\\) and the clock \\(x,y\\) are in a configuration where they do satisfy the diagonal guard. we also have  - \\(\\Sigma'=\\Sigma\\) - \\(X'=X\\) - \\(q_{0}'=(q_{0},\\top)\\) if \\(c\\ge 0\\) otherwise its \\((q_0,\\bot)\\) - \\(F'=F\\times\\{\\top,\\bot\\}\\) </p> <p>For the transitions we have the following cases - If \\(x,y\\not\\in R\\) then we have the following transitions instead of the transition with diagonal guard $$ (q, 0)\\xrightarrow[R]{\\quad g\\quad}(q',0)\\quad\\quad\\text{and}\\quad\\quad(q, 1)\\xrightarrow[R]{\\quad g\\quad}(q',1) $$   because the value of \\(x-y\\) does not change until the clocks are reset, so the guards will remain satisfied or unsatisfied and not switch. - If \\(x, y \\in R\\) then we have  $$ \\begin{align} (q,b)\\xrightarrow[R]{\\quad q\\quad}(q',1) \\text{ if } 0\\le c\\ (q,b)\\xrightarrow[R]{\\quad q\\quad}(q',0) \\text{ if } 0&gt; c\\ \\end{align} $$ - If \\(x\\in R\\) and \\(y\\notin R\\) $$ (q, b)\\xrightarrow[R]{\\quad g\\; \\land\\; -y \\leq c\\quad}(q',1)\\quad\\quad\\text{and}\\quad\\quad(q, b)\\xrightarrow[R]{\\quad g\\;\\land\\; -y&gt;c\\quad}(q',0) $$ - If \\(x\\notin R\\) and \\(y\\in R\\) $$ (q, b)\\xrightarrow[R]{\\quad g\\; \\land\\; x \\leq c\\quad}(q',1)\\quad\\quad\\text{and}\\quad\\quad(q, b)\\xrightarrow[R]{\\quad g\\;\\land\\; x&gt;c\\quad}(q',0) $$  Not he construction of \\(\\mathcal A'\\) is complete, we successfully removed one diagonal transition.</p> <p>[!summary] Generalization For any arbitrary d-timed automaton we can repeat this procedure of every clock. Hence every d-timed automata can be converted to a timed automata.</p>","tags":["Example"]},{"location":"D-Timed%20Automata%20to%20Timed%20Automata%20Construction/#related","title":"Related","text":"<ul> <li>Timed Automata with Diagonal Constraints</li> </ul>","tags":["Example"]},{"location":"DTIME%28f%29%20Complexity%20Class/","title":"DTIME(f) Complexity Class","text":"<p>202301051201</p> <p>Type : #Note Tags : [[Complexity Theory]]</p>"},{"location":"DTIME%28f%29%20Complexity%20Class/#dtimef-complexity-class","title":"DTIME(f) Complexity Class","text":""},{"location":"DTIME%28f%29%20Complexity%20Class/#dtimef","title":"DTIME(f)","text":"<pre><code>title:\nClass of decision problems that can be decided by a [ Deterministic Turing Machine](&lt;./Turing Machines.md&gt;) in $O(f(n))$ time.\n</code></pre> \\[ P=\\bigcup_{c\\in\\mathbb N}DTIME(n^c) \\]"},{"location":"DTIME%28f%29%20Complexity%20Class/#examples","title":"Examples","text":""},{"location":"DTIME%28f%29%20Complexity%20Class/#related-problems","title":"Related Problems","text":""},{"location":"DTIME%28f%29%20Complexity%20Class/#references","title":"References","text":"<p>Complexity Classes</p>"},{"location":"Decidability%20of%20Presburger%20Arithmetic/","title":"Decidability of Presburger Arithmetic","text":"<p>202311282111</p> <p>Tags : [[Logic]]</p>","tags":["Note"]},{"location":"Decidability%20of%20Presburger%20Arithmetic/#decidability-of-presburger-arithmetic","title":"Decidability of Presburger Arithmetic","text":"<p>[!note] Theorem The theory of structure \\(\\mathfrak N_{\\mathcal A}=\\{\\mathbb N; 0,S,&lt;, +\\}\\) is decidable.</p> <p>The proof is based on quantifier elimination. \\(\\mathfrak N_{A}\\) itself does not admit quantifier elimination so we look at the following arithmetic $$ \\mathfrak N^\\equiv = (\\mathbb N; 0, S,&lt;, +,\\equiv_{2},\\equiv_{3},\\dots) $$ Where \\(\\equiv_{k}\\) represents the binary relation of congruence modulo \\(k\\). Every statement in \\(\\mathfrak N_A\\) is also a statement in \\(\\mathfrak N^\\equiv\\), and the latter admits quantifier elimination.</p> <p>[!hint] We know that \\(\\mathfrak N^\\equiv\\) is a superset of \\(\\mathfrak N_A\\) so every formula in \\(\\mathfrak N_A\\) is also a formula in \\(\\mathfrak N^\\equiv\\).</p>","tags":["Note"]},{"location":"Decidability%20of%20Presburger%20Arithmetic/#structure","title":"Structure","text":"","tags":["Note"]},{"location":"Decidability%20of%20Presburger%20Arithmetic/#terms","title":"Terms","text":"<p>We assume all terms to be of the form  $$ S^n(\\mathbf{0})+ n_{1}x_{1} +\\dots + n_{k}x_{k} $$ This is obvious from the axioms - \\(S(\\varphi + \\psi)\\to S(\\varphi) + \\psi\\) - \\(S(\\varphi)+\\psi\\to\\varphi+S(\\psi)\\)</p>","tags":["Note"]},{"location":"Decidability%20of%20Presburger%20Arithmetic/#formulae","title":"Formulae","text":"<p>All formule are of the disjunctions of \\(\\exists y(\\beta_{1}\\land \\beta_{2}\\dots \\beta_{n})\\) where \\(\\beta_i\\) is an atomic formula.</p>","tags":["Note"]},{"location":"Decidability%20of%20Presburger%20Arithmetic/#quantifier-elimination-algorithm","title":"Quantifier Elimination Algorithm","text":"","tags":["Note"]},{"location":"Decidability%20of%20Presburger%20Arithmetic/#plan","title":"Plan","text":"<ul> <li>We remove negations</li> <li>Then we simplify the definitions by removing coefficients of the variable</li> <li>We deal with the special case where congruence is one of the atomic formulas</li> <li>We deal with the general case, without congruences</li> </ul>","tags":["Note"]},{"location":"Decidability%20of%20Presburger%20Arithmetic/#removing-negations","title":"Removing Negations","text":"<ul> <li>\\(\\lnot(t_1 \\equiv t_2)\\) becomes \\((t_{1}&lt;t_{2})\\lor(t_{2}&lt;t_{1})\\), something similar for \\(\\lnot (t_{1}&lt;t_{2})\\) and \\(\\lnot (t_{2}&lt;t_{1})\\)</li> <li>\\(\\lnot(t_{1}\\equiv_{k}t_{2})\\) becomes \\((t_{1} \\equiv_{k} t_{2}+S(\\mathbf{0} ))\\land(t_{1} \\equiv_{k} t_{2}+S^2(\\mathbf{0} ))\\land\\dots \\land(t_{1} \\equiv_{k} t_{2}+S^{k-1}(\\mathbf{0} ))\\) </li> </ul> <p>we then regroup them into DNF.</p> <p>Now we have a disjunction of formulas of the form \\(\\exists y (\\alpha_1\\land\\dots\\land \\alpha_m)\\) where each \\(\\alpha_i\\) has one the following forms(the second way of writing is just syntactic sugar) - \\(ny+t=u\\)  - \\(ny+t\\equiv_m u\\) - \\(ny+t&lt;u\\) - \\(u&lt;ny+t\\) which can be written as  - \\(ny=u-t\\) - \\(ny\\equiv_{k}u-t\\) - \\(ny&lt;u-t\\) - \\(u-t&lt;ny\\)</p>","tags":["Note"]},{"location":"Decidability%20of%20Presburger%20Arithmetic/#uniformizing-coefficients-of-y","title":"Uniformizing Coefficients of \\(y\\)","text":"<p>Say one of the parts of the formula is  $$ \\exists y(w&lt;4y\\land 2y&lt;u\\land 3y&lt;v\\land y\\equiv_{3}t) $$ We use the fact that - \\(a&lt;b\\) is true iff \\(ka&lt;kb\\)  - \\(a\\equiv_{k}b\\) is true iff \\(ma\\equiv_{mk}mb\\) and we get  $$ \\exists y(3w&lt;12y \\land 12y&lt;6u \\land  12y &lt; 4v \\land 12y \\equiv_{36} 12t) $$ We now simplify the above formula by saying that there is some number that can be put in place of \\(12y\\) and the number is divisible by \\(12\\). $$ \\exists x(3w&lt;x \\land x &lt; 6u \\land x&lt;4v \\land x\\equiv_{36} 12t\\land x\\equiv_{12}\\mathbf{0}) $$</p>","tags":["Note"]},{"location":"Decidability%20of%20Presburger%20Arithmetic/#dealing-with-equality","title":"Dealing with Equality","text":"<p>This is a special case that can be dealt easily If one of the atomic formulas is an equality of the form \\(x+y\\equiv u\\) then we can replace \\(\\exists x \\theta\\) with \\(\\theta[x := u-t] \\land u &gt; t\\) </p> <p>We assume the formula does not contain \\(\\equiv\\) for the last section.</p>","tags":["Note"]},{"location":"Decidability%20of%20Presburger%20Arithmetic/#quantifier-elimination-where-equiv-is-not-in-the-formula","title":"Quantifier elimination where \\(\\equiv\\) is not in the formula","text":"<p>Since we have eliminated negation, and assuming our formulas do not contain \\(\\equiv\\), we assume that our formula is the disjunction of subformulas of the form  $$ \\exists x \\left[   \\left(\\bigwedge_{i\\in[1..l]} r_{i}-s_{i}&lt;x\\right)\\land   \\left(\\bigwedge_{i\\in[1..m]} x &lt; t_{i}-u_{i}\\right) \\land   \\left(\\bigwedge_{i\\in[1..n]} x\\equiv_{m_{i}} v_{i}-w_{i}\\right) \\right] $$</p>","tags":["Note"]},{"location":"Decidability%20of%20Presburger%20Arithmetic/#without-congruencies","title":"Without Congruencies","text":"<p>[!tip] Intuition If there are no congruencies The first chunk is a bunch of lower bounds The second chunk is a bunch of upper bounds So we just need to check if the smallest upper bound is atleast 2 more the largest lower bound, But since we do not have a max operator, we just compare every upper bound-lower bound pair. We also need to make sure that upper bounds are at least 1</p> <p>The first two chunks together can be converted to  $$ \\bigwedge_{i&lt;k}\\bigwedge_{j&lt;l}\\Big(\\mathbf{S}(r_{j}-s_{j})&lt;t_{i}-u_{i}\\Big)\\land\\bigwedge_{i&lt;k}\\Big(\\mathbf{0}&lt;t_{i}-u_{i}\\Big) $$</p>","tags":["Note"]},{"location":"Decidability%20of%20Presburger%20Arithmetic/#with-congruencies","title":"With Congruencies","text":"<p>Congruencies force us to manually check for existance of such an \\(x\\), but they also restrict the search space to modulo classes, hence making it finite.</p> <p>Given all \\(m\\) such that there is an atomic formula like \\(x\\equiv_{m}v_{i}-w_{i}\\), let \\(M\\) be the least common multiple of all such \\(m\\).</p> <p>Then all modulo operators are together periodic with periodicity \\(M\\), we should just check the first \\(M\\) numbers starting after the biggest lower bound, which we again test by checking \\(M\\) numbers after each lower bound, so we get the following</p> \\[ \\begin{align} \\bigvee_{j&lt;l}\\bigvee_{1\\leq q\\leq M}\\Big[\\quad   &amp;\\bigwedge_{i&lt;l} r_{i}-s_{i} &lt; (r_{j}-s_{j}+\\mathbf{S}^q\\mathbf{0})\\quad \\land\\\\   &amp;\\bigwedge_{i&lt;m} (r_{j}-s_{j}+\\mathbf{S}^q \\mathbf{0}) &lt; t_{i}-u_{i}\\quad \\land \\\\   &amp;\\bigwedge_{i&lt;n} (r_{j}-s_{j}+\\mathbf{S}^q\\mathbf{0})\\equiv_{k_{i}} v_{i}-w_{i}\\quad \\Big] \\end{align} \\]","tags":["Note"]},{"location":"Decidability%20of%20Presburger%20Arithmetic/#references","title":"References","text":"<p>First Order Logic</p>","tags":["Note"]},{"location":"Dedekind%20Domains/","title":"Dedekind Domains","text":"<p>202306050906</p> <p>Type : #Note Tags : [[Number Theory]] [[Algebra]]</p>"},{"location":"Dedekind%20Domains/#dedekind-domains","title":"Dedekind Domains","text":"<p><pre><code>title:\nA **Dedekind Domain** is an integral domain $R$ such that \n1. Every ideal is finitely generated.\n2. Every non zero prime ideal is maximal.\n3. $R$ is _integrally closed_ in its field of fractions $$\nK = \\left\\{  \\frac{\\alpha}{\\beta} : \\alpha,\\beta \\in R, \\beta \\neq 0  \\right\\}\n$$\n</code></pre> (3) means that if \\(\\frac{\\alpha}{\\beta} \\in K\\) is a root of a monic polynomial with coefficients in \\(R\\), then \\(\\frac{\\alpha}{\\beta} \\in R\\). (1) is equivalent to :  (1') Every increasing sequence of ideals is eventually constant: $$ I_{1} \\subset I_{2} \\subset \\dots  $$ implies that all \\(I_{n}\\) are equal for sufficiently large \\(n\\). (1'') Every non empty set \\(S\\) of ideals has a (not necessarily unique) maximal element.</p> <p>A ring satisfying the equivalent conditions \\((1),(1'),(1'')\\) is called a Noetherian Ring.</p>"},{"location":"Dedekind%20Domains/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nEvery number ring is a Dedekind Domain.\n</code></pre>"},{"location":"Dedekind%20Domains/#proof","title":"Proof:","text":"<p>See Number Rings theorem 5.</p>"},{"location":"Dedekind%20Domains/#theorem-2","title":"Theorem 2:","text":"<p><pre><code>title:\nLet $I$ be any ideal of a dedekind domain $R$, then there is an ideal $J$ such that $IJ$ is principal.\n</code></pre> Let \\(\\alpha\\) be any nonzero member of \\(I\\), then define \\(J := \\{ \\beta \\in R  : \\beta I \\subset (\\alpha)\\}\\), note that \\(\\alpha \\in J\\) and that it is an ideal. Thus \\(IJ \\subset (\\alpha)\\). We will show that equality holds.</p>"},{"location":"Dedekind%20Domains/#lemma-1","title":"Lemma 1:","text":"<pre><code>title:\nIn a dedekind domain, every ideal contains a product of prime ideals.\n</code></pre>"},{"location":"Dedekind%20Domains/#proof_1","title":"Proof:","text":"<p>Suppose not, there consider the non empty set of ideals that do not contain a product of prime ideals, call it \\(S\\). This has a maximal element by condition (1'') of Dedekind Domains, call it \\(M\\). Then \\(M\\) can't be prime hence there are \\(r,s \\in R - M\\) such that \\(rs \\in M\\). \\(M + (r)\\) and \\(M+(s)\\) are bigger than \\(M\\) hence are not contained in \\(S\\), hence contain a product of prime ideals.  But then so does \\((M+(r))(M+(s)) \\subset M\\), this is a contradiction.</p>"},{"location":"Dedekind%20Domains/#lemma-2","title":"Lemma 2:","text":"<pre><code>title:\nLet $A$ be a proper ideal in a dedekind domain $R$ with field of fractions $K$. Then there is an element $\\gamma \\in K - R$ such that $\\gamma A \\subset R$.\n</code></pre>"},{"location":"Dedekind%20Domains/#proof_2","title":"Proof:","text":"<p>Let \\(\\alpha \\in A\\) be any non zero element, then \\((\\alpha)\\) contains a product of prime ideals. Let \\((\\alpha) \\supset P_{1}P_{2}\\dots P_{r}\\) where \\(P_{i}\\)'s are prime ideals and \\(r\\) is the least possible. Now \\(A\\) is contained in a maximal ideal, call it \\(P\\), it is also prime. \\(P \\supset A \\supset (\\alpha) \\supset P_{1}P_{2}\\dots P_{r}\\). Now if none of the prime ideals \\(P_{i}\\), are contained in \\(P\\), then there are elements \\(a_{i} \\in P_{i}-P\\) such that \\(\\prod_{i}a_{i} \\in P\\), this is a contradiction to the fact that \\(P\\) is prime. Thus WLOG, \\(P \\supset P_{1}\\), and since both are maximal, \\(P = P_{1}\\).</p> <p>Now \\(\\exists \\beta \\in P_{2}P_{3}\\dots P_{r} - (\\alpha)\\) (since \\((\\alpha)\\) doesn't contain the product \\(P_{2}P_{3}\\dots P_{r}\\)). Let \\(\\gamma = \\frac{\\beta}{\\alpha}\\), then \\(\\gamma A \\subset \\gamma P \\subset \\frac{1}{\\alpha}PP_{2}\\dots P_{r} \\subset \\frac{1}{\\alpha}(\\alpha)\\).</p> <p>Returning to the proof of the theorem, let \\(A = \\frac{1}{\\alpha}IJ\\), we want to show \\(A = R\\).  Suppose not, then \\(A\\) is a proper ideal, and so there exists a \\(\\gamma \\in K - R\\), such that \\(\\gamma A\\subset R\\). This implies \\(\\gamma IJ \\subset (\\alpha) \\implies \\gamma J \\subset J\\). \\(J\\) is finitely generated, let \\(\\alpha_{1},\\dots\\alpha_{m}\\) be the generators,  then $$ \\begin{pmatrix} \\gamma\\alpha_{1}  \\ \\vdots \\ \\gamma\\alpha_{m} \\end{pmatrix} = M \\begin{pmatrix} \\alpha_{1} \\ \\vdots \\ \\alpha_{m} \\end{pmatrix} $$ where \\(M \\in M_{m}(R)\\), this gives \\(\\det(M-\\gamma I) = 0\\), hence \\(\\gamma\\) satisfies a monic poly over \\(R\\), hence \\(\\gamma \\in R\\). This is a contradiction.</p>"},{"location":"Dedekind%20Domains/#corollary-1","title":"Corollary 1:","text":"<pre><code>title:\nThe ideal classes in a dedekind domain form a group.\n</code></pre>"},{"location":"Dedekind%20Domains/#note","title":"NOTE:","text":"<p>We can put an equivalence relation on the set of ideals of a commutative ring \\(R\\) as follows: \\(I \\sim J\\) iff \\(\\exists \\ \\alpha,\\beta \\in R\\) s.t. \\(\\alpha I = \\beta J\\). We can multiply ideal classes by taking representatives from each class. This multiplication is associative and commutative since the ring is commutative. The identity is the class of all principal ideals. This forms a group in the case of dedekind domains since we have inverses.</p>"},{"location":"Dedekind%20Domains/#corollary-2-cancellation-law","title":"Corollary 2 (Cancellation Law):","text":"<pre><code>title:\nIf $A,B,C$ are ideals in a dedekind domain, and $AB = AC$, then $B = C$.\n</code></pre>"},{"location":"Dedekind%20Domains/#proof_3","title":"Proof:","text":"<p>There is an ideal \\(I\\) such that \\(IA = (\\alpha)\\) for some \\(\\alpha \\in R\\). Thus, \\(\\alpha B = \\alpha C\\) from which we get \\(B = C\\).</p>"},{"location":"Dedekind%20Domains/#corollary-3","title":"Corollary 3:","text":"<pre><code>title:\nIf $A,B$ are ideals in a dedekind domain $R$, then $A \\mid B$ iff $A \\supset B$.\n</code></pre>"},{"location":"Dedekind%20Domains/#proof_4","title":"Proof:","text":"<p>If \\(A \\mid B\\), then \\(B = AC \\subset AR  = A\\). If \\(B \\subset A\\), then there is an ideal \\(J\\) such that \\(AJ = (\\alpha)\\). Then \\(B = A(\\frac{1}{\\alpha}JB)\\), and note that \\(\\frac{1}{\\alpha}JB\\) is an ideal in \\(R\\).</p>"},{"location":"Dedekind%20Domains/#theorem-3","title":"Theorem 3:","text":"<pre><code>title:\nEvery ideal in a dedekind domain $R$ is uniquely representable as a product of prime ideals.\n</code></pre>"},{"location":"Dedekind%20Domains/#proof_5","title":"Proof:","text":"<p>Consider the set of all proper ideal not representable in such a form, call it \\(S\\). Now \\(S\\) contains a maximal (in \\(S\\)) element \\(M\\). \\(M\\) is not a maximal ideal, hence it is properly contained in some maximal ideal \\(P\\). Thus by corollary 3, \\(M = PI\\) for some \\(I\\). Now \\(I \\supset M\\) again by corollary 3 above, this containment is strict since otherwise \\(I = M \\implies PI = PM \\implies RM = PM \\implies R = P\\), contradiction. Thus, \\(M = PI\\), both of which do not belong to \\(S\\), thus \\(M\\) doesn't belong to \\(S\\), thus \\(S\\) is empty.</p> <p>Now let \\(I = P_{1}P_{2}\\dots P_{n} = Q_{1}\\dots Q_{m}\\) be two representations of an ideal \\(I\\) as a product of prime ideals. Then \\(P_{1} \\supset Q_{1}\\dots Q_{m}\\), then it contains one of them, WLOG \\(P_{1} \\supset Q_{1} \\implies P_{1} = Q_{1} \\implies P_{2}\\dots P_{n} = Q_{2}\\dots Q_{m}\\). Continuing this way, we get \\(P_{i} = Q_{i}\\) and \\(n = m\\).</p>"},{"location":"Dedekind%20Domains/#definition","title":"Definition","text":"<pre><code>title:\nWe can define the $\\mathrm{gcd}$ and $\\mathrm{lcm}$ of ideals in a DD with the help of their prime decomposition in the obvious way. We also have $$\n\\begin{align}\n\\mathrm{gcd}(I,J) = I + J \\\\\n\\mathrm{lcm}(I,J) = I \\cap J\n\\end{align}\n$$\n</code></pre>"},{"location":"Dedekind%20Domains/#theorem-4","title":"Theorem 4:","text":"<pre><code>title:\nLet $I$ be an ideal in a DD $R$, and let $\\alpha$ be a non zero element of $I$. Then there exists $\\beta \\in I$ such that $(\\alpha,\\beta) = I$.\n</code></pre>"},{"location":"Dedekind%20Domains/#proof_6","title":"Proof:","text":"<p>It is enough to construct a \\(\\beta \\in R\\) such that \\(I = \\mathrm{gcd}((\\alpha),(\\beta))\\), then \\(\\beta \\in I\\) automatically.</p> <p>Let \\((\\alpha) = P_{1}^{n_{1}}\\dots P_{r}^{n_{r}}\\) where \\(P_{i}\\) are prime ideals. Let \\(Q_{1},\\dots,Q_{m}\\) denote the other prime ideals dividing \\((\\alpha)\\). Then we need to construct \\(\\beta\\) such that \\(Q_{j} \\nmid (\\beta)\\) and \\(P_{i}^{n_{i}}\\) is the exact power of \\(P_{i}\\) that divides \\((\\beta)\\). This can be done by Chinese remainder theorem. Choose \\(\\beta_{i} \\in P_{i}^{n_{i}} - P_{i}^{n_{i}+1}\\), and solve the system $$ \\begin{align} \\beta \\equiv \\beta_{i}  (\\mathrm{mod } P_{i} ^{n_{i}+1}) \\ \\beta \\equiv 1  (\\mathrm{mo d}  Q_{i}) \\end{align} $$ noting that each of the ideals \\(P_{i}^{n_{i}+1}\\) and \\(Q_{j}\\) are coprime to each other.</p>"},{"location":"Dedekind%20Domains/#theorem-5","title":"Theorem 5:","text":"<pre><code>title:\nA DD is a UFD iff it is a PID.\n</code></pre>"},{"location":"Dedekind%20Domains/#proof_7","title":"Proof:","text":"<p>We know \\(PID \\implies UFD\\). Suppose the DD is a UFD, then take any ideal \\(I\\) in \\(R\\). It divides some principal ideal \\((\\alpha)\\) by theorem 2. \\(\\alpha = p_{1}p_{2}\\dots p_{n}\\) is a product of primes in \\(R\\), hence \\((\\alpha) = (p_{1})(p_{2})\\dots(p_{n})\\). \\(I \\mid (p_{1})\\dots(p_{n})\\), now by uniqueness of prime factorisation of ideals, \\(I\\) is a product of principal ideals and hence is principal.</p>"},{"location":"Dedekind%20Domains/#lemma-3","title":"Lemma 3:","text":"<pre><code>title:\nLet $A$ and $B$ be nonzero ideals in a DD $R$ with $B \\subset A$ and $A \\neq R$.\nThen there exists $\\gamma \\in K$ such that $\\gamma B \\subset R$ but $\\gamma B \\nsubseteq A$.\n</code></pre>"},{"location":"Dedekind%20Domains/#proof_8","title":"Proof:","text":"<p>We know there is an ideal \\(C\\) such that \\(BC = (\\alpha)\\) for some \\(\\alpha\\). Then \\(BC \\nsubseteq \\alpha A\\); fix any \\(\\beta \\in C\\) such that \\(B \\beta \\nsubseteq \\alpha A\\), then \\(\\gamma = \\frac{\\beta}{\\alpha}\\) works.</p>"},{"location":"Dedekind%20Domains/#references","title":"References","text":"<p>Number Rings [[Principal Ideal Domain]]</p>"},{"location":"Deformation%20Retracts/","title":"Deformation Retracts","text":"<p>202304041604</p> <p>Type : #Note Tags :[[Topology]]</p>"},{"location":"Deformation%20Retracts/#deformation-retracts","title":"Deformation Retracts","text":"<pre><code>title:Idea\n1. $S ^{1} \\to \\{ p \\}$, $p \\in S ^{1}$ is a retraction\n2. $(x,y,z) \\mapsto (x,y,|z|)$ is a retraction $S ^{2} \\to S ^{2}_{+}$\n3. $x \\mapsto \\frac{x}{|x|}$ on $\\mathbb{R}^{2} \\setminus \\{ 0 \\} \\to S ^{1}$\n4. The map that shrinks a mobius strip to its central circle $S ^{1}$\n   $X = I \\times I / (0,y) \\sim (1,1-y)$ is a square with opposite sides twisted and identified. \n   $H : X \\times I \\to X$\n   $H((x,y),t) = (x, \\frac{1}{2}t + y(1-t))$ is the homotopy from $id_{X}$ to the retraction.\n\n- In the last two examples, $id_{X}$ can be deformed continuously in $X$ to get the retraction map, contrary to the first two examples.\n</code></pre>"},{"location":"Deformation%20Retracts/#definition","title":"Definition:","text":"<p><pre><code>title:\nLet $A \\subset X$. $A$ is a **deformation retract** of $X$ if the map $id_X$ is homotopic to a map that carries all of $X$ into $A$, such that each point of $A$ remains fixed during the homotopy.\n</code></pre> - \\(H : X \\times I \\to X\\) s.t. \\(H(x,0) = x\\), \\(H(x,1) \\in A\\) \\(\\forall \\ x \\in A\\), and \\(H(a,t) = a\\) for all \\(a \\in A\\) \\(\\forall \\ t \\in [0,1]\\). - \\(H\\) is called a deformation retraction of \\(X\\) onto \\(A\\), \\(r : X \\to A\\), \\(r(x):= H(x,1)\\) is a retraction.</p>"},{"location":"Deformation%20Retracts/#example","title":"Example:","text":"<ol> <li>\\(\\mathbb{R}^{n}\\setminus \\{ 0 \\} \\to S ^{n-1}\\) \\(H(x,t) = t \\frac{x}{|x|} + (1-t)x\\) </li> </ol>"},{"location":"Deformation%20Retracts/#lemma-1","title":"Lemma 1:","text":"<pre><code>title:\nLet $h,k : (X,x_{0}) \\to (Y,y_{0})$ be cts s.t. $h,k$ are homotopic to each other under $H$, and $x_{0} \\to y_{0}$ throughout the homotopy,\ni.e, the image of $x_{0}$ = $y_{0}$ for each function $H|_{X \\times \\{ t \\}}$. Then $h_{*}$ and $k_{*}$ are equal to each other.\n</code></pre>"},{"location":"Deformation%20Retracts/#proof","title":"Proof:","text":"<p>To show: \\(h_{*}([f]) = k_{*}([f])\\) \\(\\iff [h \\circ f] = [k \\circ f] \\iff h \\circ f \\simeq_{p} k \\circ f\\) $$ I \\times I \\xrightarrow{f \\times id} X \\times I \\xrightarrow{H} Y $$ So, \\(H \\circ (f \\times id)\\) is a homotopy between \\(h \\circ f\\) and \\(k \\circ f\\). Path homotopy because \\(f\\) is a loop at \\(x_{0}\\), and \\(H\\) maps \\(\\{ x_{0} \\} \\times I \\to \\{ y_{0} \\}\\).</p>"},{"location":"Deformation%20Retracts/#lemma-2","title":"Lemma 2:","text":"<pre><code>title:\n$h,k : X \\to Y$ cts map. $h,k$ are homotopic under homotopy $H$, say $h(x_0) = y_0$, $k(x_0) = y_1$.\n\nThen $\\alpha(t) = H(x_0,t)$ is the path defined by $H$ from $y_0$ to $y_1$ and $\\hat{\\alpha} : \\Pi_1(Y,y_0) \\to \\Pi_1(Y,y_1)$ then $k_* = \\hat{\\alpha}(h_*)$\n</code></pre>"},{"location":"Deformation%20Retracts/#proof_1","title":"Proof:","text":"<p>Consider \\(I \\times I \\xrightarrow{F} X \\times I\\) by concatenating:  1. \\((x_{0},1) \\to (x_{0},t)\\) path 2. loop \\(f\\) in \\(X \\times \\{ t \\}\\) 3. \\((x_{0},t) \\to (x_{0},1)\\) for each \\(t \\in I\\). $$ I \\times I \\xrightarrow{F} X \\times I \\xrightarrow{H} Y $$ \\(H \\circ F\\) is a path homotopy from  $$ \\bar{\\alpha}(h \\circ f) \\alpha      \\text{to}      e (k \\circ f)e $$</p>"},{"location":"Deformation%20Retracts/#lemma-3","title":"Lemma 3:","text":"<pre><code>title:\nIf $r : X \\to A$ is a deformation retract, $i : A \\hookrightarrow X$ is the inclusion then $i_* : \\Pi_1(A,x_0) \\to \\Pi_1(X,x_0)$ is an isomorphism.\n</code></pre>"},{"location":"Deformation%20Retracts/#proof_2","title":"Proof:","text":"<p>Let \\(H\\) be the homotopy defined by the deformation retract, so that \\(H(x,0) = x\\), \\(H(x,1) \\in A\\), \\(H(a,t) = a\\), for all \\(a \\in A\\). Then if \\(r : X \\to A\\) is the retraction and \\(i : A \\to X\\) is the inclusion. Then \\(r_{*}\\circ i_{*} = id_{\\Pi_{1}(A)}\\). We know \\(i \\circ r = H|_{X \\times \\{ 1 \\}}\\) is homotopic to \\(id_{X} = H|_{X \\times \\{ 0 \\}}\\). Therefore, \\(i_{*} \\circ r_{*} = id_{\\Pi_{1}(X)}\\). Hence they are inverses of each other.</p>"},{"location":"Deformation%20Retracts/#examples","title":"Examples:","text":"<ol> <li>\\(S ^{1}\\) has the same \\(\\Pi_{1}\\) as \\(S ^{1} \\times I\\) the cylinder, mobius band, \\(B ^{2}\\setminus \\{ 0 \\}, \\mathbb{R}^{2} \\setminus \\{ 0 \\}, S \\times B ^{2}\\)</li> <li>Figure eight space is a deformation retract of \\(\\mathbb{R} ^{2} \\setminus\\{ 2 \\ points \\}\\) and also of \\(S ^{1} \\times S ^{1} \\setminus \\{ 1 \\ point \\}\\).</li> <li>The theta shape : \\(S ^{1} \\cup (\\{ 0 \\} \\times [-1,1]) \\subseteq \\mathbb{R}^{2}\\). Also a deformation retract of \\(\\mathbb{R}^{2} \\setminus \\{ 2 \\ points \\}\\), so the \\(\\Pi_{1}(\\mathrm{figure \\ eight}) = \\Pi_{1}(\\mathrm{\\theta \\ shape})\\)</li> </ol>"},{"location":"Deformation%20Retracts/#references","title":"References","text":"<p>Retractions Fundamental Group Homotopy of paths S^1</p>"},{"location":"Denotational%20Semantics%20for%20lambda%20Calculus/","title":"Denotational Semantics for lambda Calculus","text":"<p>202310201610</p> <p>Tags : [[Lambda Calculus]]</p>","tags":["Note"]},{"location":"Denotational%20Semantics%20for%20lambda%20Calculus/#denotational-semantics-for-lambda-calculus","title":"Denotational Semantics for Lambda Calculus","text":"<pre><code>title:Denotational Semantics\n*Denotational Semantics* is an approach for formalizing syntactic expressions by constructing mathematical objects that describe the meaning of an expression.\n</code></pre> <p>The purpose is to assign a value to every expression in the language. Here - An expression is a syntactic object built using the rules of the language - A value is a mathematical object</p> <p>We define  the function \\(\\mathbf{Eval}\\) that takes expressions to values.</p>","tags":["Note"]},{"location":"Denotational%20Semantics%20for%20lambda%20Calculus/#here-we-use-a-variant-to-lambda-calculus-that-contains-some-built-in-functions-and-constants","title":"<pre><code>Here we use a variant to lambda calculus that contains some built in functions and constants.\n</code></pre>","text":"","tags":["Note"]},{"location":"Denotational%20Semantics%20for%20lambda%20Calculus/#the-textbfeval-function","title":"The \\(\\textbf{Eval}\\) function","text":"<p>We define \\(\\mathbf{Eval}\\) in the following way.</p> <ul> <li>We can define expressions that do not have a normal form, i.e expressions that do not terminate  as follows .<ul> <li>\\(\\textbf{Eval}\\textlbrackdbl\\; E\\;\\textrbrackdbl =\\; \\perp\\)</li> </ul> </li> <li>Given an expression \\(E\\), we need to assign values to its free variables by defining an environment \\(\\rho\\) which has definitions for free variables<ul> <li>\\(\\mathbf{Eval}\\textlbrackdbl\\; x\\;\\textrbrackdbl\\;\\rho =\\rho\\; x\\)</li> </ul> </li> <li>Given an application of 2 expressions, we will have that the first expression will evaluate to a function, that takes in the evaluation of the second expression as an argument.<ul> <li>\\(\\textbf{Eval}\\textlbrackdbl \\;E_{1}\\;E_{2}\\;\\textrbrackdbl=(\\textbf{Eval}\\textlbrackdbl \\;E_{1}\\;\\textrbrackdbl)\\;(\\textbf{Eval}\\textlbrackdbl \\;E_{2}\\;\\textrbrackdbl)\\;\\)</li> </ul> </li> <li>Given an abstraction, we can say that its evaluation is the same as the evaluation of the inner expression if we overwrite the definition of the bound variable with input that is supposed to be applied to the expression<ul> <li>\\(\\textbf{Eval}\\textlbrackdbl \\;\\lambda x.E\\;\\textrbrackdbl\\;\\rho\\;a=\\textbf{Eval}\\textlbrackdbl\\;E\\;\\textrbrackdbl\\;\\rho[x:=a]\\)</li> </ul> </li> <li>We can sort of use a Haskell like syntax to give evaluation for built-in functions <ul> <li>\\(\\mathbf{Eval}\\textlbrackdbl\\;*\\;\\textrbrackdbl\\; a\\;b=a\\times b\\) <ul> <li>Here we give the syntactic \\(*\\) a corresponding mathematical object which is the multiplication function, although we need to complete the definition by adding all possible arguments</li> </ul> </li> </ul> </li> </ul> <p>Considering all that we get the following definition for \\(\\mathbf{Eval}\\) $$ \\begin{align} &amp;\\mathbf{Eval}\\textlbrackdbl\\;E\\;\\textrbrackdbl&amp;&amp;=\\;\\perp&amp;&amp;\\text{If \\(E\\) does not terminate}\\ &amp;\\mathbf{Eval}\\textlbrackdbl\\;x\\;\\textrbrackdbl\\;\\rho&amp;&amp;=\\rho\\;x&amp;&amp;\\text{Defining by the enviroment}\\ &amp;\\mathbf{Eval}\\textlbrackdbl\\;E_{1}\\;E_{2}\\;\\textrbrackdbl&amp;&amp;=(\\mathbf{Eval}\\textlbrackdbl\\;E_{1} \\;\\textrbrackdbl)\\quad(\\mathbf{Eval}\\textlbrackdbl\\;E_{2} \\;\\textrbrackdbl)&amp;&amp;\\text{Evaluate spearately}\\ &amp;\\mathbf{Eval}\\textlbrackdbl\\;\\lambda x.E \\;\\textrbrackdbl\\;\\rho\\;a&amp;&amp;=\\mathbf{Eval}\\textlbrackdbl\\;E \\;\\textrbrackdbl\\;\\rho[x:=a]&amp;&amp;\\text{Overwrite environment}\\ &amp;\\mathbf{Eval}\\textlbrackdbl\\;\\text{Some inbuilt value}\\;\\textrbrackdbl&amp;&amp;&amp;&amp;\\text{Defined like haskell}\\ \\end{align} $$ Ideally the environment should always be mentioned, but I will not write it for clarity</p>","tags":["Note"]},{"location":"Denotational%20Semantics%20for%20lambda%20Calculus/#references","title":"References","text":"<p>Lambda Calculus Syntax Lambda Calculus Introduction Enriched Lambda Calculus</p>","tags":["Note"]},{"location":"Dependency%20analysis%20for%20letrec%20expressions/","title":"Dependency analysis for letrec expressions","text":"<p>202311071911</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note","Incomplete"]},{"location":"Dependency%20analysis%20for%20letrec%20expressions/#dependency-analysis-for-letrec-expressions","title":"Dependency analysis for letrec expressions","text":"<p>A lot of Haskell expressions like where clauses only produce let-rec because of the potential to contain recursive definitions, even though it is possible to write definitions which do not involve recursion.  Here we analyse definitions that have been converted to let-rec form and convert them to nested let(rec)</p> <p>It is desirable to do strictness analysis for the following reasons - let expressions are significantly more efficient to implement than let-rec. - Type Checking may be impossible without it. - Strictness checking also becomes significantly more efficient.</p>","tags":["Note","Incomplete"]},{"location":"Dependency%20analysis%20for%20letrec%20expressions/#example","title":"Example","text":"<pre><code>letrec x   = fac z\n       fac = \\n. IF (= n 0) 1 (* n (fac (- n 1)))\n       z   = 4\n       sum = \\x.\\y.IF (= 0 x) y (sum (- x 1) (+ y 1))\nin sum x z\n</code></pre> <p>An equivalent expression would be</p> <pre><code>let \n    z = 4 \nin \n    letrec fac  = \\n. IF (= n 0) 1 (* n (fac (- n 1)))\nin let \n    x = fac z\nin letrec \n    sum = \\x.\\y.IF (= 0 x) y (sum (- x 1) (+ y 1))\nin \n    sum x z\n</code></pre>","tags":["Note","Incomplete"]},{"location":"Dependency%20analysis%20for%20letrec%20expressions/#dependency-analysis-algorithm","title":"Dependency Analysis Algorithm","text":"<p>We use the following code as an example for dependency analysis <pre><code>letrec\n      a = ...\n      b = ... a ...\n      c = ... h ... b ... d ...\n      d = ... c ...\n      f = ... g ... h ... a ...\n      g = ... f ...\n      h = ... g ...\nin\n      ...\n</code></pre> 1. For each letrec, we create a graph where the nodes are variables variables bound by let-rec. There exists edge \\(\\alpha\\to\\beta\\) iff \\(\\beta\\) has a free occurrence in the definition of \\(\\alpha\\).     2. Existence of loops or cycles would imply that a letrec must be used to deal with the variable defined in said loop/cycle, and we find strongly connected components to get co-dependent variables which need to be put in the same letrec block 3. After quotienting the stronly connected components, we get a Directed Acyclic Graph. We can now perform topo-sort to get a linear order instead of a tree, that can be converted to the final code, keeping the smallest element in the innermost let/letrec block.      4. Using that we generate the following code</p> <pre><code>let\n     a = ...\nin let  \n     b = ... a ...\nin letrec\n     f = ... g ... h ... a\n     g = ... f ...\n     h = ... g ...\nin letrec\n     c = ... h ... b ... d ...\n     d = ... c ...\nin \n     ...\n</code></pre>","tags":["Note","Incomplete"]},{"location":"Dependency%20analysis%20for%20letrec%20expressions/#references","title":"References","text":"<p>[[let(rec)-expressions to Ordinary Lambda Calculus]]</p>","tags":["Note","Incomplete"]},{"location":"Determinants/","title":"Determinants","text":"<p>202210111310</p> <p>Type : #Note Tags : [[Calc]]</p>"},{"location":"Determinants/#determinants","title":"Determinants","text":"<p>let \\(V=\\mathbb R^n\\) \\(M_n\\xrightarrow{\\det}\\mathbb R\\) \\(M_n = \\underbrace{R^n\\times R^n\\times\\dots \\times R^n}_{n \\text{ times}}\\) \\((\\overline{v_1},\\overline{v_2}\\dots\\overline{v_n})\\mapsto[\\overline{v_1}|\\overline{v_2}|\\dots|\\overline{v_n}]\\mapsto\\det[\\overline{v_1}|\\overline{v_2}|\\dots|\\overline{v_n}]\\)</p> <p>Proposition: Let \\(T:V\\to V\\) be a linear map. Given \\(w\\in\\Lambda^n(V)\\) \\(w(Tv_1,Tv_2\\dots Tv_n) = \\det T\\times w(v_1,v_2\\dots v_n)\\) Proof: Choose a basis for \\(V\\) let \\(w = \\det\\), \\(w(\\overline{v_1},\\overline{v_2}\\dots\\overline{v_n})=\\det [\\overline{v_1}|\\overline{v_2}|\\dots|\\overline{v_n}]\\) \\(w^T(\\overline{v_1},\\overline{v_2}\\dots\\overline{v_n}) = w(T\\overline{v_1},T\\overline{v_2}\\dots T\\overline{v_n})\\) \\(w\\mapsto w^T\\) is a linear map \\(\\Lambda^n V \\to \\Lambda^n V\\). \\(w^T(\\overline{v_1},\\overline{v_2}\\dots\\overline{v_n}) =\\det[T\\overline{v_1}|T\\overline{v_2}|\\dots|T\\overline{v_n}]=\\det\\big(T[\\overline{v_1}|\\overline{v_2}|\\dots|\\overline{v_n}]\\big)= \\det T\\times \\det [\\overline{v_1}|\\overline{v_2}|\\dots|\\overline{v_n}]\\) \\(\\therefore w^T = \\det T \\times w\\) for any \\(w\\).</p>"},{"location":"Determinants/#related-problems","title":"Related Problems","text":""},{"location":"Determinants/#references","title":"References","text":"<p>Exterior Algebra</p>"},{"location":"Deterministic%20Timed%20Automata/","title":"Deterministic Timed Automata","text":"<p>202309071009</p> <p>Type : #Note  Tags : Timed Automata</p>"},{"location":"Deterministic%20Timed%20Automata/#deterministic-tomato","title":"Deterministic Tomato","text":"<pre><code>title: Determinisitc and Complete Automata\nA *Deterministic Automaton* is an Automaton such that it has a unique start state and for each letter of the input alphabet, there is at most 1 transition for each letter in the alphabet for each state.\n\nAn automaton is called *Complete* if for each state, there is a transition for each letter.\n\n*Deterministic* along with *Complete* implies that there is a unique run for each letter.\n</code></pre> <p>For a Timed Automata, The deterministic condition holds if from a state, all transitions that accept a letter have guards which are never pairwise mutually satisfiable. And Completeness condition holds when the union of all the guards always holds.</p>"},{"location":"Deterministic%20Timed%20Automata/#closure-properties","title":"Closure Properties","text":"<p>Complete and Deterministic Timed Automata are closed under - Union The union of two complete and deterministic timed automata \\(\\mathcal A_1\\) and \\(\\mathcal A_2\\) is $$ \\begin{align} \\mathcal A &amp;= \\langle \\Sigma,L, l, C, F, E\\rangle\\ \\Sigma&amp;= \\Sigma_{1}\\cup\\Sigma_{2}&amp;\\text{Alphabet}\\ L &amp;= L &amp;\\text{States}\\ l &amp;= (l_{1}, l_{2}) &amp; \\text{Start State}\\ C &amp;= C_{1}\\sqcup C_{2} &amp; \\text{Clocks}\\ F &amp;= {(a, b) : a\\in F_{1}\\lor b\\in F_{2}} &amp;\\text{Final States}\\ E &amp;= {\\langle(a_{1,}a_{2}),\\alpha,B_{1}\\land B_{2,}R_{1}\\cup R_{2}, (b_{1},b_{2})\\rangle} &amp;\\text{Transitions} \\end{align} $$ where \\(\\langle a_{i}, \\alpha,B_{i},R_{i},b_{i}\\rangle, i\\in \\{1,2\\}\\) are transitions in \\(A_{i}\\) </p> <ul> <li> <p>Intersection The Intersection of two automata \\(\\mathcal A_1\\) and \\(\\mathcal A_{2}\\) is given by an almost identical construction except \\(F = \\{(a, b)\\ :\\ a\\in F_{1}\\land b\\in F_{2}\\}\\)</p> </li> <li> <p>Complement The Complement of a complete and deterministic timed automata can be made done by making the accepting states non accepting and vice versa</p> </li> </ul> <p>These construction are essentially the same as the ones for Untimed DFAs</p> <pre><code>title: Determinizability\nSince Not all Timed Automata are Determinizable(complement closure). Then given a timed Automata is its determinizability decidable.\n\nSpoiler : Nope\n</code></pre>"},{"location":"Deterministic%20Timed%20Automata/#examples","title":"Examples","text":"<p>\\(\\{a^{k}, \\tau_{1},\\tau_{2}\\dots \\tau_{k}\\ :\\ \\tau_{k}-\\tau_{k-1}=1\\}\\) where \\(k\\ge 2\\)  ![[Drawing 2023-09-07 12.51.29.excalidraw]]</p>"},{"location":"Deterministic%20Timed%20Automata/#references","title":"References","text":"<p>Event Recording Automata</p>"},{"location":"Diagonal%20Constraints%20Offer%20Exponential%20Succinctness./","title":"Diagonal Constraints Offer Exponential Succinctness.","text":"<p>202310282118</p> <p>tags : Timed Automata</p>","tags":["Example"]},{"location":"Diagonal%20Constraints%20Offer%20Exponential%20Succinctness./#diagonal-constraints-offer-exponential-succinctness","title":"Diagonal Constraints Offer Exponential Succinctness.","text":"<p>[!abstract]  Abstract, lmao We will show that d-timed automata are exponentially more succinct that diagonal free timed automata by constructing a set of languages \\(\\{L_{i}\\}_{i\\in \\mathbb N}\\)  such that for each \\(L_i\\) there is a d-timed automaton of size polynomial in \\(n\\) and a diagonal free timed automaton that has exponential states in \\(n\\).</p> <p>Let \\(L_i\\) be defined as follows $$ \\left{\\;\\left(a<sup>{2</sup>n},\\tau \\right) | 0&lt;\\tau_{1}&lt;\\tau_{2}&lt;\\cdots&lt;\\tau_{2^n}&lt;1\\;\\right} $$</p>","tags":["Example"]},{"location":"Diagonal%20Constraints%20Offer%20Exponential%20Succinctness./#construction-of-a-small-d-timed-automaton","title":"Construction of a small d-timed automaton","text":"<p>We use a counter \\(c\\) that can store \\(n\\) bits and its initial value if \\(0\\), each time an \\(a\\) is seen, the counter is incremented and after \\(2^{n-1}\\) \\(a\\)s we accept the last one and go to the final state.</p> <p>We use \\(n\\) pairs of clocks to encode the counter bits, for the \\(i^\\text{th}\\) bit, we have clocks \\(x_i\\) and \\(x_i'\\) such that the value of the counter bit \\(b_i\\) is $$ b_{i} =  \\begin{cases} 0&amp;x_{i}-x_{i}'=0 \\ 1&amp;x_{i}-x_{i}'&gt;0 \\end{cases} $$ The incrementing of the counter works in the following manner, if the first \\(j-1\\) bits \\(1\\) and the next bit is \\(0\\). We set the first \\(j-1\\) bits to \\(0\\) and the \\(j^{\\text{th}}\\) bit to \\(1\\) and stop. This gives us a set of \\(n-1\\) transitions(to check if the first \\(n-1\\) digits are \\(1\\) and next digit is \\(0\\)).</p> <p>then the \\(j^\\text{th}\\) transition is  $$ \\begin{cases} g_{j}\\text{ is } \\left(\\bigwedge\\limits_{x=1}^{j-1} (x_{i}-x_{i}'&gt;0)\\right) \\land (x_{j} -x_{j}'=0) \\ \\ R_{j}\\text{ is } \\left(\\bigcup\\limits_{i=1}^{j-1}{x_{i},x_{i}'}\\right) \\cup{x_{j}'} \\end{cases} $$ There is a final transition that checks if all bits are 1 and \\(x_{n}&lt;1\\) and takes us to final state.  There are only \\(2\\) states and \\(n+1\\) transitions.</p>","tags":["Example"]},{"location":"Diagonal%20Constraints%20Offer%20Exponential%20Succinctness./#every-diagonal-free-timed-automata-for-l_i-has-atleast-2i-states","title":"Every Diagonal Free Timed Automata for \\(L_i\\) has atleast \\(2^i\\) states","text":"<p>FTSOC:  Lets say there is a state a diagonal free timed automaton \\(\\mathcal B_n\\) that has less thatn \\(2^n\\) states. Then consider the word defined below $$ w:= \\begin{matrix} \\left(a<sup>{2</sup>n},\\tau\\right) &amp; \\text{s.t.} &amp; \\tau_{i}= \\frac{i}{2^n+1} \\end{matrix} $$ clearly \\(w\\in L_{n}\\), there is an accepting run in \\(\\mathcal B_n\\) on \\(w\\): $$ (q_{0},v_{0}) \\xrightarrow{\\delta_{0},\\theta_{0}} (q_{1},v_{1}) \\xrightarrow{\\delta_{1},\\theta_{1}} \\dots  (q_{2<sup>n},v_{2</sup>n}) $$ but since the number of states are less than \\(2^n\\). There exists \\(i, j\\) such that \\(i\\ne j\\) and \\(q_i = q_j\\). However, the total time spent in the run is less that \\(1\\), there for every transition, all clock evaluations belong to \\((0,1)\\). Thus for the above run, we can say that during any transition, the clock would have satisfied the guards of all the transitions in the run. Hence consider the word \\(\\left(a^{2^n -(j-i)},\\tau\\right)\\) such that  $$ \\tau_{k} =  \\begin{cases} \\frac{k}{2^n+1} &amp;\\text{if } k\\leq i \\ \\ \\frac{k+j-i}{2^n+1} &amp;\\text{if } k &gt;i \\end{cases} $$ using the fact that all guards used in the previous run accept in the hypercube \\((0,1)^X\\) we have the following run $$ (q_{0},v_{0}) \\xrightarrow{\\delta_{0},\\theta_{0}} (q_{1},v_{1}) \\xrightarrow{\\delta_{1},\\theta_{1}} \\dots (q_{i},v_{i}) \\xrightarrow{\\Delta,\\theta_{j}} (q_{j+1},v_{j+1}) \\xrightarrow{\\delta_{j+1},\\theta_{j+1}} \\dots (q_{2<sup>n},v_{2</sup>n}) $$ where \\(\\Delta = \\frac{(j-i)+1}{2^n+1}\\) Hence \\(\\mathcal B_n\\) accepts the above word, which is a contradiction.</p>","tags":["Example"]},{"location":"Diagonal%20Constraints%20Offer%20Exponential%20Succinctness./#related","title":"Related","text":"<p>Timed Automata with Diagonal Constraints D-Timed Automata to Timed Automata Construction</p>","tags":["Example"]},{"location":"Diff%20Eq%20-%20Nice%20Info/","title":"Diff Eq   Nice Info","text":"<p>202301041101</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"Diff%20Eq%20-%20Nice%20Info/#diff-eq-nice-info","title":"Diff Eq - Nice Info","text":"<pre><code>title: People that contributed\nEuler, (Jacob and Daniel) Bernoulli, Lagrange, Laplace, Fourier, Cauchy, Maxwell, Poincare$\\dots$\n</code></pre> <p>Differential Equations greatly contributes to the predictive powers of maths. Connected Fields - Linear Algebra - Number Theory     - Elliptic curves - \\((\\wp'(z))^2 = 4\\wp(z)^3- g_2\\wp(z)-g_3\\)     - Transcendence - Cauchy-Reimann Equations - Calaby-Yau spaces \\(\\leadsto\\) Calabi Flow - Poincare Conjecture \\(\\leadsto\\) Ricci Flow</p>"},{"location":"Diff%20Eq%20-%20Nice%20Info/#grading","title":"Grading","text":"<p>4 quizzes (best 3) each 10 % mid term exam - 30 % final exam - 40 %</p> <p>Homework is given but not graded</p>"},{"location":"Diff%20Eq%20-%20Nice%20Info/#references","title":"References","text":"<ol> <li>ODE- V.I. Arnold(sometimes it gets unmotivaed and its a little terse)</li> <li>Theory of ODE- Coddington and levinson</li> <li> <ul> <li>GF Simmons</li> </ul> </li> <li>ODE - Birkhoff and rota (not entirely aligned w the course)</li> <li>Prof. Pramadh Sastry's notes</li> </ol>"},{"location":"Diff%20Eq%20-%20Nice%20Info/#syllabus","title":"Syllabus","text":"<p>Hopefully will get by the end of the course roughly- - first and Nth order ODE - IVP problems (autonomous and non autonomous) - Lipschitz conditions important for existance and uniqueness theorems - Systems of linear differential equations - One parameter group of transformations, etc etc etc</p>"},{"location":"Diff%20Eq%20-%20Nice%20Info/#related-problems","title":"Related Problems","text":""},{"location":"Diff%20Eq%20-%20Nice%20Info/#references_1","title":"References","text":""},{"location":"Dimension%20of%20a%20Path/","title":"Dimension of a Path","text":"<p>202311180011</p> <p>Tags : Timed Automata</p>","tags":["Note","Incomplete"]},{"location":"Dimension%20of%20a%20Path/#dimension-of-a-path","title":"Dimension of a Path","text":"<p>[!tip] Motivation We here want to embed each Path in some \\(\\mathbb R^n\\). In doing so, the idea of the Dimension of a Path naturally arises. Its the smallest \\(n\\) such that the path in some sense can be embedded in \\(R^n\\).</p> <p>The idea is to consider \\(\\tau_i\\) corresponding to each edge in the path, and use it add a dimension to the path. - If set of possible \\(\\tau_i\\) is a singleton in all cases. or if its empty. Then the edge does not add dimension to the path. - Otherwise we add a dimension</p> <p>[!example]   let \\(s_0 = (l_0,0)\\) and \\(\\pi\\) be the path \\(\\pi(s_0, e_1, e_2)\\) Then it is easy to associate the following polygon to the path \\(\\text{Pol}(\\pi)=\\{ (\\tau_{1},\\tau_{2})\\in \\mathbb R^2 \\;:\\;(0\\leq \\tau_{1}\\leq2)\\land(0\\leq \\tau_{1}+\\tau_{2}\\leq 5)\\}\\) so \\(\\pi\\) has dimension 2 but the path \\(\\pi'=\\pi(s_{0},e_{1},e_{3})\\) has dimension 1 but is embedded in 2 dimension. So we say that the dimension of \\(\\pi'\\) is undefined.</p> <p>For a constrained path is the polygon corresponding to path intersected with the polygon corresponding to the constraint.(sort of)</p> \\[ \\text{Pol}(\\pi_{\\mathbb C})=\\{ (\\tau_{i})_{1\\leq i\\leq n} \\in \\mathbb R_{+}\\;:\\;s\\xrightarrow{\\tau_{1},e_{1}}s_{1}\\cdots\\xrightarrow{\\tau_{n},e_{n}}s_{n}\\in \\pi_{\\mathcal C}(s,e_{1}\\dots e_{n}) \\} \\] <p>[!note] Definition Let \\(\\mathcal A\\) be a Timed Automaton and \\(\\pi_{\\mathcal C}=\\pi_{\\mathcal C}(s,e_{1}\\dots e_{n})\\) be a constrained path. For each \\(i\\leq n\\) we write \\(\\mathcal C_i\\) to be the projection of \\(\\text{Pol}(\\pi_{\\mathcal C})\\) over the variables of first \\(i\\) coordinates. With convention \\(C_0\\) is true. We say that the dimension of a path is undefined whenever there exist some \\(i\\) such that \\(1\\leq i \\leq n\\) such that  $$ \\text{dim}\\Big(\\text{Pol}\\big(\\pi_{\\mathcal C_{i}}(s, e_{1}\\dots e_{i})\\big)\\Big) &lt; \\text{dim}\\Big(\\bigcup_{e}\\text{Pol}\\big(\\pi_{\\mathcal C_{i}}(s, e_{1}\\dots e_{i-1}e)\\big)\\Big) $$ Then we say \\(\\text{dim}_{\\mathcal A}(\\pi_{\\mathcal C})=\\bot\\) otherwise \\(\\text{dim}_{\\mathcal A}(\\pi_{\\mathcal C})=\\top\\).</p> <p>In other words. If some of the transition from a state accept only a single valuation of clock while there are other transitions that accept guards containing non singleton intervals then a path containing the former has undefined dimension.</p>","tags":["Note","Incomplete"]},{"location":"Dimension%20of%20a%20Path/#references","title":"References","text":"<p>Timed Automata Alternate Definition</p>","tags":["Note","Incomplete"]},{"location":"Dinic%27s%20algo%20for%20max-flow/","title":"Dinic's algo for max flow","text":"<p>202310251410</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note","Incomplete"]},{"location":"Dinic%27s%20algo%20for%20max-flow/#dinics-algo-for-max-flow","title":"Dinic's algo for max-flow","text":"<p>Blocking flow: \\(f\\) is a blocking flow if it saturates at least one edge on every \\(s-t\\) path. Observe: A blocking flow need not be a max flow.</p> <ol> <li>Find a BFS layout of the network.</li> <li>Perform a DFS using only the BFS edges.</li> <li>Every time \\(t\\) is reached, send a flow along the \\(s-t\\) path (present on the stack).</li> </ol> <p>This results in a blocking flow. Compute residual graph \\(G_{f}\\).</p> <p>Observe: When a blocking flow is found, the network has no \\(s-t\\) path of length \\(d\\).</p> <p>Analysis: \\(O(n)\\) iterations since each iteration increases the length of an \\(s-t\\) path. So \\(O((m+n)n)\\) time.</p>","tags":["Note","Incomplete"]},{"location":"Dinic%27s%20algo%20for%20max-flow/#references","title":"References","text":"<p>Edmonds-Karp algo for max flow</p>","tags":["Note","Incomplete"]},{"location":"Dirichlet%20Kernels/","title":"Dirichlet Kernels","text":"<p>2022-11-12 03:11 pm</p> <p>Type : #Note Tags : [[Analysis]]</p>"},{"location":"Dirichlet%20Kernels/#dirichlet-kernels","title":"Dirichlet Kernels","text":"<pre><code>### Definition:\nThe trigonometric polynomial defined on $[-\\pi,\\pi]$ by \n$$D_N(x) = \\sum\\limits_{-N}^{N} e^{inx}$$\nis called the $N$-th dirichlet kernel.\n</code></pre> <ul> <li>A closed form is given by \\(\\(D_N(x) = \\dfrac{sin((N+1/2)x)}{sin(x/2)}\\)\\)</li> <li> \\[\\dfrac{1}{2\\pi}\\int\\limits_{-\\pi}^{\\pi}D_N(t)dt = 1\\] </li> </ul>"},{"location":"Dirichlet%20Kernels/#related-problems","title":"Related Problems","text":"<p>Good Kernels Fejer Kernel</p>"},{"location":"Dirichlet%20Kernels/#references","title":"References","text":"<p>Fourier Series</p>"},{"location":"Discriminant%20of%20an%20n-tuple/","title":"Discriminant of an n tuple","text":"<p>202302281602</p> <p>Type : #Note Tags : [[Number Theory]]</p>"},{"location":"Discriminant%20of%20an%20n-tuple/#discriminant-of-an-n-tuple","title":"Discriminant of an n-tuple","text":"<pre><code>title:\nLet $K$ be a number field of degree $n$ over $\\mathbb{Q}$. Let $\\sigma_1,\\dots,\\sigma_n$ denote the n embeddings of $K \\in \\mathbb{C}$. For any $n$-tuple of elements $\\alpha_1,\\dots,\\alpha_n \\in K$, define the discriminant as follows:\n$$\\mathrm{disc}(\\alpha_1,\\dots,\\alpha_n) = |\\sigma_i(\\alpha_j)|^2$$\nwhere $|M|$ denotes the determinant of the matrix $M$.\n</code></pre>"},{"location":"Discriminant%20of%20an%20n-tuple/#theorem-1","title":"Theorem 1:","text":"<pre><code>title: \n$$\n\\mathrm{disc}(\\alpha_{1},\\alpha_{2},\\dots,\\alpha_{n}) = \\mid T(\\alpha_{i}\\alpha_{j})\\mid\n$$\n</code></pre>"},{"location":"Discriminant%20of%20an%20n-tuple/#proof","title":"Proof:","text":"\\[ \\begin{align} \\mathrm{disc}(\\alpha_{1},\\alpha_{2},\\dots,\\alpha_{n}) &amp;= |\\sigma_{j}(\\alpha_{i})| |\\sigma_{i}(\\alpha_{j})| \\\\ &amp;= \\left|\\sum_{k=1}^{n}\\sigma_{k}(\\alpha_{i})\\sigma_{k}(\\alpha_{j})\\right| \\\\ &amp;= | T(\\alpha_{i}\\alpha_{j})| \\end{align} \\]"},{"location":"Discriminant%20of%20an%20n-tuple/#corollary","title":"Corollary","text":"<pre><code>title: \nIf $\\mathrm{disc}(\\alpha_{1},\\dots,\\alpha_{n}) \\in \\mathbb{Q}$, and if all $\\alpha_{i}'s$ are integers then $\\mathrm{disc}(\\alpha_{1},\\dots,\\alpha_{n}) \\in \\mathbb{Z}$.\n</code></pre>"},{"location":"Discriminant%20of%20an%20n-tuple/#theorem-2","title":"Theorem 2","text":"<pre><code>title:\n$\\mathrm{disc}(\\alpha_{1},\\dots,\\alpha_{n}) = 0$ iff $\\alpha_{1},\\dots,\\alpha_{n}$ are linearly dependent over $\\mathbb{Q}$.\n</code></pre>"},{"location":"Discriminant%20of%20an%20n-tuple/#proof_1","title":"Proof:","text":"<p>If they are linearly dependent, then the columns of the matrix \\([\\sigma_{i}(\\alpha_{j})]\\) are too, hence discriminant is 0. If disc is 0, then the rows of \\([T(\\alpha_{i}\\alpha_{j})]\\) are linearly dependent, hence there are \\(a_{1},a_{2},\\dots,a_{n} \\in \\mathbb{Q}\\) such that: $$ a_{1}T(\\alpha_{1}\\alpha_{j}) + \\dots + a_{n}T(\\alpha_{n}\\alpha_{j}) = 0 $$ for all \\(1 \\le j \\le n\\). Implying \\(T(\\alpha_{j}(a_{1}\\alpha_{1}+\\dots+a_{n}\\alpha_{n})) = 0\\) Now if the \\(\\alpha_{j}\\) are linearly independent over \\(\\mathbb{Q}\\), they form a basis for \\(K\\) over \\(\\mathbb{Q}\\), and \\(\\alpha := a_{1}\\alpha_{1} + \\dots a_{n}\\alpha_{n} \\neq 0\\). Thus \\(\\alpha\\alpha_{j}\\) forms a basis over \\(\\mathbb{Q}\\). But then \\(T(\\alpha\\alpha_{j}) = 0\\) for all \\(j\\), implying \\(T(\\beta) = 0\\) for all \\(\\beta \\in K\\), that's not true since \\(T(1) = n \\neq 0\\). Contradiction.</p>"},{"location":"Discriminant%20of%20an%20n-tuple/#theorem-3","title":"Theorem 3:","text":"<pre><code>title:\nSuppose $K = \\mathbb{Q}(\\alpha)$, and let $\\alpha = \\alpha_{1},\\dots,\\alpha_{n}$ denote the conjugates of $\\alpha$ over $\\mathbb{Q}$. Then \n$$\n\\mathrm{disc}(1,\\alpha, \\dots, \\alpha^{n-1}) = \\prod_{1 \\le r &lt; s \\le n} (\\alpha_{r}-\\alpha_{s})^{2} = \\pm N ^{K}(f'(\\alpha))\n$$\nwhere $f$ is the minimal polynomial of $\\alpha$ over $\\mathbb{Q}$.\nThe plus sign holds iff $n \\equiv 0 \\ \\text{or} \\ 1 (\\mathrm{mod}\\ 4)$\n</code></pre>"},{"location":"Discriminant%20of%20an%20n-tuple/#proof_2","title":"Proof:","text":"<p>Note that $$ \\begin{align} \\mathrm{disc}(1,\\alpha, \\dots,\\alpha^{n-1}) &amp;= |\\alpha_{i}<sup>{j}|</sup>{2}  \\ &amp;= \\prod_{1 \\le r &lt; s \\le n} (\\alpha_{r}-\\alpha_{s})^{2} \\ &amp;= \\pm\\prod_{r \\neq s} (\\alpha_{r}-\\alpha_{s}) \\end{align} $$ Since the determinant \\(|a_{i}^j|\\) is a vandermonde determinant. There are \\(n(n-1)\\) terms in the last product, and so, the plus sign holds iff \\(4 \\mid n(n-1)\\). Now \\(f(x) = \\prod_{i=1}^{n}(x-\\alpha_{i})\\) is the minimal poly of \\(\\alpha\\) over \\(\\mathbb{Q}\\). Then $$ \\begin{align} f'(x) &amp;= \\sum_{j=1}^{n}\\prod_{i\\neq j}(x-\\alpha_{i}) \\ \\implies f'(\\alpha) &amp;= (\\alpha-\\alpha_{2})(\\alpha-\\alpha_{3}) \\dots (\\alpha-\\alpha_{n}) \\ \\implies N ^{K}(f'(\\alpha)) &amp;= \\prod_{r=1}^{n}\\sigma_{r}(f'(\\alpha))  \\ &amp;= \\prod_{r=1}^{n}f'(\\sigma_{r}(\\alpha)) \\ &amp;= \\prod_{r=1}^{n}f'(\\alpha_{r}) \\ &amp;= \\prod_{r=1}^{n}\\prod_{r \\neq s}(\\alpha_{r}-\\alpha_{s}) \\ &amp;= \\prod_{r \\neq s}(\\alpha_{r}-\\alpha_{s}) \\end{align} $$ Thus we are done.</p>"},{"location":"Discriminant%20of%20an%20n-tuple/#note-we-write-mathrmdiscalpha-to-denote-mathrmdisc1alpha-dots-alphan-1-for-any-algebraic-number-alpha-of-degree-n-over-mathbbq","title":"NOTE: We write \\(\\mathrm{disc}(\\alpha)\\) to denote \\(\\mathrm{disc}(1,\\alpha, \\dots ,\\alpha^{n-1})\\), for any algebraic number \\(\\alpha\\) of degree \\(n\\) over \\(\\mathbb{Q}\\).","text":""},{"location":"Discriminant%20of%20an%20n-tuple/#theorem-4","title":"Theorem 4:","text":"<pre><code>title:\nIf $\\alpha_{1},\\dots\\alpha_{n} \\in R$, then they form an integral basis iff $\\mathrm{disc}(R) = \\mathrm{disc(\\alpha_{1},\\dots,\\alpha_{n})}$.\n\n$\\alpha_{1},\\dots,\\alpha_{n} \\in R$ form an integral basis if $\\mathrm{disc(\\alpha_{1},\\dots,\\alpha_{n})}$ is square free.\n</code></pre>"},{"location":"Discriminant%20of%20an%20n-tuple/#proof_3","title":"Proof:","text":"<p>Both parts follow from Problem 3 in related problems below.</p>"},{"location":"Discriminant%20of%20an%20n-tuple/#related-problems","title":"Related Problems","text":"<ol> <li>Calculate \\(\\mathrm{disc}(1,\\omega, \\dots ,\\omega^{p-2})\\) where \\(\\omega = e ^{2\\pi i/p}\\). It comes out to be \\(\\pm p ^{p-2}\\).    \\(f(x)(x-1) = x ^{p}-1\\), differentiate to get \\(f'(x)(x-1) + f(x) = px ^{p-1}\\), thus \\(f'(\\omega) = p \\frac{\\omega^{p-1}}{\\omega-1} = \\frac{p}{\\omega(\\omega-1)}\\).    \\(N(f'(\\omega)) = \\frac{N(p)}{N(\\omega)N(\\omega-1)} = \\frac{p ^{p-1}}{1 \\cdot p} = p ^{p-2}\\).</li> <li>For \\(\\mathrm{disc}(\\omega)\\) with \\(\\omega = e ^{2\\pi i/m}\\), we can show that \\(\\mathrm{disc}(\\omega) \\mid m ^{\\varphi(m)}\\).    Let \\(f\\) be min poly of \\(\\omega\\), then \\(x ^{m}-1 = f(x)g(x) \\implies f'(x)g(x) + g(x)f'(x) = mx ^{m-1} \\implies f'(\\omega)g(\\omega)=m \\omega^{m-1} \\implies m = \\omega f'(\\omega)g(\\omega)\\).    Taking norms, $$ m ^{\\varphi(m)} = N(f'(\\omega))N(\\omega g(\\omega)) $$ establishing the result.</li> <li>Let \\(G\\) be a number ring of rank \\(n\\), and \\(H\\) be a subgroup of rank \\(n\\). Then \\(|G /H|\\) is finite, and \\(\\mathrm{disc}(H) = |G /H|^{2} \\mathrm{disc}(G)\\).    To show that \\(|G /H|\\) is finite, look at proposition 3 of Free Group.    This means \\(G /H = \\mathbb{Z}/d_{1}\\mathbb{Z} \\oplus \\mathbb{Z} /d_{2}\\mathbb{Z} \\oplus \\dots \\oplus \\mathbb{Z}/d_{n}\\mathbb{Z}\\). So, there is a basis \\(\\beta_{1},\\dots \\beta_{n}\\) for \\(G\\) such that \\(d_{1}\\beta_{1},\\dots,d_{n}\\beta_{n}\\) is a basis for \\(H\\). We can get the \\(\\beta's\\) by looking at the preimages of \\((0,..0,1,0,\\dots,0)\\) type vectors in \\(G /H = \\mathbb{Z}/d_{1}\\mathbb{Z} \\oplus \\mathbb{Z} /d_{2}\\mathbb{Z} \\oplus \\dots \\oplus \\mathbb{Z}/d_{n}\\mathbb{Z}\\).    Note that now \\(\\mathrm{disc}(H) = (d_{1}d_{2}\\dots d_{n})^{2}\\mathrm{disc}(G)\\).</li> </ol>"},{"location":"Discriminant%20of%20an%20n-tuple/#references","title":"References","text":"<p>Number Field Trace and Norm Algebraic Integers Free Group</p>"},{"location":"Distance%20of%20a%20Code/","title":"Distance of a Code","text":"<p>202308101608</p> <p>Type : #Note Tags : Algorithmic Coding Theory</p>"},{"location":"Distance%20of%20a%20Code/#distance-of-a-code","title":"Distance of a Code","text":"<p>The Distance of a code is the distance between 2 codes.</p>"},{"location":"Distance%20of%20a%20Code/#theorem","title":"Theorem:","text":"<p>Given a code \\(C\\), The following are equiavlent 1. \\(C\\) has minimum distance \\(d\\ge 2\\) 2. If \\(d\\) is odd, \\(C\\) can correct \\(\\frac{d-1}{2}\\) errors, if even then it can correct \\(\\frac{d}{2}\\)-1 errors 3. It can detect \\(d-1\\) erasures</p>"},{"location":"Distance%20of%20a%20Code/#references","title":"References","text":"<p>Hamming Distance Algorithmic Coding Theory Lec 1</p>"},{"location":"Distances%20in%20Graphs/","title":"Distances in Graphs","text":"<p>202308091008</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Distances%20in%20Graphs/#distances-in-graphs","title":"Distances in Graphs","text":"<pre><code>title:\nGiven $x, y\\in V, d(x,y)=$ length of the shortest path between $x$ and $y$.\nThe diameter of a graph $G$ is the longest distance in $G$\n</code></pre>"},{"location":"Distances%20in%20Graphs/#references","title":"References","text":""},{"location":"EXP%20Complexity%20Class/","title":"EXP Complexity Class","text":"<p>202302191902</p> <p>Type : #Note Tags :[[Complexity Theory]]</p>"},{"location":"EXP%20Complexity%20Class/#exp-complexity-class","title":"EXP Complexity Class","text":""},{"location":"EXP%20Complexity%20Class/#exp","title":"EXP","text":"<p><pre><code>title:\nClass of decisin problems that can be decided by a [Determinisitic Turing Machine](&lt;./Turing Machines.md&gt;) in $O(2^{n^{c}})$ time.\n</code></pre> $$ EXP = \\bigcup_{n\\in\\mathbb{N}}DTIME\\left(2<sup>{n</sup>{c}}\\right) $$</p>"},{"location":"EXP%20Complexity%20Class/#example","title":"Example","text":""},{"location":"EXP%20Complexity%20Class/#related-problems","title":"Related Problems","text":""},{"location":"EXP%20Complexity%20Class/#references","title":"References","text":"<p>Complexity Classes DTIME(f) Complexity Class</p>"},{"location":"Edge-Transitive%2C%20Non%20Transitive%20is%20Bipartite/","title":"Edge Transitive, Non Transitive is Bipartite","text":"<p>202308140243</p> <p>type : #Example tags : [[Algebraic Graph Theory]]</p>"},{"location":"Edge-Transitive%2C%20Non%20Transitive%20is%20Bipartite/#edge-transitive-non-transitive-is-bipartite","title":"Edge-Transitive, Non Transitive is Bipartite","text":""},{"location":"Edge-Transitive%2C%20Non%20Transitive%20is%20Bipartite/#theorem","title":"Theorem:","text":"<p>Let \\(T\\) on a connected graph \\(G\\), be edge transitive bit not vertex transitive. Then \\(G\\) is Bipartite</p>"},{"location":"Edge-Transitive%2C%20Non%20Transitive%20is%20Bipartite/#proof","title":"Proof:","text":"<p>Let \\(X\\) and \\(Y\\) be two distinct orbits that share an edge (say \\(e\\)) We know that two such distinct orbits exist because the graph is not vertex transitive, hence \\(\\exists x,y\\) such that orbit of \\(x\\) and \\(y\\) are distinct, take \\(y\\) to have share an edge with \\(X\\).</p> <p>If \\(X\\sqcup Y\\ne V\\) then consider \\(z\\notin X\\sqcup Y\\), but since \\(G\\) is edge transitive, take make \\(e\\) incident of \\(z\\), but that implies \\(z\\in X\\) or \\(z\\in Y\\) which is a contradiction. Thus \\(X\\sqcup Y=V\\)</p> <p>Consider \\(e'=x_{1},x_{2}\\) then there exists an automorphism \\(\\alpha\\) such that \\(\\alpha(e')=e\\). But this is a contradiction as \\(x_1\\) or \\(x_{2}\\) cannot go to \\(Y\\). Hence \\(e'\\) does not exist, hence the graph is bipartite.</p>"},{"location":"Edge-Transitive%2C%20Non%20Transitive%20is%20Bipartite/#related","title":"Related","text":""},{"location":"Edmonds-Karp%20algo%20for%20max%20flow/","title":"Edmonds Karp algo for max flow","text":"<p>202310201410</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note","Incomplete"]},{"location":"Edmonds-Karp%20algo%20for%20max%20flow/#edmonds-karp-algorithm-for-max-flow","title":"Edmonds-Karp algorithm for max-flow","text":"<p>Ford-Fulkerson's algo takes \\(O((m+n)|f^{*}|)\\) (pseudopolynomial time)</p> <p>Edmonds-Karp Algo 1: \\(O((m+n)\\log |f^{*}|)\\) (Weakly polytime) Algo 2: \\(O(m^{2}n)\\)</p> <p>\\(m=|E|,n=|V|,|f^{*}|=\\) value of the max flow</p>","tags":["Note","Incomplete"]},{"location":"Edmonds-Karp%20algo%20for%20max%20flow/#algorithm-1","title":"Algorithm 1","text":"<p>Same as Ford-Fulkerson except choose a path with maximum capacity in each iteration.</p> <pre><code>title:**Claim:**\nIf $f^{*}$ is a max-flow, then $\\exists$ an $s-t$ path with capacity $\\geq \\frac{|f^{*}|}{m}$.\n*Proof:* Remove all edges of capacity $&lt; \\frac{|f^{*}|}{m}$ from $G$. Either $s-t$ are still connected, or we have a cut of capacity $&lt;|f^{*}|$.\n</code></pre> <p>\\(G\\) has max flow of size \\(|f^{*}|\\). After the first iteration, \\(G_{f_{1}}\\) has max flow of size \\(\\left( 1- \\frac{1}{m} \\right)|f^{*}|\\), and so on. After the \\(k^{th}\\) iteration, \\(G_{f_{k}}\\) has max flow of size \\(\\left( 1-\\frac{1}{m} \\right)^{k}|f^{*}|\\). We stop when $$ \\begin{align} \\left( 1-\\frac{1}{m} \\right)<sup>{k}|f</sup>{}|&amp;&lt;1\\ |f<sup>{*}|e</sup>{ -k/m }&amp;&lt;1\\ \\log |f^{}|-\\frac{k}{m}&amp;&lt;1,\\ \\text{and, } k&amp;&gt;m\\log |f^{}| \\end{align*} $$ We find a max-flow in \\(m\\log |f^{*}|\\) iterations.</p>","tags":["Note","Incomplete"]},{"location":"Edmonds-Karp%20algo%20for%20max%20flow/#algorithm-2","title":"Algorithm 2","text":"<p>Find an augmenting path by BFS i.e. every iteration, choose a shortest augmenting path.</p> <pre><code>title: **Lemma:**\nNumber of iterations $\\le mn$.\n$d=\\text{dist}(s,t)$ at any point.\n\nTo prove:\n1. $d$ does not decrease.\n2. After $\\le m$ augmentations, $d$ increases by at least $1$.\n\nThe algo finds an augmenting path using only forward edges as long as possible.\nAfter each augmentation, one forward edge gets saturated and disappears from $G$.\nAfter $\\le m$ iterations, there is either no $s-t$ path or all $s-t$ paths use a back or cross edge. Thus $d$ increases.\n</code></pre> <p>This algorithm takes \\(O((m+n)mn)\\) time.</p>","tags":["Note","Incomplete"]},{"location":"Edmonds-Karp%20algo%20for%20max%20flow/#generalizations","title":"Generalizations","text":"<p>Multiple sources, multiple sinks Create a super-source and a super-sink, and connect \\(s\\) to all the \\(s_{i}\\), similarly for the sinks.</p> <p>Upper limit on the amount of flow sent out by \\(s\\) Add a phantom \\(s_{in}\\), connect it to \\(s_{out}\\), and put the limit on the edge.</p> <p>Upper limits on the amounts of flow through any vertex \\(v\\) Same trick, \\(v_{in}, v_{out}\\) and put limit on the edge.</p>","tags":["Note","Incomplete"]},{"location":"Edmonds-Karp%20algo%20for%20max%20flow/#applications","title":"Applications","text":"<ol> <li>Bipartite matchings max flow = maximum matchings</li> <li>Edge-disjoint paths To find the number of edge-disjoint paths from \\(s\\) to \\(t\\), put capacity \\(1\\) on each edge and find \\(s-t\\) max-flow.</li> </ol>","tags":["Note","Incomplete"]},{"location":"Edmonds-Karp%20algo%20for%20max%20flow/#min-cost-max-flow","title":"Min cost max flow","text":"<p>Edges have capacities and costs Cost of a flow \\(f = \\sum\\limits_{(u,v)\\in E}s(u,v)f(u,v)\\) For \\(e\\in E\\) capacity \\(=c(e)\\), cost \\(=s(e)\\)</p> <p>Goal: Among all max flows, find one with min cost.</p> <p>Choose any aug path with minimum cost</p>","tags":["Note","Incomplete"]},{"location":"Edmonds-Karp%20algo%20for%20max%20flow/#references","title":"References","text":"<p>Network Flows</p>","tags":["Note","Incomplete"]},{"location":"Ehrenfeucht-Fraisse%20Games%20Proof/","title":"Ehrenfeucht Fraisse Games Proof","text":"<p>202310201210</p> <p>Tags : [[Logic]]</p>","tags":["Note"]},{"location":"Ehrenfeucht-Fraisse%20Games%20Proof/#ehrenfeucht-fraisse-games-proofs","title":"Ehrenfeucht-Fra\u00efss\u00e9 Games Proofs","text":"<p>// stuff from last lecture: definition etc</p> <pre><code>title:Lemma: TFAE\n1. $(A,\\overline{a})\\equiv_{k}(B,\\overline{b})$.\n2. $(A,\\overline{a}),(B,\\overline{b})$ agree on FO$[k]$.\n\n$\\overline{a}$ is an $m-$tuple.\n</code></pre> <p>FO\\([0]\\) has only terms and formulas without any quantifiers We have \\(m\\) free variables. \\(N=|R|.m^{j}+m^{2}\\) The number of FO formulas will be infinite. We want to count the number of FO formulas up to logical equivalence.</p> <p>Because there are no quantifiers, we can go to propositional logic for inspiration.</p> <p>In propositional logic, the number of formulas we could have is \\(2^{2^{n}}\\). We got that by observing that each partition of the set of valuations gives a unique (up to logical equivalence) formula. The size of the set of valuations is \\(2^{n}\\), hence the number of partitions of the set is \\(2^{2^{n}}\\), as claimed.</p> <p>Now, drawing a parallel to FOL, we observe that an FO formula \\(\\varphi\\) can't distinguish between two interpretations \\(\\mathcal{I}_{1}\\) and \\(\\mathcal{I}_{2}\\), \\(\\mathcal{I}_{1}\\vDash\\varphi\\) iff \\(\\mathcal{I}_{2}\\vDash\\varphi\\). In the set of all interpretations, we have \\(2^{N}\\)(why) equivalence classes. Any FO formula will partition this set of equivalence classes (there's a bijection). Thus there are \\(2^{2^{N}}\\) FO\\([0]\\) formulas.</p> <p>Up to logical equivalence FO\\([0]\\) has only finitely many formulas on \\(m\\) free variables. We will prove this for FO\\([k]\\) by induction on \\(k\\). Ind. hypo.: \\(\\varphi\\) is a FO\\([k]\\) formula on \\(m+1\\) variables. Any FO\\([k+1]\\) formula in \\(m\\) free variables is a Boolean combination of formulas of the form \\(\\exists x\\ \\varphi\\) or \\(\\forall x\\ \\varphi\\), where \\(\\varphi\\) is a FO\\([j]\\) formula for \\(j\\leq k\\), in \\(m+1\\) free variables.</p> <p>Going back to graphs with all this newly learnt knowledge If we have only FO[0] formulas, we can only talk about the vertices that are in front of us (in the induced subgraph), and the edges between them. If we have only FO[1] formulas, we can only talk about the vertices and their plus ones, i.e. for the vertices we have, we can only talk about edges among them and 2-length paths among them (so we can talk about 1-radius balls around those vertices). Similarly for a general \\(k\\).</p> <p>Formalising that stuff^ Given an \\(m-\\) tuple \\(\\overline{a}\\) of elements in a structure \\(A\\), the rank-\\(k\\) \\(m-\\)type of \\(\\overline{a}\\) in \\(A\\) is the set \\(\\{ \\varphi \\in FO[k]\\ |\\ A,\\overline{a}\\vDash\\varphi \\}\\) </p>","tags":["Note"]},{"location":"Ehrenfeucht-Fraisse%20Games%20Proof/#references","title":"References","text":"<p>First Order Logic Even is not FO-definable</p>","tags":["Note"]},{"location":"Ehrenfeucht-Fra%C3%AFss%C3%A9%20Game/","title":"Ehrenfeucht Fra\u00efss\u00e9 Game","text":"<p>202311292011</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"Ehrenfeucht-Fra%C3%AFss%C3%A9%20Game/#ehrenfeucht-fraisse-game","title":"Ehrenfeucht-Fra\u00efss\u00e9 Game","text":"<p>Ehrenfeucht-Fra\u00efss\u00e9 Games are a nice tool to for describing expressiveness of logics over finite model.</p> <p>[!important] Setup There are two players, a spoiler and a duplicator. And the board consists of two structure \\(\\mathfrak U\\) and \\(\\mathfrak B\\). The goal of the spoiler is to show that the two structures are different, and the goal of the duplicator is to show that the two structures are the same. </p>","tags":["Note","Incomplete"]},{"location":"Ehrenfeucht-Fra%C3%AFss%C3%A9%20Game/#gameplay","title":"Gameplay","text":"<p>In the classic Ehrenfeucht-Fra\u00efss\u00e9 Game, the players play a certain number of rounds, and each round consists of the following steps. 1. Spoiler picks a structure. 2. Then spoiler makes a move by picking a point in the structure. 3. the duplicator responds to the move by picking a point in the other structure.</p> <p>An example game is given in Even is not FO-definable.</p>","tags":["Note","Incomplete"]},{"location":"Ehrenfeucht-Fra%C3%AFss%C3%A9%20Game/#winning","title":"Winning","text":"<p>After \\(n\\) rounds of an Ehrenfeucht-Fra\u00efss\u00e9 Game, we have the move \\(\\vec{a}=(a_{1},a_{2}\\dots a_{n})\\) and \\(\\vec{b}=(b_{1},b_{2}\\dots b_{n})\\). And let \\(c_{1},c_{2}\\dots c_{k}\\) be the set of constants in the language. We define \\(\\vec{c^\\mathfrak U}=c^{\\mathfrak U}_{1}\\dots c^\\mathfrak U_{k}\\) and \\(\\vec{c^\\mathfrak B}=c^\\mathfrak B_{1}\\dots c^\\mathfrak B_{k}\\) is the interpretation of the constants in the two structures \\(\\mathfrak B\\) and \\(\\mathfrak U\\).</p> <p>With that machinery, we say that the spoiler failed to show that the two structures if \\(((\\vec{a}, \\vec{c^\\mathfrak U}), (\\vec{b},\\vec{c^\\mathfrak B}))\\) is a Partial Isomorphism.</p> <p>And we say \\(\\mathfrak U \\equiv_n \\mathfrak B\\) if duplicator has a winning strategy in an \\(n\\) round game.</p> <p>The Application of Ehrenfeucht-Fra\u00efss\u00e9 Games is given by the Ehrenfeucht-Fra\u00efss\u00e9 Theorem which links it to first order logic</p>","tags":["Note","Incomplete"]},{"location":"Ehrenfeucht-Fra%C3%AFss%C3%A9%20Game/#references","title":"References","text":"<ul> <li>Even is not FO-definable</li> <li>Quantifier rank</li> <li>Ehrenfeucht-Fra\u00efss\u00e9 Theorem</li> </ul>","tags":["Note","Incomplete"]},{"location":"Ehrenfeucht-Fra%C3%AFss%C3%A9%20Theorem/","title":"Ehrenfeucht Fra\u00efss\u00e9 Theorem","text":"<p>202311300011</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"Ehrenfeucht-Fra%C3%AFss%C3%A9%20Theorem/#ehrenfeucht-fraisse-theorem","title":"Ehrenfeucht-Fra\u00efss\u00e9 Theorem","text":"","tags":["Note","Incomplete"]},{"location":"Ehrenfeucht-Fra%C3%AFss%C3%A9%20Theorem/#back-and-forth-equivalence","title":"Back and Forth Equivalence","text":"<p>This is an additional equivalence that we use to make the proof easier. - \\(\\mathfrak A\\simeq_{0}\\mathfrak B\\) iff \\(\\mathfrak A \\equiv_0\\mathfrak B\\) - \\(\\mathfrak A\\simeq_{k+1}\\mathfrak B\\) iff     - Back: For every \\(b\\) in \\(B\\), there exists an \\(a\\) such that \\((\\mathfrak A,a)\\simeq_k(\\mathfrak B,b)\\)      - Forth: For every \\(a\\) in \\(A\\), there exists a \\(b\\) such that \\((\\mathfrak A,a)\\simeq_k(\\mathfrak B,b)\\)</p>","tags":["Note","Incomplete"]},{"location":"Ehrenfeucht-Fra%C3%AFss%C3%A9%20Theorem/#theorem","title":"Theorem","text":"<p>Then we have that the following are equivalent: - \\(\\mathfrak A\\) and \\(\\mathfrak B\\) agree on \\(\\text{FO}[k]\\) - \\(\\mathfrak A\\equiv_k \\mathfrak B\\) - \\(\\mathfrak A\\simeq_k\\mathfrak B\\)</p>","tags":["Note","Incomplete"]},{"location":"Ehrenfeucht-Fra%C3%AFss%C3%A9%20Theorem/#proof","title":"Proof","text":"<p>Proof is by induction Base Case: \\(k=0\\) is trivial</p> <p>Induction Step: To prove the equivalence between points \\(2\\) and \\(3\\). </p> <p>for left to right, if  spoiler picks \\(a\\) from \\(A\\) and duplicator picks \\(b\\) from \\(B\\) in the last round, then we use those choices to define the Forth relation. If the the spoiler picks \\(b\\) from \\(B\\), we define the choices for the Back: relation.</p> <p>For the reverse direction, back and forth relation directly gives a strategy for the duplicator</p> <p>To prove the equivalence between \\(1\\) and \\(3\\).</p> <p>For right to left, pick \\(a\\) from \\(A\\) and let \\(\\alpha_i\\) define its rank-\\(k\\) \\(1\\) type. Then \\(\\mathfrak A\\models\\exists x\\alpha_i(x)\\). This is a formula of quantifier rank \\(k+1\\). so \\(\\mathfrak B\\) also satisfies the formula, which shows the existence of a witnessing \\(b\\). Which means \\(\\text{tp}(\\mathfrak A,a)=\\text{tp}(\\mathfrak B, b)\\). So \\((\\mathfrak A, a)\\) and \\((\\mathfrak B,b)\\) agree on the same \\(\\text{FO}[k]\\) sentences. And from the hypotheses we have \\((\\mathfrak A,a)\\simeq_k(\\mathfrak B,b)\\).</p> <p>For Left to Right Each formula of rank \\(k+1\\) is a boolean formula that contains rank-\\(k\\) formulas and formulas of form \\(\\exists x\\varphi(x)\\) where \\(\\varphi\\) is rank-\\(k\\). So we can only deal with formulas of the type \\(\\exists x\\varphi(x)\\) If \\(\\mathfrak A\\models\\exists x\\varphi(x)\\), and let \\(a\\) be the witnessing element. Then we have  \\((\\mathfrak A,a)\\simeq (\\mathfrak B, b)\\). By hypothesis we have that these agree on sentences from \\(\\text{FO}[k]\\) So we have \\(\\mathfrak B, b\\models \\varphi(b)\\) and hence \\(\\mathfrak B\\models\\exists x\\varphi (x)\\). </p> <p>This \\(\\mathfrak A\\) and  \\(\\mathfrak B\\) agree on all formulas of rank \\(k+1\\).</p>","tags":["Note","Incomplete"]},{"location":"Ehrenfeucht-Fra%C3%AFss%C3%A9%20Theorem/#references","title":"References","text":"<p>Ehrenfeucht-Fra\u00efss\u00e9 Game Quantifier rank Rank-k Types</p>","tags":["Note","Incomplete"]},{"location":"Emptiness%20for%20Updatable%20Timed%20Automata/","title":"Emptiness for Updatable Timed Automata","text":"<p>202310301010</p> <p>Tags : Timed Automata</p>","tags":["Note","Incomplete"]},{"location":"Emptiness%20for%20Updatable%20Timed%20Automata/#emptiness-for-updatable-timed-automata","title":"Emptiness for Updatable Timed Automata","text":"<p>[!note] Theorem Emptiness problem is undecidable for Updatable Timed Automata</p> <p>We prove the following theorem by reducing the Emptiness problem for updatable timed automata to the emptiness problem of 2-Counter Automata.</p> <p>We use clocks to simulate counters of the counter automata. The only problem with using clocks for counters is that the value of clocks change with time. To counter this, we have an extra clock that is used to fix the time.</p>","tags":["Note","Incomplete"]},{"location":"Emptiness%20for%20Updatable%20Timed%20Automata/#emulating-a-2-counter-machine-using-an-updatable-timed-automata","title":"Emulating a 2-counter machine using an Updatable Timed Automata","text":"<p>Let \\(\\mathcal C\\) be a 2-counter automata \\(\\langle Q, \\Sigma, q_{0}, \\{c_{1}, c_{2}\\}, \\Delta \\rangle\\) We construct a Timed Automata \\(\\mathcal T\\) defined as \\(\\langle Q, \\Sigma, q_{0},\\{x_{1},x_{2},x_{\\text{fix}}\\}, \\Delta'\\rangle\\)</p> <p>We deal with operations on the counter automata by defining transitions in our timed automata.</p> <ol> <li>Increment<ul> <li>The for the transition on the counter automata    $$   q\\xrightarrow[c_{1}++]{\\quad a\\quad}q'   $$</li> <li>We have the following transition on the UTA   $$   q \\xrightarrow[x_{1} := x_{1}+1]{\\quad a,x_{\\text{fix}}=0\\quad}q'   $$</li> </ul> </li> <li>Increment<ul> <li>The for the transition on the counter automata    $$   q\\xrightarrow[c_{1}--]{\\quad a\\quad}q'   $$</li> <li>We have the following transition on the UTA   $$   q \\xrightarrow[x_{1} := x_{1}-1]{\\quad a,x_{\\text{fix}}=0\\quad}q'   $$</li> </ul> </li> <li>Increment<ul> <li>The for the transition on the counter automata    $$   q\\xrightarrow[c_{1}=0]{\\quad a\\quad}q'   $$</li> <li>We have the following transition on the UTA   $$   q \\xrightarrow[x_{1} =0]{\\quad a,x_{\\text{fix}}=0\\quad}q'   $$</li> </ul> </li> </ol> <p>Because of \\(x_\\text{fix}\\), time does not elapse, hence the other clocks behave perfectly like a counter.</p>","tags":["Note","Incomplete"]},{"location":"Emptiness%20for%20Updatable%20Timed%20Automata/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Empty%20Rule%20for%20Match%20Function/","title":"Empty Rule for Match Function","text":"<p>202310251510</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note"]},{"location":"Empty%20Rule%20for%20Match%20Function/#empty-rule-for-match-function","title":"Empty Rule for Match Function","text":"<p>After applying the other rules(possibly multiple times),  we will reach the following situation</p>","tags":["Note"]},{"location":"Empty%20Rule%20for%20Match%20Function/#example","title":"Example","text":"<pre><code>match []\n      [ ( [], (A u1 u3) ) ]\n      ERROR\n</code></pre> <p>which intuitively reduces to  <pre><code>A u1 u2\n</code></pre></p>","tags":["Note"]},{"location":"Empty%20Rule%20for%20Match%20Function/#empty-rule","title":"Empty Rule","text":"<p>In the general case, the list of expressions might be empty for might have multiple arguments. Hence given the expression <pre><code>match []\n      [ ([], E1),\n        ...\n        ([], Em)]\n      E\n</code></pre> where \\(m\\ge 0\\). and we define match to give the following in this situation <pre><code>E1 |&gt; ... |&gt; Em |&gt; E\n</code></pre></p>","tags":["Note"]},{"location":"Empty%20Rule%20for%20Match%20Function/#references","title":"References","text":"<ul> <li>Match Function for Enriched Lambda Calculus</li> <li>Constructor Rule for Match Function</li> <li>Variable Rule for Match Function</li> <li>Mixture Rule for Match Expression</li> </ul>","tags":["Note"]},{"location":"Emulating%20a%20Multi-tape%20Turing%20machine%20on%20a%20Single%20Tape%20Turing%20Machine%20Effeciently/","title":"Emulating a Multi tape Turing machine on a Single Tape Turing Machine Effeciently","text":"<p>202302191902</p> <p>Type : #Note Tags : [[Complexity Theory]]</p>"},{"location":"Emulating%20a%20Multi-tape%20Turing%20machine%20on%20a%20Single%20Tape%20Turing%20Machine%20Effeciently/#emulating-a-multi-tape-turing-machine-on-a-single-tape-turing-machine-efficiently","title":"Emulating a Multi-tape Turing machine on a Single Tape Turing Machine Efficiently","text":"<p>Claim: If Multi-tape Turing Machine can compute inputs of size \\(n\\) in \\(O(T(n))\\) then a Single-tape Turing Machine can emulate it in \\(O(T^{2}(n))\\). Proof:  If the multiple tape turing machine has \\(k\\) tapes, then its contents can be copied down to the single tape turing machine by interleaving the letters, as in  $$ \\begin{align} \\text{Letters from tape} &amp;\\longmapsto \\text{can be written in positions}\\ 1 &amp;\\longmapsto 1, k+1,2k+1\\dots\\ 2 &amp;\\longmapsto 2, k+2,2k+2\\dots\\ 3 &amp;\\longmapsto 3, k+3,2k+3\\dots\\ &amp;\\;\\;\\;\\vdots\\ k &amp;\\longmapsto  k,2k\\;\\;\\;\\;\\;,3k\\;\\;\\;\\;\\;\\;\\dots\\ \\end{align} $$ For each of the tapes, the letter which corresponds to the one that was pointed at by the head can be marked,eg \\(a\\) can be written as \\(\\hat a\\).</p> <p>Then in a single step, the turing machine can read the section of the tape which is filled to figure out the position of the heads, can go back and then repeat this while applying the valid changes, Hence a move of the \\(k\\) tape turing  machine can be emulated in \\(O(T(n))\\) hence, the entire execution of the machine can be done in \\(O(T^2(n))\\) steps.</p>"},{"location":"Emulating%20a%20Multi-tape%20Turing%20machine%20on%20a%20Single%20Tape%20Turing%20Machine%20Effeciently/#related-problems","title":"Related Problems","text":""},{"location":"Emulating%20a%20Multi-tape%20Turing%20machine%20on%20a%20Single%20Tape%20Turing%20Machine%20Effeciently/#references","title":"References","text":"<p>Turing Machines with multiple tapes</p>"},{"location":"Encoding%20and%20Decoding%20Functions/","title":"Encoding and Decoding Functions","text":"<p>202308101608</p> <p>Type : #Note Tags : Algorithmic Coding Theory</p>"},{"location":"Encoding%20and%20Decoding%20Functions/#encoding-and-decoding-functions","title":"Encoding and Decoding Functions","text":"<p>Consider \\(C\\subseteq \\Sigma^{n}\\) as the set of all data words. and Encoding function \\(\\text{Enc:  } [|C|]\\longrightarrow \\Sigma^{n}\\)</p> <p>while a Decoding function \\(\\text{Dec: } \\Sigma^{n}\\longrightarrow [|C|]\\)</p>"},{"location":"Encoding%20and%20Decoding%20Functions/#references","title":"References","text":"<p>Hamming Distance</p>"},{"location":"Endsem%20ACT/","title":"Endsem ACT","text":"<p>BMC202107 Aditi Muthkhod</p>"},{"location":"Endsem%20ACT/#4","title":"4.","text":"<p>We prove that the set \\(\\mathcal{C}\\) contains the set \\(RM_{q}(2,d)\\). \\(RM_{q}(2,d)=\\{ f: \\mathbb{F}_{q}^{2}\\to \\mathbb{F}_{q}\\ |\\deg(f)\\leq d \\}\\subseteq \\mathbb{F}_{q}[X,Y]\\). In other words, it is the set of evaluations of all bivariate polynomials in \\(\\mathbb{F}_{q}[X,Y]\\) of total degree at most \\(d\\) and individual degree at most \\(q-1\\) over all points in \\(\\mathbb{F}_{q}^{2}\\). \\(\\mathcal{C}\\), on the other hand, is the set of evaluations of all polynomials in \\(\\mathbb{F}_{q}[X,Y]\\) that satisfy property \\(P(d)\\) (i.e. are in \\(\\mathcal{F}\\)) over all points in \\(\\mathbb{F}_{q}^{2}\\). So it suffices to prove that \\(\\mathcal{F}\\supseteq\\{ f\\ |\\deg(f)\\leq d \\}=\\mathcal{F'}\\). (We can ignore the condition of having individual degree at most \\(d\\) because when we evaluate these polynomials over \\(\\mathbb{F}_{q}\\), all the higher degree terms will reduce to degree at most \\(q-1\\) since \\(a^{q}=a\\).) Any polynomial \\(p(X,Y)\\) in \\(\\mathcal{F'}\\) is also in \\(\\mathcal{F}\\) because \\(p(X,Y)\\) itself is of degree at most \\(d\\).</p> <p>In local correction of RM codes, say at coordinate \\(a\\in \\mathbb{F}_{q}^{2}\\), we pick a random direction \\(b\\in\\mathbb{F}_{q}^{2}\\) and look at the restriction of the received word to the line \\(L=\\{ a+bt\\ |\\ t\\in\\mathbb{F}_{q} \\}\\). This gives us evaluations of the univariate polynomial \\(Q(T)=f(a+bT)\\in\\mathbb{F}_{q}[T]\\) which is of degree at most \\(d&lt;q\\), and we can recover \\(Q(T)\\) successfully. For \\(\\mathcal{C}\\), the same argument follows through. Say the coordinate we want to locally decode at is \\(a\\in\\mathbb{F}_{q}^{2}\\), we pick a random direction \\(b\\in\\mathbb{F}_{q}^{2}\\) and look at the restriction of the received word to the line \\(L=\\{ a+bt\\ |\\ t\\in\\mathbb{F}_{q} \\}\\). This gives us evaluations of the univariate polynomial \\(f(a+bT)\\in\\mathbb{F}_{q}[T]\\). \\(L=l(T)=(b_{x}T+a_{x},b_{y}T+a_{y})\\). If \\(f\\in\\mathcal{F}\\), then \\(\\exists g\\) such that \\(f\\equiv g\\) (i.e has all the same evaluations) and \\(g\\) has degree at most \\(d\\). Let \\(Q(T)=g(a+bT)\\) Hence we can recover \\(Q\\), and evaluating \\(Q(T)\\) at \\(T=0\\) gives us the corrected value of \\(g(a)=f(a)\\).</p> <p>We can use the same decoding algorithm as that for RM codes, by reducing it to decoding RS codes.</p>"},{"location":"Endsem%20ACT/#5","title":"5.","text":"<p>We have seen in class a basic list decoding algorithm. For the root finding step to go through, we had \\(\\deg(X,P(X))\\leq \\deg_{X}(Q)+(k-1)\\deg_{Y}(Q)\\), so we required \\(t&gt;\\deg(X,P(X))\\), which is not tight because the highest degrees of \\(X\\) and \\(Y\\) might occur in different terms, which would lead to the slackness. We can counter this by using the notion of weighted degree. We can require the \\((1,(k-1))-\\)degree of \\(Q(X,Y)\\) to be at most \\(\\sqrt{ 2(k-1)n }\\), where \\((1,w)-\\)degree of \\(X^{i}Y^{j}\\) is defined to be \\(i+wj\\). We extend this as: the \\((1,w)-\\)degree of a polynomial is the highest \\((1,w)-\\)degree among all its monomials. Now the same argument as earlier goes through, and we can pick values such that \\(t&gt; \\lceil \\sqrt{ 2(k-1)n } \\rceil\\), which will give us an algorithm to decode from \\(1-\\sqrt{ 2R }\\) fraction of errors, or \\(n-\\sqrt{ 2kn }\\) errors.</p> <p>So \\(M=\\{ x^{i}y^{j}\\ |\\ i+(k-1)j\\leq \\sqrt{ 2(k-1)n } \\}\\).</p>"},{"location":"Endsem%20ACT/#6","title":"6.","text":"<p>I think there's a typo in the question: \\(\\gamma\\) and \\(\\delta\\) should be interchanged. (Or maybe the definition of an expander graph is different.) I am assuming a \\((c,d)-\\)regular \\((\\delta,\\gamma)-\\)expander. Let \\(\\gamma=(1-\\epsilon)c\\), so \\(\\gamma&gt; \\frac{2c}{3} \\implies\\epsilon&lt; \\frac{1}{3}\\).</p> <p>(a) Let the vertex added to \\(ERASE\\) at the \\(i^{th}\\) iteration be \\(i\\). Define \\(E_{0}=\\phi,E_{i}=E_{i-1}\\cup \\{ i \\},i\\geq 1\\). Let there be \\(t\\) iterations.</p> <p>FTSOC \\(|E_{t}|&gt;\\delta n-1\\). Let \\(i\\) be the last iteration for which \\(|E_{i}|\\leq\\delta n\\).</p> <p>We have that \\(|N(E_{i})|\\geq(1-\\epsilon)c|E_{i}|\\).</p> <p>We obtain an upper bound on \\(|N(E_{j})|\\) as follows: Since \\(j\\) was added to \\(E_{j-1}\\), \\(|N(j)\\cap UNHAPPY|\\geq \\frac{c}{3}\\). Any \\(k\\in N(j)\\cap UNHAPPY\\) is either in \\(N(j)\\cap N(E_{j-1})\\), or in \\(N(j)\\cap (U\\setminus N(E_{j-1}))\\).</p> <p>So \\(|N(j)\\cap UNHAPPY|=|N(j)\\cap N(E_{j-1})|+|N(j)\\cap (U\\setminus N(E_{j-1}))|\\geq \\frac{c}{3}\\). Using this, we get $$ \\begin{align} |N(E_{i})|&amp;=\\sum\\limits_{j=1}^{n}(|N(j)|-|N(j)\\cap E_{j-1}|)\\ &amp;\\leq \\sum\\limits_{j=1}^{n}\\left( c- \\frac{c}{3}+|N(j)\\cap U\\setminus N(E_{j-1})| \\right)\\ &amp;\\leq \\frac{2c}{3}i +|U|. \\end{align} $$ This and, \\(|E_{i}|=i\\), gives us \\(\\((1-\\epsilon)c|E_{i}|\\leq |N(E_{i})|\\leq \\frac{2c}{3}|E_{i}|+|U|.\\)\\)Or, $$ \\begin{align} (1-\\epsilon)c|E_{i}|&amp;\\leq \\frac{2c}{3}|E_{i}|+|U|\\ |E_{i}|c\\left( \\frac{1}{3}-\\epsilon \\right)&amp;\\leq |U|\\ &amp;\\leq c|ERRORS|\\ |E_{i}|&amp;\\leq \\frac{|ERRORS|}{\\frac{1}{3}-\\epsilon}. \\end{align} $$ Since \\(|E_{i+1}|=|E_{i}|+1&gt;\\delta n\\), \\(|E_{i}|&gt;\\delta n-1\\), so \\(|ERRORS|&gt;\\left( \\frac{1}{3}-\\epsilon \\right)(\\delta n-1)\\). So we can pick \\(\\tau=\\left( \\frac{1}{3}-\\epsilon \\right)\\delta\\), which would give a contradiction since \\(|ERRORS|\\) is an integer.</p> <p>(b) Suppose FTSOC \\(ERRORS\\not\\subseteq ERASE\\). Let \\(\\tau=\\delta\\), so \\(|ERRORS|\\leq\\delta n\\). Let \\(O=ERRORS\\setminus ERASE\\). \\(|O|&lt;|ERRORS|\\leq\\delta n\\). So, \\(|N(O)|\\geq |O|(1-\\epsilon)c\\).</p> <p>[!info] Any \\((\\delta,(1-\\epsilon)c)-\\)expander is also a \\((\\delta,(1-2\\epsilon)c)-\\)unique neighbour expander.</p> <p>We get \\(|N^{1}(O)|\\geq |O|(1-2\\epsilon)c\\). By PHP, \\(\\exists i\\in O\\) such that \\(|N^{1}(O)\\cap N(i)|\\geq(1-2\\epsilon)c&gt; \\frac{c}{3}\\).</p> <p>Claim: \\(N^{1}(O)\\cap N(i)\\subseteq UNHAPPY\\). Proof: Pick any \\(j\\in N^{1}(O)\\cap N(i)\\). \\(j\\) is not a neighbour of any vertex in \\(O\\) other than \\(i\\), because it is a unique neighbour of \\(O\\). If it is a neighbour of some vertex in \\(ERASE\\), then \\(j\\in UNHAPPY\\). Otherwise, \\(j\\) is a neighbour of exactly one vertex in \\(ERRORS\\), which means \\(j\\in U\\subseteq UNHAPPY\\).</p> <p>This gives us that \\(i\\) has \\(&gt; \\frac{c}{3}\\) neighbours in \\(UNHAPPY\\), so \\(i\\) should have been in \\(ERASE\\), which is a contradiction.</p> <p>Thus, \\(ERRORS\\subseteq ERASE\\).</p> <p>Referred to Viderman's paper [Vid13a], and \"Viderman's algorithm for quantum LDPC codes\" by Krishna, Navon, and Wootters</p>"},{"location":"Endsem%20ACT/#7","title":"7.","text":"<p>We want to check whether \\(Ay=0\\). Let the rows of the matrix be \\(A_{0},\\dots,A_{m-1}\\). We can construct a degree \\(m-1\\) polynomial as follows: \\(P(X)=\\sum\\limits_{i=0}^{m-1}A_{i}X^{i}\\). Checking if \\(Ay=0\\) is equivalent to checking if \\(P(X)\\equiv 0\\). \\(\\mathbb{F}_{q^{s}}\\) contains \\(\\mathbb{F}_{q}\\) as a subfield. So we pick a random point \\(\\alpha \\in\\mathbb{F}_{q^{s}}\\), and check if \\(P(\\alpha)=0\\), which is the same as checking if \\(\\sum\\limits_{i=0}^{m-1}\\left( \\sum\\limits_{j=1}^{n}y_{j}A_{i,j} \\right)\\alpha^{i}=0\\). Reordering the summations, we need to check if \\(\\sum\\limits_{j=1}^{n}y_{j}\\left( \\sum\\limits_{i=0}^{m-1}A_{i,j}\\alpha^{i} \\right)=0\\). Since \\(\\alpha^{i}\\) and \\(A_{i,j}\\) can be computed in \\(O(1)\\), the inner summation can be computed in time \\(O(m)\\) for each \\(j\\). So on seeing \\(y_{j}\\) we can calculate the required term, and store the sum, till the end. The algorithm uses at most \\(O(\\log q+\\log m+\\log n)\\) bits.</p> <p>If \\(Ay=0\\), the algorithm always returns \"Zero!\". If \\(Ay\\not=0\\), then the algorithm returns \"Not Zero!\" with probability at least \\(\\frac{q^{s}-m+1}{q^{s}}\\). We want \\(\\frac{q^{s}-m+1}{q^{s}}\\geq \\frac{2}{3}\\). So we can pick \\(s\\) such that \\(q^{s}\\geq 3(m-1)\\), or \\(s\\geq\\log_{q}3(m-1)\\).</p> <p>Referred to \"Data Stream Algorithms for Codeword Testing\" by Atri Rudra, Steve Uurtamo.</p>"},{"location":"Endsem%20ACT/#8","title":"8.","text":"<p>For \\(p(x)=(p_{1}(x),\\dots,p_{m}(x))\\), we can define \\(p_{i}(x)=\\sum\\limits_{j=0}^{m-1}\\lambda_{i}^{q^{j}}x^{q^{j}}\\), where \\(\\lambda_{1},\\dots,\\lambda_{m}\\in\\mathbb{F}_{q^{m}}\\) are linearly independent over \\(\\mathbb{F}_{q}\\). Since \\(|\\mathbb{F}_{q}^{m}|=|\\mathbb{F}_{q^{m}}|\\), it suffices to show that \\(p(x)\\) has image \\(\\mathbb{F}_{q}^{m}\\), which we can do by observing algebraic properties of the image of \\(p\\), which will be a subspace of \\(\\mathbb{F}_{q}^{m}\\). Let the received polynomial be \\(w\\). \\(w:\\mathbb{F}_{q}^{m}\\to \\mathbb{F}_{q}\\). \\(w\\circ p:\\mathbb{F}_{q^{m}}\\to \\mathbb{F}_{q}\\). We can look at \\(w\\circ p\\) as the received polynomial for the RS code \\([q^{m},k]_{q^{m}}\\), evaluated over all of \\(\\mathbb{F}_{q^{m}}\\). It can be shown that \\(k=rq^{m-1}+1=\\deg(w\\circ p)+1\\) in this case.</p> <p>(a) Now we can use any of the RS decoding algorithms to get the corrected polynomial, say \\(g=w\\circ p\\). We get \\(w=g\\circ p^{-1}\\). Since we can evaluate \\(p^{-1}\\) and \\(g\\), we can correct the RM code. The RS code decoding algorithm can uniquely decode up to half its minimum distance i.e. up to \\(\\frac{q^{m}-rq^{m-1}}{2}\\), which also is half the minimum distance for the RM code. The RS code decoding algorithm is polytime \\((O(q^{m})^{s}=O(n)^{s})\\), and it is easy to check that the reduction is as well.</p> <p>(b) We can use the same algorithm as in part (a), and use the algorithm mentioned in question 4 for decoding the RS code. It will take \\(O(q)\\) queries because we are restricting to a line. We can correct \\(\\frac{1}{2}\\left( 1-\\frac{r}{q} \\right)\\) fraction of errors as shown above. So \\(\\epsilon_{0}=\\frac{1}{2}\\) here.</p>"},{"location":"Enriched%20Lambda%20Calculus/","title":"Enriched Lambda Calculus","text":"<p>202310201610</p> <p>Tags : [[Lambda Calculus]], [[Programming Languages]]</p>","tags":["Note"]},{"location":"Enriched%20Lambda%20Calculus/#enriched-lambda-calculus","title":"Enriched Lambda Calculus","text":"<p>Enriched Lambda Calculus is an extended version of Lambda Calculus. Which means that every Lambda Calculus expression is also an expression in Enriched Lambda Calculus.</p> <p>The syntax of Enriched Lambda Calculus includes- - All off Lambda Calculus Syntax. - let-expressions and letrec-expressions - pattern-matching lambda abstractions: Patterns - the infix operator \\(\\triangleright\\)  - \\(\\text{case-}\\)expression The following summarizes the syntax </p>","tags":["Note"]},{"location":"Enriched%20Lambda%20Calculus/#references","title":"References","text":"","tags":["Note"]},{"location":"Enumeration%20Machine/","title":"Enumeration Machine","text":"<p>202211171911</p> <p>Type : #Note Tags : [[Theory of Computation]]</p>"},{"location":"Enumeration%20Machine/#enumeration-machine","title":"Enumeration Machine","text":"<p>[!info] Alan Turing and Enumeration Machies Turing machines were invented by Alan Turing. Originally they were presented in the form of enumeration machines, since Turing was interested in enumerating the decimal expansions of computable real numbers and values of real-valued functions. Turing also introduced the concept of nondeterminism in his original paper, although he did not develop the idea. </p> <p>An Enumeration Machine has a finite control and \\(2\\) tapes, A read/write work tape and a write-only output tape. The work tape head can move in either direction and can read and write any symbol from \\(\\Gamma\\) while the output tape can only move to the right and always writes one letter from \\(\\Sigma\\) before moving.</p> <p>There is no input and no accepted or rejected state, the machine starts in its initial configuration with both the tapes empty. According to the transistion function it occasionally writes on the output tape, and when the control reaches an enumerating state, which is just a distinguished state. The word in the output tape is said to be enumerated. Then the output tape is cleared and its head is brought back to the start of the tape and the machines indefinitely continues from there. \\(L(E)\\) is the set of words ever enumerated by the enumeration machine \\(E\\).</p>"},{"location":"Enumeration%20Machine/#equivalence-of-turing-machine-and-enumerable-machines","title":"Equivalence of Turing Machine and Enumerable Machines","text":"<p>A Turing Machine can simulate an Enumerable Machine using the following Procedure.</p> <p>We let our turing Machine have 3 tapes, the first tape will contain the input. The second and the third tape will be used to simulate the Enumerable machine, each time a new word is enumerated, it is checked to match with the input, and if it does then it is accpeted. All words that can be enumerated will be eventually accepted.</p> <p>Similarly we can create an Enumerable Machine such \\(L(E) = L(M)\\), but the following procedure does not work. We can simulate the turing machine on the work tape for all \\(x\\) that are accpeted by \\(M\\), \\(E\\) copies them to its output tape.</p> <p>This does not work because the turing machine might not halt on some of the inputs hence the enumerable machine would get stuck forever.</p> <p>The solution to this problem is time sharing. The work tape of the enumeration machine will be divided into multiple sections separated by \\(\\#\\in \\Gamma\\) on the turing machine will be simulated on all those sections. For each new input we can make a new simulation, for example we can make the turing machine simulate the first input in the first step, the first and second on the second step, and the first, second and third input on the third step, this way all words accpeted by the turing machine will eventually be accepted. </p>"},{"location":"Enumeration%20Machine/#related-problems","title":"Related Problems","text":""},{"location":"Enumeration%20Machine/#references","title":"References","text":"<p>Turing Machines</p>"},{"location":"Equations%20over%20Finite%20Fields/","title":"Equations over Finite Fields","text":"<p>202305271605</p> <p>Type : #Note Tags : [[Number Theory]] [[Algebra]]</p>"},{"location":"Equations%20over%20Finite%20Fields/#equation-x-ny-n-1-in-the-field-mathbbf_p","title":"Equation \\(x ^{n}+y ^{n} = 1\\) in the field \\(\\mathbb{F}_{p}\\).","text":"<p>Assume \\(p \\equiv 1\\ (n)\\) and consider the equation \\(x ^{n}+y ^{n} = 1\\) in the field \\(\\mathbb{F}_{p}\\).</p> <p>We have \\(N(x ^{n}+ y ^{n} = 1) = \\sum_{a+b=1}N(x ^{n}= a)N(y ^{n} = a)\\). Let \\(\\chi\\) be a character of order \\(n\\). Then $$ \\begin{align} N(x ^{n}+y ^{n} = 1) &amp;= \\sum_{a+b=1} \\left( \\sum_{i=0}<sup>{n-1}\\chi</sup>{i}(a) \\right)\\left( \\sum_{j=0}<sup>{n-1}\\chi</sup>{j}(b) \\right)   \\ &amp;= \\sum_{i,j} J(\\chi<sup>{i},\\chi</sup>{j}) \\end{align} $$ When \\(i = j = 0\\), \\(J(\\varepsilon,\\varepsilon) = p\\). When \\(i + j =n\\), \\(J(\\chi^{i},\\chi^{j}) = -\\chi^{i}(-1)\\). The sum of these terms is \\(-\\sum_{i=1}^{n-1}\\chi^{i}(-1) = 1 - \\delta_{n}(-1)\\) where \\(\\delta_{n}(-1) = 1\\) if \\(-1\\) is an nth power and 0 otherwise. When \\(i = 0, j\\neq 0\\) and \\(j = 0, i\\neq 0\\) then \\(J(\\chi^{i},\\chi^{j}) = 0\\)</p> <p>Hence $$ N(x ^{n} + y ^{n} = 1) = p + (1- \\delta_{n}(-1)) + \\sum_{1 \\le i, j \\le n-1 ;   i+j \\neq n} J(\\chi<sup>{i},\\chi</sup>{j}) $$ This gives $$ |N(x ^{n} + y ^{n} = 1) - p -(1-\\delta_{n}(-1))| \\le (n-1)(n-2)\\sqrt[]{ p } $$</p>"},{"location":"Equations%20over%20Finite%20Fields/#the-equation-x_12x_22dots-x_r2-1-over-mathbbf_p","title":"The equation \\(x_{1}^{2}+x_{2}^{2}+\\dots x_{r}^{2} =  1\\) over \\(\\mathbb{F}_{p}\\).","text":"<p>Let \\(\\chi\\) be a character of order \\(2\\) (\\(\\chi(a) = \\left( \\frac{a}{p} \\right)\\)). Then $$ \\begin{align} N(x_{1}^{2} + \\dots x_{r}^{2} = 1) &amp;= \\sum_{t_{1}+t_{2}+\\dots t_{r} = 1}N(x_{1}^{2}=t_{1})\\dots N(x_{r}^{2} = 1)  \\ &amp;= J(\\varepsilon,\\varepsilon,\\dots,\\varepsilon) + J(\\chi,\\chi, \\dots,\\chi) \\ &amp;= p ^{r-1} + J(\\chi,\\chi, \\dots,\\chi) \\end{align} $$ If \\(r\\) is odd, then \\(g(\\chi)^{r} = g(\\chi)J(\\chi, \\dots ,\\chi) \\implies J(\\chi, \\dots ,\\chi) = g ^{r-1}(\\chi)\\). Since \\(g ^{2}(\\chi) = g(\\chi)g(\\overline{\\chi}) = g(\\chi)\\overline{g(\\chi)}\\chi(-1) = p\\chi(-1) = p(-1)^{(p-1)/2}\\)  Therefore, \\(g ^{r-1}(\\chi) = p ^{(r-1)/2}(-1)^{(p-1)(r-1)/4}\\).</p> <p>If \\(r\\) is even, then \\(J(\\chi,\\chi\\dots,\\chi) = -\\chi(-1)J(\\chi, \\dots ,\\chi)\\) where the term on the right has \\(r-1\\) \\(\\chi's\\). This equals \\(-\\chi(-1)p ^{(r-2)/2}\\chi(-1)^{(r-2)/2} = -\\chi(-1)^{r/2}p ^{r/2-1}\\)</p>"},{"location":"Equations%20over%20Finite%20Fields/#proposition","title":"Proposition:","text":"<pre><code>title:\nIf $r$ is odd, then \n$$\nN(x_{1}^{2}+x_{2}^{2}+\\dots+x_{r}^{2} = 1) = p ^{r-1} + p ^{(r-1)/2}(-1)^{((r-1)/2)((p-1)/2)}\n$$\nIf $r$ is even, then $$\nN(x_{1}^{2}+x_{2}^{2}+\\dots+x_{r}^{2} = 1) = p ^{r-1} - p ^{r/2-1}(-1)^{(r/2)((p-1)/2)}\n$$\n</code></pre>"},{"location":"Equations%20over%20Finite%20Fields/#a_1x_1l_1-dots-a_rx_rl_r-b-over-mathbbf_p","title":"\\(a_{1}x_{1}^{l_{1}} + \\dots + a_{r}x_{r}^{l_{r}} = b\\) over \\(\\mathbb{F}_{p}\\)","text":"<p>Here \\(a_1,a_{2},\\dots a_{r} \\in \\mathbb{F}_{p}^{*}\\) and \\(b \\in \\mathbb{F}_{p}\\). Let \\(N\\) be the number of solutions. Then $$ N = \\sum_{a_{1}t_{1}+ \\dots a_{r}t_{r} = b} N(x_{1}^{l_{1}} = t_{1})\\dots N(x_{r}^{l_{r}}=t_r) $$ Note that \\(N(x ^{m} = a) = N(x ^{d} = a)\\) where \\(d = (m,p-1)\\). Indeed, \\(\\(N(x ^{m} = a) = \\begin{cases} d &amp;  \\text{if } a ^{(p-1)/d} = 1\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\)\\) And so, \\(N(x ^{m} = a) = \\sum_{\\chi}\\chi(a)\\) where the sum is over characters \\(\\chi\\) such that \\(\\chi^{d} = \\varepsilon\\) (similar to proposition 5 in Multiplicative Characters). We can show that if \\(x ^{m} = a\\) is unsolvable, then the character \\(\\chi = \\lambda^{(p-1)/d}\\) is such that \\(\\chi^{d} = \\varepsilon\\), and \\(\\chi(a) \\neq 1\\) (this helps us prove the above expression for \\(N(x ^{m} = a)\\)).</p> <p>So we can replace the \\(l_{i}'s\\) by \\(gcd(l_{i},p-1)\\) and hence assume \\(l_{i} | p-1\\). Now let \\(\\chi_{i}\\) vary over the characters with order dividing \\(l_{i}\\).</p> <p>$$ \\begin{align}     N(x ^{m} = a) &amp;= \\sum_{\\chi_{1},\\dots \\chi_{r}}\\sum_{a_{1}t_{1} + \\dots +a_{r}t_{r} = b} \\chi_{1}(t_{1})\\dots \\chi_{r}(t_{r}) \\end{align} $$ If \\(b = 0\\), let \\(u_{i} = a_{i}t_{i}\\). Then the inner sum becomes $$ \\sum_{u_{1}+\\dots+u_{r} = 0}\\chi_{1}\\left( \\frac{u_{1}}{a_{1}} \\right)\\dots \\chi_{r}\\left( \\frac{u_{r}}{a_{r}} \\right) = \\chi_{1} ^{-1}(a_{1})\\dots \\chi_{r}^{-1}(a_{r})J_{0}(\\chi_{1},\\chi_{2},\\dots, \\chi_{r}) $$ If \\(b\\neq 0\\), let \\(u_{i} = b ^{-1} a_{i}t_{i}\\) and the inner sum becomes $$ \\sum_{u_{1}+\\dots+u_{r}=  1} \\prod\\chi_{i}(ba_{i}^{-1}u_{i}) = \\chi_{1}\\chi_{2}\\dots \\chi_{r}(b)\\chi_{1}^{-1}(a_{1})\\dots \\chi_{r}^{-1}(a_{r})J(\\chi_{1},\\dots,\\chi_{r}) $$ In both cases, if \\(\\chi_{1} = \\dots =\\chi_{r} = \\varepsilon\\), the value becomes \\(p ^{r-1}\\). If some but not all \\(\\chi_{i}\\)'s are trivial, the value becomes 0. In the first case, the value is zero unless \\(\\chi_{1}\\dots \\chi_{r} = \\varepsilon\\).</p>"},{"location":"Equations%20over%20Finite%20Fields/#proposition_1","title":"Proposition:","text":"<pre><code>title:\nIf $b=0$ then $$N = p ^{r-1} + \\sum \\chi_{1}(a_{1}^{-1})\\dots \\chi_{r}(a_{r}^{-1})J_{0}(\\chi_{1},\\dots \\chi_{r})$$\n\nhere the sum is over all r-tuples of characters such that $\\chi_{i}^{l_{i}}=\\varepsilon$, $\\chi_{i} \\neq \\varepsilon$ for $i = 1,2,\\dots r$ and $\\chi_{1}\\dots \\chi _r=\\varepsilon$.\n\n\nIf $b \\neq 0$, then $$N = p ^{r-1} + \\sum\\chi_{1}\\chi_{2}\\dots \\chi_{r}(b)\\chi_{1}^{-1}(a_{1})\\dots \\chi_{r}^{-1}(a_{r})J(\\chi_{1},\\dots,\\chi_{r})$$\n\nhere the sum is over all r-tuples of characters such that $\\chi_{i}^{l_{i}}=\\varepsilon$, $\\chi_{i} \\neq \\varepsilon$ for $i = 1,2,\\dots r$.\nIf $M_{0}$ is the number of such $r$-tuples with $\\chi_{1}\\dots \\chi_{r} = \\varepsilon$, and $M_{1}$ the number of tuples with $\\chi_{1}\\dots \\chi_{r}\\neq\\varepsilon$, then \n$$|N-p ^{r-1} | \\le M_{0}p ^{(r/2)-1} + M_{1}p ^{(r-1)/2}$$\n</code></pre>"},{"location":"Equations%20over%20Finite%20Fields/#references","title":"References","text":"<p>Finite Fields Jacobi Sums Multiplicative Characters</p>"},{"location":"Error%20Correcting%20Codes/","title":"Error Correcting Codes","text":"<p>202308101608</p> <p>Type : #Note Tags : Algorithmic Coding Theory</p>"},{"location":"Error%20Correcting%20Codes/#error-correcting-codes","title":"Error Correcting Codes","text":"<p>Let \\(c\\in\\Sigma^{n}\\) be a code and \\(t\\ge 1\\) is an integer. \\(C\\) is a \\(t-\\)error correcting code if \\(\\exists\\) a decoding function \\(\\text{Dec}\\) such that for every message \\(m\\in [|C|]\\)(data words) $$ (\\text{Dec}\\circ Ch\\circ\\text{Enc}) = id $$</p> <p>where \\(Ch\\) is an t-error channel </p> <p>The above process is $$ \\text{data word} \\longmapsto \\text{Encoding Function} \\longmapsto {Error Channel} \\longmapsto {Decoding Function} $$</p>"},{"location":"Error%20Correcting%20Codes/#references","title":"References","text":"<p>Hamming Codes</p>"},{"location":"Evaluating%20Pattern%20Matching%20in%20Lambda%20Calculus/","title":"Evaluating Pattern Matching in Lambda Calculus","text":"<p>202310251410</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note"]},{"location":"Evaluating%20Pattern%20Matching%20in%20Lambda%20Calculus/#evaluating-pattern-matching-in-lambda-calculus","title":"Evaluating Pattern Matching in Lambda Calculus","text":"<pre><code>This note is motivation for the solution: [Match Function for Enriched Lambda Calculus](&lt;./Match Function for Enriched Lambda Calculus.md&gt;).\n</code></pre>","tags":["Note"]},{"location":"Evaluating%20Pattern%20Matching%20in%20Lambda%20Calculus/#intuitive-approach","title":"Intuitive Approach","text":"<p>Consider the following example <pre><code>zipWith f []     ys     = []\nzipWith f (x:xs) []     = []\nzipWith f (x:xs) (y:ys) = f x y : zipWith f xs ys\n</code></pre> This function, takes in <code>f</code> and two lists as arguments and there is non-trivial pattern matching on the two lists.</p> <p>Given the inputs, the intuitive way that one should proceed with pattern matching goes as follows 1. the function is always pattern matched  2. we check if the list 1 is empty, if it is, then second list is pattern matched and we return an empty list 3. If it fails we move to the next step,      1. match <code>f</code>     2. we then check if the first list is non empty, if it is we check if the second list is empty     3. If successful we return empty 4. If either of those fail then we move to the 3rd part where     1. we pattern match <code>f</code>     2. Then we check if both the lists are non empty     3. Then return the output</p> <p>This example demonstrates an obvious inefficiency in this approach. Whenever we get an input that gets evaluated on multiple guards before reaching the correct one, we recompute the same matches multiple times, for examples if our input is  <pre><code>(+) [1,2] [3,4]\n</code></pre> then we match all the inputs thrice, there should be a way to match each input at most once, like one would be able do in their head.</p>","tags":["Note"]},{"location":"Evaluating%20Pattern%20Matching%20in%20Lambda%20Calculus/#case-statement","title":"Case Statement","text":"<p>To deal with the redundant computations, we rewrite the lambda expression using match case, then we get <pre><code>zipWith = \n  \\f-&gt;(\\xs'-&gt;(\\ys'-&gt;\n    case xs' of\n      Nil    =&gt; Nil\n      (x:xs) =&gt; case ys' of\n                  Nil    =&gt; Nil\n                  (y:ys) =&gt; (f x y) : zipWith xs ys))\n</code></pre></p> <p>One way to convert pattern match expressions to case statements is the Match Function</p>","tags":["Note"]},{"location":"Evaluating%20Pattern%20Matching%20in%20Lambda%20Calculus/#references","title":"References","text":"<ul> <li>Patterns</li> <li>Enriched Lambda Calculus</li> <li>Translating Haskell Programs to Lambda Calculus</li> <li>Match Function for Enriched Lambda Calculus</li> </ul>","tags":["Note"]},{"location":"Even%20is%20not%20FO-definable/","title":"Even is not FO definable","text":"<p>202310171510</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"Even%20is%20not%20FO-definable/#even-is-not-fo-definable","title":"Even is not FO-definable","text":"<p>We want to prove that \"the size of this set is even\" is not FO-definable. Suppose we have a linear order on our sets of elements \\(A,B\\). \\(A\\) has \\(6\\) elements, \\(B\\) has \\(5\\). Now, if the number of rounds is greater than \\(log(n)\\), the spoiler has a winning strategy using binary search:</p> <pre><code>title: Strategy for Spoiler\n\n- S picks the third element in $B$.\nD should pick either the third or fourth element, say third in $A$, in response.\n- S picks the last element in $B$.\nD is forced to pick the last element in $A$ in response.\n- S picks the fourth element in $B$.\nD should pick fourth or fifth element, say fifth.\n- S picks the fourth element in $A$, and D has no response to it.\n\nSpoiler wins! \u2747\ufe0f\n</code></pre>","tags":["Note","Incomplete"]},{"location":"Even%20is%20not%20FO-definable/#references","title":"References","text":"<p>First Order Logic FOL Inexpressibility Ehrenfeucht-Fraisse Games Proof</p>","tags":["Note","Incomplete"]},{"location":"Event%20Clock%20Automata/","title":"Event Clock Automata","text":"<p>202309231609</p> <p>Tags : Timed Automata</p>","tags":["Note"]},{"location":"Event%20Clock%20Automata/#event-clock-automata","title":"Event Clock Automata","text":"<pre><code>**Event Clock Automata** are not *Timed Automata* because of the use of [Event Predicting Clocks](&lt;./Event Predicting Automata.md#event-predicting-clocks&gt;).\n</code></pre> <p>Consider the language \\(\\{aab\\}\\cup\\{abb\\}\\) where the time gap between accepting the last two characters is \\(1\\).</p> <p>We can give an Event Recording Automaton for the first word but not for the second word. And we can given an Event Recording Automaton for the second word but not the first one.</p> <p>Hence both Event Recording Automata and Event Recording Automaton are not powerful enough, and neither of them are more powerful than the other. The Expressive Power Of Event Clock Automata is discussed here.</p> <p>Now we create an Event Clock Automaton by combining the two. We are allowed to use both Event Recording Clocks and Event Predicting Clocks <pre><code>Let $L=\\{a^{*}b^{*}\\}$ such that:\n- There exists an $a$ such that the first $b$ is accepted after time $1$\n- There exists a $b$ such that it is accepted $1 unit$ after the last $a$\n\n![[Drawing 2023-09-23 16.35.03.excalidraw]]\n</code></pre></p> <p>Event Clock Automata can also be determinized using the subset construction. Event Clock Languages are closed under - Union - Intersection - Complementation</p>","tags":["Note"]},{"location":"Event%20Clock%20Automata/#references","title":"References","text":"<p>Event Predicting Automata Event Recording Automata Expressive Power Of Event Clock Automata</p>","tags":["Note"]},{"location":"Event%20Predicting%20Automata/","title":"Event Predicting Automata","text":"<p>202309231509</p> <p>Tags : Timed Automata</p>","tags":["Note"]},{"location":"Event%20Predicting%20Automata/#event-predicting-automata","title":"Event Predicting Automata","text":"<p>In an Event Predicting Automata given an Alphabet \\(\\Sigma\\), we have A set Event Predicting Clocks</p>","tags":["Note"]},{"location":"Event%20Predicting%20Automata/#event-predicting-clocks","title":"Event Predicting Clocks","text":"<p>Let \\(Y_{\\Sigma}\\) be the set of Event Predicting Clocks. The value of Every \\(Y_{a}\\) gives the time stamp after which the timed automata accepts the letter \\(a\\).</p> <pre><code>Let $L=\\{aa^*b\\}$ where the time gap between the first $a$ and $b$ is $10$. \n\n![[Drawing 2023-09-23 15.24.58.excalidraw]]\n\nThere is no [Event Recording Automata](&lt;./Event Recording Automata.md&gt;) which can have the above language(It cannot talk about the first occurence of a letter)\n</code></pre>","tags":["Note"]},{"location":"Event%20Predicting%20Automata/#semantics-of-event-predicting-automata","title":"Semantics of Event Predicting Automata","text":"<p>Given a Timed Word, The value of \\(Y_i\\) at each step would look something like { width=\"500\" } Semantics on a timed word is given by \\(\\gamma_{i}\\)  Given a word \\(\\omega = (a_{1},a_{2}\\dots a_{k},\\tau_{1},\\tau_{2}\\dots \\tau_{k})\\) $$ \\gamma_{i}(y_{a})= \\begin{cases}  t_{j}=t_{i} &amp; \\text{if }\\exists j&gt;i,a_{j}\\text{ is the first \\(a\\) after \\(a_{i}\\)} \\ \\perp &amp; \\text{otherwise} \\end{cases} $$</p> <p>Just like Event Recording Automata, The languages here are closed under - Intersection - Union - Complementation</p> <p>And Event Predicting Automata are determinizable using the subset construction</p>","tags":["Note"]},{"location":"Event%20Predicting%20Automata/#references","title":"References","text":"<p>Event Recording Automata Event Clock Automata</p>","tags":["Note"]},{"location":"Event%20Recording%20Automata/","title":"Event Recording Automata","text":"<p>202309071109</p> <p>Type : #Note  Tags : Timed Automata</p>"},{"location":"Event%20Recording%20Automata/#event-recording-automata","title":"Event Recording Automata","text":"<p>An Event Recording Automata is a timed automata, such that we have a clock for each letter in the alphabet and those are the only clock \\(\\{X_{a},X_{b}\\dots X_{n}\\}\\), and a clock is reset iff the letter corresponding to the clock is read.</p> <pre><code>title: Motivation: Subset Construction\n\nResetting of clocks which go out of sync created a problem while subset construction. \n\nConsider A Non-deterministic Timed Automata. And we attempt to determinize it by the subset construction.\n\n![[Drawing 2023-09-07 12.14.54.excalidraw|200]]\n\nSo consider the above situation.\nWe can perform the subset construction to get this situation which workds\n\n![[Drawing 2023-09-07 12.18.33.excalidraw|200]]\n\nBut the problem arises when we reset clocks\n\n![[Drawing 2023-09-07 12.22.03.excalidraw|200]]\n\nIf we try to do the same thing, we encounter the following issue.\n\n![[Drawing 2023-09-07 12.25.19.excalidraw|200]]\n\nHere reseting or not reseting the clock messes up guards and timings of atleast of the component timed automata \n</code></pre> <p>An example of a language accepted by ERA would be \\(\\{(ab*a)\\ :\\ \\text{distance between the }\\text 2\\ a's\\text{ is } 1\\}\\) An example of a language which cannot be accepted by an ERA would be \\(\\{(aaa) : \\text{distance between last } 2\\ a's \\text{ is } 1\\}\\)</p> <p>Event Recording Languages are closed under  - Intersection - Union - Complementation</p> <p>And all Event Recording Automata are determinizable by the subset construction</p>"},{"location":"Event%20Recording%20Automata/#references","title":"References","text":"<p>Deterministic Timed Automata Event Predicting Automata Event Clock Automata</p>"},{"location":"Example%20for%20Proof%20System%20in%20FOL/","title":"Example for Proof System in FOL","text":"<p>202310101437</p> <p>tags : [[Logic]]</p>","tags":["Example"]},{"location":"Example%20for%20Proof%20System%20in%20FOL/#example-for-proof-system-in-fol","title":"Example for Proof System in FOL","text":"<p>We want to prove \\(t\\equiv t\\). 1. \\(x\\equiv x\\) 2. \\(y\\equiv y\\) 3. \\(\\lnot(x\\equiv x)\\implies\\lnot(y\\equiv y)\\quad(1,PL)\\) 4. \\(\\exists x\\ \\lnot(x\\equiv x)\\implies\\lnot(y\\equiv y)\\quad(3,G)\\) 5. \\(\\lnot(t\\equiv t)\\implies\\exists x\\ \\lnot(x\\equiv x)\\quad(A_{3})\\) 6. \\(\\lnot(t\\equiv t)\\implies\\lnot(y\\equiv y)\\quad(4,5,PL)\\) 7. \\(t\\equiv t\\quad(6,2,PL)\\)</p>","tags":["Example"]},{"location":"Example%20for%20Proof%20System%20in%20FOL/#related","title":"Related","text":"","tags":["Example"]},{"location":"Existence%20and%20uniqueness%20for%20linear%20systems/","title":"Existence and uniqueness for linear systems","text":"<p>202303111503</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"Existence%20and%20uniqueness%20for%20linear%20systems/#linear-systems","title":"Linear Systems","text":"<pre><code>title:\nODEs where dependence on phase is linear. \nFor example,\n$$ \\dot{x} = kx \\leftrightarrow x(t) = x(0)e^{kt}$$\n$$\n\\dot{x} = p(t)x(t) + q(t)\n$$\n</code></pre>"},{"location":"Existence%20and%20uniqueness%20for%20linear%20systems/#existence-and-uniqueness-for-1st-order-linear-systems","title":"Existence and uniqueness for 1st order Linear systems","text":"<pre><code>title:\nLet $I \\subseteq \\mathbb{R}$, be an interval and \n$$\n\\mathcal{A} : I \\to M_{n}(\\mathbb{R})\n$$\n$$\n\\mathcal{b} : I \\to \\mathbb{R}^{n}\n$$\nbe continuous on $I$.\nLet $(t_{0},x_{0}) \\in I \\times \\mathbb{R}^{n}$. Then the IVP \n$$\\dot{\\bar{x}}(t) = \\mathcal{A}(t) + \\mathcal{ b}(t); \\ \\bar{x}(t_{0}) = x_{0} \n$$\nhas a unique solution on $I$.\n</code></pre>"},{"location":"Existence%20and%20uniqueness%20for%20linear%20systems/#proof","title":"Proof:","text":"<ol> <li>WLOG assume that \\(I\\) is compact. Since if not, then we can take a sequence \\(I_{n}\\) of compact subsets of \\(I\\), which converge to \\(I\\), such that on each of the \\(I_{n}\\)'s the solution is unique, and then the solution on \\(I\\) is given by patching together the solutions on these \\(I_{n}\\)'s.</li> <li>Take \\(I = [\\alpha,\\beta]\\). Define  $$ v : I \\times \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$$ $$ (t,\\bar{x}) \\to A(t)\\bar{x} + b(t)$$</li> <li>Check that \\(v\\) satisfies the conditions of local existence and uniqueness thm (Maximal interval of existence theorem), i.e., check that \\(v\\) is continuous and is lipschitz w.r.t. \\(x\\).</li> <li>To apply the theorem, we need open domain, so we extend \\(I\\) to \\(I_{\\eta} = (\\alpha-\\eta,\\beta+\\eta)\\) for some \\(\\eta &gt; 0\\),     $$ \\widetilde{A}(t) :=  \\begin{cases} A(\\alpha) &amp; \\mathrm{if} &amp; \\alpha-\\eta &lt; t \\le \\alpha; \\ A(t) &amp; \\mathrm{if} &amp; \\alpha&lt;t&lt;\\beta;\\  A(\\beta) &amp; \\mathrm{if} &amp; \\beta \\le t &lt; \\beta + \\eta \\end{cases} $$ Similarly for \\(\\widetilde{b}\\).</li> <li>We have \\(\\widetilde{v} : I_{\\eta} \\times \\mathbb{R}^{n} \\to \\mathbb{R}^{n}\\). $$ \\widetilde{v}(t,\\bar{x}) = \\widetilde{A}(t)\\bar{x}+ \\widetilde{b}(t) $$ being lipschitz with respect to \\(x\\), now apply the local existence and uniqueness theorem which gives a unique solution on a maximal interval \\((\\omega_{-}, \\omega_{+})\\).</li> <li>Show that \\((\\omega_{-},\\omega_{+}) = I_{\\eta} \\implies \\phi_{max}|_{I}\\) is unique solution of the given IVP.    For \\(t \\in [t_{0},\\omega_{+})\\), we have (here \\(M = \\sup\\limits_{s \\in I} |b(s)|\\), and \\(\\mid\\mid \\widetilde{A}(s)\\mid\\mid \\le L\\) for all \\(s \\in I\\))    $$    \\begin{align} \\phi_{max}(t) &amp;= x_{0} + \\int\\limits_{t_{0}}^{t} \\widetilde{v}(s,\\phi_{max}(s))\\, ds\\ \\ &amp;=  x_{0} + \\int \\limits_{t_{0}}^{t}(\\widetilde{A}(s)\\phi_{max}(s) + \\widetilde{b}(s)) \\, ds \\ \\implies |\\phi_{max}(t)|&amp;\\le |x_{0}| + \\int \\limits_{t_{0}}^{t}\\mid\\mid \\widetilde{A}(s)\\mid\\mid \\mid \\phi_{max}(s)\\mid \\, ds + M(t-t_{0})\\ &amp;\\le |x_{0}| + L \\int\\limits_{t_{0}}^{t} \\mid \\phi_{max}(s)\\mid \\, ds + M(t-t_{0})\\</li> </ol> <p>\\end{align}    $$ 7. Take \\(K = |x_{0}| + M(t-t_{0})\\), then using [[Gronwall's Inequality]], we get \\(|\\phi_{max}(t)| \\le K e^{L(t-t_{0})} \\ \\forall t \\in [t_{0},\\omega_{+})\\), hence it is bounded.    But we know \\((t,\\phi_{max}(t))\\) escapes every compact subset of \\(I_{\\eta} \\times \\mathbb{R}^{n}\\) as \\(t \\to \\omega_{+}\\), hence \\(\\omega_{+} = \\beta+\\eta\\), similarly \\(\\omega_{-} = \\alpha-\\eta\\).    Therefore, \\(\\phi_{max}\\) exists on \\(I\\) and is the unique solution to the required IVP. </p>"},{"location":"Existence%20and%20uniqueness%20for%20linear%20systems/#related-problems","title":"Related Problems","text":""},{"location":"Existence%20and%20uniqueness%20for%20linear%20systems/#references","title":"References","text":"<p>[[Gronwall's Inequality]] [[Maximal Interval of Existence]]</p>"},{"location":"Existence%20of%20Gomory-Hu%20trees/","title":"Existence of Gomory Hu trees","text":"<p>202311031411</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note","Incomplete"]},{"location":"Existence%20of%20Gomory-Hu%20trees/#existence-of-gomory-hu-trees","title":"Existence of Gomory-Hu trees","text":"<p>Definition: \\(T\\) is a Gomory-Hu tree for \\(G\\) if: - \\(T=(V,F)\\), weights \\(w'\\geq 0\\), and - for every \\((u,v)\\in F\\), \\(w'(u,v)=w(\\text{a }u-v\\text{ min-cut in }G)\\) and \\(T \\setminus \\{ (u,v) \\}\\) is the \\(u-v\\) mincut in \\(G\\).</p> <p>\\(E\\) is a finite set. Submodular function: \\(f:2^{E}\\to \\mathbb{R}\\) is said to be a submodular function if \\(\\forall A,B \\subset E,\\) \\(f(A)+f(B)\\geq f(A\\cup B)+f(A\\cap B)\\).</p> <p>Alternate definition: \\(\\forall A \\subset B,\\) \\(f(A+e)-f(A)\\geq f(B+e)-f(B)\\). (Adding an element to a smaller set would make a bigger difference to its value than adding the same element to a larger set.) Equivalence between the two definitions can be seen easily by letting \\(X=A\\cap B,Y=A,B=X\\cup Z\\), and the case where \\(Z\\) is singleton gives the second definition.</p> <p>Cut: \\(W \\subseteq V\\) is a cut of \\(G=(V,E)\\). \\(f(W)=\\sum\\limits_{e\\in\\delta(W)}w(e)\\). Call \\(f\\) the cut function. <pre><code>title:\n**Lemma:** Cut function is submodular.\n*Proof:* Let $A$ and $B$ be two cuts. Draw Venn diagram, do arithmetic, bada boom bada bang!\n</code></pre></p> <pre><code>title: Useful Fact\nAny symmetric submodular function is also posi-modular.\n**Posi-modularity:** For cuts $A,B$, $f(A)+f(B)\\geq f(A\\setminus B)+f(B\\setminus A)$. Proof is easy by the previous method, or by using the fact that the function is symmetric (i.e. $f(A)=f(\\bar{A})$) . \n</code></pre> <pre><code>title:\n**Theorem:** Let $W$ be an $s,t$ mincut in $G$. Then $\\forall u,v\\in W$ there is a $(u-v)$ mincut $X$ s.t. $X \\subseteq W$.\n*Proof:* Let $X$ be a $(u,v)$ mincut but $X\\not\\subseteq W$.\n\nCase I: $t\\not\\in X$.\n$f(X)+f(W)\\geq f(X\\cup W)+f(X\\cap W)$\n$X\\cup W: (s,t)$ cut, $X\\cap W: (u,v)$ cut\n$f(X)\\leq f(X\\cap W)$, $f(W)\\leq f(X\\cup W)$\n$\\implies f(X)=f(X\\cap W)$. Thus $X\\cap W$ is also a $(u-v)$ mincut.\n\nCase II: $t\\in X$.\n$f(W)+f(X)\\geq f(X\\setminus W)+f(W\\setminus X)$\n$W\\setminus X:(u,v)$ cut\n$X\\setminus W:(s,t)$ cut\n$\\implies W\\setminus X$ is also a $(u,v)$ mincut.\n</code></pre>","tags":["Note","Incomplete"]},{"location":"Existence%20of%20Gomory-Hu%20trees/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Expander%20Graphs/","title":"Expander Graphs","text":"<p>202311021611</p> <p>Tags : Algorithmic Coding Theory</p>","tags":["Note","Incomplete"]},{"location":"Expander%20Graphs/#expander-graphs","title":"Expander Graphs","text":"<p>Bipartite graphs: \\(G=(L,R,E)\\)</p> <p>Given any \\([n,k]_{2}\\) code \\(C\\) with parity check matrix \\(H\\), we'll call the bipartite graph \\(G_{H}\\) with \\(A_{G_{H}}=H\\). Given a bipartite graph \\(G=(L,R,E)\\), with \\(|L|\\geq |R|\\), the corresponding code has parity check matrix \\(A_{G}\\).</p> <p>\\(G\\) is sparse, each vertex in \\(L\\) has at most a fixed number of neighbours in \\(R\\),...</p> <p>Left Regularity: A bipartitite graph \\(G=(L,R,E)\\) is \\(D-\\)left regular if every vertex in \\(L\\) has degree exactly \\(D\\).</p> <p>Neighbour Set:</p> <p>Unique Neighbour Set: For any left vertex set \\(S\\subset L\\), a vertex \\(u\\in R\\) is called a unique neighbour if it is adjacent to only one neighbour of \\(S\\).</p> <p>Bipartite Expander Graphs: An \\((n,m,D,\\gamma,\\alpha)\\) bipartite expander is a \\(D-\\)left regular bipartite graph \\(G=(L,R,E)\\), \\(|L|=n\\), \\(|R|=m\\), if for every \\(S \\subset L\\) with \\(|S|\\leq\\gamma n, |N(S)|\\geq\\alpha |S|\\).</p> <p>We have existential results, but the natural question is, of course, how to construct expander graphs, which we will take as a black box for the purpose of this course.</p> <p>Let \\(G\\) be an \\((n,n-k,D,\\gamma,D(1-\\epsilon))\\) bipartite expander with \\(\\epsilon &lt; \\frac{1}{2}\\), then \\(C(G)\\) is an \\([n,k,\\gamma n]\\) binary code. Proof: FTSOC, assume that \\(C(G)\\) has distance at most \\(\\gamma n\\). \\(\\exists\\) a non zero codeword \\(\\tilde{c}\\in C(G)\\), with \\(wt(\\tilde{c})\\leq\\gamma n\\). Let \\(S\\) be the set of non zero coordinates of \\(\\tilde{c}\\). \\(r\\in U(S)\\) \\(|U(S)|\\geq D(1-2\\epsilon)|S|&gt;0\\), \\(\\epsilon&lt; \\frac{1}{2},|S|\\geq 1\\)</p>","tags":["Note","Incomplete"]},{"location":"Expander%20Graphs/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Exponential%20Generating%20Functions/","title":"Exponential Generating Functions","text":"<p>202305281305</p> <p>Type : #Note Tags : [[Enumerative Combinatorics]]</p>"},{"location":"Exponential%20Generating%20Functions/#exponential-generating-functions","title":"Exponential Generating Functions","text":"<p>Exponential Generating Functions are a type of Generating Functions that are of the form  $$ F(x) = \\sum\\limits_{n\\ge0} \\frac{f(n)x^{n}}{n!} $$ where \\(f(n)\\) is the Generating Functions. These functions can generally be written in a closed from that is in the form of an exponential, hence given the name.</p>"},{"location":"Exponential%20Generating%20Functions/#references","title":"References","text":""},{"location":"Exponential%20of%20a%20Matrix/","title":"Exponential of a Matrix","text":"<p>202303121503</p> <p>Type : #Note Tags : [[Differential Equations]] [[Algebra]]</p>"},{"location":"Exponential%20of%20a%20Matrix/#exponential-of-a-matrix","title":"Exponential of a Matrix","text":"<p><pre><code>title:\nGiven $A \\in M_n(\\mathbb{R})$; we would like to define\n$$e^A := \\sum\\limits_{n=0}^\\infty \\dfrac{A^k}{k!}$$\n</code></pre> To make sure that the definition makes sense, we need to ensure that the series always converges.</p> <p>We put a metric on \\(M_{n}(\\mathbb{R})\\) induced by the norm \\(\\|A\\| = \\sup\\limits_{\\|x\\| = 1} |Ax|\\).</p>"},{"location":"Exponential%20of%20a%20Matrix/#theorem","title":"Theorem","text":"<pre><code>title:\n$M_n(\\mathbb{R})$ is complete under the metric induced by $\\| \\cdot \\|$.\n</code></pre>"},{"location":"Exponential%20of%20a%20Matrix/#proof","title":"Proof:","text":"<p>Let \\(\\{ T_{k} \\}_{k \\in \\mathbb{N}}\\) be a cauchy sequence in \\(M_{n}(\\mathbb{R})\\) i.e., \\(\\forall \\ \\epsilon&gt;0, \\exists N_{0}\\) s.t. \\(\\forall \\ k,n &gt; N_{0}, \\ \\|T_{k}-T_{n}\\| &lt; \\epsilon\\).</p> <p>Given any \\(x_{0}\\in \\mathbb{R}^{n}\\), define \\(x_{j} := T_{j}x_{0}\\ ,\\  j\\ge 1\\). Then for \\(k,n &gt; N_{0}\\) as above, $$ |T_{k}x_{0}-T_{n}x_{0}| \\le |T_{k}-T_{n}| |x_{0}| &lt; \\epsilon|x_{0}| $$ Thus the sequence of points \\(T_{j}x_{0}\\) forms a cauchy sequence in \\(\\mathbb{R}^{n}\\). Since \\(\\mathbb{R}^{n}\\) is complete, \\(\\lim_{ j \\to \\infty }x_{j} = y_{x_{0}} \\in \\mathbb{R}^{n}\\). Define \\(\\mathcal{M} : \\mathbb{R}^{n} \\to \\mathbb{R}^{n}\\) by \\(\\mathcal{M}(x) = y_{x}\\) by the above process. Show that this is a linear transform. Then verify that \\(T_{j} \\to \\mathcal{M}\\) as \\(j\\to \\infty\\). \\(\\square\\)</p>"},{"location":"Exponential%20of%20a%20Matrix/#theorem_1","title":"Theorem","text":"<pre><code>title:\nGiven $B \\in M_{n}(\\mathbb{R})$; the series $$e^B := \\sum\\limits_{k=0}^{\\infty} \\frac{1}{k!}B^{k}$$\nconverges in $M_{n}(\\mathbb{R})$.\n</code></pre>"},{"location":"Exponential%20of%20a%20Matrix/#proof_1","title":"Proof:","text":"<p>Show that this sequence of partial sums is cauchy.</p>"},{"location":"Exponential%20of%20a%20Matrix/#theorem_2","title":"Theorem","text":"<pre><code>title:\nLet $T_{1},T_{2} \\in M_{n}(\\mathbb{R})$ s.t. $T_{1}T_{2} = T_{2}T_{1}$, then $e^{T_{1}+T_{2}} = e^{T_{1}}e^{T_{2}}$\n</code></pre>"},{"location":"Exponential%20of%20a%20Matrix/#proof_2","title":"Proof:","text":"\\[ \\begin{align} e^{T_{1}}e^{T_{2}} &amp;= \\left( \\sum\\limits_{n=0}^{\\infty} \\frac{T_{1}^{n}}{n!}\\right) \\left( \\sum\\limits_{n=0}^{\\infty} \\frac{T_{2}^{n}}{n!} \\right)  \\\\ &amp;= \\sum\\limits_{n=0}^{\\infty}\\sum\\limits_{k=0}^{n} \\dfrac{T_{1}^{k}T_{2}^{n-k}}{k!(n-k)!} \\\\ &amp;=  \\sum\\limits_{n=0}^{\\infty} \\frac{(T_{1}+T_{2})^{n}}{n!} \\\\ &amp;= e^{T_{1}+T_{2}} \\end{align} \\]"},{"location":"Exponential%20of%20a%20Matrix/#how-do-you-calculate-the-exponential","title":"How do you calculate the exponential?","text":""},{"location":"Exponential%20of%20a%20Matrix/#1-exponential-of-a-diagonal-matrix","title":"1. Exponential of a diagonal matrix","text":"\\[ A = \\begin{bmatrix} \\lambda_{1}  \\\\ &amp; \\lambda_{2}  \\\\      &amp;  &amp; \\ddots \\\\      &amp;  &amp;  &amp; \\lambda_{n} \\end{bmatrix} \\implies A^{k} = \\begin{bmatrix} \\lambda_{1}^{k}  \\\\ &amp; \\lambda_{2}^{k}  \\\\      &amp;  &amp; \\ddots \\\\      &amp;  &amp;  &amp; \\lambda_{n}^{k} \\end{bmatrix} \\implies e^{A} = \\begin{bmatrix} \\sum \\limits_{k=0}^{\\infty} \\frac{\\lambda_{1}^{k}}{k!}  \\\\      &amp; \\sum \\limits_{k=0}^{\\infty} \\frac{\\lambda_{2}^{k}}{k!} \\\\      &amp;  &amp; \\ddots  \\\\      &amp;  &amp;  &amp; \\sum \\limits_{k=0}^{\\infty} \\frac{\\lambda_{n}^{k}}{k!} \\end{bmatrix} = \\begin{bmatrix} e^{\\lambda_{1}}  \\\\      &amp; e^{\\lambda_{2}} \\\\      &amp;  &amp; \\ddots  \\\\      &amp;  &amp;  &amp; e^{\\lambda_{n}} \\end{bmatrix} \\]"},{"location":"Exponential%20of%20a%20Matrix/#2-exponential-of-a-diagonalisable-matrix","title":"2. Exponential of a diagonalisable matrix","text":"<p>Given \\(A\\in M_{n}(\\mathbb{R})\\), let \\(P \\in GL_{n}(\\mathbb{R})\\) s.t. $$ P^{-1}AP = \\mathrm{diag}[\\lambda_{1},\\lambda_{2},\\dots ,\\lambda_{n}] $$ Then \\((P^{-1}AP)^{k} = \\mathrm{diag}[\\lambda_{1}^{k},\\lambda_{2}^{k},\\dots,\\lambda_{n}^{k}] = P^{-1}A^{k}P\\) Hence, $$ e^{A} = \\sum \\limits_{ k=0}^{ \\infty } \\frac{A^{k}}{k!} = \\sum \\limits_{ k=0}^{ \\infty } \\frac{1}{k!}P \\mathrm{diag}[\\lambda_{1}<sup>{k},\\lambda_{2}</sup>{k},\\dots,\\lambda_{n}^{k}] P^{-1} = P\\left(\\sum\\limits_{ k=0}^{ \\infty } \\mathrm{diag}[\\lambda_{1}^{k}, \\lambda_{2}^{k},\\dots , \\lambda_{n}^{k}] \\right) P^{-1} $$ This gives \\(e^{A} = P \\ \\mathrm{diag}[e^{\\lambda_{1}}, e^{\\lambda_{2}},\\dots , e^{\\lambda_{n}}]P^{-1}\\).</p>"},{"location":"Exponential%20of%20a%20Matrix/#3-exponential-of-a-nilpotent-operator","title":"3. Exponential of a nilpotent operator","text":"<p>Suppose \\(A^k = 0 \\ \\forall \\ k\\ge N\\) $$ e^{A} = \\sum\\limits_{ i=0}^{ N-1 } \\frac{A^{i}}{i!} $$</p>"},{"location":"Exponential%20of%20a%20Matrix/#note-a-property-e-a-ea-1-follows-from-exy-exey-when-xy-commute","title":"Note: A property: \\(e^{-A} = (e^{A})^{-1}\\) (follows from \\(e^{X+Y} = e^{X}e^{Y}\\) when \\(X,Y\\) commute).","text":""},{"location":"Exponential%20of%20a%20Matrix/#4-exponential-of-2x2-matrices","title":"4. Exponential of 2x2 matrices.","text":"<p>CASE 1: Let \\(A = \\begin{bmatrix} a &amp; b\\\\0 &amp; a\\end{bmatrix}\\). \\(A = aI + b \\begin{bmatrix}0  &amp;  1 \\\\ 0  &amp; 0\\end{bmatrix}\\) Since \\(I\\) and \\(\\begin{bmatrix}0  &amp;  1 \\\\ 0  &amp; 0\\end{bmatrix}\\) commmute, $$ e^{A} = e^{aI + b \\begin{bmatrix}0  &amp;  1 \\ 0  &amp; 0\\end{bmatrix}} = e^{aI} \\cdot e^{b\\begin{bmatrix}0  &amp;  1 \\ 0  &amp; 0\\end{bmatrix}} $$ But \\(\\begin{bmatrix}0  &amp;  b \\\\ 0  &amp; 0\\end{bmatrix}\\) is nilpotent, hence \\(e^{\\begin{bmatrix}0  &amp;  b \\\\ 0  &amp; 0\\end{bmatrix}} = I + \\begin{bmatrix}0  &amp;  b \\\\ 0  &amp; 0\\end{bmatrix} = \\begin{bmatrix}1  &amp;  b \\\\ 0  &amp; 1\\end{bmatrix}\\) Hence, \\(e^{A} = e^{a}\\cdot \\begin{bmatrix}1  &amp;  b \\\\ 0  &amp; 1\\end{bmatrix}\\). </p> <p>CASE 2: Let \\(A = \\begin{bmatrix}a  &amp;  -b \\\\ b &amp;  a\\end{bmatrix}\\). The eigenvectors for \\(A\\) are \\(\\begin{bmatrix}1 \\\\ -i\\end{bmatrix}\\) and \\(\\begin{bmatrix}1 \\\\ i\\end{bmatrix}\\) with eigenvalues \\(a+ib\\) and \\(a-ib\\) respectively. So, $$ \\begin{align} \\begin{bmatrix} 1 &amp; 1 \\ -i &amp; i \\end{bmatrix}^{-1} \\begin{bmatrix} a &amp; -b \\ b &amp; a \\end{bmatrix}  \\begin{bmatrix} 1 &amp; 1 \\ -i &amp; i \\end{bmatrix} &amp;= \\begin{bmatrix} \\lambda &amp; 0 \\ 0 &amp; \\bar{\\lambda} \\end{bmatrix} \\ \\implies  \\begin{bmatrix} 1 &amp; 1 \\ -i &amp; i \\end{bmatrix}^{-1} \\begin{bmatrix} a &amp; -b \\ b &amp; a \\end{bmatrix} ^{k} \\begin{bmatrix} 1 &amp; 1 \\ -i &amp; i \\end{bmatrix} &amp;= \\begin{bmatrix} \\lambda^{k} &amp; 0 \\ 0 &amp; \\bar{\\lambda}^{k} \\end{bmatrix}  \\ \\implies \\begin{bmatrix} a &amp; -b  \\ b &amp; a \\end{bmatrix}^{k} &amp;= \\begin{bmatrix} 1 &amp; 1 \\ -i &amp; i \\end{bmatrix} \\begin{bmatrix} \\lambda^{k} &amp; 0 \\ 0 &amp; \\bar{\\lambda}^{k} \\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 \\ -i &amp; i \\end{bmatrix}^{-1} \\ &amp;= \\begin{bmatrix} \\mathrm{Re}(\\lambda^{k})  &amp; -\\mathrm{Im}(\\lambda^{k}) \\ \\mathrm{Im}(\\lambda^{k}) &amp; \\mathrm{Re}(\\lambda^{k}) \\end{bmatrix}  \\end{align} $$ This gives $$ e^{A} = \\begin{bmatrix} \\mathrm{Re}(e^{\\lambda}) &amp; -\\mathrm{Im}(e^{\\lambda}) \\ \\mathrm{Im}(e^{\\lambda})  &amp; \\mathrm{Re}(e^{\\lambda}) \\end{bmatrix} $$ But \\(\\lambda = a+ib\\), \\(e^{\\lambda} = e^{a}e^{ib} = e^{a}(\\cos b + i\\sin b)\\). This gives, $$ e^{A} = e^{a} \\begin{bmatrix} \\cos b &amp;  -\\sin b \\ \\sin b &amp;  \\cos b \\end{bmatrix} $$</p>"},{"location":"Exponential%20of%20a%20Matrix/#related-problems","title":"Related Problems","text":""},{"location":"Exponential%20of%20a%20Matrix/#references","title":"References","text":""},{"location":"Expressability%20of%20One-Clock%20Alternating%20Timed%20Automata/","title":"Expressability of One Clock Alternating Timed Automata","text":"<p>202311041311</p> <p>Tags : Timed Automata</p>","tags":["Note","Incomplete"]},{"location":"Expressability%20of%20One-Clock%20Alternating%20Timed%20Automata/#expressability-of-one-clock-alternating-timed-automata","title":"Expressability of One-Clock Alternating Timed Automata","text":"<p>Consider the following languages: $$ L_{1}\\quad:=\\quad{(a^n,\\tau)\\;:\\;\\not\\exists i&lt;j \\text{ such that }\\tau {j}-\\tau{i}=1 } $$ This language cannot be accepted by a Timed Automata, but it can be accepted by the following Alternating Timed Automata ![[1-clock TImed Automata Example.excalidraw|600]]</p> <p>Next, consider the following language. $$ L_{2}\\quad:=\\quad{(a^k,\\tau)\\;:\\; (\\tau_{1}&lt;\\tau_{2}&lt;1)\\land(\\tau_{1}+1&lt;\\tau_{k}&lt;\\tau_{2}+1\\text{ for exactly 1 }k)} $$ This cannot be accepted by a 1-clock ATA but the following Timed Automata accepts it ![[Finite Automata example.excalidraw|600]] Proof that \\(L_2\\) cannot be accepted by a 1-clock ATA is fairly non-trivial</p> <p>Hence The expressive power for 1-clock ATA and Timed Automata comparable.</p>","tags":["Note","Incomplete"]},{"location":"Expressability%20of%20One-Clock%20Alternating%20Timed%20Automata/#references","title":"References","text":"<ul> <li>Proof that L2 cannot be accepted by a 1 Clock Timed Automata</li> <li>Alternating Timed Automata</li> <li>Alternating Finite Automaton</li> </ul>","tags":["Note","Incomplete"]},{"location":"Expressive%20Power%20Of%20Event%20Clock%20Automata/","title":"Expressive Power Of Event Clock Automata","text":"<p>202309231609</p> <p>Tags : Timed Automata</p>","tags":["Note"]},{"location":"Expressive%20Power%20Of%20Event%20Clock%20Automata/#expressive-power-of-event-clock-automata","title":"Expressive Power Of Event Clock Automata","text":"<p>![[Drawing 2023-09-23 16.46.35.excalidraw]] - Event Recording Automata are weaker than Deterministic Timed Automata because the conversion process preserves determinism - EPL\\(\\not\\subseteq\\)ERL     - \\(L=\\{aab\\}\\) where time diference between first and last letter is \\(1\\). \\(L\\in\\) EPL but \\(L\\notin\\) ERL -  ERL\\(\\not\\subseteq\\)EPL     - \\(L=\\{abb\\}\\) where time difference between first and last letter is \\(1\\). \\(L\\in\\) ERL but \\(L\\notin\\) EPL - DTL \\(\\not\\subseteq\\) EPL     - \\(L=\\{abb\\}\\) where the last letter is accepted \\(1\\) unit after the first one - EPL\\(\\not\\subseteq\\) DTL     - \\(L=\\{a^{k}b\\}\\), there is an \\(a\\) such that the \\(b\\) is accepted \\(1\\) unit after it - DTL \\(\\not\\subseteq\\) ECL     - \\(L = \\{aaa\\}\\), the time difference between accepting the first and last \\(a\\) is \\(1\\) - ECL\\(\\not\\subseteq\\)DTL     - As EPL \\(\\not\\subseteq\\) DTL - ECL \\(\\not\\subseteq\\) EPL \\(\\cup\\) ERL     - \\(L=\\{aab\\}\\cup\\{abb\\}\\) such time the time difference between first and last letters is \\(1\\) - ECL \\(\\subseteq\\) NDTL</p>","tags":["Note"]},{"location":"Expressive%20Power%20Of%20Event%20Clock%20Automata/#references","title":"References","text":"","tags":["Note"]},{"location":"Expressive%20Power%20of%20G%C3%B6del%27s%20system%20T/","title":"Expressive Power of G\u00f6del's system T","text":"<p>202307261607</p> <p>Type : #Note Tags : [[Type Theory]]</p>"},{"location":"Expressive%20Power%20of%20G%C3%B6del%27s%20system%20T/#expressive-power-of-godels-system-t","title":"Expressive Power of G\u00f6del's system T","text":""},{"location":"Expressive%20Power%20of%20G%C3%B6del%27s%20system%20T/#booleans","title":"Booleans","text":"<p>The typical example is given by the logical connectors $$ \\begin{matrix}\\neg u=\\text {D F T} u &amp; \\text{disj \\((u, v)=\\) D T} u v &amp; \\text{conj\\((u,v)=\\) D \\(v\\) F \\(u\\)} \\end{matrix} $$ Here \\(\\text{disj}(u,\\text T)\\) does not simplify to \\(\\text T\\). hence the definition is not symmetric It is in fact not possible to make a symmetric form of disjunction which, i.e. we cannot create a \\(G\\) such that  $$ \\begin{matrix}G\\langle \\text T, x\\rangle \\rightsquigarrow \\text T &amp; G\\langle x, \\text T\\rangle\\rightsquigarrow \\text T &amp; G\\langle\\text {F,F}\\rangle\\rightsquigarrow\\text F\\end{matrix} $$</p>"},{"location":"Expressive%20Power%20of%20G%C3%B6del%27s%20system%20T/#integers","title":"Integers","text":"<p>obviously \\(\\overline n=\\text S^{n}\\text O\\) represents the integers, and then we can define addition and other functions for these terms like lambda calculus.  example $$ \\text{null}(x)=\\text{R T }(\\lambda z^{\\text{Bool}}.\\lambda z'^{\\text{Int}}.\\text F) x $$ To define Addition we have  \\(\\(t[x,y]\\rightsquigarrow\\text R\\ x\\ (\\lambda z^{\\text {Int}}.\\lambda z'^{\\text{Int}}.\\text S\\ z)\\ y\\)\\)</p> <p>Note that these examples make serious use of higher types.</p> <p>We can also define iterator in the following way</p> <p>\\(\\text{it}(f) x=\\text R\\ \\overline1(\\lambda z^{\\text{Int}}.z'^{\\text{Int}}.f\\ z)x\\) where \\(f:\\text {Int}\\to\\text{Int}\\) which gives \\(\\text{it}\\) the type \\((\\text{Int}\\to\\text{Int})\\to(\\text{Int}\\to\\text{Int})\\) and \\(\\text{it}(f)x:= f^{x}\\left(\\overline 1\\right)\\)</p> <pre><code>title:Ackerman's function in System T\nFunctions like Ackerman's function is also easily definable as we can have recursive function of complex types like $\\text{Int}\\to\\text{Int}$ .\n$$\nA := \\lambda m.\\;\\;\\; \\text R\\;\\;\\;\\; \\text S\\;\\;\\;\\; (\\lambda z.\\lambda z'. (\\lambda n. \\text{it}(z)n))\\;\\;\\;\\; m\n$$\n</code></pre> <p>Iterators an be used to make the recursion function. To do so, modify the iterator to make it take more kinds of inputs</p> \\[ \\begin{align*} I &amp;: (U\\to U) \\to \\text{Int}\\to(U\\to U)\\\\ I\\ f\\ p\\ i&amp;: \\text R\\ i\\ (\\lambda z.\\lambda z'. f\\ z)\\ p \\end{align*} \\] <p>The big difference being that we can also use the \"level\" of the recursion in the function, the generalization allows us to do that, we can now define recursion in the following way $$ R u v t = \\text I v' t \\langle u,\\text O\\rangle  $$ where \\(v' \\langle a,b \\rangle = v\\ a\\ b\\)</p>"},{"location":"Expressive%20Power%20of%20G%C3%B6del%27s%20system%20T/#references","title":"References","text":"<p>Result of Expressive Power of G\u00f6del's system T</p>"},{"location":"Extension%20Field/","title":"Extension Field","text":"<p>202210121010</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Extension%20Field/#extension-field","title":"Extension Field","text":"<p>If \\(K\\) is a field containing the sub field \\(F\\), then \\(K\\) is said to be an extension field (or simply an extension) of \\(F\\), denoted \\(K / F\\) or by the diagram $$ \\begin{matrix} K\\ \\huge|\\ F \\end{matrix} $$</p> <p>\\(F\\) is sometimes called the base field of the extension.</p> <pre><code>The notation $K/F$ is short hand for \"$K$ over $F$\" and not quotient of $K$ by $F$\n</code></pre> <p>The Degree of the field extension \\(K/F\\), denoted by \\([K:F]\\), is the dimension of \\(K\\) as a vector space over \\(F\\). The extensions is said to be finite if \\([K:F]\\) is finite, infinite otherwise.</p> <p>Theorem: Let \\(F\\) be a field and \\(p(x)\\in F[x]\\) be irreducible, then there exists a field \\(K\\) containting an isomorphic copy of \\(F\\) such that \\(p(x)\\) has a root in \\(K[x]\\).  Proof: Consider the quotient: $$ K = F[x]/(x^2+1) $$ Since \\(x^2+1\\) is irreducible \\((x^2+1)\\) is a maximal ideal. Hence, \\(K\\) is a field. Let \\(\\pi:F[x]\\to K\\) be the canonical projection. Then \\(\\pi|_{F}:F\\to K\\) is a field homomorphism. Since it is not identically \\(0\\) \\(K\\) contains and isomorphic copy of \\(F\\). Let \\(F\\) be identified with its isomophic copy in \\(K\\), then we can view \\(K\\) as an extensions of \\(F\\) let \\(\\overline x = \\pi(x)\\) $$ \\begin{aligned} p(\\overline x) &amp;= \\overline{p(x)} &amp;(\\text{Since \\(\\pi\\) is a homomorphism})\\ &amp;= p(x) \\mod p(x) &amp;(\\text{in }F[x]/p(x))\\ &amp;= 0 \\end{aligned} $$</p> <p>Hence \\(K\\) contains a root of \\(p(x)\\)</p> <p>Theorem: let \\(L\\subseteq K\\subseteq F\\) be fields then $$ [L:F] = [L:K][K:F] $$ i.e the degree of the extension is multiplicative.</p>"},{"location":"Extension%20Field/#related-problems","title":"Related Problems","text":""},{"location":"Extension%20Field/#references","title":"References","text":"<p>Fields</p>"},{"location":"Extension%20of%20Proper%20Filter%20to%20Prime%20Filter%20in%20Heyting%20Algebras/","title":"Extension of Proper Filter to Prime Filter in Heyting Algebras","text":"<p>202309121209</p> <p>Tags : [[Logic]]</p>","tags":["Example"]},{"location":"Extension%20of%20Proper%20Filter%20to%20Prime%20Filter%20in%20Heyting%20Algebras/#extension-of-proper-filter-to-prime-filter-in-heyting-algebras","title":"Extension of Proper Filter to Prime Filter in Heyting Algebras","text":"<p>Lemma: Let \\(F\\) be a proper filter in \\(\\mathcal H\\) and let \\(a\\notin F\\). Then there exists a prime filter \\(G\\) such that \\(F\\subseteq G\\) and \\(a\\notin G\\).</p> <p>Proof: Consider \\(\\mathcal F\\) to be the set of filters that do not contain \\(a\\) but contain \\(F\\) with inclusion as the partial order.</p> <p>Here the union of every chain is also in \\(\\mathcal F\\) hence all chains have upper bounds. By Zorn's Lemma, There is a Maximal element \\(G\\).</p> <p>We need to prove that \\(G\\) is prime.</p> <p>Consider \\(G_{y}=\\{x\\ :\\ x\\ge g\\sqcap y\\}\\) for some \\(g\\in G\\)(Pick \\(y\\) and \\(g\\) and find the smallest filter which contains both). If \\(c\\sqcup b\\in G\\). Then consider \\(G_{c}\\) and \\(G_{b}\\).</p> <p>If both of them contain \\(a\\) Then there are \\(g_{1},g_{2}\\in G\\) such that \\(g_{1}\\sqcap b\\le a\\) and \\(g_{2}\\sqcap c\\le a\\). since \\(g_{1}, g_{2}, b\\sqcup c\\in G\\) we have \\((g_{1}\\sqcap g_{2})\\sqcap (b\\sqcup c)\\in G\\).</p> <p>We also have \\(a=a\\sqcup a\\ge (g_{1}\\sqcap g_{2}\\sqcap b)\\sqcup(g_{1}\\sqcap g_{2}\\sqcap c) = (g_{1}\\sqcap g_{2})\\sqcap(b\\sqcup c)\\in G\\) which is a contradiction.</p> <p>Thus one of \\(G_{b}\\) and \\(G_{c}\\in\\mathcal F\\) and by primality of \\(G\\) we have either \\(b, c\\in G\\) thus \\(G\\) is prime.</p>","tags":["Example"]},{"location":"Extension%20of%20Proper%20Filter%20to%20Prime%20Filter%20in%20Heyting%20Algebras/#references","title":"References","text":"","tags":["Example"]},{"location":"Exterior%20Algebra/","title":"Exterior Algebra","text":"<p>202210100910</p> <p>Type : #Note Tags : [[Calc]]</p>"},{"location":"Exterior%20Algebra/#exterior-algebra","title":"Exterior Algebra","text":"<p>We define \\(\\forall\\ k\\) a subspace $$ \\Lambda^k(V) \\subset \\mathcal T^k(V) $$ And $$ \\Lambda^*(V) = \\mathbb{R} \\oplus \\Lambda^1(V) \\oplus \\Lambda^2(V)\\oplus\\dots\\oplus \\Lambda^n(V) $$ \\(\\Lambda\\) is a skew symmetric matrix An element \\(A\\in\\Lambda^k(V)\\) is a k-multilinear map such that $$ A(v_{\\sigma(1)}\\dots v_{\\sigma(n)}) = \\text{sgn}(\\sigma)A(v_1\\dots v_n) $$</p> <p>Lemma: \\(\\dim \\Lambda^k(V) = {n\\choose k}\\) if \\(0\\le k \\le n\\)  and \\(0\\) if \\(k &gt; n\\) Proof of Lemma: Let \\(k &gt; n\\) and let \\(A \\in \\Lambda^k(V)\\)  Choose a basis \\((e_1\\dots e_n)\\) if \\(v_1\\dots v_k\\) are \\(k\\) vectors is \\(V\\) $$ \\begin{aligned} \\text{write}  v_i &amp;= \\sum \\underbrace{v_{ji}}{\\in\\mathbb{R}}e_j\\ \\text{If} k &gt; n, e{ji_1} &amp;= e_{ji_2} \\text{ for some } i_1 \\text{ and } i_2\\ \\implies A(v_1\\dots v_k) &amp;= 0\\ \\text{If } k &amp;\\le n\\  A(v_1\\dots v_k) &amp;= \\sum \\sum v_{j1}v_{j2}\\dots A(e_{j1},e_{j2}\\dots e_{jk})\\ \\end{aligned} $$ Which can be characterized by \\(A(e_{j1},e_{j2}\\dots e_{jk})\\) Hence \\(\\dim \\Gamma^k(V) = {n\\choose k}\\) </p> <p>$$ \\begin{aligned} A&amp;\\in \\Lambda^lV &amp;\\times B&amp;\\in\\Lambda^mV &amp;\\times A\\wedge B&amp;\\in\\Lambda^{l+m} \\end{aligned} $$ <pre><code>Finish the diagram for types in multiplication in Exterior Algebra\n</code></pre></p> <p>We can use Alt: \\(\\mathcal T^k(V) \\to \\Gamma^k(V)\\) to get a multilinear map from \\(\\Gamma^K(V)\\) </p> \\[ A \\wedge B = \\frac{(l+m)!}{l!\\times m!}\\text{Alt}(A\\otimes B) $$ This product is Associative on $\\Lambda^k(V)$, more precisely $$ A\\wedge (B\\wedge C) = (A\\wedge B)\\wedge C $$ Further $$ A\\wedge B = (-1)^{lm}B\\wedge A \\] <pre><code>title: Reason to study Exterior Algebra\n[Determinants](&lt;./Determinants.md&gt;) of matrices form an Exterior Algebra which is a large part of why people study Exterior Algebra\n</code></pre>"},{"location":"Exterior%20Algebra/#related-problems","title":"Related Problems","text":"<p>Calc - Problem Session - 7 Oct</p>"},{"location":"Exterior%20Algebra/#references","title":"References","text":""},{"location":"Exterior%20Differentiation/","title":"Exterior Differentiation","text":"<p>202211040911</p> <p>Type : #Note Tags : [[Calc]]</p>"},{"location":"Exterior%20Differentiation/#exterior-differentiation","title":"Exterior Differentiation","text":"<p>\\(U\\subset V\\)  there are \\(0\\)-forms, \\(1\\)-forms, \\(2\\)-forms\\(\\dots\\) \\(l\\)-forms\\(\\dots\\)\\(n\\)-forms we make a function \\(d:l-\\)forms \\(\\to (l+1)-\\)forms having the condition 1. \\(d\\circ d = 0\\) 2. \\(d:0-\\)form \\(\\to 1-\\)form 3. \\(d(\\eta \\wedge \\rho) = d(\\eta)\\wedge \\rho + (-1)^l\\eta\\wedge d(\\rho)\\)</p>"},{"location":"Exterior%20Differentiation/#d0-forms-to-1-forms","title":"\\(d:0-\\)forms \\(\\to 1-\\)forms","text":"<p>Choose a basis \\(e_1,e_2\\dots e_n\\) for \\(V\\) get the dual basis \\(\\phi_1, \\phi_2\\dots \\phi_n\\) for \\(V^*\\) $$ df = \\sum_{i=1}^n {\\partial f \\over \\partial x_i}\\phi_i $$ by definition the derivative  \\(\\(d_tf(p)[\\vec u]= \\lim\\limits_{\\epsilon \\to 0} {f(p+\\epsilon\\vec u) - f(p)\\over\\epsilon}\\)\\) To prove that both the derivatives are equivalent: $$ \\begin{aligned} df(p)\\left[\\vec u = \\sum u_je_j\\right] &amp;= \\left{\\sum{\\partial f\\over\\partial x_i}(p)\\phi_i\\right}\\left[\\sum u_je_j\\right]\\ &amp;=\\sum_{i,j}{\\partial f\\over \\partial x_i}u_j\\phi_i[e_j]\\ \\end{aligned} $$ Applying the function on a basis $$ \\begin{aligned} df(p)[e_j] &amp;= \\sum_i{\\partial f \\over \\partial x_i}(p)\\phi_i[e_j]\\ &amp;= {\\partial f \\over \\partial x_j}(p)\\ &amp;= \\lim\\limits_{t\\to 0}{f(p+te_j) -f(p)\\over t}\\ &amp;=Df(p)[e_j]\\ \\end{aligned} $$ Putting it all back together $$ \\begin{aligned} df(p)\\left[\\vec u = \\sum u_je_j \\right] &amp;= \\sum u_j df(p)[e_j]\\ &amp;= \\sum u_j Df(p)[e_j]\\  &amp;= Df(p)[\\vec u] \\end{aligned} $$</p>"},{"location":"Exterior%20Differentiation/#related-problems","title":"Related Problems","text":""},{"location":"Exterior%20Differentiation/#references","title":"References","text":""},{"location":"Extrapolation%20for%20Zone%20Automata/","title":"Extrapolation for Zone Automata","text":"<p>202310132210</p> <p>Tags : Timed Automata</p>","tags":["Note","Incomplete"]},{"location":"Extrapolation%20for%20Zone%20Automata/#extrapolation-for-zone-automata","title":"Extrapolation for Zone Automata","text":"<pre><code>The purpose of *Extrapolation* is to ensure a finite number of _symbolic states_ using an approach which is similar to [[Region Automata]]. \n\nWe produce an operator $\\text{extra}$ which extends the zone of a symbolic state, and we change to $\\text{Trans}$ rule to accomodate that.\nThen we show that the rand of the operator is finite, that is for all infinte sequences $Z_i$ of zones we find $\\text{extra}(-,Z_{i+k})\\subseteq\\text{extra}(-,Z_{i})$ .\nThis will directly give the termination of forward analysis.\n</code></pre>","tags":["Note","Incomplete"]},{"location":"Extrapolation%20for%20Zone%20Automata/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"FOL%20Inexpressibility/","title":"FOL Inexpressibility","text":"<p>202310131210</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"FOL%20Inexpressibility/#fol-inexpressibility","title":"FOL Inexpressibility","text":"<p>Suppose \\(X\\) is a set of formulas that is satisfiable. If the logical language is countable (countably many constants), then \\(X\\) has a (at most) countable model, i.e. (there is a countable universe in which \\(X\\) is satisfied).</p> <p>Proof: Suppose \\(\\mathcal{I}\\vDash X\\). Then \\(X\\cup\\phi_{Eq}\\cup\\phi_{Q}\\cup\\phi_{H}\\) is propositionally satisfiable. So \\(X\\cup\\phi_{Eq}\\cup\\phi_{Q}\\cup\\phi_{H}\\) is satisfiable in a countable model.</p> <pre><code>title:\nAs a result of above, we can't define the set of real numbers in FOL using countably many constants.\n</code></pre>","tags":["Note","Incomplete"]},{"location":"FOL%20Inexpressibility/#lowenheim-skolem-theorem","title":"Lowenheim-Skolem theorem","text":"<p>If a set \\(X\\) of formulas has finite models that are arbitrarily large, then it also has a (at least) countable model.</p> <p>Proof: Let \\(\\lambda_{n}\\) be the formula \\(\\lambda_{n} = \\exists x_{1}\\dots x_{n}\\bigwedge\\limits_{\\substack{i\\neq j\\\\ i,j\\in\\{1,\\dots,n\\}}}x_{i}\\not\\equiv x_{j}\\) (\\(\\lambda_{n}\\) just says that there are at least \\(n\\) distinct elements in the universe.)</p> <p>\\(Y=X\\cup\\{\\lambda_{n}|n\\ge 1\\}\\)</p> <p>(We construct this (at least) countable model. Every model for \\(Y\\), if it exists, is countable. So we only need to prove its existence, aka satisfiability of \\(Y\\). We can do that by giving a model for each finite subset \\(X\\cup{\\lambda_{n}}\\) is satisfiable and then compactness go brr.)</p>","tags":["Note","Incomplete"]},{"location":"FOL%20Inexpressibility/#upward-ls-theorem","title":"Upward LS theorem","text":"<p>Suppose \\(X\\) has an infinite model. Then for every set \\(A\\), \\(X\\) has a model whose cardinality is at least that of \\(A\\).</p> <p>Proof: \\(C=\\{c_{a}|a\\in A\\}\\) \\(Y=X\\cup\\{\\lnot(c_{a}=c_{b})|a,b\\in A,a\\neq b\\}\\) \\(Y_{f}\\subset Y\\)</p>","tags":["Note","Incomplete"]},{"location":"FOL%20Inexpressibility/#connectivity-of-arbitrary-graphs-is-not-fo-definable","title":"Connectivity of arbitrary graphs is not FO-definable","text":"<p>\\(G=(V,E)\\quad L:R=\\{E\\}\\) There is an edge between \\(x,y:E(x,y)\\) There is a path of length \\(2\\) between \\(x,y:\\exists z\\ E(x,z)\\land E(z,y)\\)</p> <p>Can we have a formula for \"There is a path between \\(x,y\\)\"? No! Proof: \\(p_{n}=\\exists x_{1}\\dots x_{n}\\ E(x,x_{1})\\land E(x_{1},x_{2})\\land\\dots\\land E(x_{n},y)\\) Suppose \"Path between \\(x,y\\)\" is definable, say by the formula \\(\\varphi(x,y)\\). \\(\\varphi_{C}=\\forall x\\forall y\\ \\varphi(x,y)\\) \\(q_{n}=\\bigwedge\\limits_{1\\le i\\le n}\\lnot p_{n}(x,y)\\) \\(X=\\{\\varphi_{C}\\}\\cup\\{q_{n}|n\\ge 1\\}\\)</p> <p>Now any finite subset of \\(X\\) is satisfiable, but \\(X\\) itself will not be if we take \\(G\\) to be an infinite (say path) graph.</p> <p>What do we do if we want to say this about finite graphs? Can we say that \"Path between \\(x,y\\)\" is not definable in finite graphs?</p> <p>We can probably try compactness? Surprise! It doesn't work. Proof:</p> <p>Suppose \\(\\mathcal{I}_{1},\\mathcal{I}_{2}\\) are finite structures that agree on all FO sentences. Then \\(\\mathcal{I}_{1},\\mathcal{I}_{2}\\) are isomorphic. (We will prove this for graphs for ease, although we can prove this for any structure.) (This also gives us that given any graph \\(G\\), we can define an FO structure that models it, up to isomorphism.) Proof: \\(G=(V,E)\\), \\(|V|=n\\) \\(\\varphi_{G}:\\exists x_{1}\\dots x_{n}\\) \"\\(x_{i},x_{j}\\) are pairwise distinct\" \\(\\land\\bigwedge\\limits_{v_{1},v_{2}\\in E}E(x_{1},x_{2})\\land\\bigwedge\\limits_{v_{1},v_{2}\\not\\in E}\\lnot E(x_{1},x_{2})\\)</p> <p>For a bigger graph our nesting is deeper. Is it required intrinsically by the graph itself? If so, can we use the depth or complexity of the nesting to say something like \"If you can only go this deep then you can't distinguish between graphs of complexity more than that\". This is what we'll call the quantifier rank of the FO formula.</p>","tags":["Note","Incomplete"]},{"location":"FOL%20Inexpressibility/#references","title":"References","text":"<p>First Order Logic</p>","tags":["Note","Incomplete"]},{"location":"FOL-Examples%20for%20Definability/","title":"FOL Examples for Definability","text":"<p>202309221209</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"FOL-Examples%20for%20Definability/#first-order-logic-examples-for-definability","title":"First Order Logic-Examples for Definability","text":"<p>Terms: \\(t = c|x|f(t_{1},\\dots t_n)\\) Formulas: \\(\\phi = t_{1}\\equiv t_{2} | R(t_{1},\\dots t_{n}) | \\lnot \\phi | \\phi \\lor \\phi | \\exists x \\phi\\) </p> <p>\\(L = (R,F,C)\\) \\(R = \\{&lt; \\text{binary}\\}, F = \\{\\},C=\\{\\}\\)</p> <ol> <li> <p>\\(\\phi: \\forall x \\forall y\\) \\((x&lt;y)\\lor(x\\equiv y)\\lor(y&lt;x)\\) \\(\\mathcal{I}:\\) \\(S=\\mathbb{N}, &lt;^{\\mathcal{I}} (\\text{usual order on }\\mathbb{N})\\)</p> </li> <li> <p>\\(\\mathcal{I}:\\) \\(S=\\{a,b,c,d\\},R=\\{a&lt;b,a&lt;c,b&lt;d,c&lt;d\\})\\)</p> </li> <li> <p>\\(\\exists x \\forall y (x \\not\\equiv y \\rightarrow y&lt;x)\\) - There is a maximum element</p> </li> <li>\\(\\forall y \\exists x(x \\not\\equiv y \\rightarrow y&lt;x)\\) - Always true</li> <li>\\(\\forall x \\forall y \\forall z ((x&lt;y \\land y&lt;z) \\rightarrow x&lt;z)\\) - Checks for transitivity</li> <li>\\(\\exists x \\forall y (x \\not\\equiv y \\rightarrow x&lt;y)\\) - There is a minimum element (integers vs naturals)</li> <li>\\(\\forall x \\forall z \\exists y ((x \\not\\equiv z) \\rightarrow (x&lt;y \\land y&lt;z))\\) - Differentiates between rationals and integers</li> </ol> <p>But we can't differentiate between \\((0,\\infty)\\) and \\((0,1)\\) in the first order structure \\((S_{i},&lt;)\\). In fact we can show an isomorphism between the two.</p>","tags":["Note","Incomplete"]},{"location":"FOL-Examples%20for%20Definability/#free-variables","title":"Free variables","text":"<p>We will define by induction. - \\(fv(\\phi) \\subset\\) Var. - \\(fv(t)=\\) the set of all variables occurring in \\(t\\) - \\(fv(t_{1}\\equiv t_{2})=fv(t_{1})\\cup fv(t_{2})\\) - \\(fv(R(t_{1},\\dots,t_{n}))=fv(t_{1})\\cup\\dots\\cup fv(t_{n})\\) - \\(fv(\\lnot\\phi) = fv(\\phi)\\) - \\(fv(\\phi_{1}\\lor\\phi_{2})=fv(\\phi_{1})\\cup fv(\\phi_{2})\\) - \\(fv(\\exists x \\phi) = fv(\\phi)\\setminus\\{x\\}\\)</p>","tags":["Note","Incomplete"]},{"location":"FOL-Examples%20for%20Definability/#isomorphism","title":"Isomorphism","text":"<p>Lemma: Suppose \\(\\phi\\) is a FO formula. \\(\\sigma_{1}\\), \\(\\sigma_{2}\\) are s.t. \\(\\sigma_{1}(x)=\\sigma_{2}(x)\\) for all \\(x \\in fv(\\phi)\\). Then \\((M,\\sigma_{1})\\models\\phi\\) iff  \\((M,\\sigma_{2})\\models\\phi\\). Proof by induction on the structure of \\(\\phi\\).</p> <p>\\(\\mathcal{I}_{1}=(M_1,\\sigma_1)\\) \\(\\mathcal{I}_{2}=(M_2,\\sigma_2)\\) \\(\\mathcal{I}_1\\) and \\(\\mathcal{I}_2\\) are said to be isomorphic for \\(L=(R,F,C)\\) if there is a bijection \\(\\sigma\\) : \\(S_{1}\\to S_{2}\\) s.t. for every \\(r\\in R\\), \\((e_{1,\\dots})\\)</p>","tags":["Note","Incomplete"]},{"location":"FOL-Examples%20for%20Definability/#references","title":"References","text":"<p>Isomorphism Between First Order Interpretations</p>","tags":["Note","Incomplete"]},{"location":"False%20Assumptions%20easy%20to%20make%20about%20Patterns/","title":"False Assumptions easy to make about Patterns","text":"<p>202310291633</p> <p>tags : [[Programming Languages]]</p>","tags":["Example"]},{"location":"False%20Assumptions%20easy%20to%20make%20about%20Patterns/#false-assumptions-easy-to-make-about-patterns","title":"False Assumptions easy to make about Patterns","text":"","tags":["Example"]},{"location":"False%20Assumptions%20easy%20to%20make%20about%20Patterns/#symmetry","title":"Symmetry","text":"<p>Consider the following function</p> <pre><code>zipWith' f []     ys     = []\nzipWith' f xs     []     = []\nzipWith' f (x:xs) (y:ys) = f x y : zipWith' f xs ys\n</code></pre> <p>and consider the following evaluation</p> <pre><code>zipWith' (+) bottom []\n</code></pre> <p>where evaluations of the <code>bottom</code> would fail to terminate because it is matched against <code>[]</code> for the first pattern. </p> <p>On the other hand <pre><code>zipWith' (+) [] bottom\n</code></pre> does terminate because the first pattern matches.</p> <p>Hence even though the function definition does look like it should be symmetric in the sense that if one of the inputs is empty then other one does not matter. </p> <p>This is not true for the original definition <pre><code>zipWith f []     ys     = []\nzipWith f (x:xs) []     = []\nzipWith f (x:xs) (y:ys) = f x y : zipWith xs ys\n</code></pre> This definitions makes the asymmetry apparent.</p>","tags":["Example"]},{"location":"False%20Assumptions%20easy%20to%20make%20about%20Patterns/#ordering-non-overlapping-patterns","title":"Ordering Non-overlapping Patterns","text":"<p>Consider the following function definition</p> <pre><code>diagonal x     True  False = 1\ndiagonal False y     True  = 2\ndiagonal True  False z     = 3\n</code></pre> <p>These 3 equations are all non-overlapping, however another false assumption that is easy to make because of this is that, it should just be fair to reorder these without changing the function. However there is a slight problem with this, the slight problem being the evaluation of  <pre><code>diagonal bottom True False\n</code></pre> Under the above definition, this evaluates to \\(1\\), but if any other expressions was return before the first one, this would not have evaluated.</p> <p>[!success] Uniform Definition of Haskell Functions The set of definitions that that do have the property that they can be reordered  are called Uniform Definitions</p>","tags":["Example"]},{"location":"False%20Assumptions%20easy%20to%20make%20about%20Patterns/#related","title":"Related","text":"<ul> <li>Ordering Equations in Uniform Definitions</li> <li>Independence of meaning from changing the order on the left hand side implies uniform definition</li> <li>Uniform Definition of Haskell Functions</li> </ul>","tags":["Example"]},{"location":"Fejer%20Kernel/","title":"Fejer Kernel","text":"<p>2022-11-15 21:46 pm</p> <p>Type : #Note Tags : [[Analysis]]</p>"},{"location":"Fejer%20Kernel/#fejer-kernel","title":"Fejer Kernel","text":"<pre><code>title: Motivation\n[Dirichlet Kernels](&lt;./Dirichlet Kernels.md&gt;) fail to be good kernels, but their averages are better behaved and are good kernels.\n</code></pre> <pre><code>title: Definition\nThe N-th Fejer Kernel is defined by:\n$$F_N(x) = \\dfrac{D_0(x) + D_1(x) + \\cdots D_{N-1}(x)}{N}$$\n</code></pre>"},{"location":"Fejer%20Kernel/#-since-s_nf-fd_n-we-can-say-that-sigma_nfx-ff_nx-where-sigma_nfx-dfracs_0x-s_1x-cdots-s_n-1xn","title":"- Since \\(S_n(f) = f*D_n\\), we can say that \\(\\sigma_N(f)(x) = (f*F_N)(x)\\) where \\(\\(\\sigma_N(f)(x) = \\dfrac{S_0(x) + S_1(x) + \\cdots + S_{N-1}(x)}{N}\\)\\)","text":"<pre><code>title: Proposition\nWe have a closed form for the Fejer Kernel, closely related to that of the [Dirichlet Kernels](&lt;./Dirichlet Kernels.md&gt;).\n  $$F_N(x) = \\dfrac{1}{N}\\dfrac{\\sin^2(Nx/2)}{\\sin^2(x/2)}$$\n</code></pre>"},{"location":"Fejer%20Kernel/#proof","title":"Proof:","text":"<ul> <li>Use \\(D_n(x) = \\displaystyle\\sum\\limits_{j=-n}^{n}\\omega^j = \\dfrac{\\omega^{-n}-\\omega^{n+1}}{1-\\omega}\\)where \\(\\omega = e^{ix}\\).</li> <li>This gives \\(\\(F_N(x) = \\dfrac{1}{N}\\sum\\limits_{n=0}^{N-1}\\dfrac{\\omega^{-n}-\\omega^{n+1}}{1-\\omega}\\)\\)$$ \\implies F_N(x) = \\dfrac{1}{N(1-\\omega)}\\sum\\limits_{n=0}<sup>{N-1}(\\omega</sup>{-n}-\\omega^{n+1})$$   $$ \\implies F_N(x) = \\dfrac{1}{N(1-\\omega)<sup>2}(\\omega</sup>{1-N}-2\\omega+\\omega^{N+1})$$   $$ \\implies F_N(x) = \\dfrac{1}{N(\\omega<sup>{-1/2}-\\omega</sup>{1/2})<sup>2}(\\omega</sup>{-N}-2+\\omega^{N}) = \\dfrac{1}{N(\\omega<sup>{-1/2}-\\omega</sup>{1/2})<sup>2}(\\omega</sup>{-N/2}-\\omega<sup>{N/2})</sup>2$$   This gives the desired result.</li> </ul> <pre><code>title: Theorem\nThe Fejer Kernel is a good kernel.\n</code></pre>"},{"location":"Fejer%20Kernel/#proof_1","title":"Proof:","text":"<ul> <li>The first property of good kernels can be verified, since it also holds for dirichlet kernels.</li> <li>The second property obviously holds because the first property holds and \\(F_N(x)\\) is always positive.</li> <li>\\(sin ^2(x/2) \\ge c_\\delta &gt; 0\\) for all \\(\\delta \\le |x| \\le \\pi\\) and so, \\(F_N(x) \\le 1/Nc_\\delta\\) for such \\(x\\). Which gives that \\(\\(\\int\\limits_{\\delta\\le|x|\\le\\pi} F_N(x) \\to 0\\)\\) as \\(N \\to \\infty\\).</li> </ul> <pre><code>title: Theorem\nIf $f$ is integrable on the circle then the fourier series of $f$ is cesaro summable to $f$ at all points of continuity of $f$.\nIf $f$ is continuous, then it is uniformly cesaro summable to $f$.\n</code></pre>"},{"location":"Fejer%20Kernel/#proof_2","title":"Proof:","text":"<p>This is a direct application of property (1) in Good Kernels.</p> <p><pre><code>title: Corollary\nContinuous functions on the circle can be approximated by trigonometric polynomials.\n</code></pre> - This is closely related to the [[Weierstrass Approximation Theorem]].</p>"},{"location":"Fejer%20Kernel/#related-problems","title":"Related Problems","text":"<p>Dirichlet Kernels</p>"},{"location":"Fejer%20Kernel/#references","title":"References","text":"<p>Good Kernels Cesaro Summability Dirichlet Kernels [[Weierstrass Approximation Theorem]]</p>"},{"location":"Fields/","title":"Fields","text":"<p>202210120910</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Fields/#fields","title":"Fields","text":"<pre><code>title: Field\nA Field is a commutative ring in which every element has its inverse. Equivalently, the set $F^\\times = F \\setminus \\{0\\}$ of non zero elements is an abelian group under multiplication.\n</code></pre> <p>The Characterisic of a field \\(F\\), denoted \\(\\text{ch}(F)\\) is the smallest positive integer \\(p\\) such that \\(p\\cdot1_F=0\\) if such \\(p\\) exists, otherwise it is defined to be \\(0\\).</p> <p>Let \\(\\varphi: F\\to F'\\) be a field homomorphism, then \\(\\varphi\\) is either identically \\(0\\) or an injection. (because the kernel is an ideal in \\(F\\) which is either \\(0\\) or the ring itself.) </p> <p>If \\(K\\) is a field containing the subfield \\(F\\), \\(K\\) is called the Extension Field of \\(F\\) denoted by \\(K/F\\) or by the diagram $$ \\begin{matrix} K\\ \\huge|\\ F \\end{matrix} $$</p> <p>\\(\\alpha \\in K\\) is said to be algebraic over \\(F\\) if it is the root of some non-zero polynomial \\(f(x)\\in F[x]\\). If not, \\(\\alpha\\) is called Transcendental over \\(F\\). If \\(\\forall\\alpha\\in K\\) is algebraic over \\(F\\), \\(K\\) is called an Algebraic Extension of \\(F\\). </p> <p>If \\(K_1\\) and \\(K_2\\) are subfields of \\(K\\), then the  composite field of \\(K_1\\) and \\(K_2\\) denoted by \\(K_1K_2\\), is the smallest subfield of \\(K\\) which contains both \\(K_1\\) and \\(K_2\\). Similarly, the composite field of any collection of subfields of \\(K\\) is the smallest subfield of \\(K\\) which contains all the other ones.</p>"},{"location":"Fields/#related-problems","title":"Related Problems","text":""},{"location":"Fields/#references","title":"References","text":""},{"location":"Filters%20in%20Heyting%20Algebras/","title":"Filters in Heyting Algebras","text":"<p>202309121209</p> <p>Tags : [[Logic]]</p>","tags":["Note"]},{"location":"Filters%20in%20Heyting%20Algebras/#filters-in-heyting-algebras","title":"Filters in Heyting Algebras","text":"<p>A Filter in a Heyting Algebra \\(\\mathcal H = \\langle H,\\sqcup,\\sqcap,\\Rightarrow,1,0\\rangle\\) is a non empty subset \\(F\\) of \\(H\\) such that  - \\(a, b\\in F\\) implies \\(a\\sqcap b\\in F\\) - \\(a\\in F\\) and \\(a\\le b\\) implies \\(b\\in F\\) \\(F\\) is Proper iff \\(F\\subsetneq H\\).</p> <p>\\(F\\) is called a prime filter if \\(a\\sqcup b\\in F\\) implies \\(a\\in F\\) or \\(b\\in F\\)</p>","tags":["Note"]},{"location":"Filters%20in%20Heyting%20Algebras/#references","title":"References","text":"<p>Filters</p>","tags":["Note"]},{"location":"Filters/","title":"Filters","text":"<p>202304031304</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Filters/#filters","title":"Filters","text":""},{"location":"Filters/#references","title":"References","text":""},{"location":"Finite%20Essentially%20Uninterpreted%20%28FEU%29%20fragment/","title":"Finite Essentially Uninterpreted (FEU) fragment","text":"<p>202311141411</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"Finite%20Essentially%20Uninterpreted%20%28FEU%29%20fragment/#finite-essentially-uninterpreted-feu-fragment","title":"Finite Essentially Uninterpreted (FEU) fragment","text":"","tags":["Note","Incomplete"]},{"location":"Finite%20Essentially%20Uninterpreted%20%28FEU%29%20fragment/#satisfiability-modulo-theories-smt","title":"Satisfiability Modulo Theories (SMT)","text":"<p>Consider \\((\\mathbb{N};+;&lt;;0)f,g,p,r\\), where the things in the bracket are 'interpreted' (i.e. they have the general interpretation) and the ones outside are 'uninterpreted' (i.e. they can be anything, which can later be interpreted). We want to check the satisfiability of \\(\\varphi\\) modulo these constraints. This problem is what is solved by SMT solvers.</p>","tags":["Note","Incomplete"]},{"location":"Finite%20Essentially%20Uninterpreted%20%28FEU%29%20fragment/#skolemization","title":"Skolemization","text":"<p>We can trade existential quantified variables for (Skolem) functions. example $$ \\varphi:\\quad \\forall \\bar{x}\\exists y(\\dots r(\\dots y \\dots)\\dots) $$ means that for every tuple \\(\\bar{x}\\) we can find a \\(y\\) that satisfies the formula, This can be represented with the function \\(f:S^{k}\\to S\\) and we can rewrite the formula as $$ \\varphi:\\quad \\forall \\bar{x}(\\dots r(\\dots f(\\bar{x})\\dots)\\dots) $$</p>","tags":["Note","Incomplete"]},{"location":"Finite%20Essentially%20Uninterpreted%20%28FEU%29%20fragment/#essentially-uninterpreted-formulas","title":"Essentially uninterpreted formulas","text":"<p>A formula \\(\\varphi\\) is essentially uninterpreted if any variable in \\(\\varphi\\) appears only as an argument of uninterpreted function or predicate symbols (= relation symbols). So \\(f(g(x_{1})+0)\\leq h(x_{1})\\cup p(f(x_{1})+ b.x_{2})\\) is uninterpreted while \\(x_{1}+f(x_{2})\\) is not, because \\(x_{1}\\) is an argument of \\('+'\\), which is interpreted.</p> <p>Conjunctive Normal Form: - Literal: atomic formula or its negation - Clause: disjunction of literals - CNF formula: conjunction of clauses</p> <p>Ground formulas: formulas that don't use variables</p> <pre><code>title: Goal\nGiven a CNF formula $\\varphi$, we would like to come up with a Ground formula $\\varphi^{*}$ s.t. $\\varphi^{*}$ is satisfiable iff $\\varphi$ is satisfiable.\n</code></pre>","tags":["Note","Incomplete"]},{"location":"Finite%20Essentially%20Uninterpreted%20%28FEU%29%20fragment/#notation","title":"Notation","text":"","tags":["Note","Incomplete"]},{"location":"Finite%20Essentially%20Uninterpreted%20%28FEU%29%20fragment/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Finite%20Fields/","title":"Finite Fields","text":"<p>202304151104</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Finite%20Fields/#finite-fields","title":"Finite Fields","text":""},{"location":"Finite%20Fields/#lemma-1","title":"Lemma 1:","text":"<pre><code>title:\nIf $F$ is a finite field, the group $F ^{\\times}$ is cyclic.\n</code></pre>"},{"location":"Finite%20Fields/#proof","title":"Proof:","text":"<p>We use Lemma 1 from Abelian Groups. Let \\(|F| = q\\). Let \\(m\\) be the maximum order of elements of \\(F ^{\\times}\\). Since the order of every element divides \\(m\\), we get that \\(x ^{m} = 1\\) for all \\(x \\in F ^{\\times}\\). So the polynomial \\(X ^{m}-1\\) has all the \\(q-1\\) elements of \\(F ^{\\times}\\) as roots. But since the number of roots of a polynomial is bounded above by its degree, we get that \\(q-1 \\le m\\)  But \\(m | q-1\\) by lagrange's theorem. This gives \\(m = q-1\\) and \\(F ^{\\times}\\) is cyclic.</p>"},{"location":"Finite%20Fields/#lemma-2","title":"Lemma 2:","text":"<pre><code>title:\nEvery finite field has prime power order.\n</code></pre>"},{"location":"Finite%20Fields/#lemma-3","title":"Lemma 3:","text":"<pre><code>title:\nEvery finite field $F$ is of the form $\\mathbb{F}_p[X] / (\\pi(X))$ for some prime $p$ and some monic irreducible $\\pi(X) \\in \\mathbb{F}_p[X]$.\n</code></pre>"},{"location":"Finite%20Fields/#proof_1","title":"Proof:","text":"<p>Since \\(F ^{\\times}\\) is cyclic, it has a generator \\(\\gamma\\), now take the homomorphism \\(\\phi : \\mathbb{F}_{p}[X] \\to F\\) which is just evaluation at \\(\\gamma\\). This is a surjective homomorphism. Hence, \\(F = \\mathbb{F}_{p}[X]/ ker (\\phi)\\). But \\(ker(\\phi) = (\\pi(X))\\) for some monic irreducible polynomial \\(\\pi(X)\\) since \\(ker(\\phi)\\) is a maximal ideal (\\(F\\) is a field) and \\(\\mathbb{F}_{p}[X]\\) is a PID.</p>"},{"location":"Finite%20Fields/#lemma-4","title":"Lemma 4:","text":"<pre><code>title:\nA field of prime power order $p^n$ is a splitting field over $\\mathbb{F}_p$ of $x^{p^n} - x$.\n</code></pre>"},{"location":"Finite%20Fields/#proof_2","title":"Proof:","text":"<p>Let \\(F\\) be a field of order \\(p ^{n}\\), then each element in \\(F\\) satisfies \\(x ^{p ^{n}} - x\\) and so it splits in \\(x ^{p ^{n}}-x\\).</p>"},{"location":"Finite%20Fields/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nFor every prime power $p^n$, a field of that order exists.\n</code></pre>"},{"location":"Finite%20Fields/#proof_3","title":"Proof:","text":"<p>Let \\(F\\) be the splitting field of \\(x ^{p ^{n}}-x\\) over \\(\\mathbb{F}_{p}\\). Then this contains a subfield consisting of the roots of \\(x ^{p ^{n}}-x\\) (show that this is a subfield).</p> <p>Now show that \\(f(x) = x ^{ p ^{n}}-x\\) is separable. \\(f'(x) = -1\\) and so, \\((f(x),f'(x)) = 1\\), therefore \\(f\\) is separable. And so, the number of roots of \\(f\\) is \\(p ^{n}\\).</p>"},{"location":"Finite%20Fields/#lemma-5","title":"Lemma 5:","text":"<pre><code>title:\nEach irreducible $\\pi(X)$ in $\\mathbb{F}_p[X]$ of degree $n$ divides $x^{p^n}-x$ and is separable.\n</code></pre>"},{"location":"Finite%20Fields/#proof_4","title":"Proof:","text":"<p>\\(\\mathbb{F}_{p}[X] /(\\pi(X))\\) is of degree \\(n\\) over \\(\\mathbb{F}_{p}\\) and so is of size \\(p^n\\), hence \\(t ^{p ^{n}}  = t\\) for all \\(t \\in \\mathbb{F}_{p}[X] /(\\pi(X))\\), in particular, \\(x ^{ p ^{n}} = x \\ (\\mathrm{mod}\\ \\pi(x))\\) and so, \\(\\pi(x) | x ^{ p ^{n}}-x\\). And since \\(x ^{ p ^{ n}}-x\\) is separable, \\(\\pi(x)\\) is too.</p>"},{"location":"Finite%20Fields/#theorem-2","title":"Theorem 2:","text":"<pre><code>title:\nAll finite fields of the same size are isomorphic to one another.\n</code></pre>"},{"location":"Finite%20Fields/#-","title":"---","text":""},{"location":"Finite%20Fields/#references","title":"References","text":"<p>Abelian Groups Splitting Fields</p>"},{"location":"Finite%20Intersection%20Property/","title":"Finite Intersection Property","text":"<p>202302131602</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Finite%20Intersection%20Property/#finite-intersection-property","title":"Finite Intersection Property","text":"<pre><code>title:\nLet $\\mathcal{C}$ be a collection of open subsets of $X$. Then $\\mathcal{C}$ is said to have the finite intersection property if for each finite subcollection $\\{C_1,\\cdots C_n\\}$ the intersection $C_1 \\cap C_2 \\cap \\cdots \\cap C_n$ is non empty.\n</code></pre>"},{"location":"Finite%20Intersection%20Property/#related-problems","title":"Related Problems","text":""},{"location":"Finite%20Intersection%20Property/#references","title":"References","text":""},{"location":"Finite%20Sat%20for%20FOL%20Motivation/","title":"Finite Sat for FOL Motivation","text":"<p>202309261409</p> <p>Tags : [[Logic]]</p>","tags":["Note"]},{"location":"Finite%20Sat%20for%20FOL%20Motivation/#finite-sat-for-fol-motivation","title":"Finite Sat for FOL Motivation","text":"<p>We will try what we did in propositional logic.</p> <ol> <li> <p>\\(\\{x_{1}\\equiv x_{2},x_{2}\\equiv x_{3},x_{3}\\not\\equiv x_{1}\\}\\). Simply replacing each formula with an atomic proposition doesn't work because we lose information about the relations between the formulas. \\(\\{p_{12},p_{23},p_{31},p_{12}\\land p_{23}\\implies\\lnot p_{31}\\}\\), however, works!</p> </li> <li> <p>\\(\\{\\forall x(r(x)\\implies s(x)),\\forall x (r(x)),\\exists x (\\lnot s(x))\\}\\) The previous way shouldn't work because it's not as straightforward to see that two of these formulas imply that the third should be negated. If we were to try proving that, it would be equivalent to finding whether this set is satisfiable in FOL itself, which defeats the purpose.</p> </li> </ol> <p>Prime formulas: formulas of the form \\(t_{1}\\equiv t_{2}, r(t_{1},\\dots,t_{n}),\\text{ or }\\exists x\\varphi\\). \\(\\{\\lnot\\exists x (r(x)\\land s(x)),\\lnot\\exists x\\lnot r(x),\\exists x\\lnot s(x)\\}\\)</p> <ul> <li>\\(p_{1}:\\exists x(r(x)\\land\\lnot s(x))\\),</li> <li>\\(p_{2}:\\exists x\\lnot r(x)\\),</li> <li>\\(p_{3}:\\exists x\\lnot s(x)\\)</li> </ul> <p>\\(\\{\\lnot p_{1},\\lnot p_{2},p_{3}\\}\\)</p> <p>Prime formula structure: For FO language \\(L\\), let \\(\\mathcal{P}_{L}\\) be the set of all prime formulas.</p> <p>Lemma: Let \\(\\mathcal{I}\\) be an interpretation. There exists a valuation \\(v:\\mathcal{P}_{L}\\to\\{\\top,\\bot\\}\\) s.t. for FO formula \\(\\varphi\\), \\(\\mathcal{I}\\models_{FO}\\varphi\\) iff \\(v\\models_{\\mathcal{P}_L}\\varphi\\). (All this lemma is saying is that we can make up a valuation. The valuation loses a lot of information that the interpretation has, so the converse is not true.)</p> <ul> <li>Equality is transitive.</li> <li>Suppose \\(\\exists x\\varphi(x)\\) is true. There is a term \\(t_{\\varphi}\\) which certifies this. We add \\(\\exists x \\varphi(x)\\implies\\varphi(t_{\\varphi})\\).</li> <li>Suppose \\(\\lnot\\exists x\\varphi(x)\\) is true. Every element satisfies \\(\\lnot\\varphi\\). We add \\(\\lnot\\exists x\\varphi(x)\\implies\\lnot\\varphi(t)\\) for an arbitrary term \\(t\\).</li> </ul>","tags":["Note"]},{"location":"Finite%20Sat%20for%20FOL%20Motivation/#references","title":"References","text":"<p>First Order Logic Finite Sat for FOL</p>","tags":["Note"]},{"location":"Finite%20Sat%20for%20FOL/","title":"Finite Sat for FOL","text":"<p>202310101410</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"Finite%20Sat%20for%20FOL/#finite-sat-for-fol","title":"Finite Sat for FOL","text":"<p>If a set \\(X\\) of FO formulas is finitely satisfiable, then \\(X\\) is satisfiable.</p> <p>Proof: (FOL Sat \\(\\implies\\) prop Sat) (Prop Sat \\(\\implies\\) FOL Sat) ...</p> <ol> <li> <p>\\(\\{x_{1}\\equiv x_{2},x_{2}\\equiv x_{3},x_{3}\\not\\equiv x_{1}\\}\\). Simply replacing each formula with an atomic proposition doesn't work because we lose information about the relations between the formulas. \\(\\{p_{12},p_{23},p_{31},p_{12}\\land p_{23}\\implies\\lnot p_{31}\\}\\), however, works!</p> </li> <li> <p>\\(\\{\\forall x(r(x)\\implies s(x)),\\forall x (r(x)),\\exists x (\\lnot s(x))\\}\\) The previous way shouldn't work because it's not as straightforward to see that two of these formulas imply that the third should be negated. (In fact it only works for \\(\\equiv\\) and \\(\\exists\\).) If we were to try proving that, it would be equivalent to finding whether this set is satisfiable in FOL itself, which defeats the purpose.</p> </li> </ol> <p>Prime formulas: formulas of the form \\(t_{1}\\equiv t_{2}, r(t_{1},\\dots,t_{n}),\\text{ or }\\exists x\\ \\varphi\\). \\(\\{\\lnot\\exists x\\ (r(x)\\land s(x)),\\lnot\\exists x\\ \\lnot r(x),\\exists x\\ \\lnot s(x)\\}\\)</p> <ul> <li>\\(p_{1}:\\exists x\\ (r(x)\\land\\lnot s(x))\\),</li> <li>\\(p_{2}:\\exists x\\ \\lnot r(x)\\),</li> <li>\\(p_{3}:\\exists x\\ \\lnot s(x)\\)</li> </ul> <p>\\(\\{\\lnot p_{1},\\lnot p_{2},p_{3}\\}\\)</p> <p>Prime formula structure: For FO language \\(L\\), let \\(\\mathcal{P}_{L}\\) be the set of all prime formulas.</p> <p>Lemma: Let \\(\\mathcal{I}\\) be an interpretation. There exists a valuation \\(v:\\mathcal{P}_{L}\\to\\{\\top,\\bot\\}\\) s.t. for FO formula \\(\\varphi\\), \\(\\mathcal{I}\\models_{FO}\\varphi\\) iff \\(v\\models_{\\mathcal{P}_L}\\varphi\\). (All this lemma is saying is that we can make up a valuation. The valuation loses a lot of information that the interpretation has, so the converse is not true.)</p> <ul> <li>Equality is transitive.</li> <li>Suppose \\(\\exists x\\ \\varphi(x)\\) is true. There is a term \\(t_{\\varphi}\\) which certifies this. We add \\(\\exists x\\ \\varphi(x)\\implies\\varphi(t_{\\varphi})\\).</li> <li>Suppose \\(\\lnot\\exists x\\ \\varphi(x)\\) is true. Every element satisfies \\(\\lnot\\varphi\\). We add \\(\\lnot\\ \\exists x\\ \\varphi(x)\\implies\\lnot\\varphi(t)\\) for an arbitrary term \\(t\\).</li> </ul>","tags":["Note","Incomplete"]},{"location":"Finite%20Sat%20for%20FOL/#references","title":"References","text":"<p>First Order Logic Finite Sat for FOL Motivation</p>","tags":["Note","Incomplete"]},{"location":"First%20Countable%20space/","title":"First Countable space","text":"<p>202302081702</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"First%20Countable%20space/#first-countable-space","title":"First Countable space","text":""},{"location":"First%20Countable%20space/#definition","title":"Definition:","text":"<pre><code>title:\nA space X has a _countable basis at a point_ $x \\in X$ if there is a countable collection of open sets $\\{B_n\\}_{n=1}^{\\infty}$ containing $x$ such that each nbhd of $x$ contains at least one of the $B_n's$.\n\nA space X is _first countable_ if it has a countable basis at each point $x \\in X$. \n</code></pre>"},{"location":"First%20Countable%20space/#note-every-metric-space-is-first-countable","title":"Note: every metric space is first countable.","text":""},{"location":"First%20Countable%20space/#sequence-lemma","title":"Sequence Lemma:","text":"<pre><code>title:\nLet X be a topo space, $A \\subset X, x \\in X$.\n1) If there is a sequence $(x_n)_{n=1}^{\\infty}$ of points in A that converge to x, then $x \\in Cl(A)$\n2) If X is metrizable (first countable) and $x \\in Cl(A)$ then there is a sequence of points $x_n \\in A$ such that $x_n \\to x$.\n</code></pre>"},{"location":"First%20Countable%20space/#proof","title":"Proof:","text":"<p>1) Suppose not, then there is a nhbd of \\(x\\) that is completely outside A and so the points \\(x_n\\) cannot converge to \\(x\\), contradiction. 2) First note that \\(x \\in Cl(A) \\implies\\) every nbhd of \\(x\\) intersects A. Just take \\(x_n\\) to be a point in \\(\\bigcap \\limits_{i=1}^{n} B_i\\) other than \\(x\\), and this gives a sequence converging to \\(x\\).</p>"},{"location":"First%20Countable%20space/#lemma","title":"Lemma:","text":"<pre><code>title:\nLet $f:X \\to Y$ be a function.\n1) If f is cts, then for all convergent sequences $x_n \\to x$, $f(x_n) \\to f(x)$.\n2) If X is metrizable (first ctble) and for all convergent sequences $x_n \\to x$, $f(x_n) \\to f(x)$, then f is cts.\n</code></pre>"},{"location":"First%20Countable%20space/#proof_1","title":"Proof:","text":"<p>1) Take an open nbhd V of f(x), then \\(U = f^{-1}(V)\\) contains all \\(x_n\\) for all \\(n \\ge n_0\\) for some \\(n_0\\), and so \\(V\\) contains all \\(f(x_n)\\) for \\(n \\ge n_0\\). 2) Need to show that \\(f(Cl(A)) \\subseteq Cl(f(A))\\). Let \\(x \\in Cl(A)\\), then since X is first countable, there is a sequence \\(x_n \\to x\\) and so \\(f(x_n) \\to f(x)\\) and thus \\(f(x) \\in Cl(f(A))\\). </p>"},{"location":"First%20Countable%20space/#related-problems","title":"Related Problems","text":""},{"location":"First%20Countable%20space/#references","title":"References","text":"<p>Closure and Interior and Limit Points Continuous Functions</p>"},{"location":"First%20Order%20Logic/","title":"First Order Logic","text":"<p>202309211409</p> <p>Tags : [[Logic]]</p>","tags":["Note"]},{"location":"First%20Order%20Logic/#first-order-logic","title":"First Order Logic","text":"<pre><code>title: Motivation\nConsider the typical structures in Maths and CS. Groups, Rings, Monoids etc etc. All of them are sets, equipped with _functions_ and _relations_ on them and sometimes they have _special promoted terms_, like units in groups.\n\n**First Order Logic** gives a natural frameword to talk about these thingies.\nThe idea is to fix symbols to denote functions, relations and constants and combine the with the standard $\\lnot$ and $\\lor$ operator. In addtion to all that we have operators to quantify over all elements $\\exists$ and $\\forall$, and a $\\equiv$ operator to check equality of primitive constructs.\n</code></pre> <p>The syntax and semantics are a bit more involved for first order logic, than in Propositional Logic.</p> <p>Groups are a really good example to get introduced to first order logic</p> <p></p> <pre><code>title:Goal\nThe goal of first order logic is to capture the properties of mathematical structures.\n</code></pre>","tags":["Note"]},{"location":"First%20Order%20Logic/#references","title":"References","text":"<p>Isomorphism Between First Order Interpretations Groups In First Order Logic</p>","tags":["Note"]},{"location":"Fourier%20Series/","title":"Fourier Series","text":"<p>2022-10-27 10:10 am</p> <p>Type : #Note Tags : [[Analysis]]</p>"},{"location":"Fourier%20Series/#fourier-series","title":"Fourier Series","text":"<p>The fourier coefficients of a Riemann Integrable function \\(f\\) defined on an interval \\([a,b]\\) are given by \\(\\(\\hat{f}(n) = \\dfrac{1}{b-a}\\int_a^bf(x)e ^{-2\\pi inx/(b-a)}dx\\)\\)We generally work with functions on the circle \\(\\mathbb{T}\\) and so, the fourier coefficients of function \\(f\\) on the circle are given by: \\(\\(\\hat{f}(n) = \\dfrac{1}{2\\pi}\\int_{-\\pi}^{\\pi}f(x)e ^{-inx}dx\\)\\)</p>"},{"location":"Fourier%20Series/#when-does-the-fourier-series-converge","title":"When does the fourier series converge?","text":"<p>It depends on the sense of the desired convergence.</p> <p>For example, If we want uniform convergence, then we need \\(f\\) to be a \\(C ^2\\) function.</p> <p>For pointwise convergence, we can't ensure that it holds for general Riemann Integrable \\(f\\), since we can change the value of the function at a point, and the fourier coefficients still remain the same. If we ensure that the function is continuous, even then we can't say in general that convergence will hold. (result by Du Bois-Reymond).</p> <p>We can define the limit in the mean square sense, i.e, in the \\(L ^2\\) norm. If \\(f\\) is just integrable, then \\(\\(\\dfrac{1}{2\\pi}\\int_{-\\pi}^{\\pi}|S_Nf(\\theta)-f(\\theta)|^2d\\theta\\)\\)</p>"},{"location":"Fourier%20Series/#related-problems","title":"Related Problems","text":"<p>Convergence of Fourier Series</p>"},{"location":"Fourier%20Series/#references","title":"References","text":"<p>Uniqueness of fourier series Fourier series is the best approximation Convergence of Fourier Series</p>"},{"location":"Fourier%20series%20is%20the%20best%20approximation/","title":"Fourier series is the best approximation","text":"<p>2022-11-16 00:31 am</p> <p>Type : #Note Tags : [[Analysis]]</p>"},{"location":"Fourier%20series%20is%20the%20best%20approximation/#fourier-series-is-the-best-approximation","title":"Fourier series is the best approximation","text":"<pre><code>title: Fourier Series is the best trig approximation\nSuppose $s_N(f)(x) = \\sum\\limits_{m=-N}^{N}c_m\\phi_m$ and $t_N(x) = \\sum\\limits_{m=-N}^{N}d_m\\phi_m$ is another partial sum of a series converging to $f$.\nThen $$\\int\\limits_{-\\pi}^{\\pi}|f-s_N|^2dx \\le \\int\\limits_{-\\pi}^{\\pi}|f-t_N|^2dx$$\nand $$\\sum\\limits_{m=-N}^{N}|c_m|^2 \\le \\int\\limits_{-\\pi}^{\\pi} |f|^2dx$$\nWhere $\\phi_m(x) = \\dfrac{1}{\\sqrt{2\\pi}} e^{imx};\\ m \\in \\mathbb{Z}$\n</code></pre>"},{"location":"Fourier%20series%20is%20the%20best%20approximation/#related-problems","title":"Related Problems","text":"<p>Convergence of Fourier Series Uniqueness of fourier series</p>"},{"location":"Fourier%20series%20is%20the%20best%20approximation/#references","title":"References","text":"<p>Fourier Series Dirichlet Kernels</p>"},{"location":"Frechet%20Property/","title":"Frechet Property","text":"<p>202301181801</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Frechet%20Property/#frechet-property","title":"Frechet Property","text":""},{"location":"Frechet%20Property/#definition","title":"Definition","text":"<p>A Topological Space \\(X\\) is said to be \\(T_1\\) or Frechet if for all pairs of points \\(x, y\\) there exists a pair of open sets \\(U, V\\) such that \\(U\\) contains \\(x\\) but not \\(y\\) and \\(V\\) contains \\(y\\) but not \\(x\\).</p>"},{"location":"Frechet%20Property/#related-problems","title":"Related Problems","text":""},{"location":"Frechet%20Property/#references","title":"References","text":"<p>Separation Axioms</p>"},{"location":"Free%20Group/","title":"Free Group","text":"<p>202306041006</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Free%20Group/#free-abelian-group","title":"Free Abelian Group","text":"<pre><code>title:\nA free abelian group of rank $n$ is any group which is the direct sum of $n$ subgroups, each of which is isomorphic to $\\mathbb{Z}$, equivalently, it is isomorphic to the additive group $\\mathbb{Z}^{n}$.\n</code></pre> <p>The rank of such a group is well defined because the \\(\\mathbb{Z} ^{n}\\)'s are pairwise non isomorphic.</p>"},{"location":"Free%20Group/#proposition-1","title":"Proposition 1:","text":"<pre><code>title:\n$\\mathbb{Z}^{n} \\simeq \\mathbb{Z}^{m}$ iff $m = n$.\n</code></pre>"},{"location":"Free%20Group/#proof","title":"Proof:","text":"<p>Let \\(m &lt; n\\) and let there be an isomorphism \\(\\phi : \\mathbb{Z}^{m} \\to \\mathbb{Z}^{n}\\), let \\(A\\) be the corresponding matrix for this map. Let \\(\\widetilde{\\phi} : \\mathbb{Q}^{m} \\to \\mathbb{Q}^{n}\\) be the map corresponding to \\(A\\) but on \\(\\mathbb{Q}^{m}\\). Then since \\(m &lt; n\\), this map is not injective, thus there is a vector \\(v \\in \\mathbb{Q}^{n}\\) which has no preimage. Scaling the vector \\(v\\), we get that there is a vector in \\(\\mathbb{Z}^{n}\\) which has no preimage, hence the map \\(\\phi\\) is not injective, that's a contradiction.</p>"},{"location":"Free%20Group/#proposition-2","title":"Proposition 2:","text":"<pre><code>title:\nAny subgroup of a free abelian group of rank $n$ is free of rank $\\le n$.\n</code></pre>"},{"location":"Free%20Group/#proof_1","title":"Proof:","text":"<p>WLOG, let \\(G = \\mathbb{Z} \\oplus \\mathbb{Z} \\oplus \\dots \\oplus \\mathbb{Z}\\) (n times). We will show by induction that \\(H\\) is free of rank \\(\\le n\\). For \\(n = 1\\), we know any subgroup of \\(\\mathbb{Z}\\) is just \\(m\\mathbb{Z}\\) for some \\(m\\), in this case we are done. Let it hold for \\(n-1\\). Then let \\(\\pi : G \\to \\mathbb{Z}\\) be the projection onto the first coordinate, let \\(K\\) be the kernel of this map. \\(K \\simeq \\mathbb{Z}^{n-1}\\). Thus \\(H \\cap \\mathbb{Z}^{n-1}\\) is free of rank \\(\\le n-1\\). Now \\(\\phi(H)\\subset \\mathbb{Z}\\) is either \\(\\{ 0 \\}\\) or infinite cyclic. If it is \\(\\{ 0 \\}\\), then \\(H = H \\cap K\\) and we are done. Otherwise, \\(\\phi(H) = (\\pi(h))\\) for some \\(h \\in H\\), we will show that \\(H = \\mathbb{Z}h \\oplus (H\\cap K)\\).</p> <p>Let \\(x \\in H\\) be any element, then \\(x = \\frac{\\pi(x)}{\\pi(h)}h + (x- \\frac{\\pi(x)}{\\pi(h)}h)\\) The first term is in \\(\\mathbb{Z}h\\), and the second one is in \\(H \\cap K\\), thus \\(H = \\mathbb{Z}h \\oplus (H \\cap K)\\) (since it is easy to see that \\(\\mathbb{Z}h \\cap H \\cap K = \\{ 0 \\}\\)).</p>"},{"location":"Free%20Group/#proposition-3","title":"Proposition 3:","text":"<pre><code>title:\nLet $G,H$ be two free abelian groups of rank $n$ with $H \\subset G$, then $G /H$ is a finite group.\n</code></pre>"},{"location":"Free%20Group/#proof_2","title":"Proof:","text":"<p>Take a basis for \\(G\\), label it \\(\\mathcal{G}  = \\{g_{1},g_{2},\\dots g_{n}\\}\\). Take a basis for \\(H\\), label it \\(\\mathcal{H} = \\{ h_{1},\\dots,h_{n} \\}\\). Now there is an integer matrix \\(M\\) such that \\(\\mathcal{H} = M \\mathcal{G}\\). Now we can take the vector space \\(G'\\) over \\(\\mathbb{Q}\\) whose basis is \\(\\mathcal{G}\\). In that vector space, \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) are both bases, hence \\(M\\) is a base change matrix, hence is invertible.</p> <p>Now \\(\\det(M) M ^{-1}\\) is an integer matrix. This means \\(\\det(M)g_{i} =\\) some linear combination of \\(h_{j}'s\\) with integer coefficients. This means \\(\\det (M)G \\subset H\\). This means \\(G /H\\) is a finite group with size at most \\(|\\det(M)| ^{n}\\).</p>"},{"location":"Free%20Group/#references","title":"References","text":""},{"location":"Free%20Module/","title":"Free Module","text":"<p>202302151102</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Free%20Module/#free-module","title":"Free Module","text":"<pre><code>title:\nA module M over a ring R is called _free_ if it has a basis.\n</code></pre> <ul> <li>A module is free of rank n if it has a basis of size n. (This is well defined)</li> <li>Submodules of free modules need not be free, but the next result shows it is true if R is a PID.</li> </ul> <pre><code>title: Proposition\nLet R be a PID, let M be a free R module of finite rank n and let N be a submodule of M. Then \n1) N is free of rank m, $m \\le n$\n2) There is a basis $y_1,y_2,\\cdots y_m$ of M such that $a_1y_1,\\cdots a_my_m$ is a basis of N where $a_1,\\cdots a_m$ are non zero elements of R such that $$a_1 | a_2 | \\cdots | a_m$$\n</code></pre>"},{"location":"Free%20Module/#proof","title":"Proof:","text":"<p>The following proof is from Dummit and Foote (Pg 460, Theorem 4). <pre><code>title: Idea\nThe idea is to find a homomorphism from M to R, such that M can be written as a direct sum of its image and kernel, i.e. $M = R \\oplus \\mathrm{ker}(\\nu)$, this allows us to induct. But then we also want $N = R \\oplus \\mathrm{ker}(\\nu) \\cap N$. So we try looking at all possible homomorphisms from N to R and try to pick that which fits.\nAlso, need $N = Ry_1 \\oplus \\mathrm{ker}{\\nu}$ where $\\nu(y_1) = 1$ but that might not be possible with $y_1 \\in N$, but certainly is possible with $y_1 \\in M$. So we try to see the smallest possible multiple of 1 that we can get for $\\nu(y)$. That leads us to take the maximal element in $\\Sigma$.\n</code></pre></p> <ul> <li>Take the set of ideals \\(\\Sigma = \\{\\phi(N) \\ | \\ \\phi \\in \\mathrm{Hom}_R(M,R)\\}\\). Denote by \\(a_\\phi\\) the generator of the principal ideal \\(\\phi(N)\\).</li> <li>This is non empty since \\((0)\\) is in this collection, and there is a non trivial maximal element since R is noetherian and \\((a_{\\pi_i}) \\neq (0)\\) for some \\(\\pi_i\\) (projection map). </li> <li>Take a maximal element from \\(\\Sigma\\), let this be \\((a_\\nu) = \\nu(N)\\). Rename \\(a_\\nu = a_1\\) and let \\(y \\in N\\) such that \\(\\nu(y) = a_1\\).</li> <li>Show that \\(a_1\\) divides \\(\\phi(y)\\) for all \\(\\phi \\in \\mathrm{Hom}_R(M,R)\\).</li> <li>This gives \\(y = a_1y_1\\) where \\(y_1 = \\sum \\limits_{i=1}^n b_i x_i\\) where \\(x_1,x_2,\\dots, x_n\\) forms a basis for M.</li> <li>\\(\\nu(y_1)=1\\)</li> <li>\\(M = Ry_1 \\oplus \\mathrm{ker}(\\nu)\\)</li> <li>\\(N = Ra_1y_1 \\oplus (\\mathrm{ker}(\\nu) \\cap N)\\)</li> <li> <p>Use induction to show that N is a free module (refer to Rank of a module to show that it is of rank at most m).</p> </li> <li> <p>Prove (2) by induction on n.</p> <ul> <li>ker\\((\\nu)\\) is free of rank n-1.</li> <li>By induction hypothesis on ker\\((\\nu)\\) and ker\\((\\nu) \\cap N\\), we get a basis \\(y_2,\\dots y_n\\) of ker\\((\\nu)\\) such that \\(a_2y_2, \\dots a_my_m\\) is a basis of ker\\((\\nu)\\cap N\\) with the division condition.</li> <li>This gives \\(y_1,\\cdots y_n\\) is a basis for M and \\(a_1y_1,\\cdots a_my_m\\) is a basis for N.</li> <li>Define \\(\\phi : M \\to R\\) by \\(\\phi(y_1) = \\phi(y_2) = 1\\) and \\(\\phi(y_i) = 0\\). This gives \\(a_1 = \\phi(a_1y_1) \\in \\phi(N)\\), hence \\((a_1) \\subset \\phi(N)\\) but by the maximality of \\((a_1)\\) in \\(\\Sigma\\), we get that \\((a_1) = \\phi(N)\\). Since \\(a_2 = \\phi(a_2y_2) \\in (a_1)\\), \\(a_1 | a_2\\). </li> </ul> </li> </ul>"},{"location":"Free%20Module/#related-problems","title":"Related Problems","text":""},{"location":"Free%20Module/#references","title":"References","text":"<p>[[Principal Ideal Domain]] Module Rank of a module</p>"},{"location":"Functions%20Computable%20in%20Lambda%20Calculus/","title":"Functions Computable in Lambda Calculus","text":"<p>202304011704</p> <p>Type : #Note Tags : [[Lambda Calculus]]</p>"},{"location":"Functions%20Computable%20in%20Lambda%20Calculus/#functions-computable-in-lambda-calculus","title":"Functions Computable in Lambda Calculus","text":"<p>Computability in \\(\\lambda-\\)calculus is generally defined by the set of function of the type \\(\\mathbb N^{k}\\to\\mathbb N\\)</p> <p><pre><code>title: Recursive Functions\nThis is a set of functions which was defined by Dedikendm, Skolem, G\u00f6del, Kleen, etc. and this is equivalent to the set of functions computable by [Turing Machines](&lt;./Turing Machines.md&gt;).\n</code></pre> \\(f:\\mathbb N^{k}\\to \\mathbb N\\) is obtained by composition of function \\(g:\\mathbb N^{l}\\to\\mathbb N\\) and functions \\(h_1,h_2\\dots h_{l}:\\mathbb N^{k}\\to \\mathbb N\\) if $$ f(\\vec n)=g(h_{1}(\\vec n), h_{2}(\\vec n)\\dots h_{l}(\\vec n)) $$ and the notation for composition is  $$ f=g\\circ(h_{1}, h_{2},\\dots h_{l}) $$ \\(f:\\mathbb N^{k} \\to \\mathbb N\\) is obtained by Primitive Recursion or by Mu-Recursion</p> <p>The class of Primitive Recursive Functions is the set of functions where The initial set of functions that is allowed to be used is  + Zero Function - \\(Z(n)=0\\) + Successor Function - \\(S(n)=n+1\\) + Projection Function - \\(\\Pi_{i}^{k} (n_{1}\\dots n_{k})= n_{i}\\) Which is closed under composition and primitive recursion The class of Recursive Functions is the set of functions preverved under composition, primitive recursion and minimization, and have the initial functions.</p>"},{"location":"Functions%20Computable%20in%20Lambda%20Calculus/#references","title":"References","text":""},{"location":"Fundamental%20Group/","title":"Fundamental Group","text":"<p>202304031704</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Fundamental%20Group/#fundamental-group","title":"Fundamental Group","text":"<pre><code>title:\nSince the fundamental groupoid has more than one element, we can restrict our category to a single element $x_0 \\in X$ called the base point, and only consider paths that go from $x_0$ to itself.\nThis is called the **fundamental group** of $(X,x_0)$.\n</code></pre> <p>The fundamental group is the set of homotopy classes of loops based at \\(x_{0}\\), with a group structure on them with group operation \\(*\\).  It is denoted by \\(\\Pi_{1}(X,x_{0})\\).</p>"},{"location":"Fundamental%20Group/#proposition","title":"Proposition:","text":"<pre><code>title:\nLet $x_{0},x_{1}\\in  X$ be in the same path connected component of $X$, and $\\alpha$ be a path from $x_{0}$ to $x_{1}$.\nTake the map $\\hat{\\alpha} : \\Pi_{1}(X,x_{0}) \\to \\Pi_{1}(X,x_{1})$ $$\n[f] \\mapsto [\\alpha]^{-1}*[f]*[\\alpha]$$\nis an isomorphism of groups \n</code></pre>"},{"location":"Fundamental%20Group/#proof","title":"Proof:","text":"<p>\\(\\hat{\\alpha}(a*b) = [\\alpha]^{-1}*[a]*[\\alpha]*[\\alpha]^{-1}*[b]*[\\alpha] = \\hat{\\alpha}(a)*\\hat{\\alpha}(b)\\) Hence \\(\\hat{\\alpha}\\) is a group homomorphism.</p> <p>Denote \\(\\beta = \\bar{\\alpha}\\), then \\(\\hat{\\beta} : \\Pi_{1}(X,x_{1}) \\to \\Pi_{1}(X,x_{0})\\) is a group homomorphism. So for any \\([f] \\in \\Pi_{1}(X,x_{1})\\),  \\(\\(\\hat{\\alpha}(\\hat{\\beta}([f])) = \\hat{\\alpha}([\\bar{\\beta}]*[f]*[\\beta]) =[\\bar{\\alpha}]*[\\bar{\\beta}]*[f]*[\\beta]*[\\alpha] = [f]\\)\\) (since \\([\\alpha] = [\\beta]^{-1}\\)) Similarly \\(\\hat{\\beta} \\circ \\hat{\\alpha} = id\\).</p>"},{"location":"Fundamental%20Group/#corollary-1","title":"Corollary 1:","text":"<pre><code>title:\nFor a path connected space $X$, the fundamental group is independent of the base point.\n</code></pre>"},{"location":"Fundamental%20Group/#corollary-2","title":"Corollary 2:","text":"<pre><code>title:\nA loop $f$ at $x_0$ induces an automorphism $\\hat{f}$ of $\\Pi_1(X,x_0)$.\n</code></pre>"},{"location":"Fundamental%20Group/#pi_1-as-a-functor","title":"\\(\\Pi_{1}\\) as a functor","text":"<pre><code>Consider the category of _pointed topological spaces_, with objects $\\{(X,x_0)\\}$ ordered pairs of topological spaces $X$ and the choice of base point $x_0 \\in X$.\nThe morphisms are given by continuous functions that take base points to base points.\n$$ f:(X,x_0) \\to (Y,y_0)$$\nwhere $f$ is cts and $f(x_0) = y_0$.\n</code></pre>"},{"location":"Fundamental%20Group/#lemma","title":"Lemma:","text":"<pre><code>title:\nAny morphism of pointed topo spaces $h : (X,x_{0}) \\to (Y,y_{0})$ induces a group homomorphism $h_{*} : \\Pi_{1}(X,x_{0}) \\to \\Pi_{1}(Y,y_{0})$ defined by $$\nh_*([f]) := [h \\circ f]\n$$\n</code></pre>"},{"location":"Fundamental%20Group/#proof_1","title":"Proof:","text":"<p>Note that \\(h_{*}\\) is well defined. If \\(f \\simeq_{p} f'\\) then \\(h \\circ f \\simeq_{p} h \\circ f'\\).  \\(h_{*}([f]*[g]) = h_{*}([f*g]) = [h \\circ (f*g)] = [(h \\circ f)*(h \\circ g)] = [h \\circ f] *[h \\circ g] = h_{*}([f])h_{*}([g])\\)</p>"},{"location":"Fundamental%20Group/#lemma_1","title":"Lemma:","text":"<pre><code>title:\n$\\Pi_1$ is a functor from the category of pointed topo spaces to Groups\n</code></pre>"},{"location":"Fundamental%20Group/#proof_2","title":"Proof:","text":"<p>It takes \\((X,x_{0})\\) to \\(\\Pi_{1}(X,x_{0})\\) and \\(f : (X,x_{0}) \\to (Y,y_{0})\\) to \\(f_{*}\\). Let \\(g : (Y,y_{0}) \\to (Z,z_{0})\\). To show that composition is preserved, we need to show that \\(\\Pi_{1}(f \\circ g) = f_{*} \\circ g_{*}\\). \\(\\Pi_{1}(f \\circ g)[\\alpha] = (f \\circ g)_{*}([\\alpha]) = [f \\circ g\\circ\\alpha] = f_{*}(g_{*}([\\alpha]))\\). </p> <p>To show that identity is preserved, we need to show that for \\(id : (X,x_{0}) \\to (X,x_{0})\\) \\(\\Pi_{1}(id) = id_{\\Pi_{1}(X,x_{0})}\\) $$ \\Pi_{1}(id)[\\alpha] = [id \\circ \\alpha] = [\\alpha] $$ Hence we are done.</p>"},{"location":"Fundamental%20Group/#note-functors-carry-isomorphisms-to-isomorphisms-and-hence-we-have-the-following","title":"NOTE: functors carry isomorphisms to isomorphisms, and hence we have the following:","text":""},{"location":"Fundamental%20Group/#corollary","title":"Corollary:","text":"<pre><code>title:\nIf $h : (X,x_0) \\to (Y,y_0)$ is a homeomorphism, then $h_{*} : \\Pi_1(X,x_0) \\to \\Pi_1(Y,y_0)$ is an isomorphism.\n</code></pre>"},{"location":"Fundamental%20Group/#examples","title":"Examples","text":"<ol> <li>\\(S ^{1}\\) has fundamental group \\(\\mathbb{Z}\\).</li> </ol>"},{"location":"Fundamental%20Group/#references","title":"References","text":"<p>Fundamental Groupoid Homotopy of paths [[Category Theory]] [[Groups]] Fundamental Group Continuous Functions Homeomorphisms S^1</p>"},{"location":"Fundamental%20Groupoid/","title":"Fundamental Groupoid","text":"<p>202304031604</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Fundamental%20Groupoid/#fundamental-groupoid","title":"Fundamental Groupoid","text":"<pre><code>title:\nIf $f$ is a path in $X$ from $x$ to $y$ and $g$ is a path from $y$ to $z$, then $f * g$ is defined as a path from $x$ to $z$ with concatenates $f,g$.\n$$\n(f*g)(s) = \\begin{cases}\nf(2s) \\ 0 \\le t \\le \\frac{1}{2} \\\\\ng(2s-1) \\ \\frac{1}{2} \\le t \\le 1\n\\end{cases}\n$$\n</code></pre>"},{"location":"Fundamental%20Groupoid/#this-also-gives-a-binary-operation-on-the-set-of-path-homotopy-classes-as-well-as-long-as-f1-g0","title":"This also gives a binary operation on the set of path-homotopy classes as well, as long as \\(f(1) = g(0)\\).","text":"<p>If \\(f \\simeq_{p} f'\\) and \\(g \\simeq_{p} g'\\) with homotopies \\(F(s,t)\\) and \\(G(s,t)\\) then \\(f*g \\simeq_{p} f'*g'\\) using $$ (F*G)(s,t) = \\begin{cases} F(s,2t),  0 \\le t\\le \\frac{1}{2}  \\ G(s,2t-1),  \\frac{1}{2} \\le t\\le 1 \\end{cases} $$ So we can define \\([f] *[g] = [f*g]\\)</p>"},{"location":"Fundamental%20Groupoid/#the-operation-is-associative-on-path-homotopy-classes-and-has-identity-and-inverses","title":"The operation \\(*\\) is associative on path-homotopy classes, and has identity and inverses.","text":"<ul> <li>Associative: Let \\(f,g,h\\) be three paths with \\(f(1) = g(0)\\) and \\(g(1) = h(0)\\). We claim \\((f*g)*h \\simeq_{p} f*(g*h)\\). $$ F(s,t) = \\begin{cases} f\\left( \\frac{4s}{1+t} \\right),  0 \\le s \\le \\frac{t+1}{4} \\ g(4s-(1+t)),  \\frac{t+1}{4} \\le s \\le \\frac{t+2}{4} \\ h\\left( \\frac{4s-(2+t)}{2-t} \\right),  \\frac{2+t}{4} \\le s \\le 1 \\end{cases} $$</li> <li>Identity: Given \\(x,y \\in X\\), and \\(f\\) any path from \\(x\\) to \\(y\\), we claim that \\(id_{x}:=[e_{x}]\\) is the identity for this operation, where \\(e_{x} : I \\to X\\) is the constant path \\(e_{x}(s) = x \\ \\forall \\ s \\in I\\).</li> </ul>"},{"location":"Fundamental%20Groupoid/#claim-fid_y-id_xf","title":"Claim: \\([f]*id_{y} = id_{x}*[f]\\)","text":""},{"location":"Fundamental%20Groupoid/#proof","title":"Proof:","text":"<p>\\(f \\simeq_{p} f*id_{y}\\) by defining $$ F(s,t) = \\begin{cases} f(s(1+t)), 0 \\le s \\le \\frac{2-t}{2} \\ y, \\frac{2-t}{2} \\le s \\le 1 \\end{cases} $$ similarly \\(id_{x}*f \\simeq_{p} f\\).</p> <ul> <li>Inverse: Given a path \\(f\\) from \\(x\\) to \\(y\\), define the reverse path \\(\\bar{f}(s) = f(1-s)\\). Then \\(e_{x}\\simeq f*\\bar{f}\\) and \\(e_{y}\\simeq \\bar{f}*f\\). In the first case, $$ F(s,t) = \\begin{cases} f(2s(1-t)),  0 \\le s \\le \\frac{1}{2} \\ f(2(1-s)(1-t)),  \\frac{1}{2} \\le s \\le 1 \\end{cases} $$ Similarly for the second case.</li> </ul>"},{"location":"Fundamental%20Groupoid/#the-fundamental-groupoid-of-x","title":"The fundamental groupoid of \\(X\\)","text":"<p>The above information can be packaged into a category \\(\\mathcal{C}\\) with objects the elements of \\(X\\), and morphisms \\(\\mathrm{Hom}(x,y) =\\) set of equivalence classes of paths from \\(x\\) to \\(y\\).  The operation \\(*\\) gives us composition of morphisms in this category and is associative. The identity morphisms exist.</p> <p>This category is a groupoid since every morphism has an inverse and hence is an isomorphism.</p>"},{"location":"Fundamental%20Groupoid/#this-is-called-the-fundamental-groupoid-of-x","title":"This is called the fundamental groupoid of \\(X\\).","text":""},{"location":"Fundamental%20Groupoid/#references","title":"References","text":"<p>Homotopy of paths [[Category Theory]] [[Groupoids]]</p>"},{"location":"Galois%20Correspondence/","title":"Galois Correspondence","text":"<p>202304161004</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Galois%20Correspondence/#galois-correspondence","title":"Galois Correspondence","text":"<pre><code>title: Idea\nGiven a field extension $L /K$, a $K-$automorphism of $L$ is an automorphism of $L$ which fixes $K$. The set of all such $K-$automorphisms forms a group denoted $\\mathrm{Aut}(L/K)$.\n\nFrom each intermediate field $K \\subset F \\subset L$, we get a subgroup $\\mathrm{Aut}(L/F) := \\{\\sigma \\in \\mathrm{Aut}(L/K) | \\sigma(a) = a \\ \\forall \\ a \\in F\\}$ of $\\mathrm{Aut}(L/K)$.\nAnd, given any subgroup $H \\le \\mathrm{Aut}(L/K)$, we get a corresponding intermediate field $K \\subset L^H \\subset L$, where $L^H := \\{\\alpha \\in L : \\sigma(\\alpha) = \\alpha \\ \\forall \\ \\sigma \\in H\\}$, this is called the fixed field of $H$.\n\nIt is straightforward to check that $F \\subset L^{\\mathrm{Aut}(L/F)}$ and $H \\subset \\mathrm{Aut}(L/L^H)$.\nThe galois correspondence gives descriptions of those extensions $L/K$ where the above two inclusions become equalities.\n\nThis will give us, at a minimum, $K = L^{\\mathrm{Aut}(L/K)}$.\nThis is not true for general extensions. Consider $L = \\mathbb{Q}(\\sqrt[3]{2})$ and $K = \\mathbb{Q}$ this has no intermediate fields and its automorphism group is trivial. Which gives the fixed field of $\\mathrm{Aut}(L/K) = L$ which is not desirable in context to the above discussion.\n\nThis problem arose because some roots of the min poly of $\\sqrt[3]{2}$ were missing from the extension, so there was only one option of sending $\\sqrt[3]{2}$ under an automorphism, to itself.\n\nAnother type of problem we encounter is the following:\nLet $p$ be a prime and $F = \\mathbb{F}_p$ then $X^p - u$ has only one root in the splitting field over $F(u)$ (the polynomial in consideration is inseparable). Hence $\\mathrm{Aut}(\\mathbb{F}_p(u^{1/p})/\\mathbb{F}_p(u))$ is trivial, while the degree of the extension is $p$. \n\nThis problem arose because the extension in question was not separable, hence didn't have enough roots for the $\\mathrm{Aut}$ group to be non trivial. \n</code></pre>"},{"location":"Galois%20Correspondence/#lemma-1","title":"Lemma 1:","text":"<pre><code>title:\nIf $\\sigma \\in \\mathrm{Aut}(L/K)$ and $f(X) \\in K[X]$ then $\\sigma(f(\\alpha)) = f(\\sigma(\\alpha))$ for all $\\alpha \\in L$. In particular, a $K-$ automorphism permutes the root of any polynomial over $K$.\n</code></pre>"},{"location":"Galois%20Correspondence/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nFor every finite extension $L /K$, the group $\\mathrm{Aut}(L /K)$ is finite.\n</code></pre>"},{"location":"Galois%20Correspondence/#proof","title":"Proof:","text":"<p>Write \\(L = K(\\alpha_{1},\\alpha_{2},\\dots,\\alpha_{n})\\), and each \\(\\sigma \\in \\mathrm{Aut}(L/K)\\) is determined by the image of all \\(\\alpha_{i}\\)'s.  Each \\(\\alpha_{i}\\) has only finitely many possible images, its conjugates. Hence the number of \\(\\sigma\\)'s is finite.</p>"},{"location":"Galois%20Correspondence/#theorem-2","title":"Theorem 2:","text":"<pre><code>title:\nIf $L$ is a splitting field over $K$ of a polynomial in $K[X]$, then $|\\mathrm{Aut}(L/K)| \\le [L:K]$.\n</code></pre>"},{"location":"Galois%20Correspondence/#proof_1","title":"Proof:","text":"<p>Apply Theorem 1 of Splitting Fields to \\(L_{1} = L_{2} = L\\), we get from part (c) that the number of isomorphisms which extend \\(id_{K}: K \\to K\\) is at most \\([L : K]\\), which was desired.</p>"},{"location":"Galois%20Correspondence/#theorem-3","title":"Theorem 3:","text":"<pre><code>title:\nIf $L$ is a splitting field over $K$ of a **separable** polynomial in $K[X]$, then $|\\mathrm{Aut}(L/K)| = [L:K]$.\n</code></pre>"},{"location":"Galois%20Correspondence/#proof_2","title":"Proof:","text":"<p>Apply theorem 3 of Splitting Fields to \\(L_{1} = L_{2} = L\\) and \\(K_{1} =K_{2} = K\\).</p>"},{"location":"Galois%20Correspondence/#we-want-to-look-at-those-field-extensions-where-mathrmautl-k-l-k-previous-theorem-has-a-converse","title":"We want to look at those field extensions where \\(|\\mathrm{Aut}(L /K)| = [L : K]\\). Previous theorem has a converse.","text":""},{"location":"Galois%20Correspondence/#theorem-4","title":"Theorem 4:","text":"<pre><code>title:\nIf $L /K$ is a finite extension and $|\\mathrm{Aut}(L /K)| = [L  :K]$ then\n1) $L ^{\\mathrm{Aut}(L /K)} = K$\n2) $L /K$ is separable.\n3) For $\\alpha \\in L$, its $K-$conjugates are $\\sigma(\\alpha)$ as $\\sigma$ runs over $\\mathrm{Aut}(L)$.\n4) $L/K$ is normal.\n5) $L$ is the splitting field over $K$ of a separable polynomial.\n</code></pre>"},{"location":"Galois%20Correspondence/#proof_3","title":"Proof:","text":"<p>First we show that (2) and (4) imply (5). Since \\(L /K\\) is separable, using primitive element theorem, we get \\(L =K(\\gamma)\\). Let \\(f\\) be the min poly of \\(\\gamma\\) over \\(K\\). Then since \\(L /K\\) is normal, \\(f\\) splits completely in \\(L\\), thus \\(L\\) is the splitting field of \\(f\\) over \\(K\\).</p> <p>Now for the first 4 conditions. Set \\(F = L ^{\\mathrm{Aut}(L /K)}\\), this gives \\(\\mathrm{Aut}(L /K) \\subset \\mathrm{Aut}(L /F)\\). But we always have \\(\\mathrm{Aut}(L /F) \\subset \\mathrm{Aut} (L /K)\\), hence \\(\\mathrm{Aut}(L /K) = \\mathrm{Aut}(L /F)\\), this gives \\(F = L ^{\\mathrm{Aut}(L /F)}\\). Now we prove (2),(3) and (4) for \\(F\\) instead of \\(K\\) and then show that \\(F = K\\).</p> <p>For (2), pick an \\(\\alpha \\in L\\), let \\(S = \\{ \\sigma(\\alpha) : \\sigma \\in \\mathrm{Aut}(L /F) \\} = \\{ \\sigma_{i}(\\alpha) : i = 1 ,2,\\dots, m \\}\\).  Define \\(h(X) := \\prod \\limits_{ i=1}^{ m } (X - \\sigma_{i}(\\alpha))\\), we show that \\(h \\in K[X]\\), since we know \\(h\\) has distinct roots by definition, we will have a separable poly with root \\(\\alpha\\).  Now notice that for any given \\(\\sigma \\in \\mathrm{Aut}(L /F), \\ \\sigma(\\sigma_{i}(\\alpha)) = \\sigma_{j}(\\alpha)\\) for some \\(j\\). Hence any \\(\\sigma\\) takes \\(S\\) to itself injectively and hence bijectively. Therefore, \\(\\sigma(h(X)) = \\prod\\limits_{ i=1}^{ m }(X - \\sigma_{i}(\\alpha)) = h(X)\\), this gives \\(h(X) \\in F[x]\\).</p> <p>For (3), we show that \\(h(X)\\) is the minimal poly of \\(\\alpha\\) in \\(F\\). This is easy, just take any poly \\(f\\) with \\(\\alpha\\) as a root and it is easy to see that \\(\\sigma(\\alpha)\\) is a root of \\(f\\) too, for all \\(\\sigma\\). Hence, \\(h | f\\) and so \\(h\\) is the min poly of \\(\\alpha\\) in \\(F\\).</p> <p>For (4), Let \\(f\\) be an irreducible poly in \\(F[X]\\) with a root \\(\\gamma \\in L\\). We have shown that \\(h_{\\gamma}(X) = \\prod\\limits_{ i=1}^{ m } (X - \\sigma(\\gamma))\\) is the minimal poly of \\(\\gamma\\) over \\(F\\), hence \\(h_{\\gamma} = f\\). Hence all the conjugates of \\(\\gamma\\) lie in \\(L\\), hence \\(f\\) splits in \\(L\\).</p> <p>Now to show that \\(F = K\\), We have that (2) and (4) hold for \\(F\\) in place of \\(K\\), hence by (5), \\(L\\) is the splitting field of a separable poly over \\(F\\). Hence by the previous theorem, \\(|\\mathrm{Aut}(L /F)| = [L : F]\\) which gives \\(|\\mathrm{Aut}(L /K)| = [L : K] = [L:F]\\) and hence, \\(K = F\\).</p>"},{"location":"Galois%20Correspondence/#examples","title":"Examples:","text":"<ol> <li>Let \\(K = \\mathbb{Q}\\), and \\(L = \\mathbb{Q}(\\sqrt[3]{2},\\omega)\\) where \\(\\omega\\) is a non trivial cube root of unity. The polynomial \\(X^{3}-2\\) has 3 roots in \\(L\\). Each \\(K-\\)automorphism of \\(L\\) permutes these 3 roots. Check that all 6 permutations are realized by \\(K-\\)automorphisms of \\(L\\).</li> <li>Let \\(K = \\mathbb{Q}\\) and \\(L = \\mathbb{Q}(\\sqrt[4]{ 2},i)\\). The poly \\(X ^{4} - 2\\) has all four roots in \\(L\\). These 4 roots have 24 permutations possible, but not all of them are realised by \\(\\mathrm{Aut}(L/K)\\). The automorphism group is actually \\(D_{8}\\) (the dihedral group of the square).</li> <li>\\(|\\mathrm{Aut}(\\mathbb{Q}(\\sqrt[4]{ 2 }) /\\mathbb{Q})| = 2\\) but \\([\\mathbb{Q}(\\sqrt[4]{ 2 }) : \\mathbb{Q}] = 4\\)</li> </ol>"},{"location":"Galois%20Correspondence/#references","title":"References","text":"<p>Separable Extensions Separable Polynomials Splitting Fields Normal Extensions Primitive Element Theorem Extension Field</p>"},{"location":"Galois%20Extensions/","title":"Galois Extensions","text":"<p>202304161704</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Galois%20Extensions/#galois-extensions","title":"Galois Extensions","text":"<pre><code>title:\nA finite extension is called _Galois_ when it is both normal and separable. When $L /K$ is Galois, the group $\\mathrm{Aut}(L /K)$ is denoted $\\mathrm{Gal}(L /K)$ and is called the _Galois group_ of the extension.\n</code></pre>"},{"location":"Galois%20Extensions/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nFor a finite extension $L /K$, TFAE:\n1. $|\\mathrm{Aut}(L /K)| = [L : K]$\n2. $L ^{\\mathrm{Aut}(L /K)} = K$\n3. $L /K$ is normal and separable.\n4. $L$ is the splitting field over $K$ of a separable polynomial in $K[X]$.\n</code></pre>"},{"location":"Galois%20Extensions/#proof","title":"Proof:","text":"<p>We know from Theorem 4 in Galois Correspondence that (1) implies (2),(3),(4). We also showed in the proof of this theorem that (3) implies (4) Use the construction of \\(h(X)\\) as in the proof of the theorem mentioned above to show (2) implies (3).</p> <p>We know from Splitting Fields (corollary to theorem 3) that (4) implies (1). \\(mathrm{Aut}()\\)</p>"},{"location":"Galois%20Extensions/#corollary","title":"Corollary:","text":"<pre><code>title:\nIf $L/K$ is a finite extension then $|\\mathrm{Aut}(L/K)| \\mid [L:K]$.\n</code></pre>"},{"location":"Galois%20Extensions/#proof_1","title":"Proof:","text":"<p>Since \\(L /K\\) is finite, the order of the group \\(\\mathrm{Aut}(L /K)\\) is finite. Let \\(F = L ^{\\mathrm{Aut}(L /K)}\\), we know \\(\\mathrm{Aut}(L /F) = \\mathrm{Aut}(L /K)\\). So, \\(F = L ^{\\mathrm{Aut}(L /F)}\\) hence by the previous theorem, \\(|\\mathrm{Aut}(L /F)| =[L:F] \\mid [L:K]\\), hence we get that $$ |\\mathrm{Aut}(L /K)| \\mid [L:K] $$</p>"},{"location":"Galois%20Extensions/#corollary_1","title":"Corollary:","text":"<pre><code>title:\nIf $L /K$ is a finite extension which is either inseparable or not normal, then $|\\mathrm{Aut}(L/K)| &lt; [L:K]$\n</code></pre>"},{"location":"Galois%20Extensions/#theorem-2","title":"Theorem 2:","text":"<pre><code>title:\nEvery finite separable extension of a field can be enlarged to a finite Galois extension of the field. In particular, every finite extension of a field with characteristic 0 can be enlarged to a finite galois extension.\n</code></pre>"},{"location":"Galois%20Extensions/#proof_2","title":"Proof:","text":"<p>Take \\(L = K(\\alpha)\\) (Primitive Element Theorem), then we can extend \\(L\\) by adding all roots of the minimal polyomial of \\(\\alpha\\) over \\(K\\), to \\(K\\). This gives a galois extension of \\(K\\) extending \\(L\\) as well.</p>"},{"location":"Galois%20Extensions/#theorem-3","title":"Theorem 3:","text":"<pre><code>title:\nIf $L /K$ is a finite galois extension and $F$ is an intermediate field, then $L /F$ is galois as well.\n</code></pre>"},{"location":"Galois%20Extensions/#proof_3","title":"Proof:","text":"<p>Take \\(\\alpha \\in L\\), take its min poly over \\(K\\), call it \\(\\pi_{\\alpha}\\), since \\(\\alpha\\) is separable over \\(K\\), \\(\\pi_{\\alpha}\\) is separable.  Then the min poly of \\(\\alpha\\) over \\(F\\) divides \\(\\pi_{\\alpha}\\), and hence it has distinct roots since \\(\\pi_{\\alpha}\\) does, and so is separable. This means \\(L /F\\) is separable. Now take any irreducible poly \\(f\\) in \\(F\\), let it have a root \\(\\gamma\\) in \\(L\\). This has a min poly \\(\\pi_{\\gamma}\\) over \\(K\\). Since \\(L\\) is normal over \\(K\\), \\(\\pi_{\\gamma}\\) splits completely in \\(L\\). Now \\(f | \\pi_{\\gamma}\\) and hence it splits in \\(L\\) as well. Thus \\(L /F\\) is normal.</p>"},{"location":"Galois%20Extensions/#note-the-bottom-part-of-the-tower-f-k-need-not-be-galois-when-l-k-is-also-if-l-f-and-f-k-are-galois-l-k-need-not-be-galois-see-example-3","title":"NOTE: The bottom part of the tower \\(F /K\\) need not be galois when \\(L /K\\) is. Also, if \\(L /F\\) and \\(F /K\\) are galois, \\(L /K\\) need not be galois (See example 3)","text":""},{"location":"Galois%20Extensions/#theorem-4","title":"Theorem 4:","text":"<pre><code>title:\nIf $L_{1},L_{2}$ are finite galois extensions of $K$ inside a common field then $L_{1}L_{2}$ and $L_{1} \\cap L_{2}$ are both finite galois extensions of $K$.\n</code></pre>"},{"location":"Galois%20Extensions/#proof_4","title":"Proof:","text":"<p>Call the common field \\(L\\). Due to Corollary 2.2 of Separable Extensions, we get that \\(L_{1}, L_{2}\\) are subfields of the field of separable elements in \\(L\\) over \\(K\\) (call this \\(F\\)). Hence \\(L_{1}L_{2}\\) and \\(L_{1} \\cap L_{2}\\) are subfields of \\(F\\), hence separable over \\(K\\). Now take \\(\\alpha \\in L_{1} \\cap L_{2}\\), then its min poly over \\(K\\) has all its conjugates in \\(L_{1}\\) and in \\(L_{2}\\), hence in \\(L_{1} \\cap L_{2}\\). Thus \\(L_{1} \\cap L_{2}\\) is normal, and so galois.</p> <p>For \\(L_{1}L_{2}\\), observe that \\(L_{i}\\) is the splitting field of a separable poly \\(f_{i}\\) over \\(K\\) for \\(i=1,2\\). Thus \\(L_{1}L_{2}\\) is the splitting field of \\(f_{1}f_{2}\\) (removing repeated factors). </p>"},{"location":"Galois%20Extensions/#examples","title":"Examples","text":"<ol> <li>The extension \\(\\mathbb{Q}(\\sqrt[3]{ 2 },\\omega) / \\mathbb{Q}\\) is a galois extension since it's the splitting field of \\(X^{3}-2\\) over \\(\\mathbb{Q}\\).    So by theorem 1, \\(|\\mathrm{Gal}(\\mathbb{Q}(\\sqrt[3]{2  },\\omega) /\\mathbb{Q})| = 6\\).    Now we know that any autmorphism of this field is determined by the images of \\(\\omega, \\sqrt[3]{ 2 }\\). But we have two possibilities for \\(\\omega\\), and 3 for \\(\\sqrt[3]{ 2 }\\), hence totally 6 possibilities. But since the size of the galois group is 6, we know all of these possibilities are achieved. </li> </ol> <p>To look at this another way, any automorphism is also determined by what it does to the roots \\(\\sqrt[3]{ 2 }, \\omega\\sqrt[3]{ 2 }, \\omega^{2} \\sqrt[3]{ 2 }\\), and there are again 6 possibilities for this, so all of them are achieved.    In other words, \\(\\mathrm{Gal}(\\mathbb{Q}(\\sqrt[3]{ 2 },\\omega)/\\mathbb{Q}) = S_{3}\\).</p> <ol> <li> <p>The extension \\(\\mathbb{Q}(\\sqrt[4]{ 2 },i) / \\mathbb{Q}\\) is galois.    The degree of this extension is 8. Hence, \\(|\\mathrm{Gal}(\\mathbb{Q}(\\sqrt[4]{ 2 }, i) /\\mathbb{Q})| = 8\\). Since \\(\\sqrt[4]{ 2 }\\) has 4 choices, and \\(i\\) has 2 choices, any assignment of \\(\\sigma(\\sqrt[4]{ 2 })\\) and \\(\\sigma(i)\\) to roots of \\(X ^{4}-2\\) and \\(X ^{2}+1\\) must be realised by field automorphisms.    Imagine the 4 roots of \\(X ^{4}-2\\) as the vertices of a square, it can be shown with a bit of work that the galois group is just the group of symmetries of this square, which is \\(D_{8}\\).</p> </li> <li> <p>Let \\(L = \\mathbb{Q}(\\sqrt[4]{ 2 },i)\\) and \\(K = \\mathbb{Q}\\), take \\(F = \\mathbb{Q}(\\sqrt[4]{ 2 })\\), then \\(F /K\\) is not galois even though \\(L /K\\) is.    Let \\(L = \\mathbb{Q}(\\sqrt[4]{ 2 })\\) and \\(K = \\mathbb{Q}\\), take \\(F = \\mathbb{Q}(\\sqrt[]{ 2 })\\), then \\(L /K\\) is not galois even though \\(L /F\\) and \\(F /K\\) are.  </p> </li> </ol>"},{"location":"Galois%20Extensions/#references","title":"References","text":"<p>Normal Extensions Separable Extensions Galois Correspondence Splitting Fields Primitive Element Theorem Extension Field</p>"},{"location":"Gauss%20sums/","title":"Gauss sums","text":"<p>202305261605</p> <p>Type : #Note Tags : [[Number Theory]]</p>"},{"location":"Gauss%20sums/#gauss-sums","title":"Gauss sums","text":"<pre><code>title:\nLet $\\chi$ be a character on $\\mathbb{F}_{p}$ and $a \\in \\mathbb{F}_{p}$. Set $g_{a}(\\chi) = \\sum_{t} \\chi(t) \\zeta^{at}$, where the sum is over all $t \\in \\mathbb{F}_{p}$. and $\\zeta = e ^{2\\pi i /p}$. \n$g_{a}(\\chi)$ is called a **Gauss sum** on $\\mathbb{F}_{p}$ belonging to the character $\\chi$.\n</code></pre>"},{"location":"Gauss%20sums/#proposition-1","title":"Proposition 1:","text":"<pre><code>title:\nIf $a\\neq 0$ and $\\chi \\neq \\varepsilon$, then $g_{a}(\\chi) = \\chi(a ^{-1})g_{1}(\\chi)$.\n\nIf $a\\neq 0$ and $\\chi = \\varepsilon$, then $g_{a}(\\chi) = 0$.\n\nIf $a = 0$ and $\\chi \\neq \\varepsilon$, then $g_{0}(\\chi) = 0$.\n\nIf $a = 0$ and $\\chi = \\varepsilon$, then $g_{0}(\\varepsilon) = p$.\n</code></pre>"},{"location":"Gauss%20sums/#from-now-on-we-denote-g_1chi-gchi","title":"From now on we denote \\(g_1(\\chi) = g(\\chi)\\).","text":""},{"location":"Gauss%20sums/#proposition-2","title":"Proposition 2:","text":"<pre><code>title:\nIf $\\chi \\neq \\varepsilon$, then $|g(\\chi)| = \\sqrt[]{ p }$.\n</code></pre>"},{"location":"Gauss%20sums/#proof","title":"Proof:","text":"<p>Idea is to evaluate \\(\\sum_{a}g_a(\\chi)\\overline{g_{a}(\\chi)}\\) in two ways. First, we know \\(g_{a}(\\chi) = \\chi(a ^{-1})g(\\chi)\\).  This means $$ \\sum_{a} g_{a}(\\chi)\\overline{g_{a}(\\chi)} = \\sum_{a}\\chi(a ^{-1})\\overline{\\chi(a ^{-1})} g(\\chi)\\overline{g(\\chi)} = \\sum_{a\\neq 0} g(\\chi)\\overline{g(\\chi)} = (p-1)|g(\\chi)|^{2} $$ Also,  $$ \\begin{align} \\sum_{a}g_{a}(\\chi)\\overline{g_{a}(\\chi)} &amp;= \\sum_{a}\\left( \\sum_{t}\\chi(t) \\zeta^{at} \\right) \\left( \\sum_{s} \\chi(s) <sup>{-1}\\zeta</sup>{-as} \\right) \\  &amp;= \\sum_{a} \\sum_{t,s} \\chi(t)\\chi(s) ^{-1} \\zeta^{a(t-s)}\\  &amp;= \\sum_{t,s}\\chi(ts ^{-1})p \\cdot \\delta(t,s) \\  &amp;= p(p-1) \\end{align} $$ Here \\(\\delta(t,s) = 1\\) if \\(t=s\\) and 0 otherwise. Implying \\(|g(\\chi)|^{2} = p\\) as desired.</p>"},{"location":"Gauss%20sums/#lemma","title":"Lemma:","text":"<pre><code>title:\n1. $\\overline{g(\\chi)}= \\chi(-1)g(\\overline\\chi)$\n2. $\\chi(-1)p = g(\\chi)g(\\overline{\\chi})$\n</code></pre>"},{"location":"Gauss%20sums/#proof_1","title":"Proof:","text":"<ol> <li> \\[ \\begin{align*} \\overline{g(\\chi)} &amp;= \\sum_{t} \\overline{\\chi(t)}\\zeta^{-t} \\\\  &amp;= \\sum_{t}\\overline{\\chi(-1)}\\overline{\\chi(-t)} \\zeta^{-t} \\\\ &amp;= \\chi(-1) \\sum_{t}\\overline{\\chi(t)}\\zeta^{-t}\\\\ &amp;= \\chi(-1)g(\\overline{\\chi}) \\end{align*} \\] </li> <li>We know from proposition 2, \\(p = |g(\\chi)|^{2} = g(\\chi)\\overline{g(\\chi)} = g(\\chi)g(\\overline{\\chi})\\chi(-1)\\).</li> </ol>"},{"location":"Gauss%20sums/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nFor a quadratic gauss sum (a gauss sum over the quadratic character $\\chi$) $g(\\chi)$, \n$$\ng(\\chi) = \\begin{cases}\n\\sqrt{p} &amp; \\text{if } p \\equiv 1\\ (\\mathrm{mod} \\ 4) \\\\ \ni\\sqrt{p} &amp; \\text{if } p \\equiv 3\\ (\\mathrm{mod} \\ 4)\n\\end{cases}\n$$\n</code></pre>"},{"location":"Gauss%20sums/#proof_2","title":"Proof:","text":"<p>We know \\(|g(\\chi)|^{2} = p\\) and \\(\\overline{g(\\chi)} = g(\\chi)\\chi(-1) = g(\\chi)\\left(\\frac{-1}{p} \\right)\\). This gives $$ g(\\chi)^{2}\\left( \\frac{-1}{p} \\right) = p $$ Hence the result.</p>"},{"location":"Gauss%20sums/#references","title":"References","text":"<p>Multiplicative Characters</p>"},{"location":"Generating%20Effects/","title":"Generating Effects","text":"<p>202301270201</p> <p>Type : #Note Tags : [[Category Theory]]</p>"},{"location":"Generating%20Effects/#generating-effects","title":"Generating Effects","text":"<p>A Generating Effect is a proprerty of an interconnected system that cannot solely be explained in terms of its constituent sub-systemsYour brain as a whole is conscious</p> <p>A Simple Example - cat theory 1</p>"},{"location":"Generating%20Effects/#references","title":"References","text":"<p>Category Theory Introduction Page</p>"},{"location":"Generating%20Functions/","title":"Generating Functions","text":"<p>202305281205</p> <p>Type : #Note  Tags : [[Enumerative Combinatorics]]</p>"},{"location":"Generating%20Functions/#generating-functions","title":"Generating Functions","text":"<p>Generating Function is the most useful but most difficult to understand methods for evaluting \\(f(x)\\). The two most common types of Generating Functions are called Ordinary Generating Functions and Exponential Generating Functions.</p> <p>Generating Function is called a former series because the actual values of \\(x\\) is not relevant, and neither do we care about the covergence of the series. The term \\(x^n\\) is merely a marker for \\(f(n)\\). </p> <p>Generating Functions are useful because we can apply various operations that have combinatorial significance, for example we can add two generating functions, or multiply generating functions. There operations are what we would get by treating generating functions as standard algebraic objects and properties of algebraic structures that arise for example \\(\\mathbb C[[x]]\\) can be helpful in analysis of the functions.</p>"},{"location":"Generating%20Functions/#example","title":"Example","text":"<p>Find a simple expression for the generating function \\(F(x)=\\sum\\limits_{n\\ge0}a_{n}x^{n}\\) where \\(a_{0}=a_{1}=1\\) and \\(a_n=a_{n-1}+a_{n-2}\\) for \\(n\\ge 2\\)  We have  $$ \\begin{align} F(x)=\\sum\\limits_{n\\ge0} a_{n}x^{n} &amp;= 1 + x + \\sum\\limits_{n\\ge2} a_{n}x^{n}\\ &amp;= 1+x+\\sum\\limits_{n\\ge2}(a_{n-1}+a_{n-2})x^n\\ &amp;= 1+x+x<sup>{2}\\sum\\limits_{n\\ge0}a_{n}x</sup>{n}+x\\sum\\limits_{n\\ge0}a_{n}x^{n}\\ F(x) &amp;= 1 + x + xF(x) + x^{2}F(x)\\ F(x)&amp;= \\frac{1}{1-x-x^{2}} \\end{align} $$</p>"},{"location":"Generating%20Functions/#references","title":"References","text":""},{"location":"Generously%20Transitive/","title":"Generously Transitive","text":"<p>202308161108</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Generously%20Transitive/#generously-transitive","title":"Generously Transitive","text":"<p>\\(\\Gamma\\) is called Generously Transitive if \\(\\forall x\\ne y\\) there exists \\(\\alpha\\in \\Gamma\\) such that \\((x, y)\\xleftrightarrow{\\alpha}(y, x)\\)</p>"},{"location":"Good%20Kernels/","title":"Good Kernels","text":"<p>2022-11-13 02:11 pm</p> <p>Type : #Note Tags : [[Analysis]]</p>"},{"location":"Good%20Kernels/#good-kernels","title":"Good Kernels","text":"<pre><code>title: Definition\nLet $\\{K_n\\}$ be a sequence of functions defined on $\\mathbb{T}$ satisfying:\n- $\\dfrac{1}{2\\pi} \\displaystyle\\int_{-\\pi}^{\\pi} K_n(t)dt = 1  \\ \\forall \\ n \\ge 1$\n- $\\exists M &gt; 0, \\ s.t.\\ \\displaystyle\\int\\limits_{-\\pi}^{\\pi}|K_n(t)| \\le M$\n- $\\forall \\ \\delta &gt; 0, \\displaystyle\\int\\limits_{\\delta \\le |x| \\le \\pi} |K_n(x)|dx \\to 0 \\ as\\ n \\to \\infty$\n\nSuch a sequence of functions is called a family of *good kernels* or *approximate identity*.\n</code></pre>"},{"location":"Good%20Kernels/#properties","title":"Properties:","text":"<pre><code>title: Proposition\nIf $f$ is integrable over $\\mathbb{T}$, and $\\{K_n\\}$ be a family of good kernels, then: \n      $$lim_{n\\to\\infty} (f*K_n)(x) = f(x) \\ \\forall \\ x$$\n     where $f$ is continuous at x. If $f$ is continuous on $\\mathbb{T}$, then the convergence is uniform.\n</code></pre>"},{"location":"Good%20Kernels/#proof","title":"Proof:","text":"<ul> <li>Look at \\(|(f*K_n)(x) - f(x)|\\), write it as \\(\\displaystyle\\left|\\dfrac{1}{2\\pi} \\int \\limits_{-\\pi}^{\\pi}(f(x-y)-f(x))K_n(y)dy\\right|\\)</li> <li>Split this up into \\([-\\pi,-\\delta],[-\\delta,\\delta],[\\delta,\\pi]\\) </li> <li>Show that each of them is small.</li> </ul> <pre><code>title: Proposition\nThe [Dirichlet Kernels](&lt;./Dirichlet Kernels.md&gt;) are not good kernels.\n</code></pre>"},{"location":"Good%20Kernels/#proof_1","title":"Proof:","text":"<ul> <li>They violate the second property of good kernels.</li> <li>Use \\(D_N(x) = \\dfrac{sin(N + 1/2)x}{sin(x/2)}\\)</li> <li>\\(A = \\int |D_N(x)| \\ge 2\\int \\dfrac{|sin(N+1/2)x|}{x}dx \\ge 4\\int_0^{\\pi}\\left|\\dfrac{sin(N+1/2)x}{x}\\right|dx\\)</li> <li>change variables, \\(y = (N+1/2)x\\)</li> <li>\\(A \\ge 4\\int_0^{(N+1/2)\\pi} \\dfrac{|sin(y)|}{|y|}dy\\) </li> <li>break this up into integrals from \\(k\\pi\\) to \\((k+1)\\pi\\)</li> <li>and get a sum of the form 1 + 1/2 + 1/3 ... + 1/N</li> <li>\\(\\displaystyle \\int_{-\\pi}^{\\pi}|D_N(x)|dx \\ge c\\log(N) \\ as \\ N \\to \\infty\\)</li> </ul>"},{"location":"Good%20Kernels/#related-problems","title":"Related Problems","text":"<pre><code>title: Examples\n[Fejer Kernel](&lt;./Fejer Kernel.md&gt;)\n</code></pre>"},{"location":"Good%20Kernels/#references","title":"References","text":"<p>Convolutions Dirichlet Kernels</p>"},{"location":"Group%20Testing%20problem/","title":"Group Testing problem","text":"<p>202311141511</p> <p>Tags : Algorithmic Coding Theory</p>","tags":["Note","Incomplete"]},{"location":"Group%20Testing%20problem/#group-testing-problem","title":"Group Testing problem","text":"<pre><code>title: Problem\nThere are $N$ people, $n$ have some disease, say CoViD. We want to test the people for the disease, and the tests are costly. $n\\ll N$. The naive way would be to draw a sample from each of them, and test each sample separately, which is costly.\nSo another way to approach this is *'pooled testing'*, i.e. you pool samples together, and test them in groups.\n</code></pre> <p>Input: # individuals \\(N\\), \\(x_{1},x_{2},\\dots,x_{N}\\) upper bound on the number of infected individuals \\(d\\) Query/test: arbitrary subset of \\([N]\\) $$ A(S)=\\left{ \\begin{align} 1 &amp;&amp;\\text{iff }\\sum\\limits_{i\\in S}x_{i}\\ne 0\\ 0 &amp;&amp;\\text{otherwise.} \\end{align} \\right. $$ Goal: minimise the number of queries</p> <p>Adaptive approach: \\(t^{a}(d,N)\\) If a subset gives 0 on querying, we don't test it again (??) Non-adaptive approach: \\(t(d,N)\\) Pre-determined queries. Represent the set of tests as a matrix. \\(1\\leq t^{a}(d,N)\\leq t(d,N)\\leq N\\)</p> <p>We have the bounds for \\(1\\leq d\\leq N\\), \\(t^{a}(d,N)\\geq d\\log \\frac{N}{d}\\) \\(t(1,N)\\leq \\lceil \\log(N+1)\\rceil\\)</p> <p>\\(M_{t\\times N}:\\) binary matrix \\(M_{i}=\\{ j\\ |\\ m_{ij}=1 \\}:\\) \\(i^{th}\\) pool \\(M^{j}=\\{ i\\ |\\ m_{ij}=1 \\}:\\) set of pools that \\(j\\) belongs to</p> <p>\\(M_{t\\times N}x_{N\\times1}=y_{t\\times1}\\) (test outcome vector) \\(y=\\bigcup\\limits_{j\\in D}M^{j}\\) We want to uniquely identify \\(D\\).</p> <p>'Separable matrix:' A \\(t\\times N\\) binary matrix \\(M\\) is \\(d-\\)separable if the unions of upto \\(d\\) columns are all distinct.</p> <p>\\(M\\) is a \\(d-\\)separable \\(t\\times N\\) matrix, then time is something I didn't see.</p> <p>Decoding: Identifying the positives given the test outcome vector.</p> <p>Disjunct matrix: A \\(t\\times N\\) binary matrix \\(M\\) is said to be \\(d-\\)disjunct if the union of arbitrary \\(\\leq d\\) columns does not contain another column.</p> <p>\\((d+1)-\\)separable \\(\\implies\\ d-\\)disjunct \\(\\implies\\ d-\\)separable</p> <p>So the following algorithm works!</p> <pre><code>title: Naive decoding algorithm\n\n```python\nfor j = 1 to N\n    if item j belongs to at least one negative test, then mark j as a negative item.\n    end if\nend for\nreturn R(the remaining items)\n</code></pre>","tags":["Note","Incomplete"]},{"location":"Group%20Testing%20problem/#references","title":"References","text":"<p>Error Correcting Codes</p>","tags":["Note","Incomplete"]},{"location":"Groups%20In%20First%20Order%20Logic/","title":"Groups In First Order Logic","text":"<p>202310061810</p> <p>Tags : [[Logic]]</p>","tags":["Example"]},{"location":"Groups%20In%20First%20Order%20Logic/#groups-in-first-order-logic","title":"Groups In First Order Logic","text":"<p>A Group is a tuple \\(\\langle G,*,\\epsilon\\rangle\\) Where \\(G\\) is a set, \\(*\\) is a Binary Relation on the set and \\(\\epsilon\\in G\\) is a special element such that \\(\\epsilon * x = x * \\epsilon=x\\) for all \\(x\\in G\\). The Binary operator is also associative and every element has an inverse.</p> <p>We can now model Groups using the following Language Let \\(R=\\emptyset, F=\\{*\\}, S=\\{\\epsilon\\}\\) and let \\(L=\\langle R, F, S\\rangle\\) along with the following equations - \\(\\forall x\\forall y\\forall z\\; (x*y)*z=x*(y*z)\\) - \\(\\forall x\\; (x*\\epsilon\\equiv\\epsilon*x)\\land(\\epsilon*x=x)\\) - \\(\\forall x\\exists y\\; x*y \\equiv \\epsilon\\)</p> <p>The above three First Order Equations represent - Associativity of \\(*\\) - \\(\\epsilon\\) being the identity element for \\(*\\) - Existence of the inverse of every element.</p> <p>Any First Order Structure for an above language can be interpreted as a group.</p>","tags":["Example"]},{"location":"Groups%20In%20First%20Order%20Logic/#related","title":"Related","text":"<p>First Order Logic</p>","tags":["Example"]},{"location":"Guruswami-Sudan%27s%20list%20decoding%20of%20RS%20codes/","title":"Guruswami Sudan's list decoding of RS codes","text":"<p>202309211509</p> <p>Tags : Algorithmic Coding Theory</p>","tags":["Note","Incomplete"]},{"location":"Guruswami-Sudan%27s%20list%20decoding%20of%20RS%20codes/#guruswami-sudans-list-decoding-of-rs-codes","title":"Guruswami-Sudan's list decoding of RS codes","text":"Fraction of errors Decodability \\((0,\\frac{1-R}{2})\\) unique decoding \\((\\frac{1-R}{2},1-\\sqrt{R})\\) list decoding (poly big list) \\((1-\\sqrt{R},1-R)\\) no clue (\\(1-R,1\\)) exponentially many codewords","tags":["Note","Incomplete"]},{"location":"Guruswami-Sudan%27s%20list%20decoding%20of%20RS%20codes/#sudans-algorithm","title":"Sudan's algorithm","text":"","tags":["Note","Incomplete"]},{"location":"Guruswami-Sudan%27s%20list%20decoding%20of%20RS%20codes/#1-interpolation-step","title":"1. Interpolation step","text":"<p>Find a low-degree polynomial \\(Q(X,Y)\\) s.t. \\(Q(\\alpha_{i},y_{i})=0\\) \\(\\forall i = 1, \\dots ,n\\)</p>","tags":["Note","Incomplete"]},{"location":"Guruswami-Sudan%27s%20list%20decoding%20of%20RS%20codes/#2-root-finding-step","title":"2. Root finding step","text":"<p>Factor \\(Q(X,Y)\\) to find polynomials \\(f(X)\\) s.t. \\(Q(X,f(X)) = 0\\). Return all such \\(f\\).</p> <p>(We need some conditions to do 1. and 2.)*</p>","tags":["Note","Incomplete"]},{"location":"Guruswami-Sudan%27s%20list%20decoding%20of%20RS%20codes/#references","title":"References","text":"<p>Reed-Solomon Codes Mary Wootter's notes</p>","tags":["Note","Incomplete"]},{"location":"G%C3%B6del%27s%20system%20T/","title":"G\u00f6del's system T","text":"<p>202307261607</p> <p>Type : #Note Tags : [[Type Theory]]</p>"},{"location":"G%C3%B6del%27s%20system%20T/#godels-system-t","title":"G\u00f6del's system T","text":"<p>The extremely rudimentary type system we have studied has very little expressive power. We can represent integers and booleans, but not sufficiently many functions. So systems such as that of G\u00f6del appear.</p> <p><pre><code>title:\nSystems like **T** area step backwards from the logical viewpoint: the new schemes do not correspond to proofs in an extended logical system. In particular, that makes it difficult to stidy them.\n</code></pre> [[G\u00f6del's system F]] resolves the problems in satisfying manner.</p>"},{"location":"G%C3%B6del%27s%20system%20T/#the-calculus","title":"The Calculus","text":""},{"location":"G%C3%B6del%27s%20system%20T/#the-types","title":"The Types","text":"<p>The following types are present in the calculus - Atomic Types represented by : \\(A,B,\\dots\\) - Product Types such as \\(A\\times B,\\dots\\) - Arrow Types which represent functions : \\(A\\to B\\) - Constant Types \\(\\text{Int}\\) and \\(\\text{Bool}\\)</p>"},{"location":"G%C3%B6del%27s%20system%20T/#terms","title":"Terms","text":"<p>Besides the usual 5 of Simply types lambda calculus, there are schemes for specific constants \\(\\text{Int}\\) and \\(\\text{Bool}\\).</p> <ol> <li>\\(\\text{Int}\\)-Introduction<ol> <li>\\(O\\) is a constant type \\(Int\\).</li> <li>if \\(t\\) of type \\(Int\\), then \\(S\\ t\\) is of type \\(\\text{Int}\\)</li> </ol> </li> <li>\\(\\text{Int}\\)-elimination<ol> <li>If \\(u, v, t\\) have the types \\(U, U\\to{\\text{Int}\\to U}, \\text{Int}\\), then \\(\\text R\\ u\\ v\\ t\\) is of the type \\(U\\)</li> </ol> </li> <li>\\(\\text{Bool}\\)-Introduction<ol> <li>\\(\\text{T}\\) and \\(\\text{F}\\) are constants of type \\(\\text{Bool}\\)</li> </ol> </li> <li>\\(\\text{Bool}\\)-Reduction<ol> <li>if \\(u,v,t\\) are of the types \\(U,U,\\text{Bool}\\) then \\(\\text D\\ u\\ v\\ t\\) is of the type \\(U\\)</li> </ol> </li> </ol> <p>The intended types of these terms are 1. \\(\\text O\\) represents 0 and \\(\\text S\\) is the successor function 2. \\(\\text R\\) is the recursion operator 3. \\(\\text T\\) and \\(\\text F\\) are truth values 4. \\(\\text D\\) is the \"If..Then..Else\" operator</p>"},{"location":"G%C3%B6del%27s%20system%20T/#conversions","title":"Conversions","text":"<p>To the classical redexes we add: $$ \\begin{matrix} \\text{R} u v \\text{O}\\rightsquigarrow u &amp; \\text D u v \\text{T}\\rightsquigarrow u \\ \\text R u v (\\text S t)\\rightsquigarrow v (\\text R u v t) t &amp; \\text D u v \\text F \\rightsquigarrow v \\end{matrix} $$</p>"},{"location":"G%C3%B6del%27s%20system%20T/#references","title":"References","text":"<p>Normalization Theorem for G\u00f6del's system T</p>"},{"location":"Hamming%20Bound/","title":"Hamming Bound","text":"<p>202308101608</p> <p>Type : #Note Tags : Algorithmic Coding Theory</p>"},{"location":"Hamming%20Bound/#hamming-bound","title":"Hamming Bound","text":"<p>Every binary code with the block of length \\(n\\), Dimension \\(k\\), distance \\(3\\) then $$ 2^{k}\\le \\frac{2^{n}}{n+1} $$ The generalized version of the following formula is For every \\((n,k,d)_\\Sigma\\) code we have $$ k\\le n-\\log_{q}\\left(\\sum\\limits_{i=0}^{\\frac{d-1}{2}}  {n\\choose i}(q-1)^{i}\\right) $$</p>"},{"location":"Hamming%20Bound/#references","title":"References","text":""},{"location":"Hamming%20Codes/","title":"Hamming Codes","text":"<p>202308181608</p> <p>Type : #Note Tags : Algorithmic Coding Theory</p>"},{"location":"Hamming%20Codes/#hamming-codes","title":"Hamming Codes","text":"<p>Given a message \\((x_{1},x_{2},x_{3},x_{4})\\in\\{0,1\\}^{4}\\), Its corresponding Hamming code word(denoted by \\(C_{H}\\)) is $$ C_{H}(x_{1},x_{2},x_{3},x_{4})=(x_{1},x_{2}.x_{3},x_{4}, x_{2}\\oplus x_{3}\\oplus x_{4}, x_{1}\\oplus x_{2}\\oplus x_{4}, x_{3}\\oplus x_{1}\\oplus x_{4}) $$ and it is classified by the following parameters $$ C_{H}:q=2,k=4,n=7,R=4/7 $$</p> <pre><code>title: Hamming Weight\nGiven any code $v$, its Hamming weight is the number of non zero entires in the code. It is denoted by $\\text{wt}(v)$\n</code></pre> <p>The minimum Hamming weight is the minimum hamming distance between two codes.</p>"},{"location":"Hamming%20Codes/#references","title":"References","text":""},{"location":"Hamming%20Distance/","title":"Hamming Distance","text":"<p>202308101608</p> <p>Type : #Note Tags : Algorithmic Coding Theory</p>"},{"location":"Hamming%20Distance/#hamming-distance","title":"Hamming Distance","text":"<p>Given two vectors \\(\\vec u, \\vec v\\) the hamming distance \\(\\Delta(\\vec u, \\vec v)\\) is the number of positions at which they differ</p> <p>an \\(n-\\)symbol \\(t-\\)error channel over \\(\\Sigma\\) is a function \\(ch:\\Sigma^{n}\\to\\Sigma^{n}\\) such that  ^88a6ce $$ \\forall \\vec v\\in\\Sigma^{n}, \\Delta(\\vec v, ch(\\vec v))\\le t $$ It can be thought of as a channel that takes in \\(n-\\)length words and can make up to \\(t\\) error for an input word</p> <p>Hamming Distance is a Distance</p> <p>The selecting the valid code with the least Hamming Distance is a standard way of correcting erros</p> <p>Given a code \\(C\\) the following are equivalent - \\(C\\) has minimum distance \\(d\\ge 2\\) - \\(C\\) can correct \\(\\left\\lfloor(d-1)/2\\right\\rfloor\\) errors - \\(C\\) can correct \\(d-1\\) errors - \\(C\\) can correct \\(d-1\\) erasures</p>"},{"location":"Hamming%20Distance/#references","title":"References","text":"<p>Error Correcting Codes Hamming Bound</p>"},{"location":"Hardness%20of%20LTL%20Model%20Checking%20using%20Timed%20Automata/","title":"Hardness of LTL Model Checking using Timed Automata","text":"<p>202311182111</p> <p>Tags : [[Logic]], Timed Automata</p>","tags":["Note","Incomplete"]},{"location":"Hardness%20of%20LTL%20Model%20Checking%20using%20Timed%20Automata/#hardness-of-ltl-model-checking-using-timed-automata","title":"Hardness of LTL Model Checking using Timed Automata","text":"<p>[!note] Theorem Over finite timed words, the almost-sure and large LTL model-checking problem over non-blocking timed automata are \\(\\text{PSPACE-}\\)Complete </p> <p>Because of the Correspondence of Topological and Probabilistic Semantics for LTL, we have that the above two models are equivalent. And \\(\\text{PSPACE}\\)-Hardness comes directly from PSPACE-Hardness of LTL Model Checking.</p>","tags":["Note","Incomplete"]},{"location":"Hardness%20of%20LTL%20Model%20Checking%20using%20Timed%20Automata/#textpspace-algorithm-for-model-checking","title":"\\(\\text{PSPACE}\\) Algorithm for model checking","text":"<ul> <li>We first construct the region automaton \\(\\text{R}(\\mathcal A)\\) for the automaton.</li> <li>Then we colour each edge in the following manner.<ul> <li>We colour it red whenever \\(\\mu_s(I(s, e))=0\\) for some \\(s\\in q\\)</li> <li>Otherwise we colour it blue</li> </ul> </li> <li>Now to decide whether \\(\\mathcal A,s\\mid\\!\\not\\approx \\varphi\\), it is sufficient to guess a path in \\(\\text{R}(\\mathcal A)\\) that does not contain any red edges. This can be done in \\(\\text{PSPACE}\\).</li> </ul>","tags":["Note","Incomplete"]},{"location":"Hardness%20of%20LTL%20Model%20Checking%20using%20Timed%20Automata/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Hausdorff%20Property/","title":"Hausdorff Property","text":"<p>202301181801</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Hausdorff%20Property/#hausdorff-property","title":"Hausdorff Property","text":""},{"location":"Hausdorff%20Property/#definiton","title":"Definiton","text":"<p>A Topological Space \\(X\\) is called a Hausdorff Space if for each pair of points \\(x, y\\) there exits a pair of open sets \\(U,V\\) such that \\(x\\in U, y\\in V, U\\cap V=\\emptyset\\) </p> <p>If a Topology is finer than a Hausdorff Topology then it is also a Housdorf Topology</p>"},{"location":"Hausdorff%20Property/#examples","title":"Examples","text":"<p>1) \\(\\mathbb{R}^{\\omega}\\) with product topology is Hausdorff. (See R^w) 2) Every metric space is Hausdorff.</p>"},{"location":"Hausdorff%20Property/#related-results","title":"Related Results","text":"<p>1) Compact + Haudorff \\(\\implies\\) Normal</p>"},{"location":"Hausdorff%20Property/#references","title":"References","text":"<p>Separation Axioms R^w</p>"},{"location":"Heyting%20Algebra/","title":"Heyting Algebra","text":"<p>202308161608</p> <p>Type : #Note Tags : [[Logic]]</p>"},{"location":"Heyting%20Algebra/#heyting-algebra","title":"Heyting Algebra","text":"<pre><code>title:definition\n\nA distributive [lattice](&lt;./Lattice.md&gt;) with $0$ and $1$ such that for all $a, b\\in H$ we have $a\\implies b\\in H$\n</code></pre> <p>^definition</p> <p>Every element \\(c\\) in a Heyting Algebra is called a Relative Pseudo-Complement of \\(a\\) with respect to \\(b\\) iff \\(c\\) is the greatest element such that \\(a\\sqcap c\\le b\\) and is denoted by \\(a\\implies b\\). The special case of \\(a\\Rightarrow 0\\) can also be written as \\(-a\\) .</p> <p>A valuatation in a Heyting Algebra is a map \\(\\nu:\\text{PV}\\to H\\) given by { width=\"400\" } and we have the following notion: { width=\"450\" }</p>"},{"location":"Heyting%20Algebra/#pierces-law-and-law-of-excluded","title":"Pierce's Law and Law of Excluded","text":"<p>Both of the above two laws, are not valid in Heyting Algebras. Which can be proven by the following constructions.</p> <p>Let $$ \\mathcal H = \\langle O(\\mathcal T),\\cup,\\cap,\\Rightarrow,\\sim,\\emptyset,\\mathcal T\\rangle $$ where \\(\\mathcal T\\) is a  Topological space. let \\(\\mathcal T=\\mathbb R\\) </p> <p>Pierce's law is $$ ((p\\to q) \\to p)\\to p $$ Consider \\(\\nu(p)=\\mathbb R\\setminus\\{0\\}\\) and \\(\\nu(q)=\\emptyset\\). Then we get  $$ \\begin{align} \\textlbrackdbl p\\to q\\textrbrackdbl &amp;= \\text{Int}({0}\\cup\\emptyset)=\\emptyset\\ \\textlbrackdbl (p\\to q)\\to p\\textrbrackdbl &amp;= \\text{Int}(\\mathbb R\\cup \\mathbb R\\setminus{0})=\\mathbb R\\ \\textlbrackdbl ((p\\to q)\\to p)\\to p \\textrbrackdbl &amp;= \\text{Int}(\\emptyset\\cup\\mathbb R\\setminus{0})=\\mathbb R\\setminus{0}\\ne\\mathbb R\\ \\end{align} $$ For the law of excluded middle  take \\(\\nu(p)=(0,\\infty)\\)  Then we get \\(\\nu(-p)=(-\\infty,0)\\) and \\(\\nu(p\\lor-p)=\\mathbb R\\setminus \\{0\\}\\)</p>"},{"location":"Heyting%20Algebra/#example","title":"Example:","text":"<p>\\((\\theta(\\mathbb R),\\cup,\\cap,\\implies,\\emptyset,\\mathbb R)\\) where \\(A\\implies B=\\text{Int}(-A\\cup B)\\) \\(\\sim A=\\text{Int}(-A)\\)</p>"},{"location":"Heyting%20Algebra/#references","title":"References","text":"<p>Lattice Kripke Models</p>"},{"location":"Higman%27s%20Lemma/","title":"Higman's Lemma","text":"<p>202311031540</p> <p>tags : [[Order Theory]]</p>","tags":["Example","Incomplete"]},{"location":"Higman%27s%20Lemma/#higmans-lemma","title":"Higman's Lemma","text":"<p>[!note] Higman's Lemma Let \\(\\sqsubseteq\\) be a wqo on \\(A\\). Then the induced Monotone Domination Order \\(\\preceq\\) is a wqo on \\(A^*\\)</p> <p>^Lemma</p>","tags":["Example","Incomplete"]},{"location":"Higman%27s%20Lemma/#proof","title":"Proof","text":"","tags":["Example","Incomplete"]},{"location":"Higman%27s%20Lemma/#related","title":"Related","text":"<p>Well-Preorder Monotone Domination Order</p>","tags":["Example","Incomplete"]},{"location":"Hilbert%27s%20cube/","title":"Hilbert's cube","text":"<p>202303241103</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Hilbert%27s%20cube/#hilberts-cube","title":"Hilbert's cube","text":"<pre><code>title:\nThe hilbert cube is defined as the product $$ H := \\prod \\limits_{ n=1}^{ \\infty }\\left[ 0,\\frac{1}{n} \\right] $$\nwith the product topology.\n</code></pre> <p>We consider two metrics on \\(H\\), one is the uniform metric \\(\\rho\\): $$ \\rho(x,y) = \\sup \\limits_{ n\\geq 1}|x_{n}-y_{n}| $$ Another is the \\(\\ell^{2}\\) metric \\(d_{2}\\) given by: $$ d_{2}(x,y) = \\left( \\sum\\limits_{ n=1}^{ \\infty } (x_{n}-y_{n})^{2} \\right) ^{1/2} $$</p>"},{"location":"Hilbert%27s%20cube/#proposition","title":"Proposition:","text":"<pre><code>title:\nThe uniform metric and the $\\ell^{2}$ metric induce the same topology on $H$ as the product topology. In other words, $H$ is metrizable.\n</code></pre>"},{"location":"Hilbert%27s%20cube/#proof","title":"Proof:","text":"<p>Denote by \\(\\tau_{H}\\) the product topology on \\(H\\), \\(\\tau_{d_{2}}\\) be the metric topology induced by \\(d_{2}\\), and \\(\\tau_{\\rho}\\) be the metric topology induced by \\(\\rho.\\) We will show that \\(\\tau_{H} \\subset \\tau_{\\rho} \\subset \\tau_{d_{2}} \\subset \\tau_{H}\\). Take any basic open set in \\(H\\), call it \\(U\\). \\(U = \\prod\\limits_{ n=1}^{ \\infty }B_{n}\\) where finitely many \\(B_{n}\\) are open subsets of \\(\\left[ 0,\\frac{1}{n} \\right]\\) and the rest being full spaces. Fix a point \\(x \\in U\\), then for each \\(x_{n}, \\exists r_{n}\\) such that \\(B(x_{n},r_{n}) \\subset B_{n}\\). Take those \\(r_{n}\\)'s corresponding to the \\(B_{n}\\)'s that are not full spaces, and take their minimum, call it \\(r\\). Then \\(B_{\\rho}(x,r) \\subset U\\). This shows \\(\\tau_{H} \\subset \\tau_{\\rho}\\).</p> <p>Now take a ball \\(B_{\\rho}(x,r)\\). See that \\(B_{d_{2}}(x,r) \\subset B_{\\rho}(x,r)\\). This shows \\(\\tau_{\\rho} \\subset \\tau_{d_{2}}\\).</p> <p>Let \\(B_{d_{2}}(x,r)\\) be a ball in \\(d_{2}\\) metric, then \\(B_{d_{2}}(x,r) = \\prod\\limits_{ n=1}^{ \\infty }B_{n}\\) where only finitely many \\(B_{n}\\)'s are not the whole space. Then we know that the remaining \\(B_{n}\\)'s are open in \\(\\left[ 0, \\frac{1}{n} \\right]\\). And so, \\(B_{d_{2}}(x,r)\\) is open in \\(\\tau_{H}\\).</p>"},{"location":"Hilbert%27s%20cube/#proposition_1","title":"Proposition:","text":"<pre><code>title:\nThe space $$\\prod\\limits_{ n=1}^{ \\infty } [0,1] = [0,1]^{\\omega}$$\nis metrizable.\n</code></pre>"},{"location":"Hilbert%27s%20cube/#proof_1","title":"Proof:","text":"<p>Note that \\([0,1]^{\\omega}\\) is homeomorphic to \\(H\\). Since \\([0,1]\\) is homeomorphic to \\(\\left[ 0, \\frac{1}{n} \\right]\\).</p>"},{"location":"Hilbert%27s%20cube/#note","title":"NOTE:","text":"<p>The uniform metric defined on \\([0,1]^{\\omega}\\) is just the pulled back version of the uniform metric on \\(H\\).</p>"},{"location":"Hilbert%27s%20cube/#references","title":"References","text":"<p>Product topology Metric Topology Uniform Topology</p>"},{"location":"Homeomorphisms/","title":"Homeomorphisms","text":"<p>202301201801</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Homeomorphisms/#homeomorphisms","title":"Homeomorphisms","text":"<p>Homeomorphisms are a particular nice class of continuous functions that provide a way of detecting wether two topological spaces are the Same. We say that \\(f\\) is a Homeomorphism if it is continuous and its inverse \\(f^{-1}\\) is also continuous. In this case, we say \\((X,\\mathcal T_X)\\cong(Y,\\mathcal T_Y)\\) or simply \\(X\\cong Y\\).</p> <p>Let \\((X,\\mathcal T_X)\\) and \\((Y, \\mathcal T_Y)\\) be two topological spaces and \\(f:X\\to Y\\) be a bijective function between them, then the follwing are equivalent: - \\(f\\) is homeomorphic - \\(f\\) is open and contiuous - \\(f\\) is closed and contiuous - \\(U\\subseteq X\\) is open if and only if \\(f(U)\\subseteq Y\\) is open</p> <p>Properties of topological spaces which are preserved under homeomorphisms are called Topological Invariants.</p>"},{"location":"Homeomorphisms/#related-problems","title":"Related Problems","text":"<p>1) \\(f : X \\to Y\\) is bijective and continuous, and X is compact and Y is hausdorff, then \\(f\\) is a homeomorphism. (See Compactness)</p>"},{"location":"Homeomorphisms/#references","title":"References","text":"<p>Continuous Functions</p>"},{"location":"Homework%201/","title":"Homework 1","text":"<p>Name: Shubh Sharma Roll Number: BMC202170</p> <p>### 1  For \\(f=u+i v\\) to be analytic, \\(u, v\\) have to satisfy the Cauchy-Reimann Eqations.  $$  \\begin{aligned}  \\frac{\\partial u}{\\partial x} &amp;= 2x\\  \\frac{\\partial u}{\\partial y} &amp;= -2y\\  \\end{aligned}  $$  The Cauchy-Reimann Equations are  $$   {\\partial u \\over \\partial x} = {\\partial v \\over \\partial y}, {\\partial u \\over \\partial y} = -{\\partial v \\over \\partial x}  $$  So  $$  \\begin{aligned}  {\\partial v\\over \\partial y}&amp;= 2x \\  {\\partial v\\over \\partial x}&amp;= 2y \\  \\implies v &amp;= 2xy + c   \\end{aligned}  $$  Hence   $$  \\begin{aligned}  f(x+i y) &amp;= (x<sup>2-y</sup>2)+ i(2xy + c)\\  f(z) &amp;= z^2 + ic &amp; \\text{where } c \\in \\mathbb R   \\end{aligned}  $$  ### 2.  One way  If \\(f\\) is differentiable as a complex function, then its derivative follow Cauchy Reimann Equation, then   $$  \\begin{equation}  Df(x, y) =   \\begin{pmatrix}\\frac{\\partial u}{\\partial x} &amp; \\frac{\\partial u}{\\partial y} \\ -\\frac{\\partial u}{\\partial y} &amp; \\frac{\\partial u}{\\partial x}\\end{pmatrix}  \\end{equation} $$ which is of the form  $$ \\begin{pmatrix}a &amp; b \\ -b &amp; a\\end{pmatrix} $$ which computes with any complex number \\((x+iy)\\) of the from  $$ \\begin{pmatrix}x &amp; y \\ -y &amp; x\\end{pmatrix} $$ as the product for the two matrices is  $$ \\begin{pmatrix}ax-by &amp; ay+bx \\ -ay-ab &amp; ax-by\\end{pmatrix} $$ Converse If the derivative of \\(f\\) commtues with multiplication with complex number \\(z=x+iy\\) let the derivative of \\(f=Df\\) be \\(\\(\\begin{pmatrix}a &amp; b \\\\ c &amp; d\\end{pmatrix}\\)\\) $$ \\begin{align} Df\\times z &amp;= \\begin{pmatrix}ax+cy &amp; bx+dy\\ cx-ay &amp; dx-by\\ \\end{pmatrix}\\ z\\times Df&amp;= \\begin{pmatrix}ax - by &amp; ay+bx\\ cx-dy &amp; cy+dx\\end{pmatrix} \\end{align} $$ This implies \\(a = d\\) and \\(b=-c\\), hence the derivative is of the form $$ \\begin{pmatrix}a &amp; b  \\  -b &amp; a\\end{pmatrix} $$ which means it follows the Cauchy Reimann Inequality and hence complex function corresponding to \\(f\\) will be Analytic.   ### 3.  To Prove  $$  \\Bigg|\\sum\\limits_{i=1}^{n}a_i\\overline b_i\\Bigg|<sup>2\\le\\sum\\limits_{i=1}</sup>{n}|a_{i}|<sup>{2}\\sum\\limits_{j=1}</sup>{n}|b_{j}|^{2} $$ consider the term \\(\\sum\\limits_{i=1}^{n}|a_{i}\\overline b_{j}|\\) , then  by Triangle inequality $$ \\Bigg|\\sum\\limits_{i=1}<sup>{n}a_{i}b_{j}\\Bigg|</sup>2\\le\\sum\\limits_{i=1}^{n}\\big|a_{i}\\overline b_{j}\\big|^2 $$ replacing the terms \\(a_i\\) and \\(b_i\\) with \\(|a_i|\\) and \\(|b_i|\\) in the vectors and using the cauchy inequality for real numbers gives  $$ \\sum\\limits_{i=1}^{n}\\big|a_{i}\\overline b_{j}\\big|<sup>2\\le\\sum\\limits_{i=1}</sup>{n}|a_{i}|<sup>2\\sum\\limits_{i=1}</sup>{n}|b-i| $$ which gives the desired inequality.  ### 4.  Let \\(f= u_1 + iv_1\\) and \\(g = u_2 + iv_2\\)  $$  \\begin{aligned}  g\\circ f &amp;= u_2(u_1+iv_1) + iv_2(u_1+iv_2) \\  {\\partial (g\\circ f)\\over \\partial x} &amp;= {\\partial u_2(f)\\over \\partial x}{(u_1' + iv_1')\\over \\partial x} + i{\\partial v_2(f)\\over \\partial x}{(u_1'+iv_2')\\over \\partial x} \\  &amp;= \\left(\\frac{\\partial u_2(f)}{\\partial x}\\cdot {\\frac{\\partial u_1}{\\partial x}} - \\frac{\\partial v_2(f)}{\\partial x}\\cdot \\frac{\\partial v_1}{\\partial x}\\right)+i\\left(\\frac{\\partial u_2(f)}{\\partial x}\\cdot {\\frac{\\partial v_1}{\\partial x}} + \\frac{\\partial v_2(f)}{\\partial x}\\cdot \\frac{\\partial u_1}{\\partial x}\\right)\\  {\\partial (g\\circ f)\\over \\partial y} &amp;= {\\partial u_2(f)\\over \\partial y}{(u_1' + iv_1')\\over \\partial y} + i{\\partial v_2(f)\\over \\partial y}{(u_1'+iv_2')\\over \\partial y} \\  &amp;= \\left(\\frac{\\partial u_2(f)}{\\partial y}\\cdot {\\frac{\\partial u_1}{\\partial y}} - \\frac{\\partial v_2(f)}{\\partial y}\\cdot \\frac{\\partial v_1}{\\partial y}\\right)+i\\left(\\frac{\\partial u_2(f)}{\\partial y}\\cdot {\\frac{\\partial v_1}{\\partial y}} + \\frac{\\partial v_2(f)}{\\partial y}\\cdot \\frac{\\partial u_1}{\\partial y}\\right)\\  \\end{aligned}  $$ letting $$ \\begin{align} \\frac{\\partial u}{\\partial x}&amp;= \\left(\\frac{\\partial u_2(f)}{\\partial x}\\cdot {\\frac{\\partial u_1}{\\partial x}} - \\frac{\\partial v_2(f)}{\\partial x}\\cdot \\frac{\\partial v_1}{\\partial x}\\right)\\ \\frac{\\partial v}{\\partial x}&amp;= \\left(\\frac{\\partial u_2(f)}{\\partial x}\\cdot {\\frac{\\partial v_1}{\\partial x}} + \\frac{\\partial v_2(f)}{\\partial x}\\cdot \\frac{\\partial u_1}{\\partial x}\\right)\\ \\frac{\\partial u}{\\partial y}&amp;= \\left(\\frac{\\partial u_2(f)}{\\partial y}\\cdot {\\frac{\\partial u_1}{\\partial y}} - \\frac{\\partial v_2(f)}{\\partial y}\\cdot \\frac{\\partial v_1}{\\partial y}\\right)\\ \\frac{\\partial v}{\\partial y} &amp;= \\left(\\frac{\\partial u_2(f)}{\\partial y}\\cdot {\\frac{\\partial v_1}{\\partial y}} + \\frac{\\partial v_2(f)}{\\partial y}\\cdot \\frac{\\partial u_1}{\\partial y}\\right) \\end{align} $$ Then by applying Cauchy Reimann Equation to the components, we can prove that \\(f\\circ g\\) also follows the Cauchy Reimann Equations, hence it is also analytic.</p> <p>### 5.  The solution to the first question verifies the Cauchy Reimann Equations for \\(f(z)=z^2\\)</p> <p>For \\(f(z)=z^3\\) we can write \\(f\\) as   $$  \\begin{aligned}  f(x+iy)&amp;=x^3 +3ix^2y - 3xy^2 - iy^3\\  f(x+iy)&amp;=(x^3 - 3xy^2) + i(3x^2y - y^3)\\  \\end{aligned}  $$  and let  $$  \\begin{aligned}  u&amp;=x<sup>3-3xy</sup>2\\  v&amp;=3x^2y - y^3  \\end{aligned}  $$  Then  $$  {\\partial u \\over \\partial x} = {\\partial v \\over \\partial y} = 3x<sup>2-3y</sup>2  $$  And  $$  {\\partial u \\over \\partial y} = -{\\partial v \\over \\partial x} = -6xy  $$  Hence \\(f(z) = z^3\\) follows the Cauchy Reimann Equations.  ### 6.  Since \\(ax^3 + bx^2y + cxy^2 + dy^3\\) is harmonic  $$\\begin{aligned}  6ax + 2by &amp;= 0\\  2cx+6dy &amp;= 0  \\end{aligned}  $$  Hence \\(3a + c = 0\\) and \\(b+3d = 0\\)  Hence the cubic is of the form \\(ax^3  + 3dx^2y - 3axy^2 - dy^3\\) Integration Method  For the conjugate harmonic function   $$  \\frac{\\partial u}{\\partial x}= 3ax<sup>{2}+6dxy-3ay</sup>2=\\frac{\\partial v}{\\partial y}  $$ Hence \\(v\\) is of the form  $$ v = 3ax^{2}y+ 3dxy^{2}- ay^{3}+f(x)  $$ $$ \\frac{\\partial u}{\\partial y}= 3dx<sup>2-6axy-3dy</sup>2=-\\frac{\\partial v}{\\partial x} $$ Hence \\(v\\) is of the form $$ v=-x<sup>3+3a</sup>2xy+3dxy^{2}+f(y) $$ Hence  $$ v=-dx<sup>{3}+3ax</sup>{2}+3dxy<sup>{2}-ay</sup>{3} $$ Formal Method $$ \\begin{align} f(z)&amp;= 2u\\left(\\frac{z}{2},\\frac{z}{2i}\\right)-u(0,0)+ic\\ u(0,0) &amp;= 0\\ 2u\\left(\\frac{z}{2},\\frac{z}{2i}\\right) &amp;= a \\frac{z^{3}}{8}+3b\\frac {z<sup>{3}}{8i}+3a\\frac{z</sup>{3}}{8}+b\\frac{z^3}{8i}\\ \\therefore f(z) &amp;= z^{3}\\left(a+\\frac{b}{3i}\\right)\\ \\therefore f(z) &amp;= ax^3  + 3dx^2y - 3axy^{2} - dy<sup>{3}+i(-dx</sup>{3}+3ax<sup>{2}+3dxy</sup>{2}-ay^{3}) \\end{align} $$</p> <p>The conjugate harmonic of the equation of the following will be of the form \\(-dx^3 + 3ax^2y + 3dxy^2 - ay^3+K\\) , because they follow the Cauchy Reimann Equations.  ### 7.  FTSOC  Let \\(|f(z)| = c\\ne 0\\) be a function which has constant absolute value but is not constant value  $$  \\begin{align} |f(z)|<sup>{2}&amp;=c</sup>2\\ f(z)\\overline{f(z)}&amp;= c^2\\ \\overline {f(z)} &amp;= \\frac{c^2}{f(z)}\\ \\end{align} $$ Hence, \\(\\overline {f(z)}\\) is also analytic, so it follows the Cauchy Reimann Equations, comparing Cauchy Reimann Equations for \\(f(z)\\) and \\(\\overline{f(x)}\\) is \\(u(x, y) = v(x, y) = 0\\) where \\(f = u+iv\\) and \\(\\overline f = u -iv\\), which means that the functions is constant, which is a contradiction, hence, if an analytic function has constant absolute value then it reduces to a constant number.  ### 8.  let \\(f(x+iy) = u(x,y) + iv(x, y)\\)  then   $$  \\begin{align} \\overline {f(\\overline z)} &amp;= u(x, -y) - iv(x, -y)\\ \\frac{\\partial \\overline {f(\\overline z)}}{\\partial x}&amp;= \\frac{\\partial u(x, -y)}{\\partial x}- i\\frac{\\partial v(x, -y)}{\\partial x}\\ \\frac{\\partial \\overline f}{\\partial y}&amp;= -\\frac{\\partial u(x, -y)}{\\partial y}+ i\\frac{\\partial v(x, -y)}{\\partial y}\\ \\end{align} $$ \\(f(z)\\) is analytic \\(\\iff\\) \\(\\overline {f(\\overline z)}\\) is analytic because they satisfy the Cauchy Reimann Equations together.   ### 9.  $$  \\begin{align} u(z) &amp;= a(x, y) + ib(x, y)\\ u \\text{ is harmonic} &amp;\\implies v(x, y)=\\frac{\\partial^{2}u(x, y)}{\\partial x^{2}} + \\frac{\\partial^{2}u(x, y)}{\\partial y^{2}} = 0\\ \\frac{\\partial^2 u(\\overline z)}{\\partial x<sup>{2}}&amp;+\\frac{\\partial</sup>2 u(\\overline z)}{\\partial y^{2}}=v(x, -y)=0\\ \\therefore u(\\overline z) &amp;\\text{ is also harmonic}\\ \\end{align} $$$$</p>"},{"location":"Homework%202/","title":"Complex Analysis Homework 2","text":"<p>Name: Shubh Sharma Roll Number: BMC202170</p>"},{"location":"Homework%202/#page-47","title":"Page 47","text":"<p>### 1.   $$  \\sin z = z - \\frac{z^{3}}{3!}+ \\frac{z^{5}}{5!}-\\cdots,\\;\\;\\cos z = 1- \\frac{x^{2}}{2!}+ \\frac{x^{4}}{4!}-\\cdots $$ To show: $$ S_k(z)=\\sin z - \\sum\\limits_{i=0}<sup>{k}(-1)</sup>{i} \\frac{z^{2i+1}}{(2i+1)!},\\;\\;\\;C_k(z)=\\cos z - \\sum\\limits_{i=0}^{k} (-1)^{i} \\frac{z^{2i}}{(2i)!} $$ have the same sign as \\((-1)^{k+1}\\) Inducting on \\(n\\), which is the term in the considered series with the highest degree. Base case: The condititon is clear for \\(n = 1,0\\) Induction Hypothesis: The condition if true for all \\(n\\in\\{1,2,3,\\cdots n-1\\}\\) Induction Step: If \\(n(2k+1)\\) is odd: By the induction hypothesis, \\(C_k(z)\\) satisfies the condition, by integrating it over \\([0,z]\\) we get that \\(S_k(z)\\) also satisfies the condition. If \\(n(2k)\\) is even: By the induction hypothesis, \\(S_{k-1}(z)\\) satisfies the condition, by integrating it over \\([0,z]\\) we get \\(-C_k(z)\\) has the same sign as \\((-1)^k\\) hence \\(C_{k}(z)\\) has the same sign as \\((-1)^{k+1}\\). This completes the Induction.  ### 2.   $$  \\begin{align}  3&lt;\\pi&lt;2\\sqrt3\\ \\end{align} $$ $$ \\cos z = 1 - \\frac{z^{2}}{2!} + \\frac{z^{4}}{4!}-\\cdots $$ Hence \\(\\cos r\\) is real if \\(r\\) is real.  also \\(\\sin^{2}z+\\cos^{2}z = 1\\) Hence for all real \\(z\\), \\(-1\\le \\sin z, \\cos z \\le 1\\)  Derivative of \\(x-\\sin x\\) is \\(1-\\cos x\\) which is always greater than or equal to \\(0\\), and since \\(\\sin 0 = 0\\), \\(\\sin x &lt; x\\forall x&gt;0\\)  Hence  \\(\\(\\sin \\frac{\\pi}{6}=\\frac{1}{2}\\implies \\frac{1}{2}&lt;\\frac{\\pi}{6}\\implies 3&lt;\\pi\\)\\) From the previous question, we can say that  $$ \\sin x \\le x - \\frac{x^{3}}{3!}+ \\frac{x^{5}}{5!}- \\frac{x^{7}}{7!}+ \\frac{x^{9}}{9!} $$ putting \\(2\\sqrt 3\\) in the above polynomial returns a negative number. Hence \\(\\sin 2\\sqrt3 &lt;0\\) so there is a root \\(c_0\\in[0,2\\sqrt 3]\\), and \\(c_0\\) is hence and integral multiple of \\(\\pi\\) , thus \\(\\pi\\le c_0\\le2\\sqrt3\\)  ### 4.  $$  \\begin{align*} e^{z}&amp;= k\\ \\log(k) &amp;= \\log(|k|)+i\\arg(k)\\</p> <p>\\text{For } k&amp;= 2\\ z&amp;= \\log(2) + 2ni\\pi\\</p> <p>\\text{For } k&amp;= -1\\ z&amp;=  i\\pi+ 2ni\\pi\\</p> <p>\\text{For } k&amp;= i\\ z&amp;= \\frac{i\\pi}{2} + 2ni\\pi\\</p> <p>\\text{For } k&amp;= \\frac{-i}{2}\\ z&amp;= \\log\\left(\\frac{1}{2}\\right) + \\frac{i3\\pi}{2} +2ni\\pi\\</p> <p>\\text{For } k&amp;= -1-i\\ z&amp;= \\log\\left(\\sqrt2\\right)+ \\frac{i5\\pi}{8} + 2ni\\pi\\</p> <p>\\text{For } k&amp;= 1+2i\\ z&amp;= \\log\\left(\\sqrt5\\right) + i\\arctan\\left(\\frac{2}{\\sqrt5}\\right)  + 2ni\\pi\\</p> <p>\\end{align} $$  ### 6.  $$  \\begin{align} 2^{i}&amp;= (e^{\\log 2})<sup>{i}=e</sup>{i\\log(2)}=\\cos(\\log 2)+i\\sin(\\log 2)&amp;\\ i^{i}&amp;= (e<sup>{\\frac{i\\pi}{2}+2in\\pi})</sup>{i}=e^{\\frac{-\\pi}{2}+2n\\pi}&amp;\\ (-1)^{i}&amp;= (e^{\\log -1})<sup>{i}=e</sup>{\\pi+2n\\pi}\\ \\end{align} $$  ### 7.  $$  \\begin{align} z<sup>{z}&amp;=e</sup>{z\\times\\log z}\\ &amp;=e^{z\\times(\\log(|z|)+i\\arg(z))}\\ &amp;= e^{Re(z)\\cdot\\log(|z|)-Im(z)\\cdot(\\arg(z)+2n\\pi)}\\cdot e^{i(Re(z)\\cdot(\\arg(z)+2m\\pi)+Im(z)\\cdot\\log(z))}\\ &amp;= e^{Re(z)\\cdot\\log(|z|)-Im(z)\\cdot(\\arg(z)+2n\\pi)}\\cdot e^{i(Re(z)\\cdot\\arg(z)+Im(z)\\cdot\\log(z))}\\</p> <p>\\end{align*} $$  ### 9.  Let the triangle be formed by \\(3\\) complex numbers \\(a,b,c\\). let \\(m= \\frac{a+b+c}{3}\\) let \\(f(z) = \\frac{z-m}{a}\\).  Since \\(f\\) is conformal, the angle between \\(2\\) lines does not change, hence the \"angles of a trianlge\" do not change  If \\(Im(b')&gt;0\\):    let angles     $$  \\begin{aligned}    A=\\arg(c'-a')-\\arg(b'-a')\\    B=\\arg(a'-b')-\\arg(c'-b')\\    C=\\arg(b'-c')-\\arg(a'-c')\\  \\end{aligned}    $$    where \\(a' = f(a), b' = f(b), c' = f(c)\\)     Then \\(A+B+C = \\arg(b'-c')-\\arg(c'-b') + \\arg(c'-a')-\\arg(a'-c')+\\arg(a'-b')-\\arg(b'-a')\\)    Thus \\(A+B+C = -\\pi + \\pi + \\pi = \\pi\\)</p>"},{"location":"Homework%202/#page-72","title":"Page 72","text":"<p>### 1.   The domain for a single valued branch of \\(\\sqrt z\\) is the complement of \\(\\{x : x\\in\\mathbb R, x\\le0\\}\\), hence for \\(\\sqrt {1+z}\\) the domain would be \\(\\{x:x\\in\\mathbb R, x\\le-1\\}\\) and for \\(\\sqrt {1-z}\\) the domain would be \\(\\{x:x\\in\\mathbb R, x\\ge1\\}\\) Hence, for the function \\(f(z) = \\sqrt {1+z}+\\sqrt {1-z}\\) the domain would be the intersection of two which is \\(\\{x+iy:x,y\\in\\mathbb R, y\\ne 0\\}\\cup\\{x:x\\in\\mathbb R, -1\\le x\\le1\\}\\)   ### 3.  if \\(Re\\ f(z) = 0\\) then \\(f(z)=iy\\) and \\(|f(z)^2-1|&lt;1\\) implies \\(|-y^2-1|&lt;1\\) which is false for all \\(y\\in\\mathbb R\\) hence \\(Re\\ f(z)\\) is never zero in the domain. Since the domain is connected \\(Re\\ f(z)\\) is either greater than \\(0\\) or less than \\(0\\) in the entire domain.</p>"},{"location":"Homework%202/#page-78","title":"Page 78","text":"<p>### 1.  FTSOC: If there exists a transformation, let that be   $$  \\overline z = \\frac{az+b}{cz+d} $$ \\(\\overline 0 = 0\\) hence \\(b=0\\) if \\(r\\in\\mathbb R, \\overline r = r\\) $$ \\begin{align} r &amp;= \\frac{ar}{cr+d}\\ cr^{2}&amp;= (a-d)r\\ cr &amp;= a-d \\end{align} $$ which is a contradiction, hence such a linear transformation does not exist  ### 2.  If a linear transform is represented in the following way  $$  Tz= \\frac{az+b}{cz+d}=\\begin{bmatrix}a &amp; b \\ c &amp; d\\end{bmatrix} $$ Then composition of maps will correspond to multiplication of matrices \\(\\(T_1=\\begin{pmatrix}1 &amp; 2 \\\\ 1 &amp; 3\\end{pmatrix}\\)\\) \\(\\(T_2=\\begin{pmatrix}1 &amp; 0 \\\\ 1 &amp; 1\\end{pmatrix}\\)\\) \\(\\(T_1^{-1}=\\begin{pmatrix}3 &amp; -2 \\\\ -1 &amp; 1\\end{pmatrix}\\)\\) Then  \\(\\(T_1T_{2}= \\begin{pmatrix}1 &amp; 2 \\\\ 1 &amp; 3\\end{pmatrix}\\begin{pmatrix}1 &amp; 0 \\\\ 1 &amp; 1\\end{pmatrix} = \\begin{pmatrix}3 &amp; 2 \\\\ 4 &amp; 3\\end{pmatrix}\\)\\) \\(\\(T_1T_{2}= \\begin{pmatrix}1 &amp; 0 \\\\ 1 &amp; 1\\end{pmatrix}\\begin{pmatrix}1 &amp; 2 \\\\ 1 &amp; 3\\end{pmatrix} = \\begin{pmatrix}1 &amp; 2 \\\\ 2 &amp; 5\\end{pmatrix}\\)\\) \\(\\(T_1^{-1}T_{2}= \\begin{pmatrix}3 &amp; -2 \\\\ -1 &amp; 1\\end{pmatrix}\\begin{pmatrix}1 &amp; 0 \\\\ 1 &amp; 1\\end{pmatrix} = \\begin{pmatrix}1 &amp; -2 \\\\ 0 &amp; 1\\end{pmatrix}\\)\\) Hence  $$ \\begin{matrix}T_{1}T_{2}z= \\frac{3z+2}{4z+3} &amp; T_{2}T_{1}z= \\frac{z+2}{2z+5} &amp; T_{1}^{-1}T_{2}z= z-2\\end{matrix} $$  ### 3. \\(|az -  aw| = |a||z-w| = |z=w|\\) if \\(|a|=1\\) Hence rotation preserves distances. Since distance is preserved \\(|z| = |Tz|\\). Hence all points of a circle of radius \\(r\\) around \\(0\\) will remain on the cirlcle. Let \\(a = T1\\)  Let \\(b = Tz\\) for some \\(z\\in \\mathbb C\\) \\(| \\frac{b}{a} |= |z|\\) and \\(|z-1| = \\left| \\frac{b}{a} - \\frac{a}{a}\\right| = \\left| \\frac{b}{a}-1\\right|\\) Since the distance from \\(0\\) and \\(1\\) is the same for the points \\(z\\) and \\(\\frac{b}{a}\\),  \\(\\frac{b}{a}= z\\) or \\(\\frac{b}{a}=\\overline z\\) . Hence the transformation followed by a rotation that sends 1's image to 1 sends all points to themselves or thier relfection in the real number line.  Let that transformation be \\(T'z = \\frac{Tz}{T1}\\)  For non real numbers \\(a=x+iy, b=z+iw\\) and \\(T'a=\\overline a\\)  Since \\(T'\\) is distance preserving  $$ \\begin{align} |a-b| &amp;= |Ta - Tb|\\ |(x-z)+i(y-w)| &amp;= |(x-z)+i(-y-w')| \\end{align} $$ is true if and only if \\(y-w = -y-w'\\) where \\(w' = w\\) or \\(-w\\) and for  \\(w,y\\ne 0\\) the statement is true iff \\(w' = -w\\) or \\(T'b=\\overline b\\) Hence if one of the points goes to its mirror image, all points go to their mirror image. so \\(T'\\) is either identity or reflection about the real number line. any such \\(T\\)  can be obtained by following \\(T'\\) with a rotation that maps \\(1\\) to \\(T1\\). Hence the most general distance preserving transformations are identitiy or relfection about the real numberline followed by a rotation. If the transfomation is relfection followed by rotation, it maps \\(r\\cdot e^{i\\theta}\\) to \\(r\\cdot e^{-i\\theta+i\\alpha}\\) which can be rewritten as \\(r\\cdot e^{-(i\\theta-i\\alpha)}\\) which is rotation by \\(-\\alpha\\) followed by reflection. Hence any transformation that preserves distances is of the form, a rotation followed by identity or reflection along the real number line.  ### 4.  Let the transformation be   \\(\\(Tz'= \\frac{a'z+b'}{c'z+d'}\\)\\)  If \\(a'\\ne 0\\) divide the numerator and denominator by  \\(|a'|^2\\)  $$  Tz = \\frac{z+b}{cz+d} $$ putting \\(z=0\\) we get \\(b/d\\) is real and taking the sequence of natural numbers \\(Tz\\) converges to \\(\\frac{1}{c}\\) Hence \\(c\\) is also real.</p> <p>Taking \\(b=x+iy\\) and \\(z=-x\\) we get  \\(\\(T(-x)= \\frac{iy}{(-cx+kx)+kiy}\\)\\) Which is real iff \\(c=k\\) so  $$ Tz = \\frac{z+b}{cz+cb}=1/c $$ otherwise,  if \\(c=rd\\) where \\(r\\) is real we get $$ Tz = \\frac{b}{d(rz+1)} = \\frac{b/d}{rz+1} $$ which would make \\(\\frac{b}{d}, r, 1\\) real otherwise, We can find real numbers \\(a\\) and \\(b\\) such that they would make the real and imaginary parts of the denominator \\(0\\) respectively. Hence To make the fraction always real, we \\(b\\) would have to be both real and imaginary so \\(T(z)=0\\).</p>"},{"location":"Homogenous%20Linear%20Systems/","title":"Homogenous Linear Systems","text":"<p>202303111603</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"Homogenous%20Linear%20Systems/#homogenous-linear-systems","title":"Homogenous Linear Systems","text":"<pre><code>title:\nA homogenous Linear system is of the form \n$$\\dot{\\bar{x}}(t) = A(t)x(t)$$\nWhere $A : I \\to M_n(\\mathbb{R})$\n</code></pre>"},{"location":"Homogenous%20Linear%20Systems/#how-do-we-find-its-solutions","title":"How do we find its solutions?","text":"<p>Assume henceforth that \\(A(t)\\) is continuous.</p> <p>Consider \\(\\(T_{A} : C^{1}(I,\\mathbb{R}^{n}) \\to C^{0}(I,\\mathbb{R}^{n})\\)\\)\\(\\(f \\mapsto \\frac{ df }{ dt } - A(t)f(t)\\)\\)</p>"},{"location":"Homogenous%20Linear%20Systems/#claim-1-t_a-is-a-linear-map","title":"Claim 1: \\(T_{A}\\) is a linear map.","text":""},{"location":"Homogenous%20Linear%20Systems/#claim-2-the-map-t_a-is-surjective","title":"Claim 2: The map \\(T_{A}\\) is surjective.","text":""},{"location":"Homogenous%20Linear%20Systems/#proof","title":"Proof:","text":"<p>Let \\(g \\in C^{0}(I,\\mathbb{R}^{n})\\). Then \\(g \\in Im(T_{A})\\) if there exists \\(\\varphi \\in C^{1}(I,\\mathbb{R}^{n})\\) s.t.  \\(\\(\\frac{ d\\varphi }{ dt } - A(t)\\varphi(t) = g(t)\\)\\) This is true by Existence and uniqueness for linear systems.</p>"},{"location":"Homogenous%20Linear%20Systems/#claim-3-dim-ker-t_a-n","title":"Claim 3: \\(\\dim (\\ker T_{A}) = n\\).","text":""},{"location":"Homogenous%20Linear%20Systems/#proof_1","title":"Proof:","text":"<p>Fix \\(t_{0} \\in I\\), let \\(\\varepsilon_{t_{0}} : \\ker T_{A} \\to \\mathbb{R}^{n}\\) \\(\\(\\varphi \\mapsto \\varphi(t_{0})\\)\\) Then \\(\\varepsilon_{t_{0}}\\) is linear.  Suppose \\(\\varphi(t_{0}) = 0\\) for some \\(\\varphi\\). Then \\(\\varphi\\) is a solution of $$ \\dot{x} = Ax;    x(t_{0}) = 0 $$ By uniqueness of solution, \\(\\varphi(t) = 0 \\ \\forall t \\in I\\) as \\(\\varphi \\equiv 0\\) is a solution.</p> <p>Thus \\(\\varepsilon_{t_{0}}\\) is an injective linear map.</p> <p>Now for any \\(x_{0} \\in \\mathbb{R}^{n}\\), we need to find a \\(\\varphi \\in \\ker T_{A}\\) such that \\(\\varphi(t_{0}) = x_{0}\\) But this is a consequence of Existence and uniqueness for linear systems. Thus \\(\\varepsilon_{t_{0}}\\) is an isomorphism, hence we are done.</p> <p>Let \\(\\varphi_{1}, \\varphi_{2} \\dots \\varphi_{n}\\) be a basis of \\(\\ker T_{A}\\), then any element of \\(\\ker T_{A}\\) can be written as a linear combination of these.</p>"},{"location":"Homogenous%20Linear%20Systems/#related-problems","title":"Related Problems","text":"<p>Inhomogenous Linear Systems</p>"},{"location":"Homogenous%20Linear%20Systems/#references","title":"References","text":"<p>Existence and uniqueness for linear systems</p>"},{"location":"Homotopy%20Equivalence/","title":"Homotopy Equivalence","text":"<p>202304041604</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Homotopy%20Equivalence/#homotopy-equivalence","title":"Homotopy Equivalence","text":"<pre><code>title:\nLet $f : X \\to Y$ and $g : Y \\to X$ be cts maps. If $g \\circ f : X \\to X$ is homotopic to $id_{X}$ and $f \\circ g : Y \\to Y$ is homotopic to $id_{Y}$, then we say that $f,g$ are **homotopy equivalences**. $X$ and $Y$ have the same **homotopy type**.\n</code></pre>"},{"location":"Homotopy%20Equivalence/#examples","title":"Examples:","text":"<ol> <li>If \\(r : X \\to A\\) deformation retract, \\(f = r\\), \\(g = i : A \\hookrightarrow X\\)</li> <li>\\(A =\\) figure eight space, \\(X = \\mathbb{R}^{2} \\setminus \\{ 2 \\ points \\}\\), \\((i,r)\\) maps    \\(A' =\\) Theta figure \\((i',r')\\) maps    $$ A \\xrightarrow{i} X \\xrightarrow{r'} A' \\xrightarrow{i'} X \\xrightarrow{r} A $$ This is homotopic to \\(A \\xrightarrow{i} X \\xrightarrow{r}A\\) because \\(r' \\circ i'\\) is homotopic to \\(id_{X}\\). So \\(r \\circ i\\) is homotopic to \\(id_{A}\\) and similarly in the other direction. So, \\(A,A'\\) are homotopy equivalent with \\(f = r' \\circ i\\) and \\(g = r \\circ i'\\).</li> </ol>"},{"location":"Homotopy%20Equivalence/#proposition","title":"Proposition:","text":"<pre><code>title:\nIf $f:X \\to Y$ and $g:Y \\to Z$ are homotopy equivalences then $g \\circ f:X\\to Z$ is a homotopy equivalence.\n</code></pre>"},{"location":"Homotopy%20Equivalence/#proof","title":"Proof:","text":"<p>Take $$ X \\xrightarrow{f} Y \\xrightarrow{g} Z \\xrightarrow{g'} Y \\xrightarrow{f'} X $$ This is homotopic to \\(id_{X}\\). So \\(X\\) and \\(Z\\) are homotopy equivalent with \\(g \\circ f\\) and \\(f' \\circ g'\\) as the homotopy equivalence maps.</p>"},{"location":"Homotopy%20Equivalence/#definition","title":"Definition:","text":"<p><pre><code>title:\n$X$ is contractible if $id_X$ is homotopic to a constant map $X \\to \\{p\\}$, $p\\in X$.\n</code></pre> This gives that \\(\\{ p \\}\\to X\\) is a homotopy equivalence.</p>"},{"location":"Homotopy%20Equivalence/#theorem","title":"Theorem:","text":"<pre><code>title:\nLet $f : (X,x_0)\\to (Y,y_0)$ be a cts map. If $f$ is a homotopy equivalence, then $f_*$ is an isomorphism.\n</code></pre>"},{"location":"Homotopy%20Equivalence/#proof_1","title":"Proof:","text":"<p>Recall Lemma 2 from Deformation Retracts. Let \\(g : Y \\to X\\) be the homotopy inverse to \\(f\\). Let \\(g(y_{0}) = x_{1}\\) and let \\(f(x_{1}) = y_{1}\\). Consider the sequence of maps $$ (X,x_{0}) \\xrightarrow{f} (Y,y_{0}) \\xrightarrow{g} (X,x_{1}) \\xrightarrow{f} (Y,y_{1}) $$ This gives rise to the following homomorphisms: $$ \\Pi_{1}(X,x_{0}) \\xrightarrow{f_{}} \\Pi_{1}(Y,y_{0}) \\xrightarrow{g_{}} \\Pi_{1}(X,x_{1}) \\xrightarrow{f_{*}'} \\Pi_{1}(Y,y_{1}) $$ Since we know that \\(g \\circ f\\) is homotopic to \\(id_{X}\\), so by the lemma applied to \\(h = g \\circ f\\) and \\(k = id_{X}\\), we get that \\(h_{*} = \\hat{\\alpha}(k_{*}) \\implies (g \\circ f)_{*} = \\hat{\\alpha}(id_{\\Pi_{1}(X,x_{0})}) = \\hat{\\alpha}\\). Which means that \\(g_{*} \\circ f_{*}\\) is an isomorphism.</p> <p>Similarly, \\(f \\circ g\\) is homotopic to \\(id_{Y}\\) and so the lemma gives that \\(f_{*}' \\circ g_{*}\\) is an isomorphism, this gives that \\(g_{*}\\) is bijective, and hence an isomorphism. Which gives that \\(f_{*}\\) is an isomorphism.</p>"},{"location":"Homotopy%20Equivalence/#references","title":"References","text":"<p>Homotopy of paths Deformation Retracts Fundamental Group</p>"},{"location":"Homotopy%20of%20paths/","title":"Homotopy of paths","text":"<p>202304031504</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Homotopy%20of%20paths/#homotopy-of-paths","title":"Homotopy of paths","text":"<pre><code>title:\nLet $X,Y$ be two topological spaces.\nLet $f,g:X \\to Y$ be two continuous maps between them. Then a _homotopy_ between $f,g$ is a continuous map $F : X \\times I \\to Y$ such that $F(x,0) = f(x)$ and $F(x,1) = g(x)$.\n\n\nIf such a map exists, we say that $f,g$ are homotopic and we write $g \\simeq f$. If $f$ is homotopic to the constant map, we say it is nullhomotopic.\n</code></pre>"},{"location":"Homotopy%20of%20paths/#note-when-we-talk-of-homotopy-of-paths-fg-01-to-x-we-assume-that-the-homotopy-fixes-their-endpoints-since-otherwise-all-paths-are-homotopic-to-the-constant-path","title":"NOTE: When we talk of homotopy of paths \\(f,g : [0,1] \\to X\\) we assume that the homotopy fixes their endpoints, since otherwise all paths are homotopic to the constant path.","text":"<pre><code>title:\nTwo paths $f,g : [0,1] \\to X$ are _path homotopic_ if there exists $F : [0,1] \\times [0,1] \\to X$ such that $F(s,0) = f(s)$ and $F(s,1) = g(s)$ and $F(0,t) =x_0$ and $F(1,t) = x_1$.\n\nSuch an $F$ is called a path homotopy and we write $f \\simeq_p g$ for this relation between $f,g$.\n</code></pre> <ul> <li>In particular, for all time \\(t \\in [0,1]\\), \\(f_{t} : [0,1] \\to X\\) is a path from \\(x_{0}\\) to \\(x_{1}\\).</li> </ul>"},{"location":"Homotopy%20of%20paths/#lemma","title":"Lemma:","text":"<pre><code>title:\nThe relations $\\simeq$ and $\\simeq_p$ are equivalence relations on paths.\n</code></pre>"},{"location":"Homotopy%20of%20paths/#proof","title":"Proof:","text":"<p>The relations are obviously symmetric and reflexive. For transitivity, we can concatenate homotopies. Say \\(f \\simeq g\\) and \\(g \\simeq h\\) and \\(F,G\\) are the respective homotopies. Then we can just take \\(\\(H = \\begin{cases} F(s,2t)  &amp; 0 \\le t \\le \\frac{1}{2}\\\\ G(s,2t-1) &amp; \\frac{1}{2} \\le t \\le 1 \\end{cases}\\)\\) giving us a homotopy from \\(f\\) to \\(h\\).</p>"},{"location":"Homotopy%20of%20paths/#note-we-denote-the-homotopy-equivalence-class-of-f-by-f","title":"NOTE: We denote the homotopy equivalence class of \\(f\\) by \\([f]\\).","text":""},{"location":"Homotopy%20of%20paths/#examples","title":"Examples:","text":"<ol> <li>If \\(f,g\\) are paths in \\(\\mathbb{R}^{2}\\), we can define the straight line homotopy between \\(f,g\\) by $$ F(s,t) = (1-t)f(s) + tg(s) $$</li> <li>In \\(X = \\mathbb{R}^{2} \\setminus \\set{(0,0)}\\), let \\(f,g\\) be paths from \\((-1,0)\\) to \\((1,0)\\) such that \\(f\\) stays in the upper half plane and \\(g\\) stays in the lower half plane. Then there is no homotopy between them. </li> </ol>"},{"location":"Homotopy%20of%20paths/#references","title":"References","text":"<p>Continuous Functions</p>"},{"location":"How%20to%20count/","title":"How to count","text":"<p>202305280005</p> <p>Type : #Note Tags : [[Enumerative Combinatorics]]</p>"},{"location":"How%20to%20count/#how-to-count","title":"How to count","text":"<p>Enumerative Combinatorics is mainly about counting the number of elements in a finite set. Usually in an infinite family of finite sets \\(S_i\\) where \\(i\\) ranges over some indexing set \\(I\\). </p> <p>One of the ways to achive that is by finding a counting function \\(f(i):\\mathbb N \\to \\mathbb N\\) which returns the size of the \\(i^{th}\\) set.</p> <p>Some of the Standard ways to give a counting function are</p> <ol> <li>Closed Form: ^8d213d<ol> <li>The nicest form and generally the most desired one, but one's preference towards this would decrease as the complexity of the formula decreases</li> <li>Ex: Number of subsets of \\([n]\\), its corresponding counting function would be \\(f(i)=2^{i}\\)</li> </ol> </li> <li>Recurrence Relation:<ol> <li>Where \\(f(i)\\) is calculated based on smaller \\(f(i)\\)'s </li> <li>Ex: Number of subsets of \\([n]\\) That do not contain consecutive numbers gives \\(f(i) = f(i-1) + f(i-1)\\) where \\(f(1)=2, f(2)=3\\)</li> </ol> </li> <li>Algorithm:<ol> <li>This one subsumes a few other methods</li> <li>We need the algorithm to be effient, should require much less steps than \\(f(i)\\) itself, otherwise is useless</li> </ol> </li> <li>Estimation by Asymptotic Functions:<ol> <li>This one is prefferered when the solution to the first one is extremely ugly, which makes it in some sense better than the Closed Form. </li> <li>Ex. Something like \\(e^{-2} 36^{-n} (3n)!\\) seems more useful than \\(\\(6^{-n} n!^{2}\\sum \\frac{(-1)^{\\beta}(\\beta+3\\gamma)!2^{\\alpha}3^{\\beta}}{\\alpha!\\beta!\\gamma!6^{\\gamma}}\\)\\)For non negative \\(\\alpha + \\beta + \\gamma = n\\)</li> <li>\\(f\\) and \\(g\\) are said to be aymptotic if \\(\\lim\\limits_{x\\to\\infty}\\frac{f}{g}(x)=1\\)</li> </ol> </li> <li>Generating Function:<ol> <li>Functions that can be written as a polynomial series where the coefficient of \\(x^n\\) is \\(f(i)\\) or \\(\\frac{f(i)}{n!}\\) </li> <li>Ex: For number of elements in \\([n]\\) gives the exponential generating function \\(F(x)=e^{x+1}\\) </li> </ol> </li> </ol>"},{"location":"How%20to%20count/#references","title":"References","text":"<p>Generating Functions</p>"},{"location":"IFPL%20Lec%202/","title":"IFPL Lec 2","text":"<p>Front-End : Translate Haskell to enriched lc then lc. Back-End : Graph Reduction + Code Generation.</p> <p>Lc language $$ \\begin{matrix}K &amp; | &amp; v &amp; | &amp; \\lambda v.E &amp; | &amp; E_{1}.E_{2}\\end{matrix} $$</p> <p>We have beta eta and alpha reductions.</p> <p>Eval stuff. There will be an Undefined element \\(\\perp\\) in your domain. and Eval of an expression is \\(\\perp\\) if it does not have a normal form.</p> <p>Fatbar stuff for multiple line definitions.</p> <p>I AM NOT SURE ANYMORE</p>"},{"location":"IFPL%20Note%20Order/","title":"IFPL Note Order","text":"<ul> <li>Denotational Semantics for lambda Calculus</li> <li>Enriched Lambda Calculus</li> <li>let-expressions in Enriched Lambda Calculus</li> <li>let-expressions in Enriched Lambda Calculus</li> <li>Patterns</li> <li>Translating Haskell Programs to Lambda Calculus</li> <li>Translation Scheme from Haskell to Lambda Calculus for Expressions</li> <li>Translation Scheme from Haskell to Lambda Calculus for Definitions</li> <li>Translation Scheme from Haskell to Lambda Calculus for Some RHS only options</li> <li>Evaluating Pattern Matching in Lambda Calculus</li> <li>Match Function for Enriched Lambda Calculus</li> <li>Variable Rule for Match Function</li> <li>Constructor Rule for Match Function</li> <li>Empty Rule for Match Function</li> <li>Mixture Rule for Match Expression</li> <li>Optimisations for Overlapping Patterns</li> <li>Optimisations for Expressions containing FAIL and I&gt;</li> <li>Uniform Definition of Haskell Functions</li> <li>False Assumptions easy to make about Patterns</li> <li>Ordering Equations in Uniform Definitions</li> <li>Independence of meaning from changing the order on the left hand side implies uniform definition</li> <li>Converting Enriched Lambda Calculus to Ordinary Lambda Calculus</li> <li>Pattern Matching to Ordinary Lambda Calculus</li> <li>Constant Pattern to Lambda Calculus</li> <li>Product Constructor Pattern Matching to Lambda Calculus</li> <li>Sum Constructor Pattern Matching to Lambda Calculus</li> <li>[[let(rec)-expressions to Ordinary Lambda Calculus]]</li> <li>Dependency analysis for letrec expressions</li> <li>Irrefutable let(rec)</li> <li>Conformality Transformation on let(rec) expressions</li> <li>Irrefutable let(rec)s to Simple let(rec)s</li> <li>letrec-expressions to Irrefurtable let-expressions</li> <li>Simple let expressions to Ordinary Lambda Calculus</li> <li>Converting Case Expressions to Ordinary Lambda Calculus</li> </ul>"},{"location":"Inapproximability%20of%20Max%20Independent%20Set/","title":"Inapproximability of Max Independent Set","text":"<p>202311201511</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note","Incomplete"]},{"location":"Inapproximability%20of%20Max%20Independent%20Set/#inapproximability-of-max-independent-set","title":"Inapproximability of Max Independent Set","text":"<p>We will assume that MAX-3SAT has no approx \\(&gt;\\frac{7}{8}\\) unless P=NP, and reduce it to this problem.</p> <p>\\(\\phi=(x_{1}\\lor \\bar{x}_{2}\\lor x_{3})\\land(x_{2}\\lor \\bar{x}_{3}\\lor x_{4})\\land(x_{1}\\lor \\bar{x}_{2}\\lor\\bar{x}_{4})\\) We make a triangle for each clause, with each variable as a vertex, and then join all the edges (possibly across triangles) that are of the form \\((x_{i},\\bar{x}_{i})\\). Call this graph \\(G_{\\phi}\\). Claim: max # satisfiable clauses = size of a max ind set in \\(G_{\\phi}\\). \\(OPT(\\phi)=OPT(G_{\\phi})\\). Thus, max ind set cannot be approximated to \\(&gt; \\frac{7}{8}\\) unless P=NP.</p>","tags":["Note","Incomplete"]},{"location":"Inapproximability%20of%20Max%20Independent%20Set/#theorem","title":"Theorem","text":"<p>If there is an \\(\\alpha-\\)approximation for max ind set then there is a \\(\\sqrt{\\alpha}-\\)approximation for the same.</p> <p>\\(G=(V,E)\\) Define \\(G^{2}=G\\times G,V^{2}=V\\times V,E^{2}=\\{ ((a,b)(c,d))\\ |\\ (a,c)\\in E or (b,d)\\in E \\}\\). Claim: \\((OPT(G))^{2}=OPT(G^{2})\\) where \\(OPT\\) is max ind set.</p> <p>Proof: If \\(S\\) is an ind set of size \\(k\\) in \\(G\\) then \\(S\\times S\\) is an ind set in \\(G^{2}\\) of size \\(k^{2}\\), If \\(a,b\\in S\\) then \\((a,b),(b,a),(a,a),(b,b)\\in S\\times S\\) and no two have an edge.</p> <p>Let \\(S'\\) be an ind set in \\(G^{2}\\). \\(S'=\\{ (a_{1},b_{1}),(a_{2},b_{2}),\\dots,(a_{k},b_{k}) \\}\\) \\(S_{1}=\\{ a\\ |\\ \\exists b(a,b)\\in S' \\}\\) \\(S_{2}=\\{ b\\ |\\ \\exists a(a,b)\\in S' \\}\\) Both \\(S_{1},S_{2}\\) are ind sets in \\(G\\). \\(S'\\subseteq S_{1}\\times S_{2}\\) The larger of \\(S_{1},S_{2}\\), let's say \\(S_{1}\\) has \\(|S_{1}|\\geq \\sqrt{ |S'| }\\). \\((OPT(G))^{2}=OPT(G^{2})\\).</p> <p>\\(\\alpha-\\)approx for max ind set in \\(G\\) \\(\\implies \\alpha OPT(G^{2})\\) size ind set in \\(G^{2}\\) \\(\\implies \\sqrt{ \\alpha }\\sqrt{ OPT(G^{2}) }\\) size ind set in \\(G^{2}\\) i.e. \\(\\sqrt{ \\alpha  }OPT(G)\\) size ind set in \\(G\\).</p> <p>We can get \\((1-\\epsilon)-\\)approx i.e. PTAS for ind set: Apply the reduction \\(k\\) times. Get \\(G^{2^{k}}\\), apply \\(\\alpha-\\)approx algo to \\(G^{2^{k}}\\). Get \\(\\alpha^{1/2^{k}}-\\)approx for \\(G\\). We want \\(\\alpha^{1/2^{k}}\\geq 1-\\epsilon\\). So \\(k\\geq \\log \\log \\frac{1}{\\alpha}-\\log \\log \\frac{1}{1-\\epsilon}\\).</p>","tags":["Note","Incomplete"]},{"location":"Inapproximability%20of%20Max%20Independent%20Set/#references","title":"References","text":"<p>Probabilistically Checkable Proof</p>","tags":["Note","Incomplete"]},{"location":"Incidence%20Matrix/","title":"Incidence Matrix","text":"<p>202308140108</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Incidence%20Matrix/#incidence-matrix","title":"Incidence Matrix","text":"<p>The incidence matrix of a graph \\(G=\\langle V, E\\rangle\\) is a matrix \\(M\\) of order \\(|V|\\times|E|\\) such that if the edge \\(m\\) is incident on the vertex \\(n\\) then \\(M_{n, m}\\) is \\(1\\) otherwise \\(0\\). </p> <p>Nice Properties of incidence matrices. \\(MM^{t}=\\text{diag}(d_{1}, \\dots,d_{n})+L(G)\\) \\(M^{t}M=2I+G\\)</p>"},{"location":"Incidence%20Matrix/#title-peterson-graph-is-a-line-graph","title":"<pre><code>title:\nPeterson Graph is a line graph.\n</code></pre>","text":""},{"location":"Incidence%20Matrix/#references","title":"References","text":"<p>Line Graph</p>"},{"location":"Independence%20of%20meaning%20from%20changing%20the%20order%20on%20the%20left%20hand%20side%20implies%20uniform%20definition/","title":"Independence of meaning from changing the order on the left hand side implies uniform definition","text":"<p>202310292154</p> <p>tags : [[Programming Languages]]</p>","tags":["Example"]},{"location":"Independence%20of%20meaning%20from%20changing%20the%20order%20on%20the%20left%20hand%20side%20implies%20uniform%20definition/#independence-of-meaning-from-changing-the-order-on-the-left-hand-side-implies-uniform-definition","title":"Independence of meaning from changing the order on the left hand side implies uniform definition","text":"<p>[!note] Theorem If the left-hand sides of a definition are such that the order of equations does not matter (regardless of the right hand side or condition parts of the equations), the definition is uniform </p> <p>^b54fe4</p> <p>We prove this by induction on number of variables to be pattern matched. - Empty Rule for Match Function     - Empty rule always picks the first equation in the set and falls through if it evaluates to \\(\\text{FAIL}\\).     - If there are more than one definitions, consider the case where neither of them evaluate to \\(\\text{FAIL}\\), and are different, in that case ordering them will change the meaning and hence the set of equations can have at most 1 element.     - By definition of uniform definitions we get the base case of induction - Variable Rule for Match Function     - If the set of equations follow the variable pattern, we apply the variable rule to get a set of uniform equations(by induction) and get that the set of equations was uniform - Constructor Rule for Match Function     - If the set of equations follow the constructor pattern, we apply the constructor pattern and then apply the variable rule repeatedly (can be 0) to get rid of all newly introduced variables.     - If the original order of equations did not change the meaning of the function, then the order for this set will not matter either, so the new set of equations is uniform by induction.     - Given that we have proved that if we apply the variable rule to get a set of uniform equations then the original set of was also uniform, we get that just after applying the constructor rule we get that for each constructor we get a set of uniform equations     - This proves that the original set of equations was uniform - Mixture Rule for Match Expression     - This happens when the some of the elements of the set of equations match the variable patterns and some match the constructor pattern.     - If we have a function that discard the first argument entirely, and gives some output, the order matters, because if the first equation follows the variable pattern or constructor pattern decides if the program terminates.     - Hence there if we have to use the mixture rule, then the order of the left hand side matters hand hence the hypothesis fails. \\(\\square\\) </p>","tags":["Example"]},{"location":"Independence%20of%20meaning%20from%20changing%20the%20order%20on%20the%20left%20hand%20side%20implies%20uniform%20definition/#related","title":"Related","text":"<p>Uniform Definition of Haskell Functions Ordering Equations in Uniform Definitions</p>","tags":["Example"]},{"location":"Inhomogenous%20Linear%20Systems/","title":"Inhomogenous Linear Systems","text":"<p>202303111803</p> <p>Type : #Note Tags :[[Differential Equations]]</p>"},{"location":"Inhomogenous%20Linear%20Systems/#inhomogenous-linear-systems","title":"Inhomogenous Linear Systems","text":"<pre><code>title:\nGiven the IVP \n$$\n\\dot{\\bar{x}}(t) = A(t)x(t) + b(t)\n$$\nwith $x(t_{0}) = x_{0}$.\nThe set of all solutions is given by $\\{ c_{1}\\varphi_{1} + c_{2}\\varphi_{2} + \\dots + c_{n} \\varphi_{n} + \\varphi_{b}\\mid c_{1},c_{2},\\dots c_{n} \\in \\mathbb{R} \\}$, where $\\varphi_{1}, \\varphi_{2},\\dots \\varphi_{n}$ is a basis for $\\ker T_{A}$ and $\\varphi_{b}$ is one specific solution to the given IVP.\n\nFor the definition of $T_A$, look at [Homogenous Linear Systems](&lt;./Homogenous Linear Systems.md&gt;).\n</code></pre>"},{"location":"Inhomogenous%20Linear%20Systems/#how-do-we-find-a-specific-solution-varphi_b-to-this-ivp-given-a-basis-for-ker-t_a","title":"How do we find a specific solution (\\(\\varphi_{b}\\)) to this IVP given a basis for \\(\\ker T_{A}\\)?","text":"<pre><code>title: Idea\nAllow coefficients of $\\varphi_1, \\varphi_2, \\dots, \\varphi_n$ to be functions $c_1(t),c_2(t),\\dots, c_n(t)$, i.e., $$\\varphi := c_1(t)\\varphi_1(t) + \\dots + c_n(t)\\varphi_n(t)$$\nwhere $c_i : I \\to \\mathbb{R}$.\n</code></pre> <p>This gives $$ \\begin{align} \\dot{\\varphi} &amp;= (c_{1}(t)\\dot{\\varphi_{1}}(t) + c_{2}(t)\\dot{\\varphi_{2}}(t) + \\dots + c_{n}(t)\\dot{\\varphi_{n}}(t)) + (\\dot{c_{1}}(t)\\varphi_{1}(t) + \\dot{c_{2}}(t)\\varphi_{2}(t) + \\dots + \\dot{c_{n}}(t)\\varphi_{n}(t)) \\ &amp;= (c_{1}(t) A(t)\\varphi_{1}(t) + c_{2}(t) A(t) \\varphi_{2}(t) + \\dots + c_{n}(t) A(t) \\varphi_{n}(t)) + (\\dot{c_{1}}\\varphi_{1} + \\dots + \\dot{c_{n}}\\varphi_{n})\\ &amp;= A(t)\\varphi(t) + (\\dot{c_{1}}\\varphi_{1} + \\dots + \\dot{c_{n}}\\varphi_{n}) \\end{align}  $$ Therefore it suffices to find \\(c_{1},\\dots ,c_{n}\\) satisfying \\(\\dot{c_{1}}\\varphi_{1} + \\dots + \\dot{c_{n}}\\varphi_{n} = b\\). Writing this as a matrix equation, we get: $$ \\begin{bmatrix} \\varphi_{1}  \\varphi_{2}  \\dots  \\varphi_{n} \\end{bmatrix}{n\\times n} \\begin{bmatrix} \\dot{c{1}} \\  \\dot{c_{2}}\\ \\vdots \\ \\dot{c_{n}} \\end{bmatrix}{n\\times 1}  =   \\begin{bmatrix} b(t) \\end{bmatrix}{n\\times_{1}} $$ Thus  $$ \\begin{bmatrix} \\dot{c_{1}} \\ \\dot{c_{2}}  \\ \\vdots \\ \\dot{c_{n}} \\end{bmatrix}  =   \\begin{bmatrix} \\varphi_{1} &amp; \\varphi_{2} &amp; \\dots &amp; \\varphi_{n} \\end{bmatrix}^{-1} \\begin{bmatrix} b(t) \\end{bmatrix} $$ Hence $$ \\begin{bmatrix} c_{1} \\ c_{2} \\ \\vdots  \\ c_{n} \\end{bmatrix}  = \\int \\begin{bmatrix} \\varphi_{1} &amp; \\varphi_{2} &amp; \\dots &amp; \\varphi_{n} \\end{bmatrix}^{-1}  \\begin{bmatrix} b(t) \\end{bmatrix}\\, dt  $$ So we are done.</p>"},{"location":"Inhomogenous%20Linear%20Systems/#related-problems","title":"Related Problems","text":"<ol> <li>How to find the \\(\\varphi_{1}, \\varphi_{2},\\dots,\\varphi_{n}\\)? We do this in Linear systems with constant coefficients, for the special case of linear systems with constant coefficients.</li> </ol>"},{"location":"Inhomogenous%20Linear%20Systems/#references","title":"References","text":"<p>Homogenous Linear Systems</p>"},{"location":"Integers%20in%20Simply%20Typed%20Lambda%20Calculus/","title":"Integers in Simply Typed Lambda Calculus","text":"<p>202309071009</p> <p>Type : #Note Tags : [[Lambda Calculus]], [[Type Theory]]</p>"},{"location":"Integers%20in%20Simply%20Typed%20Lambda%20Calculus/#integers-in-simply-typed-lambda-calculus","title":"Integers in Simply Typed Lambda Calculus","text":"<p>To Give Church Numerals a type in simply typed lambda calculus. We try to figure out the types</p> <p>$$ \\begin{align} \\lambda fx.x &amp;: a\\to b\\to b\\ \\lambda fx.fx &amp;: (p\\to q)\\to p\\to q\\ \\lambda fx.f(fx) &amp;: (p\\to p) \\to p \\to p\\ \\vdots \\end{align} $$ So we can define the type Int to be \\((p\\to p)\\to p\\to p\\) Hence we can represent the functions \\(f:\\mathbb N^{k}\\to\\mathbb N\\) in terms of lambda terms. $$ F c_{n_{1}}\\dots c_{n_{k}} =c_{f(n_{1},\\dots n_{k})} $$ The class of Extended Polynomials is the smallest class of functions over \\(\\mathbb N\\) which is closed under composition, contains constant functions \\(0\\) and \\(1\\), projections, addition, multiplication and conditional statements</p> \\[ \\text{Cond}(n_{1}, n_{2}, n_{3}) =  \\left\\{ \\begin{align*} n_{2} &amp;&amp; \\text{if }n=0\\\\ n_{3} &amp;&amp; \\text{otherwise} \\end{align*}\\right. \\] <p>Theorem(Schwichtenberg): The \\(\\lambda_{\\to}\\)-definable terms are exactly the extended polynomials.</p>"},{"location":"Integers%20in%20Simply%20Typed%20Lambda%20Calculus/#references","title":"References","text":"<p>Expressive Power of G\u00f6del's system T Functions Computable in Lambda Calculus</p>"},{"location":"Integral%20Curves/","title":"Integral Curves","text":"<p>202301210401</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"Integral%20Curves/#integral-curves","title":"Integral Curves","text":"<p>Consider A vector field assigned to a Phase Space. Given a point in the phase space, we can make a curve that passes through a point, and is tangent to every single vector on the points it passes through. Such a curve is called the Integral curve of the direction field.</p> <p>The name is motivated by the fact that in certain cases they can be found by the process of integration. { width=\"500\" } The problem of finding an integral curve is precisely the problem of integration given a continuous function.</p> <p>In general the problem of finding integral curve does not reduce to the process of integration. Even for very simply defined direction fields, the equation may not be possible to represent with finitely many integrals and elementary functions.</p>"},{"location":"Integral%20Curves/#related-problems","title":"Related Problems","text":""},{"location":"Integral%20Curves/#references","title":"References","text":"<p>Phase Space</p>"},{"location":"Integrality%20Gap/","title":"Integrality Gap","text":"<p>202310062210</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note","Incomplete"]},{"location":"Integrality%20Gap/#integrality-gap","title":"Integrality Gap","text":"<p>\\(OPT(LP)\\le OPT(ILP)\\le\\) our solution</p> <p>Integrality gap \\(=\\max_\\limits{\\text{all instances}}\\frac{OPT(ILP)}{OPT(LP)}\\) integrality gap \\(\\le\\) approximation ratio</p>","tags":["Note","Incomplete"]},{"location":"Integrality%20Gap/#references","title":"References","text":"<p>Linear Programming (Weighted) Vertex Cover using LP</p>","tags":["Note","Incomplete"]},{"location":"Intermediate%20Value%20Theorem/","title":"Intermediate Value Theorem","text":"<p>202302101302</p> <p>Type : #Note Tags : [[Analysis]]</p>"},{"location":"Intermediate%20Value%20Theorem/#intermediate-value-theorem","title":"Intermediate Value Theorem","text":"<pre><code>title:\nFor any continuous function $f$ on an interval $[a,b] \\subset \\mathbb{R}$, and any $r$ such that $f(a) &lt; r &lt; f(b)$, then there exists $c \\in [a,b]$ such that $f(c) = r$.\n</code></pre>"},{"location":"Intermediate%20Value%20Theorem/#note-this-generalises-to-connected-topological-spaces-see-references","title":"Note: this generalises to connected topological spaces. (See references)","text":""},{"location":"Intermediate%20Value%20Theorem/#related-problems","title":"Related Problems","text":""},{"location":"Intermediate%20Value%20Theorem/#references","title":"References","text":"<p>Connectedness</p>"},{"location":"Intuitionistic%20Logic/","title":"Intuitionistic Logic","text":"<p>202308141408</p> <p>Type : #Note Tags : [[Logic]], [[Type Theory]]</p>"},{"location":"Intuitionistic%20Logic/#intuitionistic-logic","title":"Intuitionistic Logic","text":"<pre><code>title: Suresh\n$2+5 = 3\\times6$\n</code></pre> <p>The logic interpretation was first developed as BHK - Interpretation which stands for Brouwer, Heyting and Kolmogorov which probably independently made contributions to it.</p> <p><pre><code>title: syntax\nThe Main syntax of the language of intuitionistic logic is\n$$\n\\begin{matrix}\\perp &amp; | &amp; (\\phi\\to\\psi) &amp; | &amp; (\\phi\\land\\psi) &amp; | &amp; (\\phi\\lor\\psi)\\end{matrix}\n$$\nalong with short hand notations\n$$\n\\begin{align*}\n\\lnot \\phi &amp;= (\\phi\\to\\perp)\\\\\n\\phi\\leftrightarrow\\psi &amp;= (\\phi\\to\\psi)\\land(\\psi\\to\\phi)\\\\\n\\top &amp;= \\perp\\to\\perp\\\\\n\\end{align*}\n$$\n</code></pre> Intuitionistic Logic was made for Constructivist Mathematics, so instead of basing our logical statements on Truth, we base them on Constructions. </p> <ul> <li>A construction of \\(\\varphi_{1}\\land\\varphi_{2}\\) consists of a construction of \\(\\varphi_{1}\\) and a construction of \\(\\varphi_{2}\\).</li> <li>A construction of \\(\\varphi_{1}\\lor\\varphi_{2}\\) consists of a an indicator \\(i\\in\\{1,2\\}\\) and a construction of \\(\\varphi_{i}\\) </li> <li>A construction of \\(\\varphi_1\\to\\varphi_2\\) is a function that transforms every construction of \\(\\varphi_1\\) to a construction of \\(\\varphi_2\\) </li> <li>There is no construction for \\(\\perp\\) </li> </ul> <p>The language of Intuitionistic Logic was later simplified in a language called Natural Deduction</p> <p>Where as Heyting Algebra and Kripke Models are some of the ways to proved semantics to Intuitionistic Logic.</p>"},{"location":"Intuitionistic%20Logic/#references","title":"References","text":"<p>Algebraic Semantics of Logic: Lattice</p>"},{"location":"Irrefutable%20let%28rec%29/","title":"Irrefutable let(rec)","text":"<p>202311071911</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note","Incomplete"]},{"location":"Irrefutable%20let%28rec%29/#irrefutable-letrec","title":"Irrefutable let(rec)","text":"<p>Irrefutable let(rec) expressions are expressions that cannot return a \\(\\text{FAIL}\\) when evaluated. Sum Patterns and Constant Patterns are the only cases where \\(\\text{FAIL}\\) can be produced, hence we define Irrefutable let(rec)s as</p> <p>[!note] Definition A pattern \\(p\\) is said to be irrefutable if it is  1. either a variable \\(v\\) 2. or a product pattern \\((t p_1, \\dots p_r)\\) where each \\(p_i\\) is irrefutable</p>","tags":["Note","Incomplete"]},{"location":"Irrefutable%20let%28rec%29/#references","title":"References","text":"<p>Conformality Transformation on let(rec) expressions</p>","tags":["Note","Incomplete"]},{"location":"Irrefutable%20let%28rec%29s%20to%20Simple%20let%28rec%29s/","title":"Irrefutable let(rec)s to Simple let(rec)s","text":"<p>202311071911</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note","Incomplete"]},{"location":"Irrefutable%20let%28rec%29s%20to%20Simple%20let%28rec%29s/#irrefutable-letrecs-to-simple-letrecs","title":"Irrefutable let(rec)s to Simple let(rec)s","text":"<p>The process of converting Irrefutable lets to simple letrecs is almost identical to the process of converting irrefutable lets to simple lets</p> <pre><code>letrec (t p1 ... pr) = B\n       &lt; Other Definitions &gt;\nin E\n===\nletrec v = B\n       p1 = SEL-t-1 v\n       ...\n       pr = SEL-t-r v\n       &lt; Other Definitions&gt;\nin E\n</code></pre> <p>where <code>v</code> is a new variable.</p>","tags":["Note","Incomplete"]},{"location":"Irrefutable%20let%28rec%29s%20to%20Simple%20let%28rec%29s/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Isomorphism%20Between%20First%20Order%20Interpretations/","title":"Isomorphism Between First Order Interpretations","text":"<p>202309221209</p> <p>Tags : [[Logic]]</p>","tags":["Note"]},{"location":"Isomorphism%20Between%20First%20Order%20Interpretations/#isomorphism-between-first-order-interpretations","title":"Isomorphism Between First Order Interpretations","text":"<pre><code>title: Motivation\nLet $R=\\{&lt;\\}, F=\\emptyset$\nConsider two structures, one where $S=\\mathbb{Q}\\cap(0,1)$ and one where $S=\\mathbb{Q}\\cap(0,\\infty)$\nThen we cannot have any **First Order Formula** which is valid for one of the model and not for the other one, hence that is not strong enough to differentiate between the two models.\n\nHere we show that the two models are isomorphic.\n</code></pre> <p>Let \\(\\mathcal I_{1} = \\{\\mathcal{M}_{1},\\sigma_{1}\\}\\) and \\(\\mathcal I_{2} = \\{\\mathcal M_{2},\\sigma_{2}\\}\\) Then we say that the above two interpretations isomorphic if there exists a bijection \\(\\tau:S_{1}\\to S_{2}\\) such that: - For every \\(r\\in R\\) we have \\((e_{1},e_{2}\\dots e_{n})\\in r^{\\mathcal M_{1}}\\) iff \\((\\tau(e_{1}),\\tau(e_{2})\\dots \\tau(e_{n}))\\in r^{\\mathcal M_{2}}\\)  - For every \\(f\\in F\\) we have \\(\\tau(f^{\\mathcal M_{1}}(e_{1},e_{2}\\dots e_n))=f^{\\mathcal M_{2}}(\\tau(e_{1}),\\tau(e_{2})\\dots\\tau(e_{n})))\\) - For every \\(c\\in C\\) we have \\(\\tau(c^{\\mathcal M_{1}})=c^{\\mathcal M_{2}}\\) - For every \\(x\\in Var\\) we have \\(\\tau(\\sigma_{1}(x))=\\sigma_{2}(x)\\)</p> <pre><code>title:Lemma\nIf $\\mathcal I_{1},\\mathcal I_{2}$ are isomorphic, then for every formula $\\varphi$, $\\mathcal I_{1}\\models\\varphi$ iff $I_{2}\\models\\varphi$.\n\n*Proof:* Induction on structure of $\\varphi$\n</code></pre>","tags":["Note"]},{"location":"Isomorphism%20Between%20First%20Order%20Interpretations/#references","title":"References","text":"<p>Syntax of First Order Logic Semantics of First Order Logic</p>","tags":["Note"]},{"location":"Jacobi%20Sums/","title":"Jacobi Sums","text":"<p>202305261905</p> <p>Type : #Note Tags : [[Number Theory]]</p>"},{"location":"Jacobi%20Sums/#jacobi-sums","title":"Jacobi Sums","text":"<pre><code>title: Idea\nConsider the equation $x^{2}+y^{2} = 1$ in the field $\\mathbb{F}_{p}$. Since $\\mathbb{F}_{p}$ is finite, the equation only has finitely many solutions.\nLet $N(x^{2}+y^{2} = 1)$ be the number of solutions.\nThen,\n$$\n\\begin{align}\nN(x^{2}+y^{2} = 1) = \\sum_{a+b=1}N(x^{2} =a)N(y^{2}=b)\n\\end{align}\n$$\nwhere $a,b \\in \\mathbb{F}_{p}$.\nSince $N(x^{2} = a) = 1 + \\left( \\frac{a}{p} \\right)$ (refer to [Multiplicative Characters](&lt;./Multiplicative Characters.md&gt;)), substituting we get:\n$$\n\\begin{align}\nN(x^{2}+y^{2}=1) &amp;= \\sum_{a+b=1}1 + \\left( \\frac{a}{p} \\right) +\\left( \\frac{b}{p} \\right) +\\left( \\frac{ab}{p} \\right) \\\\\n&amp;= p + \\sum_{a}\\left( \\frac{a}{p} \\right) +\\sum_{b}\\left( \\frac{b}{p} \\right) +\\sum_{a+b=1}\\left( \\frac{ab}{p} \\right)  \\\\\n&amp;= p + \\sum_{a+b=1}\\left( \\frac{a}{p} \\right) \\left( \\frac{b}{p} \\right) \n\\end{align}\n$$\n\nAnother example,\n$$\n\\begin{align}\nN(x^{3}+y^{3}=1) &amp;= \\sum_{a+b=1}N(x^{3}=a)N(y^{3}=b)  \\\\\n&amp;= \\sum_{a+b=1}\\left(\\sum_{i=0}^{2}\\chi^{i}(a)\\right)\\left( \\sum_{j=0}^{2} \\chi^{j}(b) \\right) \\\\\n&amp;= \\sum_{a+b=1}\\left( \\sum_{i,j} \\chi^{i}(a)\\chi^{j}(b) \\right)  \\\\\n&amp;= \\sum_{i,j}\\left( \\sum_{a+b=1}\\chi^{i}(a)\\chi^{j}(b) \\right)   \n\\end{align}\n$$\nwhere $\\chi$ is a character of order 3. \n\nThis means that understanding sums of the form $\\sum_{a+b=1}\\chi(a)\\lambda(b)$ where $\\chi$ and $\\lambda$ are characters, will help us understand the number of solutions of equations in $\\mathbb{F}_{p}$.\n</code></pre> <pre><code>title:\nLet $\\chi$ and $\\lambda$ be characters on $\\mathbb{F}_{p}$ and set $J(\\chi,\\gamma) = \\sum_{a+b=1}\\chi(a)\\lambda(b)$. \n\n$J(\\chi,\\lambda)$ is called a **Jacobi sum**.\n</code></pre>"},{"location":"Jacobi%20Sums/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nLet $\\chi$ and $\\lambda$ be non trivial characters. Then\n1. $J(\\varepsilon,\\varepsilon) = p$\n2. $J(\\chi,\\varepsilon) = 0$\n3. $J(\\chi,\\chi^{-1}) = -\\chi(-1)$\n4. If $\\chi\\lambda \\neq \\varepsilon$, then $$\nJ(\\chi,\\lambda) = \\frac{g(\\chi)g(\\lambda)}{g(\\chi\\lambda)}\n$$\n</code></pre>"},{"location":"Jacobi%20Sums/#proof","title":"Proof:","text":"<ol> <li>$$ \\begin{align} J(\\chi,\\chi^{-1}) &amp;= \\sum_{a+b=1}\\chi(a)\\chi^{-1}(b) \\ &amp;= \\sum_{a+b=1} \\chi\\left( \\frac{a}{b} \\right) \\ &amp;= \\sum_{a\\neq 1} \\chi \\left( \\frac{a}{1-a} \\right) \\ &amp;= \\sum_{c \\neq -1} \\chi(c) \\ &amp;= 0-\\chi(-1) \\end{align} $$ We have used the fact that the function \\(\\phi : \\mathbb{F}_{p} \\setminus \\{ 1 \\} \\to \\mathbb{F}_{p} \\setminus \\{ -1 \\}\\) taking \\(a\\) to \\(\\frac{a}{1-a}\\) is bijective.</li> <li>$$ \\begin{align} g(\\chi)g(\\lambda) &amp;= \\left( \\sum_{t} \\chi(t)\\zeta^{t} \\right) \\left( \\sum_{s}\\lambda(s)\\zeta^{s} \\right) \\ &amp;= \\sum_{t,s}\\chi(t)\\lambda(s)\\zeta^{s+t} \\ &amp;= \\sum_{t+s=0}\\chi(t)\\lambda(s)+\\sum_{a\\neq 0}\\sum_{t+s=a}\\chi(t)\\lambda(s)\\zeta^{a} \\ &amp;=\\sum_{t}\\chi(t)\\lambda(-t) + \\sum_{a \\neq 0} \\zeta^{a} \\left( \\sum_{t'+s' = 1} \\chi(at')\\lambda(as') \\right) \\ &amp;= \\lambda(-1)\\sum_{t}\\chi\\lambda(t) + \\sum_{a \\neq 0} \\zeta^{a} \\chi\\lambda(a) \\left( \\sum_{t'+s'=1}\\chi(t')\\lambda(s') \\right)  \\ &amp;= 0 + J(\\chi,\\lambda)g(\\chi\\lambda) \\end{align} $$ In the last equality we use the fact that \\(\\chi \\lambda\\) is non trivial.</li> </ol>"},{"location":"Jacobi%20Sums/#corollary","title":"Corollary:","text":"<pre><code>title:\nIf $\\chi,\\lambda,\\chi\\lambda \\neq \\varepsilon$ then $|J(\\chi,\\lambda)| = \\sqrt[]{ p }$\n</code></pre>"},{"location":"Jacobi%20Sums/#note","title":"NOTE:","text":"<ul> <li>Observe that \\(\\sum_{a+b=1}\\left( \\frac{a}{p} \\right)\\left( \\frac{b}{p} \\right) = J(\\chi,\\chi^{-1})\\) where \\(\\chi\\) is the Legendre symbol (character of order 2). And so the value of the sum is \\(-\\chi(-1) = -(-1)^{(p-1)/2}\\). This gives a precise value of the number of solutions of the concerned equation.</li> <li>Similarly, \\(\\(N(x^{3}+y^{3} = 1) = \\sum_{i,j}\\left( \\sum_{a+b=1}\\chi^{i}(a)\\chi^{j}(b) \\right) =p + J(\\chi,\\chi) + 2J(\\chi,\\chi^{2}) + J(\\chi^{2},\\chi^{2}) = p - 2\\chi(-1) + J(\\chi,\\chi) + J(\\chi^{2},\\chi^{2})\\)\\) Since \\(\\chi(-1) = \\chi((-1)^{3}) = \\chi(-1)^{3} = \\varepsilon(-1) = 1\\), we get $$ N(x<sup>{3}+y</sup>{3}=1) = p-2 + 2 \\mathrm{Re}(J(\\chi,\\chi)) $$ Giving us \\(|N(x^{3}+y^{3}=1) - p+ 2| \\le 2 \\sqrt[]{ p }\\) Which gives us asymptotic information about the number of solutions.</li> </ul>"},{"location":"Jacobi%20Sums/#proposition-1","title":"Proposition 1:","text":"<pre><code>title:\nIf $p \\equiv 1 (\\mathrm{mod} \\ 4)$ then there exists integers $a,b$ such that $p = a^{2}+b^{2}$.\nIf $p \\equiv 1(\\mathrm{mod} \\ 3)$ then there exists integers $a,b$ such that $p = a^{2}+b^{2}-ab$.\n</code></pre>"},{"location":"Jacobi%20Sums/#proof_1","title":"Proof:","text":"<p>If \\(p\\equiv 1(\\mathrm{mo d} \\ 4)\\), there is a character \\(\\chi\\) of order \\(4\\) on \\(\\mathbb{F}_{p}\\), since the order of the group of characters is \\(p-1\\).  The values this character takes belongs to \\(\\{ \\pm {1},\\pm i \\}\\). Hence, \\(J(\\chi,\\chi) = \\sum_{a+b=1}\\chi(a)\\chi(b) \\in \\mathbb{Z}[i]\\).  Noting that \\(|J(\\chi,\\chi)|^{2} =p\\), we get that \\(p = (x+yi)(x-yi)\\) since \\(J(\\chi,\\chi) = x+yi; \\ x,y \\in \\mathbb{Z}\\). Thus, \\(p = x^{2}+y^{2}\\).</p> <p>If \\(p \\equiv 1(\\mathrm{mo d} \\ 3)\\), there is a character \\(\\chi\\) of order 3 on \\(\\mathbb{F}_{p}\\). The values this character takes belongs to \\(\\{ {1},\\omega,\\omega^{2}\\}\\). Hence, \\(J(\\chi,\\chi) = \\sum_{a+b=1}\\chi(a)\\chi(b) \\in \\mathbb{Z}[\\omega]\\).  Noting that \\(|J(\\chi,\\chi)|^{2} =p\\), we get that \\(p = (x+y \\omega)(x+y \\omega^{2})\\) since \\(J(\\chi,\\chi) = x+y \\omega; \\ x,y \\in \\mathbb{Z}\\). Thus, \\(p = x^{2}+y^{2} -xy\\).</p>"},{"location":"Jacobi%20Sums/#note_1","title":"Note:","text":"<ol> <li>We can show that \\(p = a^{2}+b^{2}\\) is a unique representation given that \\(a\\) is odd and \\(b\\) is even, and \\(a,b &gt; 0\\). (Just use unique factorisation in \\(\\mathbb{Z}[i]\\)).</li> <li>The representation \\(p = a^{2}+b^{2}-ab\\) is not unique even if we assume that \\(a,b &gt; 0\\).    $$ a<sup>{2}+b</sup>{2}-ab = (b-a)<sup>{2}+b</sup>{2}-(b-a)b = (a-b)<sup>{2}+a</sup>{2}-(a-b)a $$</li> <li>But we can reformulate the result so that it is unique.     If \\(p=a^{2}+b^{2}-ab\\) then \\(4p = (2a-b)^{2}+3b^{2} = (2b-a)^{2}+3a^{2} = (a+b)^{2} + 3(a-b)^{2}\\).    We claim that \\(3\\) divides either \\(b,a\\) or \\((a-b)\\). Suppose \\(3 \\nmid a,b\\) and \\(a \\equiv 1 (\\mathrm{mo d}\\ 3)\\) and \\(b \\equiv 2(\\mathrm{mo d}\\ 3)\\) then     \\(p \\equiv 0 (\\mathrm{mo d}\\ 3)\\) that's a contradiction.    Similarly \\(a \\equiv 2(\\mathrm{mo d}\\ 3)\\) and \\(b \\equiv 1(\\mathrm{mo d}\\ 3)\\) gives the same contradiction.</li> </ol> <p>This gives the following proposition:</p>"},{"location":"Jacobi%20Sums/#proposition-2","title":"Proposition 2:","text":"<pre><code>title:\nIf $p \\equiv 1(\\mathrm{ mo d}\\ 3)$ then there are integers $A,B$ such that $4p = A^{2}+27B^{2}$. In this representation of $4p$, $A,B$ are uniquely determined up to sign.\n</code></pre>"},{"location":"Jacobi%20Sums/#proof_2","title":"Proof:","text":"<p>Since \\(A\\) is not divisible by \\(3\\), \\(A \\equiv 1,2(\\mathrm{mo d}\\ 3)\\). But since we can replace \\(A\\) by \\(-A\\), WLOG we can assume that \\(A \\equiv 1 (\\mathrm{mo d}\\ 3)\\). So we show that under the assumption \\(A \\equiv 1(\\mathrm{mo d} \\ 3)\\), the representation is unique. $$ \\begin{align} 4p = (A + 3 \\sqrt[]{ 3 }iB)(A-3 \\sqrt[]{ 3 }iB) &amp;= ((A+3B) + 6B \\omega)(A-3B - 6B \\omega)  \\ \\implies p &amp;= \\left( \\frac{A+3B}{2} + 3B \\omega \\right) \\left( \\frac{A-3B}{2}-3B \\omega \\right)  \\ \\implies p &amp;= \\pi \\overline{\\pi}  \\end{align} $$ where \\(\\pi = \\frac{A+3B}{2}+3B \\omega\\). \\(\\pi\\) is a prime since its norm is a prime in \\(\\mathbb{Z}\\).</p> <p>Now if \\(4p = C^{2}+27D^{2}\\), with \\(C \\equiv 1(\\mathrm{mo d}\\ 3)\\), then $$ p = \\left( \\frac{A+3B}{2} + 3B \\omega \\right) \\left( \\frac{A-3B}{2}-3B \\omega \\right) = \\left( \\frac{C+3D}{2}+3D \\omega \\right) \\left( \\frac{C-3D}{2}-3D \\omega \\right) $$ Since \\(\\mathbb{Z}[\\omega]\\) is a UFD, \\(\\left( \\frac{A+3B}{2} +3B \\omega\\right)\\) and \\(\\left( \\frac{C + 3D}{2} + 3D \\omega \\right)\\) are associates WLOG, since we can replace \\(D\\) by \\(-D\\) if necessary. Therefore one of the following holds: $$ \\begin{align} \\left( \\frac{A+3B}{2} +3B \\omega\\right) &amp;= \\pm \\left( \\frac{C + 3D}{2} + 3D \\omega\\right)  \\ &amp;= \\pm \\omega \\left( \\frac{C + 3D}{2} + 3D \\omega\\right)  \\ &amp;= \\pm \\omega^{2} \\left( \\frac{C + 3D}{2} + 3D \\omega\\right)  \\end{align} $$ By exhausting each case, we see that $$ \\left( \\frac{A+3B}{2}+3B \\omega \\right)  = \\left( \\frac{C+3D}{2} + 3D \\omega \\right) $$ is the only possibility. Hence, \\(A = C\\) as desired.</p>"},{"location":"Jacobi%20Sums/#proposition-3","title":"Proposition 3:","text":"<pre><code>title:\nSuppose that $p \\equiv 1(\\mathrm{mo d}\\ n)$ and that the character $\\chi$ is of order $n &gt; 2$. Then $$\ng(\\chi)^{n} = p\\chi(-1)J(\\chi,\\chi)J(\\chi,\\chi^{2})\\dots J(\\chi,\\chi^{n-2})\n$$\n</code></pre>"},{"location":"Jacobi%20Sums/#proof_3","title":"Proof:","text":"<p>We know that \\(g(\\chi)^{2} = g(\\chi^{2})J(\\chi,\\chi)\\) (by part (4) of theorem 1). \\(g(\\chi)^{3} = J(\\chi,\\chi)g(\\chi^{2})g(\\chi) = J(\\chi,\\chi)J(\\chi,\\chi^{2})g(\\chi^{3})\\) Proceeding this way, \\(g(\\chi) ^{n-1} = J(\\chi,\\chi)\\dots J(\\chi,\\chi^{n-2})g(\\chi^{n-1})\\) \\(g(\\chi)^{n} = J(\\chi,\\chi)\\dots J(\\chi,\\chi^{n-2}) g(\\chi^{-1})g(\\chi)\\) But \\(g(\\chi^{-1}) = g(\\overline{\\chi}) = \\overline{g(\\chi)}\\chi(-1)\\).</p> <p>Hence, \\(g(\\chi)^{n} = J(\\chi,\\chi)\\dots J(\\chi,\\chi^{n-2})|g(\\chi)|^{2}\\chi(-1)\\) \\(\\implies g(\\chi)^{n} = p\\chi(-1)J(\\chi,\\chi)\\dots J(\\chi,\\chi^{p-2})\\).</p>"},{"location":"Jacobi%20Sums/#corollary_1","title":"Corollary:","text":"<pre><code>title:\nIf $\\chi$ is a cubic character, then $$\ng(\\chi)^{3} = pJ(\\chi,\\chi)\n$$\n</code></pre>"},{"location":"Jacobi%20Sums/#note_2","title":"Note:","text":"<p>Looking at \\(N(x^{3}+y^{3}=1)\\) again, we have seen that \\(J(\\chi,\\chi) = a+b \\omega\\) where \\(\\chi\\) is a character of order 3.</p>"},{"location":"Jacobi%20Sums/#proposition-4","title":"Proposition 4:","text":"<pre><code>title:\nSuppose $p \\equiv 1(\\mathrm{mo d}\\ 3)$ and that $\\chi$ is a cubic character. Set $J(\\chi,\\chi) = a+b \\omega$. Then \n\na) $b\\equiv 0(\\mathrm{mo d}\\ 3)$\n\nb) $a \\equiv 2(\\mathrm{mo d}\\ 3)$\n</code></pre>"},{"location":"Jacobi%20Sums/#proof_4","title":"Proof:","text":"<p>$$ \\begin{align} g(\\chi)^{3} = \\left(\\sum_{t}\\chi(t)\\zeta<sup>{t}\\right)</sup>{3} &amp;\\equiv \\sum_{t}\\chi(t)<sup>{3}\\zeta</sup>{3t}  (3)  \\ &amp;\\equiv  \\sum_{t \\neq 0}\\zeta^{3t}  (3) \\ &amp;\\equiv -1  (3) \\end{align} $$ This gives \\(pJ(\\chi,\\chi) \\equiv J(\\chi,\\chi) \\equiv 2 \\ (3)\\). This gives the result.</p>"},{"location":"Jacobi%20Sums/#corollary_2","title":"Corollary:","text":"<pre><code>title:\nLet $A = 2a-b$ and $B = \\frac{b}{3}$. Then $A\\equiv 1 \\ (3)$ and $4p =A^{2}+27B^{2}$.\n</code></pre>"},{"location":"Jacobi%20Sums/#proof_5","title":"Proof:","text":"<p>Since \\(J(\\chi,\\chi) = a+b \\omega\\) and \\(|J(\\chi,\\chi)|^{2} = p\\), we have \\(p = a^{2}+b^{2}-ab\\). Thus \\(4p = (2a-b)^{2}+3b^{2}\\) and \\(4p = A^{2}+27B^{2}\\).</p> <p>By the previous proposition, \\(3 \\mid b\\) and \\(a \\equiv 2 \\ (3)\\), hence \\(A \\equiv 1 \\ (3)\\).</p>"},{"location":"Jacobi%20Sums/#theorem-2","title":"Theorem 2:","text":"<pre><code>title:\nSuppose that $p \\equiv 1 \\ (3)$. Then there are integers $A,B$ such that $4p = A^{2}+27B^{2}$. If we require that $A \\equiv 1 \\ (3)$ then $A$ is uniquely determined and $$\nN(x^{3}+y^{3}=1) = p-2 + A\n$$\n</code></pre>"},{"location":"Jacobi%20Sums/#proof_6","title":"Proof:","text":"<p>We have shown that \\(N(x^{3}+y^{3}=1) = p-2 + 2 \\mathrm{Re}(J(\\chi,\\chi))\\). Since \\(J(\\chi,\\chi) = a+b \\omega\\), \\(\\mathrm{Re}(J(\\chi,\\chi)) = \\mathrm{Re}\\left( a + b\\left( \\frac{-1 + \\sqrt[]{ 3 }}{2}\\right) \\right) = \\frac{2a-b}{2}\\). This gives the required formula since \\(A = 2a-b\\). We have also shown uniqueness before.</p>"},{"location":"Jacobi%20Sums/#generalised-jacobi-sums","title":"Generalised Jacobi Sums","text":""},{"location":"Jacobi%20Sums/#definition","title":"Definition:","text":"<pre><code>title:\nLet $\\chi_{1},\\chi_{2},\\dots \\chi_{l}$ be characters on $\\mathbb{F}_{p}$. A Jacobi sum is defined by $$\nJ(\\chi_{1},\\chi_{2},\\dots \\chi_{l}) = \\sum_{t_{1}+\\dots+t_{l} = 1}\\chi_{1}(t_{1})\\chi_{2}(t_{2})\\dots \\chi_{l}(t_{l})\n$$\n</code></pre>"},{"location":"Jacobi%20Sums/#definition_1","title":"Definition:","text":"<pre><code>title:\n$J_{0}(\\chi_{1},\\dots \\chi_{l}) = \\sum_{t_{1}+\\dots+t_{l}=0}\\chi_{1}(t_{1})\\dots \\chi_{l}(t_{l})$.\n</code></pre>"},{"location":"Jacobi%20Sums/#proposition-5","title":"Proposition 5:","text":"<pre><code>title:\n1. $J_{0}(\\varepsilon,\\varepsilon,\\dots,\\varepsilon) = J(\\varepsilon,\\varepsilon, \\dots,\\varepsilon) = p ^{l-1}$\n2. If some but not all of the $\\chi_{i}'s$ are trivial, then $J_{0} = J = 0$.\n3. Assume that $\\chi_{l} \\neq \\varepsilon$. Then $$\nJ_{0}(\\chi_{1},\\dots \\chi_{l}) = \\begin{cases}\n0, \\ \\ &amp;\\text{if } \\chi_{1}\\chi_{2}\\dots \\chi_{l} \\neq \\varepsilon  \\\\\n\\chi_{l}(-1)(p-1)J(\\chi_{1},\\dots \\chi_{l-1}), \\ &amp;\\text{otherwise}\n\\end{cases}\n$$\n</code></pre>"},{"location":"Jacobi%20Sums/#proof_7","title":"Proof:","text":"<ol> <li> \\[ \\begin{align} J_{0}(\\varepsilon,\\varepsilon, \\dots \\varepsilon) = \\sum_{t_{1}+t_{2}+\\dots+t_{l}  = 0} 1 = p ^{l-1} = J(\\varepsilon, \\varepsilon \\dots \\varepsilon) \\end{align} \\] </li> <li> <p>$$ \\begin{align} J_{0}(\\chi_{1},\\dots,\\chi_{l-1},\\varepsilon) &amp;= \\sum_{t_{1}+\\dots+t_{l-1}+ t_{l} = 0} \\chi_{1}(t_{1})\\dots \\chi_{l-1}(t_{l-1}) \\ &amp;= \\sum_{t_{1},t_{2},\\dots t_{l-1}} \\chi_{1}(t_{1})\\chi_{2}(t_{2})\\dots \\chi_{l-1}(t_{l-1}) \\ &amp;= \\prod\\left( \\sum_{t_{i}}\\chi_{i}(t_{i}) \\right) = 0 \\end{align} $$ Since at least one of the terms in the product is 0.</p> </li> <li> <p>$$ \\begin{align} J_{0}(\\chi_{1},\\chi_{2},\\dots \\chi_{l}) &amp;= \\sum_{t_{1}+t_{2}+\\dots+t_{l}=0} \\chi_{1}(t_{1})\\chi_{2}(t_{2})\\dots \\chi_{l}(t_{l}) \\ &amp;= \\sum_{t_{l}\\neq 0}\\sum_{t_{1}+t_{2}+\\dots+t_{l-1} = -t_{l}} \\chi_{1}(t_{1})\\dots \\chi_{l-1}(t_{l-1})\\chi_{l}(t_{l})  \\ &amp;= \\sum_{t_{l}\\neq 0} \\sum_{t_{1}'+\\dots+t_{l-1}' = 1} \\chi_{1}(-t_{l}t_{1}')\\dots \\chi_{l-1}(-t_{l}t_{l-1}')\\chi_{l}(t_{l})  \\ &amp;= \\sum_{t_{l}\\neq 0}(\\chi_{1}\\chi_{2}\\dots \\chi_{l})(-t_{l})\\chi_{l}(-1) J(\\chi_{1},\\chi_{2},\\dots \\chi_{l}) \\ &amp;= \\chi_{l}(-1)J(\\chi_{1},\\chi_{2},\\dots \\chi_{l-1}) (g(\\chi_{1}\\chi_{2}\\dots \\chi_{l}) - \\chi_{1}\\chi_{2}\\dots \\chi_{l}(0)) \\end{align} $$ This gives the desired result.</p> </li> </ol>"},{"location":"Jacobi%20Sums/#theorem-3","title":"Theorem 3:","text":"<p><pre><code>title:\n</code></pre> <pre><code>title:\nAssume that $\\chi_{1},\\chi_{2},\\dots \\chi_{l}$ are non trivial and their product is also non trivial.\nThen $$\ng(\\chi_{1})g(\\chi_{2})\\dots g(\\chi_{l}) = J(\\chi_{1},\\chi_{2},\\dots \\chi_{l})g(\\chi_{1}\\chi_{2}\\dots \\chi_{l})\n$$\n</code></pre></p>"},{"location":"Jacobi%20Sums/#proof_8","title":"Proof:","text":"\\[ \\begin{align} g(\\chi_{1})\\dots g(\\chi_{l}) &amp;= \\prod_{i=1}^{l}\\sum_{t}\\chi_{i}(t)\\zeta^{t}  \\\\ &amp;= \\sum_{s=0}^{p-1}\\sum_{t_{1}+t_{2}+\\dots t_{l} = s} \\chi_{1}(t_{1})\\dots \\chi_{l}(t_{l})\\zeta^{s}  \\\\ &amp;= J_{0}(\\chi_{1},\\chi_{2},\\dots \\chi_{l}) + \\sum_{s=1}^{p-1}\\zeta^{s}\\left( \\sum_{t_{1}'+t_{2}'\\dots+t_{l}' = 1}\\chi_{1}\\chi_{2}\\dots \\chi_{l}(s) \\chi_{1}(t_{1})\\chi_{2}(t_{2})\\dots \\chi_{l}(t_{l}) \\right) \\\\ &amp;= J_{0}(\\chi_{1},\\chi_{2},\\dots \\chi_{l}) + \\sum_{s = 1}^{p-1}\\zeta^{s}\\chi_{1}\\chi_{2}\\dots \\chi_{l}(s)J(\\chi_{1},\\dots \\chi_{l}) \\\\ &amp;=  0 + J(\\chi_{1},\\chi_{2},\\dots \\chi_{l})g(\\chi_{1}\\dots \\chi_{l}) \\end{align} \\]"},{"location":"Jacobi%20Sums/#corollary-1","title":"Corollary 1:","text":"<pre><code>title:\nSuppose that $\\chi_{1},\\chi_{2},\\dots \\chi_{l}$ were non trivial but their product is trivial.\nThen $$\ng(\\chi_{1})\\dots g(\\chi_{l})= \\chi_{l}(-1)p J(\\chi_{1},\\dots \\chi_{l-1})\n$$ \n</code></pre>"},{"location":"Jacobi%20Sums/#proof_9","title":"Proof:","text":"<p>We know $$ \\begin{align} g(\\chi_{1})\\dots g(\\chi_{l-1}) &amp;= g(\\chi_{1}\\dots \\chi_{l-1})J(\\chi_{1},\\dots \\chi_{l-1})  \\ \\implies g(\\chi_{1})\\dots g(\\chi_{l}) &amp;= g(\\chi_{1}\\dots \\chi_{l-1})g(\\chi_{l})L(\\chi_{1},\\chi_{2},\\dots \\chi_{l-1}) \\ &amp;= g(\\overline{\\chi_{l}})g(\\chi_{l})J(\\chi_{1},\\dots \\chi_{l-1}) \\ &amp;= p\\chi_{l}(-1)J(\\chi_{1},\\dots \\chi_{l-1}) \\end{align} $$</p>"},{"location":"Jacobi%20Sums/#corollary-2","title":"Corollary 2:","text":"<pre><code>title:\nLet the hypotheses as in corollary 1, then $$\nJ(\\chi_{1},\\dots \\chi_{l}) = - \\chi_{l}(-1)J(\\chi_{1},\\dots \\chi_{l-1})\n$$\n</code></pre>"},{"location":"Jacobi%20Sums/#proof_10","title":"Proof:","text":"<p>We know \\((\\sum_{s=1}^{p-1}\\zeta^{s} )J(\\chi_{1},\\dots ,\\chi_{l}) + J_{0}(\\chi_{1},\\dots \\chi_{l}) = g(\\chi_{1})g(\\chi_{2})\\dots g(\\chi_{l})\\), from the proof of theorem 3. This means $$ \\begin{align} J(\\chi_{1},\\dots \\chi_{l}) &amp;= -g(\\chi_{1})g(\\chi_{2})\\dots g(\\chi_{l}) + J_{0}(\\chi_{1},\\dots \\chi_{l}) \\ &amp;= \\chi_{l}(-1)(p-1)J(\\chi_{1},\\dots \\chi_{l-1}) - \\chi_{l}(-1)pJ(\\chi_{1},\\dots \\chi_{l}) \\ &amp;= -\\chi_{l}(-1)J(\\chi_{1},\\dots \\chi_{l-1}) \\end{align} $$</p>"},{"location":"Jacobi%20Sums/#theorem-4","title":"Theorem 4:","text":"<p>Assume that \\(\\chi_{1},\\dots \\chi_{r}\\) are non trivial. 1. If \\(\\chi_{1}\\dots \\chi_{r}\\neq\\varepsilon\\), then $$ |J(\\chi_{1},\\chi_{2},\\dots \\chi_{r})| = p ^{(r-1)/2} $$ 2. If \\(\\chi_{1}\\dots \\chi_{r}= \\varepsilon\\), then $$ |J_{0}(\\chi_{1},\\dots,\\chi_{r})| = (p-1)p ^{r/2-1} $$ and $$ |J(\\chi_{1},\\dots \\chi_{r})| = p ^{r/2-1} $$</p>"},{"location":"Jacobi%20Sums/#references","title":"References","text":"<p>Gauss sums Multiplicative Characters</p>"},{"location":"Johnson%20Graphs/","title":"Johnson Graphs","text":"<p>202308201208</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Johnson%20Graphs/#johnson-graphs","title":"Johnson Graphs","text":"<p>Johnson Graphs are an important set of graphs that translate many combinatorial problems about sets into graph theory.</p> <pre><code>title: definition\nLet $v,k,i$ be fixed positive integers with $v\\ge k\\ge i$. Let $\\Omega$ be a fixed set of size $v$. Then the vertices of the graph $J(v, k ,i)$ are the subsets of $\\Omega$ of size $k$, where two subsets are adjacent if their intersection has size $i$\n</code></pre> <p>The function that maps each set to its compliment gives the following theorem $$ J(v, k ,i)\\cong J(v, v-k, v-2k+i) $$</p> <p>Which also implies that it is safe to assume \\(v\\ge 2k\\).</p> <p>Johnson Graphs for which \\(i=0\\) are called Kneser Graphs.</p> <p>\\(J(n, m, j)\\) is Vertex Transitive \\(J(2m+1, m, 0)\\) is [[Arc-Transitivity of a Graph|2-Arc-Transitive]] </p>"},{"location":"Johnson%20Graphs/#references","title":"References","text":""},{"location":"Karger%27s%20min-cut%20algorithm/","title":"Karger's min cut algorithm","text":"<p>202310181410</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note","Incomplete"]},{"location":"Karger%27s%20min-cut%20algorithm/#kargers-min-cut-algorithm","title":"Karger's min-cut algorithm","text":"<p>Problem:</p>","tags":["Note","Incomplete"]},{"location":"Karger%27s%20min-cut%20algorithm/#randomised-algorithm","title":"Randomised Algorithm","text":"<p>Pick an edge uniformly at random and contract it, remove self loops, keep multiple edges. Continue this until two nodes are left. Output the cut between the two nodes.</p> <p>Observe: - Cut size does not go down in the course of the algorithm - If min-cut size \\(=k\\), then min degree \\(\\ge k\\). So \\(|E|=m\\ge \\frac{nk}{2}\\).</p>","tags":["Note","Incomplete"]},{"location":"Karger%27s%20min-cut%20algorithm/#analysis","title":"Analysis","text":"<p>Let \\(C\\) be a min-cut, \\(|C|=k\\). \\(Pr[C\\text{ survives first round}]=1-\\frac{k}{m}\\ge 1-\\dfrac{k}{\\frac{nk}{2}}=\\frac{n-2}{n}\\). Let's say \\(C\\) survives \\(i^{th}\\) round. \\(Pr[C\\text{ survives } (i+1)\\text{st round}]\\ge \\frac{n-i-2}{n-i}\\). \\(Pr[C\\text{ survives until the end}]\\ge \\frac{n-2}{n} \\frac{n-3}{n-1} \\dots \\frac{1}{3} = \\frac{2}{n(n-1)}\\)</p> <p>If \\(G\\) has \\(q\\) min-cuts, \\(Pr[\\text{a mincut survives}]\\ge \\dfrac{q}{n \\choose 2}\\). No. of mincuts in \\(G\\le {n \\choose 2}\\), so \\(Pr\\le 1\\) Time \\(=O(n^{2})\\).</p>","tags":["Note","Incomplete"]},{"location":"Karger%27s%20min-cut%20algorithm/#improving-the-success-prob","title":"Improving the success prob","text":"<p>Repeat the algo \\(M\\) times. Find cuts \\(C_{1},\\dots,C_{M}\\). Output \\(min\\{C_{1},\\dots,C_{M}\\}\\).</p> <p>\\(Pr[C\\text{ is not a mincut}]=Pr[\\text{the mincut algo fails }M\\text{ times}]\\le \\left( 1-\\frac{1}{n^{2}} \\right)^{M}\\le e^{ \\frac{-M}{n^{2}} }\\). Choose \\(M=n^{2}\\log n\\), then failure probability \\(\\le e^{\\frac{-M}{n^{2}}}\\le e^{-\\log n}= \\frac{1}{n}\\). Time \\(=O(n^{4}\\log n)\\)</p>","tags":["Note","Incomplete"]},{"location":"Karger%27s%20min-cut%20algorithm/#references","title":"References","text":"<p>[[Randomised Algorithms]]</p>","tags":["Note","Incomplete"]},{"location":"Kneser%20Graph/","title":"Kneser Graph","text":"<p>202308161108</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Kneser%20Graph/#kneser-graph","title":"Kneser Graph","text":"<p>Let \\(X=\\) set of all \\(3-\\)subsets of the set \\([7]\\) \\(|X|={7\\choose 3}=35\\)  Represent such graphs has \\(J(7, 3, 0)\\)  which represents \\(J(\\) Size of the total set, size of the subset, interesection between subsets\\()\\)</p> <p>\\(\\text{Aut}(J(7,3,0))=\\text{Aut}(J(7,3,2))=S_{7}\\) \\(\\text{Aut}(J(7,3,1))=S_{8}\\) </p>"},{"location":"Kneser%20Graph/#references","title":"References","text":"<p>Johnson Graphs</p>"},{"location":"Kolmogorov%20Property/","title":"Kolmogorov Property","text":"<p>202301181801</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Kolmogorov%20Property/#kolmogorov-property","title":"Kolmogorov Property","text":""},{"location":"Kolmogorov%20Property/#definition","title":"Definition","text":"<p>A topological Space \\(X\\) is called \\(T_0\\) if for all pairs of points \\(x, y\\) where \\(x\\ne y\\exists\\) a set \\(U\\) such that \\(x\\in U, y\\notin U\\). </p>"},{"location":"Kolmogorov%20Property/#related-problems","title":"Related Problems","text":""},{"location":"Kolmogorov%20Property/#references","title":"References","text":"<p>Separation Axioms</p>"},{"location":"Kolmogorov%20Translation/","title":"Kolmogorov Translation","text":"<p>202310091410</p> <p>Tags : [[Lambda Calculus]], [[Type Theory]]</p>","tags":["Note","Incomplete"]},{"location":"Kolmogorov%20Translation/#kolmogorov-translation","title":"Kolmogorov Translation","text":"<p>\\(K(\\alpha)=\\lnot\\lnot\\alpha\\)</p> <p>\\(K(\\varphi\\to\\psi)=\\lnot\\lnot(\\varphi\\to\\psi)\\)</p> <p>\\(K(\\varphi)=\\lnot\\lnot\\varphi^*\\) \\(\\alpha^*=\\alpha\\) \\((\\varphi\\to\\psi)^{*}=(\\lnot\\lnot\\varphi^*\\to\\lnot\\lnot\\psi^*)\\)</p> <pre><code>title:Theorem\n1. $\\vdash \\varphi\\to K(\\varphi)$ and $\\vdash K(\\varphi)\\to\\varphi$ in $\\text{CPC}(\\rightarrow,\\perp)$\n2. $\\vdash_{\\text{CPC}}\\varphi\\iff \\vdash_{\\text{IPC}}K(\\varphi)$\n</code></pre> <p>[[Continuation Passing Translations]]</p> <pre><code>sum :: [Int] -&gt; Int\nsum []        = 0\nsum (x:xs)    = x + sum xs\n\n---\n\nsum           = sum' id\nsum' f []     = f 0\nsum' f (x:xs) = sum' ( f.(x+) ) xs\n</code></pre> <p>Translation from lambda mu calculus to lambda calculus</p>","tags":["Note","Incomplete"]},{"location":"Kolmogorov%20Translation/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Kripke%20Models%20are%20Equivalent%20to%20Heyting%20Algebra/","title":"Kripke Models are Equivalent to Heyting Algebra","text":"<p>202309111245</p> <p>tags : [[Logic]]</p>","tags":["Example"]},{"location":"Kripke%20Models%20are%20Equivalent%20to%20Heyting%20Algebra/#kripke-models-are-equivalent-to-heyting-algebra","title":"Kripke Models are Equivalent to Heyting Algebra","text":"<p>Let \\(C\\) be the set of all prime filters  in \\(\\mathcal H\\), where \\(0\\ne 1\\). And let inclusion be the partial order. We define \\(F\\Vdash p\\) iff \\(\\nu(p)\\in F\\) and we want to show that  $$ F\\Vdash\\varphi \\iff  \\textlbrackdbl     \\varphi \\textrbrackdbl_{\\nu} \\in F $$</p> <p>The only non-trivial case is \\(\\varphi = \\varphi'\\to\\varphi''\\) Suppose \\(F\\Vdash f=\\varphi'\\to\\varphi''\\) but \\(\\textlbrackdbl \\varphi'\\to\\varphi''\\textrbrackdbl_{\\nu}=\\textlbrackdbl \\varphi'\\textrbrackdbl_{\\nu}\\Rightarrow\\textlbrackdbl\\varphi''\\textrbrackdbl_{\\nu}\\). So take the Smallest filter \\(G\\) which contains \\(f\\)  and \\(\\varphi'\\). And by Lemma 1 we have \\(G = \\{b\\ :\\ b\\ge f\\sqcap\\varphi\\text{ for some }f\\in F\\}\\) And we have \\(\\textlbrackdbl \\varphi''\\textrbrackdbl_{\\nu}\\notin G\\) there is an \\(f\\) where \\(\\nu(\\varphi'')\\ge f\\sqcup \\varphi'\\) , and this \\(\\nu(\\varphi'')\\in F\\) and we would be done.</p> <p>By Lemma 2 we extend \\(G\\) to a prime filter \\(H\\) that does not contain \\(\\nu(\\varphi'')\\). but \\(H\\) contains \\(\\varphi'\\) and \\(\\varphi'\\to\\varphi''\\) and by primality it contains \\(\\varphi''\\) which is a contradiction.</p> <p>For the converse assume we have \\(\\nu(\\varphi'\\to\\varphi'')\\) and \\(F\\subseteq H\\Vdash\\varphi'\\) . And by induction hypothesis we have \\(\\varphi'\\in H\\) and \\(\\nu(\\varphi')\\Rightarrow\\nu(\\varphi'')\\) so we have \\(\\varphi''\\in H\\) and hence \\(H\\Vdash \\varphi''\\).</p> <p>Other cases are trivial, but primality is necessary for disjunctions.</p>","tags":["Example"]},{"location":"Kripke%20Models%20are%20Equivalent%20to%20Heyting%20Algebra/#related","title":"Related","text":"<p>Heyting Algebra Kripke Models</p>","tags":["Example"]},{"location":"Kripke%20Models/","title":"Kripke Models","text":"<p>202309111109</p> <p>Tags : [[Logic]]</p>","tags":["Note"]},{"location":"Kripke%20Models/#kripke-models","title":"Kripke Models","text":"<pre><code>title: Intuition\n_Kripke Models_ reflect the following idea. From a constructive point of view, we can only assert the validity of propositions that we are sure of. But acquiring more information lets us talk about more propositions.\n\nWhat is known to be *True* will remain *True* forever.\nBut what is not recognized as *True* today might be recognized as *True* tomorrow.\nWe only assert that a proposition is *False* when we can prove that it will never be recognized as *True*.\n</code></pre> <p>A Kripke Model is a triple of the form: $$ \\mathcal C = \\langle                    C                 , \\le                 , \\Vdash               \\rangle $$ Here: - \\(C\\) is a non-empty set, whose elements are called states or possibilities worlds. - \\(\\le\\) is a partial order on \\(C\\) which represent the \"Timeline\" - \\(\\Vdash\\) is a binary relation between elements of \\(C\\) and propositions which represents \"The knowledge we know at that point in the timeline\".</p> <p>We extend the binary relation to use it for propositions too - \\(c\\Vdash \\varphi\\land\\psi\\) iff \\(c\\Vdash\\varphi\\) and \\(c\\Vdash\\psi\\)  - \\(c\\Vdash \\varphi\\lor\\psi\\) iff \\(c\\Vdash\\varphi\\) or \\(c\\Vdash\\psi\\) - \\(c\\Vdash \\varphi\\to\\psi\\) iff \\(c'\\Vdash \\psi\\) whenever \\(c'\\Vdash\\psi\\) for all \\(c'\\ge c\\) (In the future, if we find \\(\\varphi\\), we will also, always get \\(\\psi\\) along with it) - \\(c\\Vdash \\lnot\\varphi\\) if \\(c'\\not\\Vdash\\varphi\\) for all \\(c'\\ge c\\)</p> <p>[[Drawing 2023-09-11 12.11.46.excalidraw|Here]] is an example of a Kripke Model such that \\(c\\Vdash\\lnot\\lnot(p\\lor q)\\) and \\(c\\Vdash(p\\to q)\\to q\\) but \\(c\\not\\Vdash p\\lor\\lnot p\\) </p> <p>[[Drawing 2023-09-11 12.27.42.excalidraw|Here]] is an example of a Kripke Model where \\((\\lnot q\\to\\lnot p)\\to(p\\to q)\\) is not proved.</p> <p>Kripke Models are Equivalent to Heyting Algebra</p>","tags":["Note"]},{"location":"Kripke%20Models/#references","title":"References","text":"","tags":["Note"]},{"location":"LECTURE/","title":"LECTURE","text":"<ol> <li>What is a discrete valuation on a field.  \\(v : K^{\\times} \\to \\mathbb{Z}\\) such that \\(v(a+b) \\ge \\min(v(a),v(b))\\) and \\(v(ab) = v(a)+v(b)\\).</li> <li>This can be extented to whole of \\(K\\) by defining \\(v(0) = \\infty\\). We follow the first definition in this lecture.</li> <li>Example: P-adic valuation on Q v_p</li> <li>The ring \\(A := \\{ x \\in K \\mid v(x) \\ge 0\\}\\) is called the valuation ring of \\(K\\). </li> <li>What is an absolute value or multiplicative valuation on a field. Archimedean, Non archimedean.</li> <li>Example: P-adic abs val on Q.</li> <li>Why is it called non archimedean?</li> <li>Show examples as in the notes. Define p-adic absolute value on \\(\\mathbb{Q}\\). </li> <li>Can go from a valuation to an absolute value and vice versa. \\(v(x) = -\\log(|x|)\\)</li> <li>Relation between absolute value and additive valuations:    A discrete (additive) valuation \\(\\mathrm{ord} : K^{\\times} \\to \\mathbb{Z}\\)  determines an absolute value by \\(|x| = e^{-\\mathrm{ord}(x)}\\)    An absolute value on \\(K\\) determines an additive valuation (need not be discrete) on \\(K^{\\times}\\) by \\(\\log_{e}|x| = -\\mathrm{ord}(x)\\)</li> <li> <p>### Lemma:    Absolute value is non archimedean iff it takes bounded values on \\(\\{m \\cdot 1 | m \\in \\mathbb{Z} \\}\\)</p> <ol> <li>Corollary: If \\(\\mathrm{char} K \\neq 0\\) then \\(K\\) has only non archimedean abs values.</li> </ol> </li> <li> <p>Say that the absolute value is discrete if the image of \\(K^{\\times}\\) is a discrete subgroup of \\(\\mathbb{R}_{&gt;0}\\).</p> </li> <li>Let \\(v\\) be a discrete valuation on \\(K\\), then $$ A := { a \\in K : v(a) \\geq 0 } $$ is a PID with the unique maximal ideal $$ \\mathfrak{m} := { a \\in K : v(a) &gt; 0 } $$</li> </ol>"},{"location":"LECTURE/#proof","title":"Proof:","text":"<p>Take an ideal \\(I\\) of \\(A\\), \\(I\\) does not contain any element with valuation 0, since they are units of A. So, take the element \\(a\\) with least valuation, this generates \\(I\\). any element with equal valuation as that of \\(a\\), is an associate of \\(a\\). Now if \\(v(b) = qv(a) + r\\) with \\(0&lt;r&lt;b\\), we get that \\(v(b a^{-q}) = r &gt; 0\\) hence \\(ba ^{-q} \\in A\\), but has smaller valuation than \\(a\\). Contradiction. So this is a PID.</p> <p>Easy to see that \\(\\mathfrak{m}\\) is the unique maximal ideal.</p>"},{"location":"LECTURE/#proposition-76-discrete-iff-m-is-principal-which-makes-a-a-dvr-pid-local","title":"Proposition 7.6 : | | discrete iff m is principal, which makes A a DVR (PID + local)","text":"<p>##### Proof:    If \\(\\mid\\cdot\\mid\\) is discrete, then take \\(\\pi \\in \\mathfrak{m}\\) such that \\(\\mid \\pi\\mid\\) is maximum. Then take any \\(\\pi_{1} \\in \\mathfrak{m}\\) and let \\(\\mid \\pi\\mid^{q+1} &lt; \\mid \\pi_{1}\\mid &lt; \\mid \\pi\\mid^{q}\\).    and so, \\(|\\pi|&lt;\\mid \\frac{\\pi_{1}}{\\pi^{q}}\\mid &lt; 1\\) and so, \\(\\frac{\\pi_{1}}{\\pi^{q}} \\in \\mathfrak{m}\\) and has larger absolute value than \\(\\pi\\). Contradiction. So, \\(|\\pi_{1}| = |\\pi|^{q}\\), this gives \\(\\pi_{1} = \\pi^{q} \\cdot u\\) where \\(u\\) is a unit in \\(A\\), hence \\(\\pi_{1} \\in (\\pi) \\implies \\mathfrak{m} \\subset (\\pi) \\implies \\mathfrak{m} = (\\pi)\\).</p> <p>If \\(\\mathfrak{m} = (\\pi)\\) then take any \\(\\alpha \\in K^{\\times}\\), let \\(|\\pi|^{q+1} &lt; |\\alpha| &lt; |\\pi|^{q}\\), then \\(\\mid\\frac{\\alpha}{\\pi^{q}}\\mid &lt; 1 \\implies \\frac{\\alpha}{\\pi^{q}} \\in (\\pi) \\implies \\alpha = \\pi^{q+1}\\beta\\) where \\(\\beta \\in A\\). This gives \\(|\\alpha| \\leq |\\pi|^{q+1}\\) This is a contradiction, hence \\(|\\alpha| = |\\pi|^{q}\\) for some \\(q \\in \\mathbb{Z}\\). Hence \\(\\mid K^{\\times}\\mid =(|\\pi|)\\).</p> <p>Take an ideal \\(I\\) of \\(A\\), \\(I\\) does not contain any element with abs value 1, since they are units of A. So, take the element \\(a\\) with greatest abs val, this generates \\(I\\). Why? Because any element with equal abs val as that of \\(a\\), is an associate of \\(a\\). Now if \\(|a|^{q+1} &lt; |b|&lt;|a|^q\\) we get that \\(|ba^{-q}| &lt; 1\\) hence \\(ba ^{-q} \\in A\\), but has greater abs val than \\(a\\). Contradiction.</p> <p>So this is a PID.</p> <ol> <li>Abs value induce metric induce topology</li> <li>Abs value equiv if they induce same topology</li> <li>Q with p adic abs value, what is it? what does closeness look like here? 1,p,p^2,.... converges to 0</li> <li>Equivalent abs values</li> <li>Ostrowski</li> <li>Completion of Q under p- adic abs value</li> <li>Completions in non archimedean case</li> <li>|K| = |\\(\\hat{K}\\)| </li> <li>\\(\\hat{A}\\) is the closure of A, \\(\\hat{\\mathfrak{m}}\\) is the closure of \\(\\mathfrak{m}\\), \\(\\hat{\\mathfrak{m}}^{n}\\) is the closure of \\(\\mathfrak{m}^{n}\\).</li> <li>\\(\\hat{\\mathfrak{m}} = (\\pi) = \\{ a \\in \\hat{K}\\mid \\ |a| \\le |\\pi| \\}\\)</li> <li>For every \\(n\\), \\(\\(\\frac{A}{\\mathfrak{m}^{n}} \\to \\frac{\\hat{A}}{\\hat{\\mathfrak{m}}^{n}}\\)\\)     Is an isomorphism.</li> </ol>"},{"location":"LECTURE/#proof_1","title":"Proof:","text":"<p>The function is \\(f(a+\\mathfrak{m}^{n}) = a + \\hat{\\mathfrak{m}}^{n}\\). Note that \\(\\mathfrak{m}^{n} = \\{ a \\in A \\mid \\ \\mid a\\mid \\le \\mid \\pi\\mid^{n} \\} = \\{ a \\in A \\mid \\ \\mid a\\mid &lt; \\mid \\pi\\mid^{n-1} \\}\\) is open and closed as a subset of \\(A\\).  Now \\(\\ker(f) = \\{ a + \\mathfrak{m}^{n} \\mid a \\in A, a \\in \\hat{\\mathfrak{m}}^{n} \\}\\) Since \\(\\mathfrak{m}\\) is dense in \\(\\hat{\\mathfrak{m}}\\), \\(\\mathfrak{m}^{n}\\) is dense in \\(\\hat{\\mathfrak{m}}^{n}\\) (\\(\\mathfrak{m}^{n} = (\\pi^{n}) \\subset A\\), and \\(\\hat{\\mathfrak{m}}^{n} = (\\pi^{n}) \\subset \\hat{A}\\)). Since \\(a \\in A\\) and \\(a\\) is a limit point of \\(\\mathfrak{m}^{n}\\), it must be in \\(\\mathfrak{m}^{n}\\) since it is closed in \\(A\\). This gives \\(\\ker(f) = \\{ 0 \\}\\).</p> <p>Now, let \\(\\hat{a} \\in \\hat{A}\\), \\(\\lim_{ n \\to \\infty }a_{n} = \\hat{a}\\), This gives \\(a_{k} \\in \\hat{a} + \\hat{\\mathfrak{m}}^{n}\\) for all large \\(k\\). (Since \\(\\hat{\\mathfrak{m}}^{n}\\) is an open set around 0). Hence, \\(f(a_{k} + \\mathfrak{m}^{n}) = a_{k} + \\hat{\\mathfrak{m}}^{n} = \\hat{a} + \\hat{\\mathfrak{m}}^{n}\\). Hence, \\(f\\) is surjective</p> <ol> <li> <p>Choose a set \\(S\\) of representatives of \\(\\frac{A}{\\mathfrak{m}}\\), and let \\(\\pi\\) generate \\(\\mathfrak{m}\\). Then each element of \\(\\hat{K}\\) is expressible as a cauchy series of the form $$ a_{-n}\\pi^{-n} + \\dots + a_{0} + a_{1}\\pi + a_{2}\\pi^{2}\\dots,  a_{i}\\in S $$ and such a representation is unique.</p> </li> <li> <p>Use this to show how elements of \\(\\mathbb{Q}_{p}\\) look like</p> </li> <li> <p>Let \\(K = \\mathbb{Q}\\), then \\(\\hat{K} = \\mathbb{Q}_{p}\\), \\(A = \\left\\{  \\frac{a}{b} \\in \\mathbb{Q} \\mid p \\nmid b, (a,b) = 1  \\right\\}\\), \\(\\mathfrak{m} = \\left\\{  \\frac{a}{b} \\in \\mathbb{Q} \\mid p |a; (a,b) = 1  \\right\\}\\)</p> </li> <li>Then the representatives for \\(A/\\mathfrak{m}\\) are just \\(0,1,\\dots p-1\\)</li> <li>In other words, \\(A / \\mathfrak{m} \\cong \\mathbb{Z}/p\\mathbb{Z}\\).</li> <li>\\(A = \\mathbb{Z}_{(p)}\\), \\(\\mathbb{Z}_{p} := \\hat{A}\\). So, \\(\\mathbb{Z}_{p}\\) is the elements of \\(\\mathbb{Q}_{p}\\) with series expansion starting from non negative integers.</li> <li>Given a element of \\(\\prod \\limits_{ n=1}^{ \\infty } \\mathbb{Z} /p ^{n} \\mathbb{Z}\\), with \\(a_{n+1} = a_{n}(\\mathrm{mod} \\ p ^{n})\\), can change it into a series representation.</li> <li>Conversely, If then \\(\\alpha = \\sum\\limits_{ i=0}^{ \\infty }c_{i}p ^{i}\\), \\(0 \\le c &lt; p-1\\), then \\(c_{0},c_{1},\\dots\\) is the unique sequence of integers such that      \\(\\alpha \\equiv \\sum\\limits_{ i=0}^{ n-1 }c_{i}p ^{i}\\ \\mathrm{mod} \\ p^n\\)</li> <li>So basically, elements of \\(\\mathbb{Z}_{p}\\) have \"compatibility\" among the coefficients, and any truncation of the series gives mod p^n of that number</li> <li>Any element \\(\\alpha \\in \\mathbb{Q}_{p}\\), can multiply it by high power of \\(p\\) and bring it in \\(\\mathbb{Z}_{p}\\). hence, coefficients of \\(\\mathbb{Q}_{p}\\) elements also have compatibility.</li> <li>Hensel's lemma</li> <li>Nakayama lemma for local rings:      Let \\(A\\) be a local ring and \\(\\mathfrak{a}\\) is a proper ideal of \\(A\\). Let \\(M\\) be a f.g. module over \\(A\\), then \\(\\mathfrak{a}M = M\\) implies \\(M = 0\\).</li> </ol>"},{"location":"Lambda%20Calculus%20Evaluation%20Rules/","title":"Lambda Calculus Evaluation Rules","text":"<p>202303150303</p> <p>Type : #Note Tags : [[Lambda Calculus]]</p>"},{"location":"Lambda%20Calculus%20Evaluation%20Rules/#lambda-calculus-evaluation-rules","title":"Lambda Calculus Evaluation Rules","text":""},{"location":"Lambda%20Calculus%20Evaluation%20Rules/#beta-reductions","title":"\\(\\beta\\)-reductions","text":"<p>^7a5311</p> <p>The only rule that is really required to evaluate any Lambda Expression is \\(\\beta\\)-reduction, which is related to the second rule of the lambda calculus syntax which is  </p> <p>\\(\\beta\\)-reduction is the rewriting of a lambda expression when one of the from \\(\\lambda x.M\\) is applied to another lambda term \\(N\\). We replace all instances of \\(x\\) bound with the parameter with \\(N\\) which can also be written as  \\(\\((\\lambda x.M)N\\longmapsto M[x:= N]\\)\\) Left part of the equation \\((\\lambda x.M)N\\) is called a Redex(Reducible Expression). While the right part of the equation \\(M[x:=N]\\) is called the Contractum(Contracted Term?? I think??). An example of this can be: $$ (\\lambda xy.xy)z\\longmapsto(\\lambda y.xy)[x:=z]\\longmapsto(\\lambda y.zy) $$</p> <p>When no more \\(\\beta\\)-reductions can be performed, then the lambda term is said to be in a \\(\\beta\\)-normal form.</p>"},{"location":"Lambda%20Calculus%20Evaluation%20Rules/#alpha-conversion","title":"\\(\\alpha\\)-conversion","text":"<p>There is one slight problem with \\(\\beta\\)-reductions, that of variable collision, which can be easily fixed using \\(\\alpha\\)-conversions. \\(\\alpha\\)-conversions are just formal change of variables in a lambda expression, like $$ \\lambda ab. ab\\equiv_{\\alpha}\\lambda cb.cb $$</p> <p>There two lamba term are considered to be intrinsionally equivalent. The following example will show the problem with \\(\\beta\\)-reduction. It is renaming a binding variable and all variables bound to it. (see Lambda Calculus Syntax#^BindingsOfVariables)</p> <p>Let \\(N=(\\lambda xy.xy)\\) and \\(M=Ny\\) In this case, applying a beta reduction would result in the expression  \\(M=\\lambda y.yy\\) which is not desired,  The desired goal was, that \\(N\\) would take \\(2\\) inputs, and apply the first one on the second one, but the following lambda expression would ignore the first input provided by \\(M\\) and just take one input, applying it to itself.</p> <p>This can be fixed in the follwing way let \\(N=(\\lambda xz.xz)\\), which is an \\(\\alpha\\)-conversion of the inner lambda term, replacing \\(y\\) with \\(z\\) and now the \\(\\lambda\\) expression \\(M\\) evaluates to \\((\\lambda z.yz)\\), which was the desired result.</p>"},{"location":"Lambda%20Calculus%20Evaluation%20Rules/#eta-reduction","title":"\\(\\eta\\)-reduction","text":"<p>\\(\\eta-\\)reduction is a method to simplify a lambda expression by eliminating redundent inputs and modifying the defintion such that the function. \\(\\lambda x.N\\; x\\longmapsto N\\) is the simplest example of \\(\\eta-\\)reduction an example of a slightly more complex \\(\\eta-\\)reudction would be  $$ \\begin{aligned} \\lambda mnfx.\\; (n\\; m (\\lambda x.fx))\\; x&amp;\\longmapsto \\lambda mnf.\\; n\\;m\\;(\\lambda x.fx)\\ &amp;\\longmapsto \\lambda mnf.\\;n\\;m\\;f\\ &amp;\\longmapsto \\lambda mn.\\;n\\;m \\end{aligned} $$</p>"},{"location":"Lambda%20Calculus%20Evaluation%20Rules/#references","title":"References","text":"<p>Lambda Calculus Syntax</p>"},{"location":"Lambda%20Calculus%20Introduction/","title":"Lambda Calculus Introduction","text":"<p>202303150203</p> <p>Type : #Note Tags : [[Lambda Calculus]]</p>"},{"location":"Lambda%20Calculus%20Introduction/#lambda-calculus","title":"Lambda Calculus","text":"<p>Lambda Calculus also knows as \\(\\lambda\\)-Calculus is a model of computation developed by Alonzo Church in the 1930s. </p> <p>One of the important things it deals with is :- The way we usually define functions is Extrinsional, as in, a function is uniquely defined by the relation it crates between its inputs and outputs, while this way to identify a function is good for most of mathematics, it is important to define the exact way in which a function computes. This problem is solved by lambda calculus making it an Intrinsional Syntax for function defintion. It captures an intrinsional definition of a function in its very minimal syntax and simple evaluation rules.</p> <p>The interesting thing about lambda calculus is that, in the world of lambda calculus, there are ONLY functions, no values, no constants......  Then what do functions act on? Other functions!!!</p> <p>But even with this restrictions, this model of computation is Turing Complete, as in its a Universal Computation Model This was done by finding a set of computable functions by \\(\\lambda-\\)calculus which was later found to be the same as the set of functions computable by Turing Machines.</p> <p>we can encode Booleans and Natural Numbers using lambda calculus.</p> <p>It was used by Alonzo Church to solve the Undecidability Problem a year before Alan Turing did!!</p>"},{"location":"Lambda%20Calculus%20Introduction/#references","title":"References","text":""},{"location":"Lambda%20Calculus%20Syntax/","title":"Lambda Calculus Syntax","text":"<p>202303150203</p> <p>Type : #Note Tags : [[Lambda Calculus]]</p>"},{"location":"Lambda%20Calculus%20Syntax/#lambda-calculus-syntax","title":"Lambda Calculus Syntax","text":""},{"location":"Lambda%20Calculus%20Syntax/#syntax","title":"Syntax","text":"<p>Lambda Calculus has a minimal syntax which captures the intrinsional definition of a function, the syntax is as follows</p> <p>You have a countably Infinite set of variables, which can be any character except \\(\\lambda\\; .\\; (\\;)\\) because these characters have predefined meanings</p> <p>Lambda calculus is build from expressions/terms of 3 types: ^fe7958 1. \\(x\\) - variable 2. \\((\\lambda x.M)\\) - abstraction, here the variable \\(x\\) will be bound in the definition of \\(M\\) ^a88f80 3. \\((M\\ N)\\) - application, Applying a function \\(M\\) on the argument \\(N\\), where both of there are lambda terms</p> <p>The variables that are written just after \\(\\lambda\\) and before \\(.\\) are called the Binding variables, the variables whose values are associated with it(possibly affected because of a \\(\\beta\\)-reduction), are called Bound like, in the expression ^BindingsOfVariables $$ x(\\lambda y.y) $$ Here \\(x\\) is unbounded, while the first occurence of \\(y\\) is binding and the second one is bound.</p> <p>Composition of lambda expression is left associated:- \\(ABC=(AB)C\\)</p> <p>Rules for a more shorthand notation is  1. \\(\\lambda x.(\\lambda y.M)\\) is abbreviated as \\(\\lambda x.\\lambda y.M\\) 2. \\(\\lambda x_1.(\\lambda x_2. (\\dots(\\lambda x_n.M)\\dots)\\) is abbreviated as \\((\\lambda x_1 x_2\\dots x_n.M)\\) 3. \\(\\lambda x.MN\\) is the same as \\(\\lambda x.(MN)\\)</p>"},{"location":"Lambda%20Calculus%20Syntax/#examples","title":"Examples","text":"<ul> <li>\\(\\lambda a.a\\)  -  Which is the identitiy function</li> <li>\\(\\lambda x.y\\)  -  Which is the contant function, returning \\(y\\) for any input \\(x\\)</li> <li>\\((\\lambda a.b)(\\lambda c.d)\\)  -  Which is applying the left lambda term on the right one</li> <li>\\(x\\)  -  a variable</li> </ul>"},{"location":"Lambda%20Calculus%20Syntax/#references","title":"References","text":"<p>Lambda Calculus Evaluation Rules</p>"},{"location":"Lambda%20Mu%20Calculus/","title":"Lambda Mu Calculus","text":"<p>202309251409</p> <p>Tags : [[Lambda Calculus]], [[Type Theory]]</p>","tags":["Note","Incomplete"]},{"location":"Lambda%20Mu%20Calculus/#lambdamu-calculus","title":"\\(\\lambda\\mu-\\)Calculus","text":"<pre><code>title: History\nUntil around 1990, it was a common belief that there is no [Curry Howard Isomorphism](&lt;./Curry Howard Isomorphism.md&gt;) for **Classical Logic**. Timothy Griffin then discovered that $\\lnot\\lnot p\\rightarrow p$ can be implemented in the typing of *Control Flow* \n</code></pre> <pre><code>title:Motivation\nOne of main uses of *Type Theory* is in *Static Analysis* for *Software Verification*. The process being, we have built the calculus in such a way that, it will correspond to some valid logical statement. \nCorrespondingly, If a program *crashes*(which it cannot for $\\lambda$-calculus, but say it does) and corresponds to a logical statement. A good way to interpret it would be that the statement if **False**. This is exactly the idea behind $\\lambda\\mu$ calculus, where we use control flow to model something like *try-catch*. We use the $\\mu$ stuff where we guarantee that a statement is going to be false, hence safely giving it a negated type, and evaluating it can lead to elimination of double negation, and hence help us prove satements from classical logic.\n</code></pre> <p>Types are formulas of the form \\(\\Phi(\\rightarrow,\\perp)\\) Variables are of the from \\(x,y,z:\\tau\\in\\Phi(\\rightarrow,\\perp)\\) Addresses are of the form \\(a,b,c:\\lnot\\sigma\\in\\Phi(\\rightarrow,\\perp)\\)</p> <p>The Context-free grammar for \\(\\lambda\\mu-\\)calculus is $$ \\begin{matrix}x &amp; | &amp; MN &amp; | &amp; (\\lambda x^{\\sigma}.M) &amp; | &amp; [a]M &amp; | &amp; \\mu a^{\\lnot\\sigma}.M\\end{matrix} $$ We write \\(\\varepsilon_\\varphi(M)= \\mu a:\\lnot\\varphi M\\) where \\(a\\notin \\text{FV}(M)\\) which represents \\(\\perp\\), but has \\(\\varphi\\) associated to it for type checking purposes.</p> <p>The Syntax for the calculus is described by the following Context-Free Grammar</p> \\[ \\begin{align*} &amp;\\Gamma, x:\\sigma \\vdash x:\\sigma\\\\\\\\ \\frac{\\Gamma, x:\\sigma \\vdash M:\\tau}{\\Gamma\\vdash(\\lambda x:\\sigma.M):\\sigma\\to\\tau}&amp;&amp;\\frac{\\Gamma\\vdash M:\\sigma\\rightarrow\\tau\\;\\;\\;\\;\\Gamma\\vdash N:\\sigma}{\\Gamma\\vdash (MN):\\tau}\\\\\\\\ \\frac{\\Gamma, a:\\lnot\\sigma\\vdash M:\\perp}{\\Gamma\\vdash (\\mu a:\\lnot\\sigma M):\\sigma} &amp;&amp; \\frac{\\Gamma, a:\\lnot\\sigma\\vdash M:\\sigma}{\\Gamma, a:\\lnot\\sigma\\vdash ([a]M):\\perp} \\end{align*} \\]","tags":["Note","Incomplete"]},{"location":"Lambda%20Mu%20Calculus/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Lambda%20Term%20In%20Combinatory%20Logic/","title":"Lambda Term In Combinatory Logic","text":"<p>202309210909</p> <p>Tags : [[Logic]], [[Lambda Calculus]]</p>","tags":["Note"]},{"location":"Lambda%20Term%20In%20Combinatory%20Logic/#lambda-term-in-combinatory-logic","title":"Lambda Term In Combinatory Logic","text":"<p>For any \\(F\\) and \\(x\\), we can define  \\(\\lambda^{*}xF\\in\\mathcal C\\) $$ \\begin{align} \\lambda^{}x.x &amp;= \\underline I\\ \\lambda^{}x.F &amp;= \\underline K F &amp;\\text{where }x\\notin\\text{vars}(F)\\ \\lambda^{}x.FG &amp;= \\underline S(\\lambda<sup>{*}x.F)(\\lambda</sup>{}x.G) \\end{align} $$</p> <p>As an example, we have the \\(Y\\) Combinator \\(\\lambda f.(\\lambda x.f(xx))(\\lambda x.f(xx))\\)</p> <p>We can add Types to our combinatory logic analogous to Lambda Calculus and get Typed Combinatory Logic</p> <p>This shows that Combinatory Logic is just as strong as Lambda Calculus and that the \\(S,K\\) Combinators are Complete.</p>","tags":["Note"]},{"location":"Lambda%20Term%20In%20Combinatory%20Logic/#references","title":"References","text":"<p>Combinatory Logic Lambda Calculus</p>","tags":["Note"]},{"location":"Lattice%20Graphs/","title":"Lattice Graphs","text":"<p>202308301108</p> <p>Type : #Note #Incomplete  Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Lattice%20Graphs/#lattice-graphs","title":"Lattice Graphs","text":"<p>Let \\(m\\) be a positive integer. Let \\(M=[m]\\) then the Lattice Graph \\(V=M\\times M\\) with \\((i, j)\\sim (i',j')\\) if - \\(i\\ne i'\\) and \\(j=j'\\) or - \\(i=i'\\) and \\(j\\ne j'\\)</p> <p>Intuitively it is a square grid of vertices where nodes that are along the same vertical or horizontal are adjacent.</p> <pre><code>Draw Diagram\n</code></pre> <p>The Lattice Graph is strongly regular with $$ \\begin{align} n&amp;= m^2\\ k&amp;= 2(m-1)\\ a&amp;= m-2 \\ c&amp;= 2  \\end{align} $$</p>"},{"location":"Lattice%20Graphs/#references","title":"References","text":""},{"location":"Lattice/","title":"Lattice","text":"<p>202308161508</p> <p>Type : #Note Tags : [[Logic]]</p>"},{"location":"Lattice/#lattice","title":"Lattice","text":"<p>A Lattice is a partial order \\(\\langle A, \\le\\rangle\\) containing as top elment (denoted by \\(1\\)) and a bottom element(denoted by \\(0\\)) such that every two-element subset has a: - Least Upper Bound     - Represented as \\(\\inf_{A}\\{a, b\\}\\) or \\(a\\sqcap b\\)     - Also called the intersection or the meet - Greatest Lower Bound     - Represented as \\(\\sup_A\\{a,b\\}\\) or \\(a\\sqcup b\\)     - Also called the union or the join $$ \\begin{matrix}a\\le a\\sqcup b &amp; &amp;a\\sqcap b\\le a \\b\\le a\\sqcup b &amp;&amp; a\\sqcap b\\le b \\ c\\le a,c\\le b\\implies c\\le a\\sqcap b &amp;&amp; a\\le c, b\\le c\\implies a\\sqcup b\\le c \\end{matrix} $$</p> <pre><code>title: distributive lattices\nA lattice which satisfies\n$(a\\sqcap b)\\sqcup c=(a\\sqcup c)\\sqcap(b\\sqcup c)$\n$(a\\sqcup b)\\sqcap c=(a\\sqcap c)\\sqcup(b\\sqcap c)$\nis called a **Distributive Lattice**\n</code></pre> <p>If a lattice has \\(0\\) and \\(1\\), \\(b\\) is said to be a complement of \\(a\\) if \\(a\\sqcap b=0\\) and \\(a\\sqcup b=1\\)</p> <p></p> <p>Heyting Algebra: </p>"},{"location":"Lattice/#references","title":"References","text":""},{"location":"Lebesgue%20Number%20Lemma/","title":"Lebesgue Number Lemma","text":"<p>202303012303</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Lebesgue%20Number%20Lemma/#lebesgue-number-lemma","title":"Lebesgue Number Lemma","text":"<pre><code>title: \nLet $\\mathcal{C}$ be an open cover of a compact metric space $(X,d)$. There exists a $\\delta&gt;0$ such that for each subset $B \\subset X$ of diameter $&lt; \\delta$, there exists an element $U \\in \\mathcal{C}$ with $B \\subset U$.\nThe number $\\delta$ is called a Lebesgue number of $\\mathcal{C}$.\n</code></pre>"},{"location":"Lebesgue%20Number%20Lemma/#proof","title":"Proof:","text":"<p>If \\(X \\in \\mathcal{C}\\) then we are done. Otherwise, since \\(X\\) is compact, there is a finite subcover \\(U_{1},\\dots,U_{n}\\) of \\(X\\). Now let \\(C_{i} := U_{i}^{c}\\).  Define  $$ f(x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} d(x,C_{i})  $$ Since \\(X\\) is compact, this achieves a minimum value \\(\\delta\\), we claim this is the required Lebesgue Number.</p> <p>Let \\(B\\) be a set with diameter \\(&lt;\\delta\\), then there is a point \\(p \\in B\\) such that \\(B \\subset B_{\\delta}(p)\\). But then \\(\\delta \\le f(p) \\le d(p,C_{m})\\) for some \\(m \\in \\{ 1,2,\\dots n \\}\\). This gives \\(B_{\\delta}(p) \\subset U_{m}\\).  </p>"},{"location":"Lebesgue%20Number%20Lemma/#related-problems","title":"Related Problems","text":""},{"location":"Lebesgue%20Number%20Lemma/#references","title":"References","text":""},{"location":"Lecture%20local%20fields/","title":"Lecture local fields","text":"<ol> <li>What is a discrete valuation on a field.  \\(v : K^{\\times} \\to \\mathbb{Z}\\) such that \\(v(a+b) \\ge \\min(v(a),v(b))\\) and \\(v(ab) = v(a)+v(b)\\).</li> <li>This can be extented to whole of \\(K\\) by defining \\(v(0) = \\infty\\). We follow the first definition in this lecture.</li> <li>The ring \\(A := \\{ x \\in K \\mid v(x) \\ge 0\\}\\) is called the valuation ring of \\(K\\). </li> <li>What is an absolute value or multiplicative valuation on a field. Archimedean, Non archimedean.</li> <li>Why is it called non archimedean?</li> <li>Show examples as in the notes. Define p-adic absolute value on \\(\\mathbb{Q}\\). </li> <li>Can go from a valuation to an absolute value and vice versa. \\(v(x) = -\\log(|x|)\\)</li> <li>Relation between absolute value and additive valuations:    A discrete (additive) valuation \\(\\mathrm{ord} : K^{\\times} \\to \\mathbb{Z}\\)  determines an absolute value by \\(|x| = e^{-\\mathrm{ord}(x)}\\)    An absolute value on \\(K\\) determines an additive valuation (need not be discrete) on \\(K^{\\times}\\) by \\(\\log_{e}|x| = -\\mathrm{ord}(x)\\)</li> <li>### Lemma:    Absolute value is non archimedean iff it takes bounded values on \\(\\{m \\cdot 1 | m \\in \\mathbb{Z} \\}\\)<ol> <li>Corollary: If \\(\\mathrm{char} K \\neq 0\\) then \\(K\\) has only non archimedean abs values.</li> </ol> </li> </ol>"},{"location":"Lecture%20local%20fields/#proposition","title":"Proposition:","text":"<pre><code>title:\nLet $\\mid \\cdot\\mid$ be a non trivial non archimedean absolute value, and let $v(x) = -\\log|x|$ (base $e &gt; 1$). Then $v : K^{\\times} \\to \\mathbb{R}$ satisfies:\n\n(a) $v(xy) =v(x) + v(y)$\n\n(b) $v(x+y) \\ge \\min(v(x),v(y))$\n\nIf $v(K^{\\times})$ is discrete in $\\mathbb{R}$, then $v$ is a multiple of a discrete valuation $\\mathrm{ord}$ on $K^{\\times} \\twoheadrightarrow \\mathbb{Z}$.\n</code></pre>"},{"location":"Lecture%20local%20fields/#proof","title":"Proof:","text":"<p>Note that \\(v(K^{\\times})\\) is a subgroup of \\(\\mathbb{R}\\) under addition, and is discrete and hence is a lattice and so, \\(v(K^{\\times}) = c \\cdot \\mathbb{Z}\\) for some \\(c\\). Now \\(\\mathrm{ord} := c^{-1} \\cdot v\\) is an additive discrete valuation on \\(K^{\\times} \\twoheadrightarrow \\mathbb{Z}\\).</p> <p>(there is a theorem that says a discrete subgroup of a real vector space is a lattice iff it is a discrete subgroup)</p> <ol> <li>Say that the absolute value is discrete if the image of \\(K^{\\times}\\) is a discrete subgroup of \\(\\mathbb{R}_{&gt;0}\\).</li> <li>Let \\(v\\) be a discrete valuation on \\(K\\), then $$ A := { a \\in K : v(a) \\geq 0 } $$ is a PID with the unique maximal ideal $$ \\mathfrak{m} := { a \\in K : v(a) &gt; 0 } $$</li> </ol>"},{"location":"Lecture%20local%20fields/#proof_1","title":"Proof:","text":"<p>Take an ideal \\(I\\) of \\(A\\), \\(I\\) does not contain any element with valuation 0, since they are units of A. So, take the element \\(a\\) with least valuation, this generates \\(I\\). any element with equal valuation as that of \\(a\\), is an associate of \\(a\\). Now if \\(v(b) = qv(a) + r\\) with \\(0&lt;r&lt;b\\), we get that \\(v(b a^{-q}) = r &gt; 0\\) hence \\(ba ^{-q} \\in A\\), but has smaller valuation than \\(a\\). Contradiction. So this is a PID.</p> <p>Easy to see that \\(\\mathfrak{m}\\) is the unique maximal ideal.</p> <ol> <li>### Proposition 7.6    ##### Proof:    If \\(\\mid\\cdot\\mid\\) is discrete, then take \\(\\pi \\in \\mathfrak{m}\\) such that \\(\\mid \\pi\\mid\\) is maximum. Then take any \\(\\pi_{1} \\in \\mathfrak{m}\\) and let \\(\\mid \\pi\\mid^{q+1} &lt; \\mid \\pi_{1}\\mid &lt; \\mid \\pi\\mid^{q}\\).    and so, \\(|\\pi|&lt;\\mid \\frac{\\pi_{1}}{\\pi^{q}}\\mid &lt; 1\\) and so, \\(\\frac{\\pi_{1}}{\\pi^{q}} \\in \\mathfrak{m}\\) and has larger absolute value than \\(\\pi\\). Contradiction. So, \\(|\\pi_{1}| = |\\pi|^{q}\\), this gives \\(\\pi_{1} = \\pi^{q} \\cdot u\\) where \\(u\\) is a unit in \\(A\\), hence \\(\\pi_{1} \\in (\\pi) \\implies \\mathfrak{m} \\subset (\\pi) \\implies \\mathfrak{m} = (\\pi)\\).</li> </ol> <p>If \\(\\mathfrak{m} = (\\pi)\\) then take any \\(\\alpha \\in K^{\\times}\\), let \\(|\\pi|^{q+1} \\leq |\\alpha| \\leq |\\pi|^{q}\\), then \\(\\mid\\frac{\\alpha}{\\pi^{q}}\\mid &lt; 1 \\implies \\frac{\\alpha}{\\pi^{q}} \\in (\\pi) \\implies \\alpha = \\pi^{q+1}\\beta\\) where \\(\\beta \\in A\\). This gives \\(|\\alpha| \\leq |\\pi|^{q+1}\\) This is a contradiction, hence \\(|\\alpha| = |\\pi|^{q}\\) for some \\(q \\in \\mathbb{Z}\\). Hence \\(\\mid K^{\\times}\\mid =(|\\pi|)\\).</p> <ol> <li>Topology induced by the p-adic absolute value is called the p-adic topology.</li> <li>Proposition 7.8</li> <li>Let \\(\\mathrm{ord}\\) be an additive valuation on \\(K\\), then \\(\\mathrm{ord}(a+b) \\ge\\min(\\mathrm{ord(a)},\\mathrm{ord(b)})\\) with equality if \\(\\mathrm{ord}(a) \\neq \\mathrm{ord}(b)\\).</li> <li>Similarly, \\(\\mid a+b\\mid \\le \\max(\\mid a\\mid,\\mid b\\mid)\\) with equality if \\(\\mid a\\mid \\neq \\mid b\\mid\\).</li> <li>If \\(x\\) is closer to \\(b\\) than it is to \\(a\\), then \\(d(x,a) = d(b,a)\\).$$ |b-a| = |b-x+x-a| = |x-a| $$</li> <li>Ostrowski's theorem <pre><code>title:\nLet $\\mid\\cdot\\mid$ be a non trivial absolute value on $\\mathbb{Q}$.\n(a) If $\\mid\\cdot\\mid$ is archimedean, then $\\mid\\cdot\\mid$ is equivalent to $\\mid\\cdot\\mid_{\\infty}$\n(b) Otherwise, it is equivalent to $\\mid\\cdot\\mid_{p}$ for some prime $p$.\n</code></pre></li> <li> <p>Lemma 7.18 <pre><code>title:\nIf $\\mid\\cdot\\mid_{1}, \\mid\\cdot\\mid_{2},\\dots,\\mid\\cdot\\mid_{n}$ are non trivial inequivalent absolute values of $K$, then there is an element $a \\in K$ such that $$\n\\begin{cases}\n\\mid a\\mid_{1}\\ &gt;1   \\\\\n\\mid a\\mid_{i}\\ &lt; 1, \\ i \\neq 1 \n\\end{cases}\n$$\n</code></pre></p> </li> <li> <p>Lemma 7.19</p> </li> <li>Theorem 7.20</li> <li>Theorem 7.23</li> <li>Absolute value on a field is continuous w.r.t. itself</li> <li>An absolute value preserving function between fields is continuous.</li> <li>COMPLETIONS IN NON ARCHIMEDEAN CASE     Let \\(\\mid \\cdot \\mid\\) be a discrete non archimedean absolute value on \\(K\\). (Why discrete? we are only interested in the discrete case since \\(\\mathbb{Q}_{p}\\) has a discrete absolute value)     This gives that \\(\\mathfrak{m} = (\\pi)\\) for some \\(\\pi \\in K\\) with largest value \\(&lt;1\\).     The set of values \\(\\mid K\\mid = \\{ c ^{m} | m \\in \\mathbb{Z}\\} \\cup \\{ 0 \\}\\), \\(c = \\mid \\pi\\mid\\).</li> <li>What does this AV look like on \\(\\widehat{K}\\)?      Let \\(a \\in \\widehat{K}\\) and let \\(a_{n} \\to a\\), Then \\(\\mid a_{n}\\mid \\to \\mid a\\mid\\), so \\(\\mid a\\mid\\) is a limit point of \\(\\mid K ^{\\times}\\mid\\) and so \\(\\mid a\\mid = 0\\) or \\(\\mid a\\mid \\in |K ^{ \\times}|\\), but \\(\\mid a\\mid = 0 \\implies a = 0\\), hence \\(\\mid \\widehat{K} \\mid = \\mid K \\mid\\).     So \\(\\mid \\cdot \\mid\\) is discrete on \\(\\widehat{K}\\) also.</li> <li>Let \\(\\mathrm{ord}\\) be a normalised discrete valuation on \\(K ^{ \\times}\\) corresponding to \\(\\mid \\cdot\\mid\\) then it extends to a normalised discrete val on \\(\\hat{K}\\).</li> <li>Given \\(a_{n} \\to a\\), \\(|a_{n}| \\to |a|\\) but \\(\\mid \\cdot \\mid\\) being discrete, \\(|a_{n}| = |a|\\) for large enough \\(n\\).</li> <li>\\(\\hat{A}\\) is the closure of \\(A\\). Since any cauchy sequence in \\(A\\) has its limit point in \\(\\hat{A}\\). Any point in \\(\\hat{A}\\) has a cauchy sequence in \\(A\\) converging to itself, \\(a_{n} \\to a\\), but \\(|a_{n}| = |a|\\) for large \\(n\\). and so, \\(\\hat{A} \\subset Cl(A)\\)</li> <li>\\(\\hat{\\mathfrak{m}} = Cl(\\mathfrak{m})\\) </li> <li>For every \\(n\\), \\(\\(\\frac{A}{\\mathfrak{m}^{n}} \\to \\frac{\\hat{A}}{\\hat{\\mathfrak{m}}^{n}}\\)\\)     Is an isomorphism.</li> </ol>"},{"location":"Lecture%20local%20fields/#proof_2","title":"Proof:","text":"<p>The function is \\(f(a+\\mathfrak{m}^{n}) = a + \\hat{\\mathfrak{m}}^{n}\\). Note that \\(\\mathfrak{m}^{n} = \\{ a \\in A \\mid \\ \\mid a\\mid \\le \\mid \\pi\\mid^{n} \\} = \\{ a \\in A \\mid \\ \\mid a\\mid &lt; \\mid \\pi\\mid^{n-1} \\}\\) is open and closed as a subset of \\(A\\).  Now \\(\\ker(f) = \\{ a + \\mathfrak{m}^{n} \\mid a \\in A, a \\in \\hat{\\mathfrak{m}}^{n} \\}\\) Since \\(\\mathfrak{m}\\) is dense in \\(\\hat{\\mathfrak{m}}\\), \\(\\mathfrak{m}^{n}\\) is dense in \\(\\hat{\\mathfrak{m}}^{n}\\) (\\(\\mathfrak{m}^{n} = (\\pi) \\subset A\\), and \\(\\hat{\\mathfrak{m}}^{n} = (\\pi) \\subset \\hat{A}\\)). Since \\(a \\in A\\) and \\(a\\) is a limit point of \\(\\mathfrak{m}^{n}\\), it must be in \\(\\mathfrak{m}^{n}\\) since it is closed in \\(A\\). This gives \\(\\ker(f) = \\{ 0 \\}\\).</p> <p>Now, let \\(\\hat{a} \\in \\hat{A}\\), \\(\\lim_{ n \\to \\infty }a_{n} = \\hat{a}\\), This gives \\(a_{k} \\in \\hat{a} + \\hat{\\mathfrak{m}}^{n}\\) for all large \\(k\\). (Since \\(\\hat{\\mathfrak{m}}^{n}\\) is an open set around 0). Hence, \\(f(a_{k} + \\mathfrak{m}^{n}) = a_{k} + \\hat{\\mathfrak{m}}^{n} = \\hat{a} + \\hat{\\mathfrak{m}}^{n}\\). Hence, \\(f\\) is surjective</p> <ol> <li> <p>Choose a set \\(S\\) of representatives of \\(\\frac{A}{\\mathfrak{m}}\\), and let \\(\\pi\\) generate \\(\\mathfrak{m}\\). Then each element of \\(\\hat{K}\\) is expressible as a cauchy series of the form $$ a_{-n}\\pi^{-n} + \\dots + a_{0} + a_{1}\\pi + a_{2}\\pi^{2}\\dots,  a_{i}\\in S $$ and such a representation is unique.</p> </li> <li> <p>Let \\(K = \\mathbb{Q}\\), then \\(\\hat{K} = \\mathbb{Q}_{p}\\), \\(A = \\left\\{  \\frac{a}{b} \\in \\mathbb{Q} \\mid p \\nmid b, (a,b) = 1  \\right\\}\\), \\(\\mathfrak{m} = \\left\\{  \\frac{a}{b} \\in \\mathbb{Q} \\mid p |a; (a,b) = 1  \\right\\}\\)</p> </li> <li>Then the representatives for \\(A/\\mathfrak{m}\\) are just \\(0,1,\\dots p-1\\)</li> <li>In other words, \\(A / \\mathfrak{m} \\cong \\mathbb{Z}/p\\mathbb{Z}\\).</li> <li>\\(A = \\mathbb{Z}_{(p)}\\), \\(\\mathbb{Z}_{p} := \\hat{A}\\). So, \\(\\mathbb{Z}_{p}\\) is the elements of \\(\\mathbb{Q}_{p}\\) with series expansion starting from non negative integers.</li> <li>The maps \\(\\mathbb{Z} /(p ^{n}) \\to \\mathbb{Z}_(p) /(p ^{n}) \\to \\mathbb{Z}_{p} /(p ^{n})\\), which just takes \\(a + (p ^{n}) \\mapsto a + (p ^{n}) \\mapsto a + (p ^{n})\\) are bijections.</li> <li>Let \\(\\alpha \\in \\mathbb{Z}_{p}\\), because the map is bijective, for all \\(n\\), there is an \\(a_{n}\\), such that \\(\\alpha \\equiv a_{n} (p ^{n})\\).</li> <li>If \\(n &lt; m\\), \\(a_{n} \\equiv a_{m} (p ^{n})\\), hence \\((a_{n})\\) is a cauchy sequence. Let \\(a_{n} = c_{0} + c_{1}p + \\dots c_{n-1}p ^{n-1} (\\mathrm{mod} \\ p^n)\\), \\(0\\le c_i&lt;p-1\\)</li> <li>Then \\(\\alpha = \\sum\\limits_{ i=0}^{ \\infty }c_{i}p ^{i}\\)</li> <li>Conversely, If then \\(\\alpha = \\sum\\limits_{ i=0}^{ \\infty }c_{i}p ^{i}\\), \\(0 \\le c &lt; p-1\\), then \\(c_{0},c_{1},\\dots\\) is the unique sequence of integers such that      \\(\\alpha \\equiv \\sum\\limits_{ i=0}^{ n-1 }c_{i}p ^{i}\\ \\mathrm{mod} \\ p^n\\)</li> <li>So basically, elements of \\(\\mathbb{Z}_{p}\\) have \"compatibility\" among the coefficients, and any truncation of the series gives mod p^n of that number</li> <li>Any element \\(\\alpha \\in \\mathbb{Q}_{p}\\), can multiply it by high power of \\(p\\) and bring it in \\(\\mathbb{Z}_{p}\\). hence, coefficients of \\(\\mathbb{Q}_{p}\\) elements also have compatibility.</li> <li>Example 7.27</li> <li>Show another definition of \\(\\mathbb{Z}_{p}\\), which is made by sequences where the nth term is from \\(\\mathbb{Z}/p ^{n}\\mathbb{Z}\\).</li> <li>Show that each such sequence is the same as a cauchy series in our previous definition and vice versa.</li> <li>Let \\(f(X) \\in A[X]\\) and let \\(a_{0}\\) be a simple (not multiple) root of \\(f(X)\\) mod \\(\\pi\\). Then there is a unique root \\(a \\in A\\) of \\(f(X)\\) with \\(a \\equiv a_{0} \\ \\mathrm{mod} \\pi\\)</li> <li>Hensel's lemma</li> <li>Use nakayama lemma for local rings</li> <li></li> </ol>"},{"location":"Legendre%20Symbol/","title":"Legendre Symbol","text":"<p>202305271605</p> <p>Type : #Note Tags : [[Number Theory]]</p>"},{"location":"Legendre%20Symbol/#legendre-symbol","title":"Legendre Symbol","text":""},{"location":"Legendre%20Symbol/#references","title":"References","text":""},{"location":"Lemma-Number%20of%20isomorphism%20classes%20on%20graphs/","title":"Lemma Number of isomorphism classes on graphs","text":"<p>202308210142</p> <p>type : #Example tags : [[Algebraic Graph Theory]]</p>"},{"location":"Lemma-Number%20of%20isomorphism%20classes%20on%20graphs/#lemma-number-of-isomorphism-classes-on-graphs","title":"Lemma-Number of isomorphism classes on graphs","text":""},{"location":"Lemma-Number%20of%20isomorphism%20classes%20on%20graphs/#lemma","title":"Lemma:","text":"<p>Lemma: The actual number of isomorphism classes on graphs are  \\(\\((1+o(1))\\frac{2^{n\\choose 2}}{n!}\\)\\) ^8f4dba</p>"},{"location":"Lemma-Number%20of%20isomorphism%20classes%20on%20graphs/#proof","title":"Proof:","text":"<p>Let Support be the set of points that are not fixed by a permutation. If the support is the  even number \\(2r\\) then the maximum number of orbits are given by \\(r\\) cycles of length \\(2\\). Looking at the effect of these permutations on edges. there are \\(2\\) ways an edge can not be fixed by the permutation. First is if both the edges of the edge are in the support and the second is if exactly of the ends are in the support. Given that there are \\(2r\\) vertices that in the support, the number of edges that are not fixed are  $$ r(n-r-1) $$ Hence the total number of orbits of a permutation \\(g\\) on the edges are $$ \\text{orb}_2(g)={n\\choose 2}-r(n-r-1) $$ Now we fix \\(m\\le n-2\\) and partition the permutation of vertices into \\(3\\) categories. The first one being identity, the second one being where the number of fixed points \\(\\le m\\) We can estimate the size of the partitions as follows  $$ \\begin{align} |\\mathcal C_{1}|&amp;= 1 \\ |\\mathcal C_{2}|&amp;\\le {n\\choose m}m!\\le n^{m} \\ |\\mathcal C_{3}|&amp;\\le n! \\le n^{m}\\ \\end{align} $$ A permutation belonging to \\(\\mathcal C_{2}\\) has at most \\({n\\choose 2}-{n-m \\choose 2}\\) such orbits A permutation belonging to \\(\\mathcal C_{3}\\) as atleast \\(m\\) points in the support, so it has maximum number of orbits if it has \\(\\frac{m}{2}\\) cycles in which case it has less than  { width=\"300\" } many orbits Hence   Where the sum of last to terms is \\(o(1)\\) </p>"},{"location":"Lemma-Number%20of%20isomorphism%20classes%20on%20graphs/#related","title":"Related","text":""},{"location":"Lifted%20Reed%20Solomon%20Codes/","title":"Lifted Reed Solomon Codes","text":"<p>202310191610</p> <p>Tags : Algorithmic Coding Theory</p>","tags":["Note","Incomplete"]},{"location":"Lifted%20Reed%20Solomon%20Codes/#lifted-reed-solomon-codes","title":"Lifted Reed Solomon Codes","text":"<pre><code>title: Motivation\nUsing RM codes we can have constant distance and $n^{\\beta}$ query complexity codes, but the rate has to be less than $1/2$, once $m&gt;1$. We would like to achieve higher rates because that would mean less redundancy. Hence, lifted RS codes.\n</code></pre> <p>\\(\\{f:\\mathbb{F}_{q}^{n}\\to \\mathbb{F}_{q}\\ |\\ \\text{For every line }L=\\{ \\vec{a}+\\vec{b}t \\}, f|_{L}\\text{ agrees with a poly of degree at most }d\\}\\)</p> <p>Any codeword \\(f\\) of RM (\\(m,d,\\mathbb{F}_{q}\\)) code is also a codeword of lifted RS. But it is not immediate why there are more codewords in lifted RS.</p> <p>\\(m=2\\), \\(\\mathbb{F}_{q}\\) of characteristic \\(2\\) Which functions \\(f:\\mathbb{F}_{q}^{2}\\to \\mathbb{F}_{q}\\) lie in the lifted RS code?</p>","tags":["Note","Incomplete"]},{"location":"Lifted%20Reed%20Solomon%20Codes/#references","title":"References","text":"<p>Reed-Solomon Codes</p>","tags":["Note","Incomplete"]},{"location":"Lifting%20Correspondence/","title":"Lifting Correspondence","text":"<p>202303281003</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Lifting%20Correspondence/#lifting-correspondence","title":"Lifting Correspondence","text":""},{"location":"Lifting%20Correspondence/#theorem","title":"Theorem:","text":"<pre><code>title:\nLet $p: E\\to B$ a covering map. $f : [0,1]\\to B$ be a path with $f(0)=b$. Let $e \\in p ^{-1}(b)$, then there exists a unique lift $\\widetilde{f}:[0,1]\\to E$ of the path $f$ such that $\\widetilde{f}(0)=e$.\n</code></pre>"},{"location":"Lifting%20Correspondence/#proof","title":"Proof:","text":"<p>\\(\\exists\\) an open covering \\(\\{ U_{\\alpha} \\}_{\\alpha \\in J}\\) of \\(B\\) such that each \\(U_{\\alpha}\\) is evenly covered by \\(p\\). Preimages \\(\\{ f ^{-1}(U_{\\alpha}) \\}_{\\alpha \\in J}\\) yield an open covering of \\([0,1] \\implies\\) Lebesgue Number lemma \\(\\implies \\ \\delta&gt;0\\) s.t. for all \\(x\\), we have \\((x,x+\\delta) \\subset f ^{-1}(U_{\\alpha}) \\implies\\) We can find a finite subdivision \\(0 = \\delta_{0} &lt;  \\delta_{1} &lt; \\dots &lt; \\delta_{n} = 1\\) such that each \\(f([s_{i},s_{i+1}])\\) lies in one of the \\(U_{\\alpha}\\).  Define \\(\\widetilde{f}(0) = \\widetilde{f}(s_{0}):=e\\) Assume we have defined \\(\\widetilde{f}(s)\\) for \\(s \\in [0,s_{i}]\\). Define \\(\\widetilde{f}(s)\\) for \\(s \\in [s_{i},s_{i+1}]\\) as follows. We know \\(f([s_{i},s_{i+1}]) \\subset U\\) for some U that is evenly covered by \\(p\\).</p> <p>\\(p ^{-1}(U) = \\bigsqcup_{ i \\in I} V_{i}\\). Choose \\(V = V_{i}\\) which contains \\(\\widetilde{f}(s_{i})\\). Then \\(p\\mid_{V} : V \\to U\\) is a homeomorphism, it has a continuous inverse \\(\\implies \\widetilde{f}(s) := p\\mid_{V} ^{-1}(f(s)), \\ s \\in [s_{i},s_{i+1}]\\) \\(\\implies\\) repeated finitely many times \\(\\widetilde{f}:[0,1]\\to E\\) \\(\\implies\\) Uniqueness follows.</p>"},{"location":"Lifting%20Correspondence/#theorem_1","title":"Theorem:","text":"<pre><code>title:\nLet $p: E\\to B$ a covering map. $F : [0,1] \\times [0,1]\\to B$ be a cts map with $F((0,0))=b$. Let $e \\in p ^{-1}(b)$, then there exists a unique lift $\\widetilde{F}:[0,1]\\times [0,1]\\to E$ such that $\\widetilde{F}((0,0))=e$.\n\nIf $F$ is a path homotopy, then $\\widetilde{F}$ is a path homotopy.\n</code></pre>"},{"location":"Lifting%20Correspondence/#proof_1","title":"Proof:","text":"<p>Similar idea as before for the first part. Second part, if \\(F\\) is a path homotopy, then it carries \\(\\{ 0 \\}\\times [0,1]\\) into a single point \\(b_{0} \\in B\\). \\(\\widetilde{F}\\) is a lifting \\(\\implies\\) this edge is carried by \\(\\widetilde{F}\\) to \\(p ^{-1}(b_{0})\\) - Discrete topology.</p> <p>Since \\(\\{ 0 \\}\\times[0,1]\\) is connected, \\(\\widetilde{F}(\\{ 0 \\}\\times [0,1])\\) is a singleton set. Same with \\(\\{ 1 \\} \\times [0,1]\\) implies that \\(\\widetilde{F}\\) fixes endpoints for all \\(t\\) is \\(F\\) does.</p>"},{"location":"Lifting%20Correspondence/#remark-loops-dont-always-go-to-loops","title":"Remark: Loops don't always go to loops","text":"<p>Example: \\(f:[0,1]\\to S ^{1}\\) \\(f(s) = (\\cos(2\\pi s),\\sin 2\\pi s)\\)</p>"},{"location":"Lifting%20Correspondence/#given-a-starting-point-e_0-in-p-1b_0-there-is-a-uniquely-determined-endpoint-of-any-lift","title":"Given a starting point \\(e_{0} \\in p ^{-1}(b_{0})\\), there is a uniquely determined endpoint of any lift.","text":"\\[ \\varphi : \\Pi_{1}(B,b_{0}) \\to p ^{-1}(b_{0}) \\] \\[ \\varphi ([f]) = \\widetilde{f}(1), \\ \\widetilde{f}(0) = e_{0} \\]"},{"location":"Lifting%20Correspondence/#theorem_2","title":"Theorem:","text":"<pre><code>title:\nIf $E$ is path connected, $\\varphi$ is surjective. If $E$ is simply connected, then it is bijective.\n</code></pre>"},{"location":"Lifting%20Correspondence/#proof_2","title":"Proof:","text":"<p>If \\(E\\) is path connected, for any \\(e_{1} \\in p ^{-1}(b_{0})\\), \\(\\exists\\) a path \\(g\\) from \\(e_{0}\\) to \\(e_{1}\\). Then \\(f := p \\circ g\\) is a loop in \\(B\\) based at \\(e_{0}\\), and \\(\\varphi([f])=e_{1}\\) by definition.</p> <p>If \\(E\\) is simply connected, take \\([f]\\) ; \\([g]\\) s.t. \\(\\varphi([f]) = \\varphi([g])\\). \\(\\widetilde{f},\\widetilde{g}\\) unique liftings beginning at \\(e_{0}\\). Then \\(\\widetilde{f}(1) = \\widetilde{g}(1) \\implies \\widetilde{f} = \\widetilde{g}\\) are path homotopic with homotopy \\(\\widetilde{F}(s,t)\\). Then \\(F(s,t) = p \\circ \\widetilde{F}(s,t)\\) is a path homotopy between \\(f,g\\).</p> <p>Since simply connected are path connected by definition, \\(\\varphi\\) is surjective.</p>"},{"location":"Lifting%20Correspondence/#theorem_3","title":"Theorem:","text":"<pre><code>title:\n$\\Pi_{1}(S ^{1}) = \\mathbb{Z}$ as a group.\n</code></pre>"},{"location":"Lifting%20Correspondence/#proof_3","title":"Proof:","text":"<p>Need to show \\(\\varphi([f]*[g]) = \\varphi([f])*\\varphi([g])\\) \\(\\widetilde{f}(1) = n, \\widetilde{g}(1)=m\\) \\(h(s)\\)</p>"},{"location":"Lifting%20Correspondence/#references","title":"References","text":""},{"location":"Lifts/","title":"Lifts","text":"<p>202304040004</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Lifts/#lifts","title":"Lifts","text":"<pre><code>title:\nGiven $p : E \\to B$ a cts map, a _lifting_ of a cts map $f : X \\to B$ is a map $\\widetilde{f}:X \\to E$ such that $p \\circ \\widetilde{f} = f$.\n</code></pre> <p>We can always locally lift \\(f\\), that means we can restrict \\(f\\) to a subspace of \\(Y\\) of\\(X\\) so that \\(f(Y) \\subset U \\subset X\\) where \\(U\\) is evenly covered by \\(p\\), then we can lift \\(f\\) to one of the slices in \\(E\\) above \\(U\\).</p>"},{"location":"Lifts/#theorem-lifting-of-paths","title":"Theorem (Lifting of paths):","text":"<p>```ad-note: title: Let \\(p: E \\to B\\) be a covering map. Let \\(f : [0,1] \\to B\\) be a path with \\(f(0) = b\\). Let \\(e \\in p ^{-1}(b)\\), then there is a unique lift \\(\\widetilde{f}:[0,1] \\to E\\) of the path \\(f\\) such that \\(\\widetilde{f}(0)=e\\). <pre><code>##### Proof:\n$\\exists$ an open covering $\\{ U_{\\alpha} \\}_{\\alpha \\in J}$ of $B$ such that each $U_{\\alpha}$ is evenly covered by $p$.\nPreimages $\\{ f ^{-1}(U_{\\alpha}) \\}_{\\alpha \\in J}$ yield an open covering of $[0,1] \\implies$ Lebesgue Number lemma $\\implies \\ \\delta&gt;0$ s.t. for all $x$, we have $(x,x+\\delta) \\subset f ^{-1}(U_{\\alpha}) \\implies$ We can find a finite subdivision $0 = \\delta_{0} &lt;  \\delta_{1} &lt; \\dots &lt; \\delta_{n} = 1$ such that each $f([s_{i},s_{i+1}])$ lies in one of the $U_{\\alpha}$. \nDefine $\\widetilde{f}(0) = \\widetilde{f}(s_{0}):=e$\nAssume we have defined $\\widetilde{f}(s)$ for $s \\in [0,s_{i}]$. Define $\\widetilde{f}(s)$ for $s \\in [s_{i},s_{i+1}]$ as follows.\nWe know $f([s_{i},s_{i+1}]) \\subset U$ for some U that is evenly covered by $p$.\n\n$p ^{-1}(U) = \\bigsqcup_{ i \\in I} V_{i}$. Choose $V = V_{i}$ which contains $\\widetilde{f}(s_{i})$. Then $p\\mid_{V} : V \\to U$ is a homeomorphism, it has a continuous inverse $\\implies \\widetilde{f}(s) := p\\mid_{V} ^{-1}(f(s)), \\ s \\in [s_{i},s_{i+1}]$\n$\\implies$ repeated finitely many times $\\widetilde{f}:[0,1]\\to E$\n$\\implies$ Uniqueness follows since in each step, we only had a unique choice for the extension.\n\n### Theorem(Lifting of homotopies):\n```ad-note\ntitle:\nLet $p: E\\to B$ a covering map. $F : [0,1] \\times [0,1]\\to B$ be a cts map with $F((0,0))=b$. Let $e \\in p ^{-1}(b)$, then there exists a unique lift $\\widetilde{F}:[0,1]\\times [0,1]\\to E$ such that $\\widetilde{F}((0,0))=e$.\n\nIf $F$ is a path homotopy, then $\\widetilde{F}$ is a path homotopy.\n</code></pre></p>"},{"location":"Lifts/#proof","title":"Proof:","text":"<p>Similar idea as before for the first part. (Partition \\(I \\times I\\) into squares)</p> <p>Second part, if \\(F\\) is a path homotopy, then it carries \\(\\{ 0 \\}\\times [0,1]\\) into a single point \\(b_{0} \\in B\\). \\(\\widetilde{F}\\) is a lifting \\(\\implies\\) this edge is carried by \\(\\widetilde{F}\\) to \\(p ^{-1}(b_{0})\\) - Discrete topology.</p> <p>Since \\(\\{ 0 \\}\\times[0,1]\\) is connected, \\(\\widetilde{F}(\\{ 0 \\}\\times [0,1])\\) is a singleton set. Same with \\(\\{ 1 \\} \\times [0,1]\\) implies that \\(\\widetilde{F}\\) fixes endpoints for all \\(t\\) as \\(F\\) does.</p>"},{"location":"Lifts/#note-loops-dont-always-lift-to-loops-see-example-1","title":"NOTE: Loops don't always lift to loops (See example 1)","text":""},{"location":"Lifts/#definition","title":"Definition:","text":"<p><pre><code>title:\nGiven a covering map $p : E \\to B$ and $b_0 \\in B$. Let $e_0 \\in p^{-1}(b_0)$.\nGiven a starting point $e_{0}$, there is a uniquely determined endpoint of any lift.\n\n$$\n\\varphi : \\Pi_{1}(B,b_{0}) \\to p ^{-1}(b_{0})\n$$\n\n$$\n\\varphi ([f]) = \\widetilde{f}(1), \\ \\widetilde{f}(0) = e_{0}\n$$\n</code></pre> This function is well defined because if \\(f \\simeq_{p} g\\) as loops in \\((B,b_{0})\\) then the homotopy \\(F\\) between them lifts to a homotopy \\(\\widetilde{F}\\) between \\(\\widetilde{f}, \\widetilde{g}\\), and thus, \\(\\widetilde{f}(1) = \\widetilde{g}(1)\\).</p> <p>This function \\(\\varphi\\) is called the lifting correspondence derived from the covering map \\(p\\) (this depends on the choice of \\(e_{0}\\)).</p>"},{"location":"Lifts/#theorem","title":"Theorem:","text":"<pre><code>title:\nIf $E$ is path connected, $\\varphi$ is surjective. If $E$ is simply connected, then it is bijective.\n</code></pre>"},{"location":"Lifts/#proof_1","title":"Proof:","text":"<p>If \\(E\\) is path connected, for any \\(e_{1} \\in p ^{-1}(b_{0})\\), \\(\\exists\\) a path \\(g\\) from \\(e_{0}\\) to \\(e_{1}\\). Then \\(f := p \\circ g\\) is a loop in \\(B\\) based at \\(e_{0}\\), and \\(\\varphi([f])=e_{1}\\) by definition.</p> <p>If \\(E\\) is simply connected, take \\([f]\\) ; \\([g]\\) s.t. \\(\\varphi([f]) = \\varphi([g])\\). \\(\\widetilde{f},\\widetilde{g}\\) unique liftings beginning at \\(e_{0}\\). Then \\(\\widetilde{f}(1) = \\widetilde{g}(1) \\implies \\widetilde{f}, \\widetilde{g}\\) are path homotopic with homotopy \\(\\widetilde{F}(s,t)\\). Then \\(F(s,t) = p \\circ \\widetilde{F}(s,t)\\) is a path homotopy between \\(f,g\\). Hence \\(\\varphi\\) is injective as well as surjective (simply connected is path connected by definition).</p>"},{"location":"Lifts/#theorem-lifting-of-any-cts-function","title":"Theorem (Lifting of any cts function):","text":"<pre><code>title:\nGiven a connected space $Y$ and a map $f : Y \\to X$, let $p : E \\to X$ be a covering map. Then if for two lifts $\\widetilde{f_{1}}$ and $\\widetilde{f_{2}}$ of $f$, $\\widetilde{f_{1}}(y_{0}) = \\widetilde{f_{2}}(y_{0})$ for some $y_{0} \\in Y$, then $\\widetilde{f_{1}} = \\widetilde{f_{2}}$. (Path and Homotopy lifting is a special case of this.)\n</code></pre>"},{"location":"Lifts/#proof_2","title":"Proof:","text":"<p>Consider the diagram: <pre><code>\\usepackage{tikz-cd}\n\\begin{document}\n\\begin{tikzcd}\nY\n\\arrow[rrd,\"\\tilde{f_2}\",bend left]\n\\arrow[rdd, \"\\tilde{f_1}\", bend right]\n\\arrow[rd,\"! \\ g\", dashed] \n&amp; &amp; \\\\\n&amp; E \\times_X E\n\\arrow[r,\"\\pi_2\"] \n\\arrow[d,\"\\pi_1\"] \n&amp; E \n\\arrow[d,\"p\"]\n\\\\\n&amp; E \n\\arrow[r,\"p\"] \n&amp; X\n\\end{tikzcd}\n\\end{document}\n</code></pre></p> <p>Since \\(p \\circ \\widetilde{f_{1}} = p \\circ \\widetilde{f_{2}}\\) \\(\\exists ! \\ g\\) s.t.  \\(\\pi_{1}\\circ g = \\widetilde{f_{1}}\\) and \\(\\pi_{2}\\circ g = \\widetilde{f_{2}}\\) but \\(Y\\) is connected so \\(g(Y)\\) will be connected, also there exists \\(y_{0} \\in Y\\) s.t. \\(g(y_{0}) = (y_{0},y_{0}) \\in \\Delta(E)\\) and since \\(\\Delta(E)\\) is a connected component of \\(E \\times_{X} E\\), we get that \\(g(Y) \\subseteq \\Delta(E)\\) and so, \\(\\pi_{1}\\circ g = \\pi_{2} \\circ g\\), and hence \\(\\widetilde{f_{1}} = \\widetilde{f_{2}}\\).</p>"},{"location":"Lifts/#note-here-deltae-ee-e-in-e-cap-e-times_x-e-we-can-show-that-this-is-a-clopen-subset-of-e-times_x-e","title":"NOTE: Here \\(\\Delta(E) = \\{ (e,e)  | e \\in E\\} \\cap E \\times_{X} E\\). (We can show that this is a clopen subset of \\(E \\times_{X} E\\))","text":""},{"location":"Lifts/#examples","title":"Examples","text":"<ol> <li>Consider \\(f(s) = (\\cos 2\\pi s,\\sin 2\\pi s)\\), \\(s \\in [0,1]\\), this has many lifts to \\(\\mathbb{R}\\) corresponding to each interval \\([n,n+1] \\subset \\mathbb{R}\\), if 0 is mapped to \\(n\\) in \\(\\mathbb{R}\\).</li> <li></li> </ol>"},{"location":"Lifts/#references","title":"References","text":"<p>Homotopy of paths Lebesgue Number Lemma Connectedness Path Connectedness Simply Connected</p>"},{"location":"Lindelof%20Space/","title":"Lindelof Space","text":"<p>202302131502</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Lindelof%20Space/#lindelof-space","title":"Lindelof Space","text":"<pre><code>title:\n$X$ is said to be _lindelof_ if every open cover of $X$ has a finite subcover.\n</code></pre>"},{"location":"Lindelof%20Space/#examples","title":"Examples","text":"<ol> <li>\\(\\mathbb{R}_{\\ell}\\)</li> <li>Any compact space.</li> <li>\\(\\mathbb{R}^{n}\\)</li> <li>Any 2nd countable space.</li> <li></li> </ol>"},{"location":"Lindelof%20Space/#related-results","title":"Related Results","text":"<p>1) Regular + Lindelof \\(\\implies\\) Normal 2) Metrizable + Lindelof \\(\\implies\\) 2nd Countable  3) 2nd countable \\(\\implies\\) Lindelof</p>"},{"location":"Lindelof%20Space/#related-problems","title":"Related Problems","text":"<p>Compactness</p>"},{"location":"Lindelof%20Space/#references","title":"References","text":""},{"location":"Line%20Graph/","title":"Line Graph","text":"<p>202308140108</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Line%20Graph/#line-graph","title":"Line Graph","text":"<p>Given a Graph \\(G\\), A line graph \\(L(G)\\) is a graph such that for edge in \\(G\\) there is a corresponding node in \\(L(G)\\) and if two edges are adjacent in \\(G\\) then the corresponding notes have an edge connecting them.</p> <p>{ width=\"200\" }</p> <p>Claim: If \\(G_{1}\\cong G_{2}\\) then \\(L(G_{1})\\cong L(G_{2})\\) </p>"},{"location":"Line%20Graph/#theorem","title":"Theorem","text":"<p>Let \\(K\\) be a simple graph. The  \\(\\exists G\\) such that \\(L(G)=K\\) iff for every vertex \\(v\\) of \\(K\\), \\(v\\) is in atmost \\(2\\) maximal cliques</p>"},{"location":"Line%20Graph/#references","title":"References","text":"<p>Incidence Matrix</p>"},{"location":"Line%20Integral/","title":"Line Integral","text":"<p>202210141110</p> <p>Type : #Note Tags : [[Calc]]</p>"},{"location":"Line%20Integral/#line-integral","title":"Line Integral","text":"<p>A parameterized \\(C^1\\) path from \\(A\\) to \\(B\\) is a \\(C^1\\) map \\(\\vec \\gamma:[a, b]\\to V\\) such that \\(\\vec\\gamma(a) = A, \\vec\\gamma(b) = B\\) and \\(A,B\\in V\\) which is a vector space. Given a vector field \\(V\\) over \\(U\\) and a parameterized path \\(\\vec\\gamma\\) from \\(A\\to B\\) , we define the line integral of \\(\\vec v\\) along \\(\\vec\\gamma\\) as $$ \\int_a^b\\vec v(\\vec\\gamma(t))\\cdot\\vec\\gamma'(t)dt $$</p> <p></p>"},{"location":"Line%20Integral/#related-problems","title":"Related Problems","text":""},{"location":"Line%20Integral/#references","title":"References","text":""},{"location":"Linear%20Programming/","title":"Linear Programming","text":"<p>Tags : [[Advanced Algorithms]]</p>","tags":["Note"]},{"location":"Linear%20Programming/#linear-programming","title":"Linear Programming","text":"<p>Linear objective function: Minimise \\(c_1x_1 + \\dots + c_nx_n = c^Tx\\) subject to linear constraints:     \\(a_{11}x_1 + \\dots a_{1n}x_n \\geq b_1\\)     .     .     .     \\(a_{m1}x_1 + \\dots a_{mn}x_n \\geq b_m\\) \\(x_i \\geq 0\\)</p> <p>Variables: \\(x_i\\) Every solution \\(x\\) is a point in \\(\\mathbb{R}^n\\).</p>","tags":["Note"]},{"location":"Linear%20Programming/#properties","title":"Properties","text":"<ul> <li>The feasible region is a convex polyhedron in \\(\\mathbb{R}^n\\).</li> <li>The optimum is always attained at a vertex/extreme point of the feasible region.</li> <li>Linear programs can be solved in polytime.</li> </ul>","tags":["Note"]},{"location":"Linear%20Programming/#references","title":"References","text":"<p>Integrality Gap (Weighted) Vertex Cover using LP Uncapacitated Facility location</p>","tags":["Note"]},{"location":"Linear%20Temporal%20Logic/","title":"Linear Temporal Logic","text":"<p>202311171411</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"Linear%20Temporal%20Logic/#linear-temporal-logic","title":"Linear Temporal Logic","text":"<p>[!hint] Motivation Classical Logic is really good for modelling structures that are static. That is, they do not change over time. But most real work structures are dynamic. They do change over time. Temporal Logics introduce operators that describe how the world changes without explicitly referring to time. This helps us guarantee properties like - Safety: Anything bad will not happen - Liveness: Something good will happen - Fairness: Evolution of subsystems</p> <p>Linear Temporal Logic introduces operators with the assumption that time is Linear. There is one execution path of the \"world\" that is being talked about evolves on a given execution paths. </p>","tags":["Note","Incomplete"]},{"location":"Linear%20Temporal%20Logic/#syntax","title":"Syntax","text":"<p>Linear Temporal Logic is defined relative to a set of primitive/atomic propositions \\(\\mathcal P\\).</p> <p>The set of formulas in Linear Temporal Logic can be given by the grammar $$ \\begin{align} \\varphi\\quad:=\\quad p\\;&amp;|\\;\\varphi\\lor\\varphi\\;|\\;\\varphi\\land\\varphi\\;|\\;\\lnot \\varphi\\;|\\;\\varphi \\Rightarrow \\varphi \\ &amp;|\\;\\square \\varphi\\;|\\;\\diamond\\varphi\\;|\\;\\bigcirc \\varphi\\;|\\;\\varphi \\cup\\varphi \\end{align} $$ Where the last \\(4\\) operators are temporal operators which are supposed to be interpreted as  - \\(\\bigcirc f\\) Next \\(f\\): \\(f\\) is true in the next state - \\(\\square f\\) Henceforth \\(f\\): \\(f\\) will be true in all future states  - \\(\\diamond f\\) Eventually \\(f\\): \\(f\\) is true at some future state - \\(f\\cup g\\) \\(f\\) Until \\(g\\): \\(g\\) will be true in some state in the future, \\(f\\) is true in each state until then  </p>","tags":["Note","Incomplete"]},{"location":"Linear%20Temporal%20Logic/#references","title":"References","text":"<p>Probabilistic Semantics for LTL Topological Semantics for LTL Semantics for LTL</p>","tags":["Note","Incomplete"]},{"location":"Linear%20systems%20with%20constant%20coefficients/","title":"Linear systems with constant coefficients","text":"<p>202303121003</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"Linear%20systems%20with%20constant%20coefficients/#linear-systems-with-constant-coefficients","title":"Linear systems with constant coefficients","text":"<pre><code>title:\nThis is just a linear system $$\\dot{x}(t) = A(t)x(t)$$\nBut where $A(t)$ is a constant matrix valued function.\n</code></pre>"},{"location":"Linear%20systems%20with%20constant%20coefficients/#for-n1","title":"For n=1:","text":"<p>\\(\\dot{x} = kx \\to x(t) = x(0)e^{kt}\\)</p>"},{"location":"Linear%20systems%20with%20constant%20coefficients/#for-n-geq-1-uncoupled","title":"For \\(n \\geq 1\\) uncoupled:","text":"\\[ \\begin{align} \\dot{x_{1}}= \\lambda_{1}x_{1} \\\\ \\dot{x_{2}} = \\lambda_{2}x_{2} \\\\ \\vdots \\\\ \\dot{x_{n}}= \\lambda_{n}x_{n} \\end{align} $$ This gives $\\varphi_{j}(t) = e^{\\lambda_{j}t}$. Now general solution $\\varphi(t) = c_{1}\\varphi_{1}(t) + \\dots + c_{n}\\varphi_{n}(t)$ In this case, we had $$ A = \\begin{bmatrix} \\lambda_{1} \\\\ &amp; \\lambda_{2}  \\\\  &amp; &amp; \\lambda_{3} \\\\ &amp; &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; &amp; \\lambda_{n} \\end{bmatrix} \\]"},{"location":"Linear%20systems%20with%20constant%20coefficients/#for-ngeq-1-coupled","title":"For \\(n\\geq 1\\), coupled:","text":"<p>Suppose \\(A\\) is diagonalisable with all eigenvalues real.</p> <p>Let \\(P^{-1}AP = \\mathrm{diag}[\\lambda_{1},\\lambda_{2},\\dots\\lambda_{n}]\\) and \\(v_{j}\\) be an eigenvector corresponding to \\(\\lambda_{j}\\). We can reduce the coupled system to an uncoupled one as follows.</p> <p>Let \\(y := P^{-1}x\\), so that \\(\\dot{y} = P^{-1} \\dot{x} = P^{-1}Ax = P^{-1}APy \\implies  \\dot{y} = \\mathrm{diag}[\\lambda_{1},\\lambda_{2},\\dots, \\lambda_{n}] y\\) Thus $$ y(t) = \\begin{bmatrix} c_{1}e^{\\lambda_{1}t}  \\ c_{2}e^{\\lambda_{2}t} \\ \\vdots \\ c_{n}e^{\\lambda_{n}t} \\end{bmatrix} \\implies x(t) = Py(t) = P  \\begin{bmatrix} c_{1}e^{\\lambda_{1}t}  \\ c_{2}e^{\\lambda_{2}t} \\ \\vdots \\ c_{n}e^{\\lambda_{n}t} \\end{bmatrix} $$</p>"},{"location":"Linear%20systems%20with%20constant%20coefficients/#for-the-general-case","title":"For the general case:","text":"<p>The answer is exponential of the matrix A.</p> <pre><code>title: Theorem\nGiven $A \\in M_n(\\mathbb{R})$; $\\dot{x} = Ax$ and $x(0) = x_0$ has the unique solution, $\\varphi(t) = e^{At}x_0$.\n</code></pre>"},{"location":"Linear%20systems%20with%20constant%20coefficients/#proof","title":"Proof:","text":"<p>Let \\(\\varphi(t) = e^{At}x_{0}\\). Then $$ \\begin{align} \\dot{\\varphi}(t) &amp;= \\lim_{ h \\to 0 } \\frac{\\varphi(t+h)-\\varphi(t)}{h} \\ &amp;= \\lim_{ h \\to 0 } \\frac{e^{(t+h)A}x_{0}- e^{tA}x_{0}}{h} \\ &amp;= \\left( \\lim_{ h \\to 0 } \\frac{(e<sup>{hA}-I)e</sup>{tA}}{h} \\right)x_{0} \\end{align}$$ This is due to the fact that \\(e^{X+Y} = e^{X}e^{Y}\\) if \\(X,Y\\) commute. $$ \\begin{align} \\implies  \\dot{\\varphi}(t) &amp;= \\left(\\lim_{ h \\to 0 } \\frac{e^{hA}-I}{h}\\right) e^{tA}x_{0} \\ &amp;= \\left( \\lim_{ h \\to 0 } \\sum\\limits_{k=1}^{\\infty} \\frac{A<sup>{k}h</sup>{k-1}}{k!}\\right) e^{tA}x_{0} \\ &amp;=Ae^{tA}x_{0} = A\\varphi(t)  \\end{align} $$ So, \\(\\varphi(t)\\) satisfies the ODE and \\(\\varphi(0) = x_{0}\\).</p>"},{"location":"Linear%20systems%20with%20constant%20coefficients/#related-problems","title":"Related Problems","text":""},{"location":"Linear%20systems%20with%20constant%20coefficients/#references","title":"References","text":"<p>Homogenous Linear Systems Exponential of a Matrix</p>"},{"location":"List%20decoding%20Folded%20RS/","title":"List decoding Folded RS","text":"<p>202311211511</p> <p>Tags : Algorithmic Coding Theory</p>","tags":["Note","Incomplete"]},{"location":"List%20decoding%20Folded%20RS/#list-decoding-folded-rs","title":"List decoding Folded RS","text":"<p>Guruswami-Sudan algo gives efficient list decoding upto \\(1-\\sqrt{ R }\\).</p> <p>[!question] Can we efficiently list decode RS codes upto \\(1-R\\)? What if the evaluation points are all the elements of \\(\\mathbb{F}_{q}\\)?</p>","tags":["Note","Incomplete"]},{"location":"List%20decoding%20Folded%20RS/#folded-rs-codes","title":"Folded RS codes","text":"<p>RS codes with evaluation points \\(1,\\gamma,\\gamma^{2},\\dots,\\gamma^{n-1}\\), \\(n=q-1\\), \\(\\gamma\\) is a primitive element of \\(\\mathbb{F}_{q}\\)</p>","tags":["Note","Incomplete"]},{"location":"List%20decoding%20Folded%20RS/#references","title":"References","text":"<p>Reed-Solomon Codes</p>","tags":["Note","Incomplete"]},{"location":"List%20decoding%20RS%20codes%20up%20to%20the%20Johnson%20radius/","title":"List decoding RS codes up to the Johnson radius","text":"<p>202309211509</p> <p>Tags : Algorithmic Coding Theory</p>","tags":["Note","Incomplete"]},{"location":"List%20decoding%20RS%20codes%20up%20to%20the%20Johnson%20radius/#list-decoding-rs-codes-up-to-the-johnson-radius","title":"List decoding RS codes up to the Johnson radius","text":"","tags":["Note","Incomplete"]},{"location":"List%20decoding%20RS%20codes%20up%20to%20the%20Johnson%20radius/#toy-case-1","title":"Toy case 1:","text":"<p>2 unknown polynomials \\(P_1(x)\\), \\(P_2(x)\\) of degree \\(k&lt;\\frac n2\\). We are given \\((a_i,b_i)\\) s.t. \\(a_i = P_1(\\alpha_i), b_i = P_2(\\alpha_i)\\), or \\(a_i = P_2(\\alpha_i), b_i = P_1(\\alpha_i)\\).</p> <p>Basically we have two polynomials, and their evaluations on some points, but for each pair, we don't know which of \\(P_1\\) and \\(P_2\\) each came from.</p> <p>We can get the sum polynomial \\(P_1+P_2\\), and the product polynomial \\(P_1P_2\\), and get \\(P_1-P_2\\), and get back the original polynomials in some order either directly or by using fanciness such as 'auxiliary bivariate polynomial', Hensel's lifting lemma.</p> <p>Auxiliary bivariate polynomial is just \\(f(X,Y) = (Y-P_1(x))(Y-P_2(x))\\). Then you factor this into irreducible polynomials which can be done in poly time.</p>","tags":["Note","Incomplete"]},{"location":"List%20decoding%20RS%20codes%20up%20to%20the%20Johnson%20radius/#toy-case-2","title":"Toy case 2:","text":"<p>2 unknown polynomials \\(P_1(x)\\), \\(P_2(x)\\) of degree \\(k&lt;\\frac n6\\). We have one evaluation for each of \\(n\\) points \\(\\alpha_1, \\dots, \\alpha_n\\), but we don't know which polynomial was evaluated at what point. We are told, however, that both the polynomials were evaluated at at least one-third of the points.</p>","tags":["Note","Incomplete"]},{"location":"List%20decoding%20RS%20codes%20up%20to%20the%20Johnson%20radius/#pattern-matching","title":"Pattern matching","text":"<p>All these algorithms - Berlekamp-Welch, Sudan, Guruswami-Sudan, have the following two step structure: Step 1 (Interpolation step): Find non zero \\(Q(X,Y)\\) st \\(Q(\\alpha_i, y_i)=0\\) Step 2 (Root finding): If \\(Y-P(X)\\) is a factor of \\(Q(X,Y)\\) then output \\(P(X)\\).</p> <p>The first step seems to be the creative one.</p> <p>In general, we try to prove a polynomial is zero, by saying it has degree at most say \\(d\\), but vanishes at \\(&gt;d\\) points.</p>","tags":["Note","Incomplete"]},{"location":"List%20decoding%20RS%20codes%20up%20to%20the%20Johnson%20radius/#algorithm-1","title":"Algorithm 1","text":"<p>Input: \\(n \\geq k \\geq 1\\),         \\(e = n-t\\), \\(l \\geq 1\\).         \\(n\\) pairs \\(\\{(\\alpha_i, y_i)\\}\\) Output: List of polynomials \\(P(X)\\) of degree at most \\(k-1\\). (can be empty)</p>","tags":["Note","Incomplete"]},{"location":"List%20decoding%20RS%20codes%20up%20to%20the%20Johnson%20radius/#interpolation-step","title":"Interpolation step","text":"<ol> <li>Find a non zero \\(Q(X,Y)\\) st \\(\\deg_X(Q) \\leq l\\), \\(\\deg_Y(Q) \\leq \\frac nl\\), st \\(Q(\\alpha_i, y_i)=0, 1 \\leq i \\leq n\\)</li> <li>\\(\\mathbb{L} \\leftarrow \\phi\\)</li> <li>Add \\(P(X)\\) to \\(\\mathbb{L}\\) under some conditions*</li> </ol> <p>Prove correctness of Step 1.</p> <p>There's nothing more to do in list decoding of RS codes. Johnson bound is the best thing we can do. Sudan had found an algorithm upto \\((1-\\sqrt{R})\\), and then Guruswami-Sudan made it better \\((1-2\\sqrt{R})\\).</p>","tags":["Note","Incomplete"]},{"location":"List%20decoding%20RS%20codes%20up%20to%20the%20Johnson%20radius/#references","title":"References","text":"<p>https://people.eecs.berkeley.edu/~venkatg/pubs/papers/listdecoding-NOW.pdf Guruswami-Sudan's list decoding of RS codes Reed-Solomon Codes [[Johnson bound]]</p>","tags":["Note","Incomplete"]},{"location":"Local%20Compactness/","title":"Local Compactness","text":"<p>202303021303</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Local%20Compactness/#local-compactness","title":"Local Compactness","text":"<pre><code>title:\nA space $X$ is called **locally compact** at $x$ if there is a compact subspace $C$ of $X$ such that it contains an open nbhd $V$ of $x$.\n$$\nx \\in V \\subset C \\subset X\n$$\n</code></pre>"},{"location":"Local%20Compactness/#proposition","title":"Proposition","text":"<pre><code>title:\nLet $X$ be a Hausdorff space. Then $X$ is locally compact iff for every $x \\in X$ and every $U$ such that $x \\in U \\in \\tau_{X}$, there is a neighbourhood $V$ of $x$ such that $$\nx \\in V \\subset Cl(V) \\subset U \\subset X $$\nand $Cl(V)$ is compact.\n</code></pre>"},{"location":"Local%20Compactness/#proof","title":"Proof:","text":"<p>If \\(X\\) is locally compact, then take \\(x \\in X\\) and \\(U\\) a nbhd of \\(x\\). Since \\(X\\) is locally compact, we have a compact subset \\(C\\) of \\(X\\) (it is also closed since \\(X\\) is hausdorff) which contains an open nbhd \\(V\\) of \\(x\\).  Now take \\(W = V \\cap U\\) and \\(K = C-W\\), note that \\(K\\) is a closed subset of \\(C\\) and hence is compact, and doesn't contain \\(x\\). Therefore, there is a pair of disjoint open sets \\(V_{1},V_{2}\\) s.t. \\(x \\in V_{1}, K \\subset V_{2}\\). This gives \\(Cl(V_{1} \\cap W) \\subset W \\subset U\\) and we are done since \\(Cl(V_{1} \\cap W) \\subset C\\) and hence is compact. </p> <p>The other direction is trivial.</p>"},{"location":"Local%20Compactness/#proposition_1","title":"Proposition","text":"<pre><code>title:\nLet $X$ be locally compact Hausdorff. If $A \\subset X$ is an open or closed subset of $X$, then $A$ is locally compact.\n</code></pre>"},{"location":"Local%20Compactness/#proof_1","title":"Proof:","text":"<p>1) If \\(A\\) is closed:    Since \\(X\\) is locally compact, there is a compact set \\(C\\) and a nbhd \\(U\\) of \\(x\\) such that \\(x \\in U \\subset C\\). Intersected everything with A. 2) If A is open:    Use the proposition above.</p>"},{"location":"Local%20Compactness/#related-problems","title":"Related Problems","text":"<ol> <li>\\(\\mathbb{Q}\\) is not locally compact.</li> <li>Any compact space is locally compact.</li> <li>\\(\\mathbb{R}^{n}\\) is locally compact.</li> <li>\\(\\mathbb{R}^{\\mathbb{N}}\\) is not locally compact in the product topology.</li> <li>One Point Compactification</li> </ol>"},{"location":"Local%20Compactness/#references","title":"References","text":"<p>Compactness Hausdorff Property</p>"},{"location":"Local%20Connectedness/","title":"Local Connectedness","text":"<p>202302101302</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Local%20Connectedness/#local-connectedness","title":"Local Connectedness","text":"<pre><code>title: \n$X$ is _locally connected_ at $x \\in X$ if $\\forall$ nbhds $U$ of $x$, $\\exists$ a connected nbhd $V$ s.t. $x \\in V \\subseteq U$.\n</code></pre> <pre><code>title: Proposition\n$X$ is locally connected iff for every open set $U$ of $X$, each connected component of $U$ is open in $X$.\n</code></pre>"},{"location":"Local%20Connectedness/#proof","title":"Proof:","text":"<p>If \\(X\\) is LC, then take an open set \\(U\\), and an element \\(x\\) in \\(U\\), then there is a connected nbhd \\(V\\) of \\(x\\) contained in \\(U\\), and so if \\(C_x\\) denotes the connected component of \\(x\\), \\(V\\) is contained in \\(C_x\\). And so each path connected component of \\(U\\) is open in \\(X\\). </p> <p>Conversely, for any nbhd \\(U\\) of \\(x\\), there is a connected component of \\(U\\) that contains \\(x\\), take this to be \\(V\\). \\(\\square\\)</p>"},{"location":"Local%20Connectedness/#references","title":"References","text":"<p>Connectedness</p>"},{"location":"Local%20Correcting%20and%20Decoding%20of%20RM%20codes/","title":"Local Correcting and Decoding of RM codes","text":"<p>202310171610</p> <p>Tags : Algorithmic Coding Theory</p>","tags":["Note","Incomplete"]},{"location":"Local%20Correcting%20and%20Decoding%20of%20RM%20codes/#local-correcting-and-decoding-of-rm-codes","title":"Local Correcting and Decoding of RM codes","text":"<pre><code>title: Motivation\nWe know that we can get a univariate polynomial back from its evaluations on some points that are equally spaced apart by taking consecutive differences, which will be a lower degree polynomial, the differences of which will be a polynomial with degree yet lower and so on.\nSo if we want to correct the evaluation of a polynomial at $(\\vec{a})$, then we can pick a random direction $(\\vec{b})$ and look at the evaluations on the line $L=\\{\\vec{a}+\\vec{b}t\\}$ and \n</code></pre> <p>Theorem: \\(RM_{2}(m,1)\\) is \\((\\delta,2,1-2\\delta)\\)-LCC for any \\(\\delta&lt; \\frac{1}{q}\\). Proof:</p> <p>Ideas for self-correction - Basic decoding using interpolation on lines \\((d+1,\\delta,(d+1)\\delta)\\)-locally correctable - Improved decoding using Reed Solomon decoding on lines \\(\\left( q-1,\\delta, \\frac{2\\delta}{1-\\sigma} \\right)\\) where \\(0&lt;\\sigma&lt;1,d\\leq\\sigma(q-1)-1\\). - Decoding on curves</p>","tags":["Note","Incomplete"]},{"location":"Local%20Correcting%20and%20Decoding%20of%20RM%20codes/#basic-decoding","title":"Basic decoding","text":"<p>Shoot a random line through \\(\\vec{w}\\). \\(L=\\{ \\vec{w}+\\lambda  \\vec{v} \\ |\\ \\lambda \\in\\mathbb{F^{*}_{q}} \\}\\) Each individual query of the corrector samples a uniformly random location. With probability at least \\(1-(d+1)\\delta\\) it never queries a corrupted coordinate.</p>","tags":["Note","Incomplete"]},{"location":"Local%20Correcting%20and%20Decoding%20of%20RM%20codes/#references","title":"References","text":"<p>[[Reed-Muller Codes]] Local Decoding</p>","tags":["Note","Incomplete"]},{"location":"Local%20Decoding/","title":"Local Decoding","text":"<p>202309261809</p> <p>Tags : Algorithmic Coding Theory</p>","tags":["Note"]},{"location":"Local%20Decoding/#local-decoding","title":"Local Decoding","text":"<p>For some applications, we can't afford to look at the entire codeword, e.g. if we have Distributed Storage, or in the following setting: We want to ask for the \\(i^{th}\\) bit in an \\(n\\) bit string, but we don't want them to know what \\(i\\) is. We want to decode in sub-linear time. \\(Q\\) should be \\(o(n)\\).</p> <p>Locally correctable code: \\(C\\subset \\mathbb{F}_{q}^{n}\\) is a \\((\\delta,Q)\\)-locally correctable code if there is an algorithm \\(A\\) s.t. the following holds: For all \\(w \\in \\mathbb{F}_{q}^{n}\\) s.t. \\(\\exists c \\in C\\) s.t. \\(\\Delta(c,w)\\le\\delta n\\) and \\(\\forall i \\in [n], A^{(w)}(i)\\) makes at most \\(Q\\) oracle queries to \\(w\\).</p> <p>If we have a deterministic algorithm, an adversary can ensure that there is no codeword satisfying the given parameters. So we appeal to randomised algorithms! We allow for a failure with low probability. So we want \\(A^{(w)}(i)=C_{i}\\) with probability at least \\(1-\\gamma\\), locally decodable codes.</p> <pre><code>Any linear $LCC$ is also $LDC$.\n</code></pre> Q \\(n\\) (as a function of \\(k\\)) 2 \\(n = \\Theta(2^{k})\\) 3 \\(k^{2}\\le n\\le e^{e^{\\log(\\log(n)^{0.99})}}\\) \\(O(\\log(n))\\) \\(k \\le n \\le \\text{poly}(k)\\) \\(O(n+\\epsilon)\\) \\(k\\le n\\le (1+\\alpha^{k})\\)","tags":["Note"]},{"location":"Local%20Decoding/#hadamard-code","title":"Hadamard Code","text":"<p>For Hadamard Code, we have a \\(2-\\)query \\(LDC\\), \\(2-\\)query \\(LCC\\), and a \\(3-\\)query \\(LTC\\). It is an exponential length linear code \\(H:\\mathbb{F}_{2}^{k}\\to\\mathbb{F}_{2}^{n}\\), \\(n=2^{k}\\). How you map is, for \\(x\\in\\mathbb{F}_{2}^{k}\\), you take the dot product of \\(x\\) and \\(i\\) in binary, and write the result at the \\(i^{th}\\) index. More formally, \\(H(x)_{y}=\\langle x,y\\rangle\\). (It's like you're evaluating a linear map on \\(n\\) points.)</p> <p>The minimum distance is \\(n/2\\). Local correction is only possible up to half of minimum distance.</p> <p>Lemma: For every \\(\\delta\\in(0,\\frac{1}{4})\\), Hadamard code is a \\((2,\\delta,\\frac{1}{2}-2\\delta)\\) \\(LCC\\). Proof: Suppose we are given a corrupted version of a codeword \\(H(x)\\), say \\(\\tilde{H}\\) s.t. \\(\\Delta(H(x),\\tilde{H}(x))\\le\\delta n\\). To correct the symbol at \\(y\\in\\mathbb{F}_{2}^{k}\\), the local correcter algorithm makes queries \\(\\tilde{H}(x)\\) at \\(z\\) and \\(z+y\\) for a uniformly random \\(z\\in\\mathbb{F}_{2}^{k}\\). With probability at least \\(1-2\\delta\\), \\(\\tilde{H}(x)_{z}+\\tilde{H}(x)_{z+y}=\\langle x,z\\rangle +\\langle x,z+y\\rangle=\\langle x,y\\rangle\\). And then just compute the parity of the two bits.</p>","tags":["Note"]},{"location":"Local%20Decoding/#references","title":"References","text":"","tags":["Note"]},{"location":"Local%20Path%20Connectedness/","title":"Local Path Connectedness","text":"<p>202302101302</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Local%20Path%20Connectedness/#local-path-connectedness","title":"Local Path Connectedness","text":""},{"location":"Local%20Path%20Connectedness/#title-x-is-_locally-path-connected_-at-x-in-x-if-for-every-nbhd-u-of-x-there-is-a-path-connected-nbhd-v-of-x-st-x-in-v-subset-u","title":"<pre><code>title:\nX is _locally path connected_ at $x \\in X$, if for every nbhd U of x, there is a path connected nbhd $V$ of x s.t. $x \\in V \\subset U$. \n</code></pre>","text":"<pre><code>title: Proposition\nX is locally path connected iff for every open set U of X, each path connected component of U is open in X.\n</code></pre>"},{"location":"Local%20Path%20Connectedness/#proof","title":"Proof:","text":"<p>If \\(X\\) is LPC, then take an open set \\(U\\), and an element \\(x\\) in \\(U\\), then there is a path connected nbhd \\(V\\) of x contained in \\(U\\), and so if \\(P_x\\) denotes the path connected component of \\(x, V\\) is contained in \\(P_x\\). And so each path connected component of \\(U\\) is open in \\(X\\). </p> <p>Conversely, for any nbhd \\(U\\) of \\(x\\), there is a path component of \\(U\\) that contains \\(x\\), take this to be \\(V\\). \\(\\square\\)</p>"},{"location":"Local%20Path%20Connectedness/#related-problems","title":"Related Problems","text":""},{"location":"Local%20Path%20Connectedness/#references","title":"References","text":"<p>Path Connectedness Connectedness</p>"},{"location":"Local%20Self-correction%20of%20Bivariate%20Multiplicity%20Codes/","title":"Local Self correction of Bivariate Multiplicity Codes","text":"<p>202311021511</p> <p>Tags : Algorithmic Coding Theory</p>","tags":["Note","Incomplete"]},{"location":"Local%20Self-correction%20of%20Bivariate%20Multiplicity%20Codes/#local-self-correction-of-bivariate-multiplicity-codes","title":"Local Self-correction of Bivariate Multiplicity Codes","text":"<p>Bivariate Multiplicity Codes of Order 2: \\(c(P)=\\langle (P(\\bar{a})), \\frac{\\partial P}{\\partial X}(\\bar{a}),\\frac{\\partial P}{\\partial Y}(\\bar{a})\\rangle\\), \\(a\\in\\mathbb{F}_{q}^{2}\\), \\(c(P)\\in(\\mathbb{F}_{q}^{3})^{q^{2}}\\)</p> <p>To correct just \\(P(\\bar{a})\\), we can use the evaluations along a random line. How many lines do we need to use in this case?</p> <p>Let \\(Q(T)=P(\\bar{a}+\\bar{b}T)\\). For every \\(t\\in\\mathbb{F}_{q}\\), the \\(\\bar{a}+\\bar{b}T\\in L\\) coordinate of \\(c(P)\\) completely determines both the value and first order derivative of \\(Q(T)\\) at point \\(t\\).</p> <p>\\(\\left( Q(t), \\frac{\\partial Q}{\\partial T}(t) \\right)=\\left( P(\\bar{a}+\\bar{b}t),b_{1} \\frac{\\partial P}{\\partial X}(\\bar{a}+\\bar{b}t)+b_{2} \\frac{\\partial P}{\\partial Y}(\\bar{a}+\\bar{b}t) \\right)\\)</p> <p>We got a linear combination of the partial derivatives, we wanted them individually. So we pick \\(b_{1},b_{2}\\) and some \\(\\tilde{b}_{1},\\tilde{b}_{2}\\) and then solve the linear equations to get them back.</p> <p>We can achieve rates arbitrarily close to 1, by increasing the number of variables (multivariate), and the order of the derivatives we are sending.</p>","tags":["Note","Incomplete"]},{"location":"Local%20Self-correction%20of%20Bivariate%20Multiplicity%20Codes/#references","title":"References","text":"<p>Multiplicity Codes</p>","tags":["Note","Incomplete"]},{"location":"Local%20problems%20on%20codes/","title":"Local problems on codes","text":"<p>202310191510</p> <p>Tags : Algorithmic Coding Theory</p>","tags":["Note","Incomplete"]},{"location":"Local%20problems%20on%20codes/#local-problems-on-codes","title":"Local problems on codes","text":"<p>Algorithmic problems on codes that can be solved \"locally\": - Locally testable codes (LTCs) - Locally correctable codes (LCCs) - Locally decodable codes (LDCs)</p> <p>Local testing: (Is it a valid codeword or is it far from being a codeword? - Property testing) Given a word \\(r\\in\\Sigma^{n}\\), is it a valid codeword? If it is a valid codeword, we should output True with probability 1. If it's not a codeword, then probability that we output False should be at least \\(\\frac{1}{4}\\Delta(r,C)\\). (It's local because we will be reading \\(r\\) by making at most \\(q\\) oracle queries.)</p> <p>Hadamard code is the standard example for local decoding but it has decreasing rate. Local codes in the constant rate regime are constant-variable RM codes over large fields. They can achieve any rate \\(R&lt; \\frac{1}{2}\\), constant distance, but number of queries is \\(O(n^{\\log R})\\).</p> <p>LDCs/LCCs with constant rate regime have query complexity at least \\(\\Omega(\\log n)\\). LTCs, however, with constant rate, constant distance and constant query complexity exist. This was open for a while till Dinur's paper (talk in Simon's Institute).</p> <p>RM codes with \\(m=1\\) are just RS codes. RS codes were so good, then why move to RM codes? RM codes, for \\(m\\geq 2\\) have \\(R&lt; \\frac{1}{2}\\). But RS codes have increasing alphabet size. So we increased \\(m\\) to decrease the alphabet size. This is one of the motivations for RM codes. Also, increasing \\(m\\) endows the code with 'local' properties.(??)</p> <p>Restriction of a \\(d-\\)degree multivariate polynomial to a line is a degree \\(d\\) univariate polynomial.</p>","tags":["Note","Incomplete"]},{"location":"Local%20problems%20on%20codes/#references","title":"References","text":"<p>Local Decoding</p>","tags":["Note","Incomplete"]},{"location":"Los-Vaught%20Test/","title":"Los Vaught Test","text":"<p>202311291813</p> <p>tags : [[Logic]]</p>","tags":["Example"]},{"location":"Los-Vaught%20Test/#los-vaught-test","title":"Los-Vaught Test","text":"<p>We say that a theory \\(T\\) is \\(\\kappa\\)-categorical for some infinte cardinal \\(\\kappa\\) iff all models of size \\(\\kappa\\) are isomorphic.</p> <p>[!note] Theorem Let \\(T\\) be a theory in a countable language. Assume that \\(T\\) has no finite models. - If \\(T\\) is \\(\\aleph_0\\) categorical, then \\(T\\) is complete - If \\(T\\) is \\(\\kappa\\)-categoriacal, for some uncountable cardinal \\(\\kappa\\), then \\(T\\) is complete.</p>","tags":["Example"]},{"location":"Los-Vaught%20Test/#proof","title":"Proof","text":"<p>Assume there are no finite models of \\(T\\).</p> <p>FTSOC: Let \\(T\\) be \\(\\kappa\\)-categorical for some uncountable \\(\\kappa\\) and be incomplete. Find \\(\\phi\\) such that \\(T\\cup\\{\\phi\\}\\) and \\(T\\cup\\{\\lnot\\phi\\}\\)</p> <p>By the [[Upward L\u00f6wenheim-Skolem Theorem]], we have that, there exits \\(\\kappa\\) models of both \\(T\\cup\\{\\phi\\}\\) and \\(T\\cup\\{\\lnot\\phi\\}\\). This is a contradiction as all \\(\\kappa\\) models are isomorphic.</p>","tags":["Example"]},{"location":"Los-Vaught%20Test/#related","title":"Related","text":"<p>Natural Numbers with Successor Function is Complete</p>","tags":["Example"]},{"location":"Lovasz%20Local%20Lemma/","title":"Lovasz Local Lemma","text":"<p>202311151411</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note","Incomplete"]},{"location":"Lovasz%20Local%20Lemma/#lovasz-local-lemma","title":"Lovasz Local Lemma","text":"<p>\\(n\\) bad events: \\(A_{1},A_{2},\\dots,A_{n}\\), each can occur with probability \\(p_{i}\\leq p&lt;1\\). So we know that we can avoid each event individually, we want to show that we can avoid all of them simultaneously. \\(Pr[\\bigcap\\limits_{i=1}^{n}\\bar{A}_{i}]&gt;0\\) If the events are independent, then \\(Pr[\\bigcap\\limits_{i=1}^{n}\\bar{A}_{i}]\\geq(1-p)^{n}&gt;0\\).</p> <p>LLL: If \\(A_{1},A_{2},\\dots,A_{n}\\) are events s.t. \\(Pr[A_{i}]\\leq p&lt;1\\ \\forall i\\) and each \\(A_{i}\\) depends on at most \\(d\\) \\(A_{j}\\)s, and \\(ep(d+1)\\leq 1\\) then \\(Pr[\\bigcap\\limits_{i=1}^{n}\\bar{A}_{i}]&gt;0\\).</p>","tags":["Note","Incomplete"]},{"location":"Lovasz%20Local%20Lemma/#k-sat","title":"k-SAT","text":"<p>\\(\\phi=C_{1}\\land C_{2}\\land\\dots \\land C_{m}\\) \\(n\\) variables Let \\(a=(a_{1},\\dots,a_{n})\\) be an assignment \\(A_{i}:\\) Clause \\(C_{i}\\) is not satisfied by \\(a\\)</p> <p>Lemma: Any \\(k-SAT\\) formula where each variable appears in at most \\(\\dfrac{2^{k-2}}{k}\\) clauses is always satisfiable. Proof: \\(Pr[A_{i}]=\\frac{1}{2^{k}}=p\\), \\(d=2^{k-2}\\) (each clause can share a variable with \\(\\leq \\frac{2^{k-2}}{k}k\\) clauses) \\(4pd=\\dfrac{4.2^{k-2}}{2^{k}}=1\\) By LLL, \\(Pr[\\bigcap\\limits_{i=1}^{n}\\bar{A}_{i}]&gt;0\\). So there exists an assignment that satisfies all clauses.</p>","tags":["Note","Incomplete"]},{"location":"Lovasz%20Local%20Lemma/#proof-of-lll","title":"Proof of LLL","text":"","tags":["Note","Incomplete"]},{"location":"Lovasz%20Local%20Lemma/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Machinery%20for%20Probabilistic%20Semantics%20for%20Timed%20Automata/","title":"Machinery for Probabilistic Semantics for Timed Automata","text":"<p>202311162211</p> <p>Tags : Timed Automata</p>","tags":["Note","Incomplete"]},{"location":"Machinery%20for%20Probabilistic%20Semantics%20for%20Timed%20Automata/#machinery-for-probabilistic-semantics-for-timed-automata","title":"Machinery for Probabilistic Semantics for Timed Automata","text":"<p>[!hint] Motivation Here we give Timed Automata as a model to give probabilistic interpretation to delays so that unlikely events will happen with probability \\(0\\)</p> <p>We fix an automata \\(\\mathcal A =\\langle L,X,E, \\mathcal I, \\mathcal L\\rangle\\) which we assume to be non blocking. For every state \\(s\\) we define a probability measure \\(\\mu_s\\) over \\(\\mathbb R_+\\) with the following requirements - \\(\\mu_s(I(s)) = \\mu_s(\\mathbb R_+)=1\\)     - Intuitively it means that the probability of taking some transition is \\(1\\). This is possible because we assume that the timed automata is non-blocking - We use \\(\\lambda\\) for the Lebesgue measure, if \\(\\lambda(I(s))&gt;0\\) then \\(\\mu_s\\) is equivalent to \\(\\lambda\\), otherwise \\(\\mu_s\\) is the uniform distribution over the set of points in \\(I(s)\\) - For every state \\(s\\) we also assume a probability distribution \\(p_s\\) over the edges such that \\(p_s(e)&gt;0\\) iff \\(e\\) is enabled in \\(s\\).</p> <p>[!example] - Uniform Distribution over \\(I(s)\\) if it is finite. - Lebesgue Measure normalised to have a probability measure if \\(I(s)\\) is a finite set of bounded intervals. - Exponential distribution if \\(I(s)\\) contains an unbounded interval.</p> <p>Given that we have the above machinery, we can define a Probability Measure over Finite Paths in a Timed Automata.</p>","tags":["Note","Incomplete"]},{"location":"Machinery%20for%20Probabilistic%20Semantics%20for%20Timed%20Automata/#references","title":"References","text":"<ul> <li>Probability Measure over Finite Paths in a Timed Automata</li> <li>Timed Automata Alternate Definition</li> </ul>","tags":["Note","Incomplete"]},{"location":"Match%20Function%20for%20Enriched%20Lambda%20Calculus/","title":"Match Function for Enriched Lambda Calculus","text":"<p>202310251410</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note"]},{"location":"Match%20Function%20for%20Enriched%20Lambda%20Calculus/#match-function-for-enriched-lambda-calculus","title":"Match Function for Enriched Lambda Calculus","text":"<p>Our goal is to convert an equation of the form $$ \\begin{align} &amp;((\\lambda p_{1,1}\\dots\\lambda p_{1,n}.E_{1})\\; u_{1}\\dots u_{n})\\ \\triangleright\\quad &amp;\\vdots\\ \\triangleright\\quad &amp;((\\lambda p_{m,1}\\dots p_{m,n}.E_{m})\\; u_{1}\\dots u_{n})\\ \\triangleright\\quad&amp;E \\end{align} $$ We first convert the above from into the following and then use the match function to simply it in multiple steps, starting from the leftmost pattern to the rightmost one $$ \\begin{align} \\textbf{match}\\quad &amp;[u_{1}, \\dots, u_n]\\ &amp;[(\\; [p_{1,1}, \\dots, p_{1,n}], E_1\\;),\\ &amp;\\vdots\\ &amp;(\\; [p_{m,1}, \\dots, p_{m,n}], E_{m}\\;)]\\ &amp;E \\end{align} $$</p>","tags":["Note"]},{"location":"Match%20Function%20for%20Enriched%20Lambda%20Calculus/#example-of-conversion-from-lambda-calculus-to-match-function","title":"Example of conversion from Lambda Calculus to Match function","text":"<pre><code>demo f []      ys     = A f ys\ndemo f (x: xs) []     = B f x xs\ndemo f (x: xs) (y:ys) = C f x xs y ys\n</code></pre> <p>This can be converted to the following program in Enriched Lambda Calculus</p> \\[ \\begin{align*} = \\lambda u_{1}.\\lambda u_{2}.\\lambda u_{3}&amp;. &amp;&amp;((\\lambda f.\\lambda [].\\lambda ys.\\ A\\ f\\ ys)\\; u_{1}\\ u_{2}\\ u_{3})\\\\ &amp;\\triangleright &amp;&amp;((\\lambda f.\\lambda (x:xs).\\lambda [].\\ B\\ f\\ x\\ xs)\\; u_{1}\\ u_{2}\\ u_{3})\\\\ &amp;\\triangleright &amp;&amp;((\\lambda f.\\lambda (x:xs).\\lambda (y:ys).\\ C\\ f\\ x\\ xs\\ y\\ ys)\\; u_{1}\\ u_{2}\\ u_{3})\\\\ &amp;\\triangleright &amp;&amp;\\text{ERROR} \\end{align*} \\] <p>This can be converted to use the match function in the following way</p> <pre><code>demo =\n  \\u1. \\u2. \\u3. \n    match [u1, u2, u3]\n          [ ( [f, [],     ys],     (A f ys)  ),\n            ( [f, (x:xs), []],     (B f x xs)),\n            ( [f, (x:xs), (y:ys)], (C f x xs y ys)) ]\n          ERROR\n</code></pre> <p>^92bf09</p>","tags":["Note"]},{"location":"Match%20Function%20for%20Enriched%20Lambda%20Calculus/#simplification-rules","title":"Simplification Rules","text":"<p>The following rules are used repeatedly to convert match syntax to \\(\\text{case-}\\)expressions ^b05ec8 - Variable Rule when all patterns are variables - Constructor Rule when all patterns are constructor - Mixed Rule when some of the patters are variables and some are constructors - Empty Rule to remove the match function to get only case expressions.</p>","tags":["Note"]},{"location":"Match%20Function%20for%20Enriched%20Lambda%20Calculus/#references","title":"References","text":"<ul> <li>Optimisations for Overlapping Patterns</li> <li>Optimisations for Expressions containing FAIL and I&gt;</li> <li>Uniform Definition of Haskell Functions</li> </ul>","tags":["Note"]},{"location":"Mealy%20Machines/","title":"Mealy Machines","text":"<p>202310151410</p> <p>Tags : [[Theory of Computation]]</p>","tags":["Note","Incomplete"]},{"location":"Mealy%20Machines/#mealy-machines","title":"Mealy Machines","text":"<pre><code>title: History - Sequential Circuits\nSequential Circuits are machines whose inputs depend not only on the current input but also the previous sequence of inputs received. \nD.A. Huffman was the first person to give a useful description of such model which would describe the machine like a *Flow Chart* which after some complicated procedures were reduced to a form with minimal storage elements (for the previous sequence of inptus).\nIndependently E.F. Moore worked on experiment that can infer the workings of such a machine only based on input output experiments from a terminal. His approach turned out to be idenitcal to  Huffman's description, i.e **Flow Chart Manipulation**.\n\nHuffman's method didn't usually work in its unmodified form with the type of circuitry found in the digital computers, so Mealy extended Huffman's machines to make it more compatible with the computers.\n</code></pre> <p>The Mealy Automata are a simplified description of the circuits described my mealy that mainly focus of the \"state\" of the circuit.</p>","tags":["Note","Incomplete"]},{"location":"Mealy%20Machines/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Measurement%20of%20Redundancy%20in%20a%20code/","title":"Measurement of Redundancy in a code","text":"<p>202308161908</p> <p>Type : #Note Tags : Algorithmic Coding Theory</p>"},{"location":"Measurement%20of%20Redundancy%20in%20a%20code/#measurement-of-redundancy-in-a-code","title":"Measurement of Redundancy in a code","text":"<p>The measurement of redundancy in a code is done using two related metrics, those being Dimension and Rate.</p>"},{"location":"Measurement%20of%20Redundancy%20in%20a%20code/#dimension-of-a-code","title":"Dimension of a code","text":"<p>The dimension \\(k\\) of a code is defined as: $$ k=\\log_{q}(|C|) $$ where \\(q=|\\Sigma|\\) \\(|C|\\) is the set of data words. This can be thought of as size of the vector that can correspond to all the code words where each entry can be one of \\(q\\) characters.</p>"},{"location":"Measurement%20of%20Redundancy%20in%20a%20code/#rate-of-a-code","title":"Rate of a code","text":"<p>The rate of the code is the average amount of information being carried by each character of the code. This is defined as  $$ R=\\frac{k}{n} $$ where \\(k\\) is the dimension of the code and \\(n\\) is the size of the code.</p> <p>The intuitive explanation would be that, you need \\(k\\) characters to carry all the information required, but you are using \\(n\\), hence each character carries \\(\\frac{k}{n}\\) of the information it would have carried without redundancies.</p> <p>A high rate implies less redundancies.</p>"},{"location":"Measurement%20of%20Redundancy%20in%20a%20code/#references","title":"References","text":""},{"location":"Metric%20Topology/","title":"Metric Topology","text":"<p>202302071202</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Metric%20Topology/#metric","title":"Metric","text":"<pre><code>title: \nA _metric_ $d$ on a set $X$ is a function $d : X \\times X \\to \\mathbb{R}_{\\ge0}$ such that:\n1) $d(x,y) \\ge 0 \\ \\forall \\ x,y \\in X$ with equality iff $x = y$.\n2) $d(x,y) = d(y,x)$\n3) $d(x,y) + d(y,z) \\ge d(z,x)$ $\\forall \\ x,y,z \\in X$. \n</code></pre>"},{"location":"Metric%20Topology/#metric-topology","title":"Metric Topology","text":"<p>```ad-note: title:  Given a metric space \\((X,d)\\), the metric topology \\(\\tau_d\\) on X is the topology generated by \\(\\mathcal{B} = \\{B_{\\epsilon}(x) : \\epsilon&gt;0, x\\in X\\}\\)</p>"},{"location":"Metric%20Topology/#note-xd-is-not-the-same-object-as-xtau_d-metrisable-space-ad-note-title-a-topological-space-is-called-_metrisable_-if-tau-tau_d-for-some-metric-d-on-x","title":"<pre><code>#### note: $(X,d)$ is not the same object as $(X,\\tau_d)$. \n\n## Metrisable space\n```ad-note\ntitle: \nA topological space is called _metrisable_ if $\\tau = \\tau_d$ for some metric $d$ on $X$.\n</code></pre>","text":""},{"location":"Metric%20Topology/#proposition","title":"Proposition","text":"<pre><code>title:\n(X,d) is a metric space. Let\n$$\\overline{d} : X\\times X \\to \\mathbb{R}$$\n$$\\overline{d}(x,y) = \\mathrm{min}(d(x,y),1)$$\nThen $\\tau_d$ = $\\tau_{\\overline{d}}$\n</code></pre>"},{"location":"Metric%20Topology/#proof","title":"Proof:","text":"<p>Easy to see that \\(\\overline{d}\\) is a metric. \\(B_{\\epsilon,d}(x) = B_{\\epsilon,\\overline{d}}(x)\\) for \\(\\epsilon &lt; 1\\). Hence is a set is open in \\(\\tau_d\\), it is open in \\(\\tau_{\\overline{d}}\\), and vice versa. Hence both are the same topology.</p>"},{"location":"Metric%20Topology/#this-also-shows-that-boundedness-is-not-a-topological-property","title":"This also shows that boundedness is not a topological property.","text":""},{"location":"Metric%20Topology/#lemma","title":"Lemma:","text":"<pre><code>title:\nLet d,d' be metrics on X. Then $\\tau_{d'}$ is finer than $\\tau_d$ iff $\\forall \\ x, \\ \\forall \\ \\epsilon &gt; 0, \\exists \\delta&gt;0$ such that $B_{\\delta,d'}(x) \\subset B_{\\epsilon,d}(x)$.\n</code></pre>"},{"location":"Metric%20Topology/#proposition_1","title":"Proposition:","text":"<pre><code>title:\nThe Topologies induced on $\\mathbb{R}^n$ by the euclidean metric (d) and the square metric ($\\rho$) are the same, which are infact the same as the product topology on $\\mathbb{R}^n$.\n</code></pre>"},{"location":"Metric%20Topology/#proof_1","title":"Proof:","text":"<p>Use the fact that \\(\\rho(x,y) \\le d(x,y) \\le \\sqrt{n}\\rho(x,y)\\), and show that \\(\\tau_d = \\tau_{\\rho}\\).  Next show for that any basis element of the product topology and a point \\(x\\) in it,  the basis element contains a basis element of the square topology containing \\(x\\) and vice versa. \\(\\square\\) </p>"},{"location":"Metric%20Topology/#uniform-metric-metric-on-an-arbitrary-product-space","title":"Uniform metric (Metric on an arbitrary product space)","text":"<pre><code>title:\nGiven an index set J and given points $\\textbf{x} = (x_{\\alpha})_{\\alpha \\in J}$ and $\\textbf{y} = (y_{\\alpha})_{\\alpha \\in J}$ on $\\mathbb{R}^J$ by the equation:\n$$\\overline{\\rho}(\\textbf{x},\\textbf{y}) = \\sup \\{\\overline{d}(x_{\\alpha},y_{\\alpha}) | \\alpha\\in J\\}$$\nIt is easy to check that $\\overline{\\rho}$ is a metric, this is called the uniform metric on $\\mathbb{R}^J$ and the topology induced by it is called the uniform topology.\n</code></pre>"},{"location":"Metric%20Topology/#theorem","title":"Theorem:","text":"<pre><code>title:\nThe uniform topology on $\\mathbb{R}^J$ is finer than the product topology and coarser than the box topology; these three are all different if $J$ is infinite.\n</code></pre>"},{"location":"Metric%20Topology/#proof_2","title":"Proof:","text":"<p>Given a basis element of the product toplogy, \\(U := \\prod_{\\alpha}U_{\\alpha}\\), where \\(U_{\\alpha_i} \\neq \\mathbb{R}\\) for \\(i = 1,2,\\cdots,n\\). For a point x in U, we can find \\(\\epsilon_i\\) such that \\((x_i-\\epsilon_i,x_i+\\epsilon_i) \\subset U_{\\alpha_i}\\). Let \\(\\epsilon = \\min_i{\\epsilon_i}\\). Then \\(B_{\\bar\\rho,\\epsilon}(x) \\subset U\\). Hence uniform topo finer than product.</p> <p>Now given a basis element of the uniform topo \\(B_{\\bar\\rho,\\epsilon}(x)\\), we have  \\(\\prod_{\\alpha} (x_{\\alpha} - \\epsilon/2,x_{\\alpha} + \\epsilon/2) \\subset B_{\\bar\\rho,\\epsilon}(x)\\). </p>"},{"location":"Metric%20Topology/#hence-product-topology-subseteq-uniform-topology-subseteq-box-topology","title":"HENCE: Product topology \\(\\subseteq\\) uniform topology \\(\\subseteq\\) box topology","text":"<p>If \\(J\\) is infinite, we already know that box topology and product topology are different.  Take the set \\(\\prod_{\\alpha} U_{\\alpha}\\) where \\(U_{\\alpha_i} = (-1/i,1/i)\\) for all \\(i \\in \\mathbb{N}\\) and the rest are \\(\\mathbb{R}\\).  This is not open in the uniform topology.</p> <p>\\(\\prod_{\\alpha}(-\\epsilon,\\epsilon)\\) is open in uniform topology but not in product topology (does not contain a basis element of product topology).</p>"},{"location":"Metric%20Topology/#theorem_1","title":"Theorem:","text":"<pre><code>title:\nThe only one of these cases where $\\mathbb{R}^J$ is metrizable with $J$ infinite, is the case where $J$ is countable and $\\mathbb{R}^J$ has the product topology.\n</code></pre>"},{"location":"Metric%20Topology/#proof_3","title":"Proof:","text":"<p>TO DO</p>"},{"location":"Metric%20Topology/#related-problems","title":"Related Problems","text":""},{"location":"Metric%20Topology/#references","title":"References","text":"<p>Product topology</p>"},{"location":"Metrizable%20Spaces/","title":"Metrizable Spaces","text":"<p>202303071503</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Metrizable%20Spaces/#metrizable-spaces","title":"Metrizable Spaces","text":"<pre><code>title:\nA topo space that can be endowed with a metric inducing the same topology is called _metrizable_.\n</code></pre>"},{"location":"Metrizable%20Spaces/#related-results","title":"Related Results","text":"<p>1) Metrizable \\(\\implies\\) Normal 2) Metrizable + separable \\(\\implies\\) 2nd countable 3) Metrizable + Lindelof \\(\\implies\\) 2nd countable 4) \\(\\mathbb{R}^{J}\\) with \\(J\\) uncountable is NOT metrizable 5) \\(\\mathbb{R}^{\\omega}\\) in the box topo is NOT metrizable. (For proof, refer to R^w)</p>"},{"location":"Metrizable%20Spaces/#examples","title":"Examples","text":"<p>1) \\(\\mathbb{R}^{\\omega}\\) with the product topology is metrizable. (Look at R^w)</p>"},{"location":"Metrizable%20Spaces/#related-problems","title":"Related Problems","text":""},{"location":"Metrizable%20Spaces/#references","title":"References","text":"<p>Second Countability Separable Spaces Normal Spaces Lindelof Space Product topology R^w</p>"},{"location":"Minimum%20k-cut%20Problem/","title":"Minimum k cut Problem","text":"<p>202311011411</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note","Incomplete"]},{"location":"Minimum%20k-cut%20Problem/#minimum-k-cut-problem","title":"Minimum k-cut Problem","text":"<p>Given \\(G=(V,E)\\) undirected graph with weight function on edges, find min wt set \\(E'\\subset E\\) s.t. removal of \\(E'\\) disconnects \\(G\\) into \\(k\\) connected components.</p> <p>There exists an \\(\\tilde{O}(n^{k^{2}})\\) algorithm. NP hard for non constant \\(k\\). (We call the optimisation version of an NP complete decision problem NP hard.)</p> <p>Today we will look at a \\(2\\left( 1- \\frac{1}{k} \\right)\\) approximation using Gomory-Hu trees.</p> <p>We can store pairwise \\(s-t\\) min-cuts \\(\\forall s,t\\in V\\), in a \\(V\\times V\\) matrix, which takes \\(O(V^{2})\\) space. Gomory-Hu trees can do this in \\(O(V)\\) space. We can find a global min-cut, and recursively find global min-cuts in the two components.</p> <p>\\(T\\) is a Gomory-Hu tree for \\(G\\) if: - \\(T=(V,F)\\), weights \\(w'\\geq 0\\), and - for every \\((u,v)\\in F\\), \\(w'(u,v)=w(\\text{a }u-v\\text{ min-cut in }G)\\) and \\(T \\setminus \\{ (u,v) \\}\\) is the \\(u-v\\) mincut in \\(G\\).</p> <pre><code>title:\n**Theorem:** Let $(s-t)$ be a min wt edge on the path between $u$ and $v$ in $T$.\nThen $w'(s,t)=w(\\text{a }u-v\\text{ min-cut in }G)$.\n*Proof:* Induction on the length of $u-v$ path\nLength = 1; it follows from definition.\n\n$\\text{mincut}_{G}(a,b)\\geq \\min\\{ \\text{mincut}_{G}(a,c),\\text{mincut}_{G}(b,c) \\}$\n$w$ is the neighbour of $u$ on a $u-v$ path in $T$.\n$$\n\\begin{align*}\n\\text{mincut}_{G}(u,v)&amp;\\geq \\min\\{ \\text{mincut}_{G}(u,w),\\text{mincut}_{G}(w,v) \\}\\\\\n&amp;\\geq \\text{mincut}_{G}(s,t)=w'(s,t)\n\\end{align*}\n$$\n</code></pre>","tags":["Note","Incomplete"]},{"location":"Minimum%20k-cut%20Problem/#construction","title":"Construction:","text":"<p>Maintain a tree on subsets of vertices \\(S_{1},\\dots S_{k}\\subset V\\), \\(S_{1}=V\\). At each step, pick an \\(S_{i}\\) s.t. \\(S_{i}\\geq 2\\). Let \\(u,v\\in S_{i}\\). In \\(G\\), contract all the vertices contained in each of the subtrees rooted at \\(S_{i}\\) in \\(T\\). Find a \\(u-v\\) mincut \\((W,V'\\setminus W)\\). Split \\(S_{i}\\) as \\(S_{i_{1}}=S_{i}\\cap W,S_{i_{2}}=S_{i}\\cap(V'\\setminus W)\\).</p> <p>Prove that this gives a Gomory-Hu tree.</p> <p>Back to the algorithm:</p> <ol> <li>Find a Gomory-Hu tree \\(T\\) of \\(G\\) and remove its \\(k-1\\) lightest edges. The corresponding cut in \\(G\\) is the required approximation to min-k-cut of \\(G\\).</li> <li>Let \\(OPT\\) partitions be \\(V_{1}',\\dots V_{k}'\\). Let \\(OPT_{i}=\\) the set of edges with exactly one endpoint in \\(V_{i}\\). \\(\\sum\\limits_{i=1}^{k}w(OPT_{i})=2w(OPT)\\). \\(w(OPT_{k})\\geq \\frac{2}{k}w(OPT)\\). If \\(w(OPT_{k})\\) is the max among \\(i=1,\\dots,k\\), then \\(\\sum\\limits_{i=1}^{k-1}w(OPT_{i})\\leq 2\\left( 1- \\frac{1}{k} \\right)w(OPT)\\)</li> <li>Shrink each \\(V_{i}'\\) into one node. Get a new tree \\(T'\\). Root \\(T'\\) at \\(V'_{k}\\). \\(w(V_{1}',\\text{parent}(V_{1}'))\\leq w(OPT)\\). \\(w'(\\text{edges in }T')\\geq w'(k-1 \\text{ lightest edges in }T)=w(\\text{our cut})\\).</li> </ol>","tags":["Note","Incomplete"]},{"location":"Minimum%20k-cut%20Problem/#references","title":"References","text":"<p>[[Min-cut Problem]] Existence of Gomory-Hu trees</p>","tags":["Note","Incomplete"]},{"location":"Mixture%20Rule%20for%20Match%20Expression/","title":"Mixture Rule for Match Expression","text":"<p>202310251710</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note"]},{"location":"Mixture%20Rule%20for%20Match%20Expression/#mixture-rule-for-match-expression","title":"Mixture Rule for Match Expression","text":"<p>The Mixture Rule deals with the case where a set of patterns contain both variables and constructors, hence neither of the other rules apply.</p> <p>We combine the Variable Rule and Constructor Rule in this situation.</p>","tags":["Note"]},{"location":"Mixture%20Rule%20for%20Match%20Expression/#example","title":"Example","text":"<p>Consider the variant of the <code>demo</code> function that was defined in the Main Page <pre><code>demo' f []     ys     = A f ys\ndemo' f xs     []     = B f xs\ndemo' f (x:xs) (y:ys) = C f x xs y ys\n</code></pre></p> <p>In this case we reach the following situation after applying the Variable Rule once.</p> <pre><code>match [u2, u3]\n      [ ([NIL   , ys]    , A u1 ys),\n        ([xs    , NIL]   , B u1 xs),\n        ([(x:xs), (y:ys)], C u1 x xs y ys)]\n       Error\n</code></pre> <p>due to the semantics of match, if all patterns fail, the function returns its third input, we can use that by only considering the patterns until it changes from variable to constant or vice versa. Check just until then, and move to the other patterns if the first set fails. We write that in the following way</p> <pre><code>match [u2, u3]\n      [([NIL, ys], A u1 xs)]\n      (\n        match [u2, u3]\n              [ ([xs, NIL], B u1 xs),\n                ([(x:xs), (y:ys) C u1 x xs y ys])]\n              Error\n      )    \n</code></pre>","tags":["Note"]},{"location":"Mixture%20Rule%20for%20Match%20Expression/#mixture-rule","title":"Mixture Rule","text":"<p>Using the above example, we can say the following <pre><code>match us qs E\n</code></pre></p> <p>where <code>qs = qs1 ++ ... ++ qsk</code>, and each <code>qsi</code> is a partition that contains only Constructors or only Variables for the first argument.</p> <p>we can rewrite it as <pre><code>match us qs1 (match us qs2(...(match us qsk E)...))\n</code></pre></p>","tags":["Note"]},{"location":"Mixture%20Rule%20for%20Match%20Expression/#references","title":"References","text":"<ul> <li>Match Function for Enriched Lambda Calculus</li> <li>Variable Rule for Match Function</li> <li>Constructor Rule for Match Function</li> <li>Empty Rule for Match Function</li> </ul>","tags":["Note"]},{"location":"Module/","title":"Module","text":"<p>202302151102</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Module/#module","title":"Module","text":"<pre><code>title: Definition\n</code></pre>"},{"location":"Module/#related-problems","title":"Related Problems","text":""},{"location":"Module/#references","title":"References","text":""},{"location":"Monolithic%20Timed%20Automaton%20for%20a%20Network%20of%20Timed%20Automata/","title":"Monolithic Timed Automaton for a Network of Timed Automata","text":"<p>202310121910</p> <p>Tags : Timed Automata</p>","tags":["Note"]},{"location":"Monolithic%20Timed%20Automaton%20for%20a%20Network%20of%20Timed%20Automata/#monolithic-timed-automaton-for-a-network-of-timed-automata","title":"Monolithic Timed Automaton for a Network of Timed Automata","text":"<p>Given a Network of Timed Automata, we want to be use all the tools constructed for Timed Automata to analyze it. One the things which we might want to analyze is reachability of states to ensure that none of the problematic states are reachable, or that an important state is.</p> <p>One way to do that is to Construct a single timed automaton that corresponds to the network and analyze that.</p> <p>The following construction will represent all possible combinations of states as tuples, and transitions would be build using transitions between the component states.</p> <p>Consider the network \\(\\langle \\mathcal A_{1},\\mathcal A_{2}\\dots \\mathcal A_{k}\\rangle\\). We construct and automata \\(\\mathcal A\\) as follows - \\(Q = \\prod Q_{i}\\), which makes the set of states. - \\(S = \\prod S_{i}\\) is the set of starting states. - \\(F = \\prod F_{i}\\) is the of final states. - \\(\\Sigma = \\bigcup \\Sigma_{i}\\) which is the Alphabet. - \\(X = \\bigsqcup X_{i}\\) which is the set of clocks.</p> <p>For the transitions, we need to be careful about synchronizations.</p> <p>Given a state \\(\\langle p_{1}, p_{2}\\dots p_{k}\\rangle\\) and a letter \\(\\alpha\\). Without Loss of dots generality, let \\(\\alpha\\in \\Sigma_{1}\\dots \\Sigma_{i}\\) and \\(\\alpha\\notin \\Sigma_{i+1}\\dots \\Sigma_{k}\\). If There exists transitions from \\(p_{x}\\) to \\(q_{x}\\) accepting \\(\\alpha\\) with guard \\(G_x\\) and resets \\(R_{x}\\) for all \\(x\\le i\\). we can make the transition from \\(\\langle p_{1}\\dots p_{i},p_{i+1}\\dots p_{k}\\rangle\\) to \\(\\langle q_{1}\\dots q_{i},p_{i+1}\\dots p_{k}\\rangle\\) accepting \\(\\alpha\\) with guards \\(\\bigwedge\\limits_{x\\le i}G_{x}\\) and resets \\(\\bigsqcup\\limits_{x\\le i}R_{x}\\).</p> <pre><code>The number of states this leads to is $\\prod |Q_{i}|$ which gets big very quickly. Evaluating the states lazily can give significantly better results.\n</code></pre>","tags":["Note"]},{"location":"Monolithic%20Timed%20Automaton%20for%20a%20Network%20of%20Timed%20Automata/#references","title":"References","text":"<p>Networks of Timed Automata</p>","tags":["Note"]},{"location":"Monotone%20Domination%20Order/","title":"Monotone Domination Order","text":"<p>202311031511</p> <p>Tags : [[Order Theory]]</p>","tags":["Note","Incomplete"]},{"location":"Monotone%20Domination%20Order/#monotone-domination-order","title":"Monotone Domination Order","text":"<p>[!note] Definition Given a Preorder \\(\\sqsubseteq\\) on \\(A\\).  We can define a monotone domination order \\(\\preceq\\) on \\(A*\\) as follows \\(a_1,a_2\\dots a_n \\preceq b_1,b_2\\dots b_m\\) if there exists a strictly  increasing function \\(f:\\{1\\dots n\\}\\rightarrow \\{1\\dots m\\}\\) such that \\(a_i\\sqsubseteq b_{f(i)}\\) for all \\(i\\)</p> <p>Intuitively I can pick the first element of \\(a\\) and match with an element \\(b\\). Then I pick the next element of \\(a\\) and match it with an element of \\(b\\) which is after the previous choice and so on.</p> <p>An important Lemma for wqo is </p>","tags":["Note","Incomplete"]},{"location":"Monotone%20Domination%20Order/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Mu-Recursion/","title":"Mu Recursion","text":"<p>202304011804</p> <p>Type : #Note Tags : [[Lambda Calculus]]</p>"},{"location":"Mu-Recursion/#mu-recursion","title":"Mu-Recursion","text":"<p>\\(f:\\mathbb N^{k}\\) is obtained by  \\(\\mu-\\)recursion from \\(g:\\mathbb N^{k}\\to \\mathbb N\\) if  $$ \\begin{equation} f(\\vec n)=      \\begin{cases}         i &amp; \\text{if \\(g(i,\\vec n)=0\\) and \\(\\forall j&lt;i,g(j,n)&gt;0\\)} \\         \\text{undefined}  &amp; \\text{otherwise}     \\end{cases} \\end{equation} $$</p> <p>This method is equivalent ot the while loop <pre><code>i=0\nwhile g(i, n1 .. nk) &gt; 0:\n    i = i+1\nreturn i\n</code></pre></p>"},{"location":"Mu-Recursion/#references","title":"References","text":"<p>Functions Computable in Lambda Calculus</p>"},{"location":"Multi-choose/","title":"Multi choose","text":"<p>202306091706</p> <p>Type : #Note Tags : [[Enumerative Combinatorics]]</p>"},{"location":"Multi-choose/#multi-choose","title":"Multi-choose","text":"<p>A Multi-choose is the Multiset version of combination \"choose\" for simple sets. It is denoted as $$ \\left(!!{n\\choose k}!!\\right) $$ Which denotes the number of \\(k\\) order multi-subsets of a set of order \\(n\\). Or the number of was of choose \\(k\\), not necessarily distinct objects from a set of \\(n\\) elements.</p> <p><pre><code>title: Proposition\n$$\n\\left(\\!\\!{n\\choose k}\\!\\!\\right)={n+k-1\\choose k}\n$$\n</code></pre> One combinatorial proof with its explanation is the following. Proof: We create a set \\(Y = \\{y_{1}, y_{2},\\dots\\}\\) which represents the multiset \\(M\\) and let the underlying set \\(S=\\{x_{1},x_{2},\\dots,x_{n}\\}\\). Then we start creating a function \\(f:Y\\to S\\) and that function defines the sub-multiset in the following way. \\(f\\) will be a surjection and elements mapping to the same element in \\(S\\) will represent copies of that element in \\(M\\). We start by mapping \\(y_{1}\\mapsto x_{1}\\) then \\(y_{2}\\mapsto x_{2}\\) until we find the first element \\(x_{i}\\) that is in our multiset, to denote that, say \\(y_{j}\\mapsto x_{i}\\) and \\(x_{i}\\in M\\) then we say \\(y_{i+1}\\mapsto x_{i}\\). since for each element of the sub-multiset we are adding atmost 1 extra element to \\(Y\\), the size of why should be \\(n+k\\) and to identify a function, we assume the procedure used to for the set and mark each elment that represents one in the multiset. they can be any \\(k\\) except for the last one hence the total number being \\(n+k-1\\choose k\\). This is a bijection as different functions would give different sub-mulitsets, which will differ atleast at the first place where the functions will disagree. The following is a diagramatic description of the procedure.</p>"},{"location":"Multi-choose/#references","title":"References","text":""},{"location":"Multiplicative%20Characters/","title":"Multiplicative Characters","text":"<p>202305261505</p> <p>Type : #Note Tags : [[Number Theory]] [[Algebra]]</p>"},{"location":"Multiplicative%20Characters/#multiplicative-characters","title":"Multiplicative Characters","text":"<p><pre><code>title:\nA multiplicative character on $\\mathbb{F}_p$ is a group homomorphism $\\chi$ from $\\mathbb{F}_p^*$ to the non zero complex numbers.\n</code></pre> - The Legendre Symbol \\((a /p)\\) is such an example. - Another example is the trivial character \\(\\varepsilon\\) which takes everything to 1.</p> <p>Often we extend the domain of definition of the character to include \\(0 \\in \\mathbb{F}_{p}\\), and we define \\(\\chi(0) = 0\\) if \\(\\chi \\neq \\varepsilon\\) and \\(\\varepsilon(0) = 1\\).</p>"},{"location":"Multiplicative%20Characters/#proposition-1","title":"Proposition 1:","text":"<pre><code>title:\nLet $\\chi$ be a multiplicative character and $a \\in \\mathbb{F}_{p}^{*}$. Then \n1. $\\chi(1) = 1$\n2. $\\chi(a)$ is a $(p-1)$th root of unity.\n3. $\\chi(a ^{-1}) = \\chi(a) ^{-1} = \\overline{\\chi (a)}$\n</code></pre>"},{"location":"Multiplicative%20Characters/#proposition-2","title":"Proposition 2:","text":"<pre><code>title:\nLet $\\chi$ be a multiplicative character. If $\\chi \\neq \\varepsilon$, then $\\sum_{t} \\chi(t) = 0$ where the sum is over all $t \\in \\mathbb{F}_{p}$. If $\\chi = \\varepsilon$, the value of the sum is p.\n</code></pre>"},{"location":"Multiplicative%20Characters/#proof","title":"Proof:","text":"<p>For \\(\\chi \\neq \\varepsilon\\), there is an \\(a \\in \\mathbb{F}_{p}^{*}\\) such that \\(\\chi(a) \\neq 1\\). Now let the sum in question be equal to \\(T\\). Then $$ \\chi(a) T = \\sum_{t} \\chi(at) = \\sum_{s} \\chi(s) = T $$ Now since \\(\\chi(a)\\neq 1\\), \\(T = 0\\).</p>"},{"location":"Multiplicative%20Characters/#note","title":"Note:","text":"<ol> <li>The characters on \\(\\mathbb{F}_{p}\\) form a group, with group operation defined as pointwise multiplication.</li> <li>And inverses just reciprocals in the normal sense. \\(\\chi^{-1}(a) := \\chi(a) ^{-1}\\).</li> <li>Identity is \\(\\varepsilon\\).</li> </ol>"},{"location":"Multiplicative%20Characters/#proposition-3","title":"Proposition 3:","text":"<pre><code>title:\nThe group of characters is of order $p-1$. If $a \\in \\mathbb{F}_{p}^{*}$ and $a \\neq 1$, then there is a character such that $\\chi(a) \\neq 1$.\n</code></pre>"},{"location":"Multiplicative%20Characters/#proof_1","title":"Proof:","text":"<p>Since \\(\\mathbb{F}_{p}^{*}\\) is cyclic, it has a generator say \\(g\\). Then any character is determined by its value at \\(g\\). Since \\(\\chi(g)\\) is a \\((p-1)\\)th root of unity, there are exactly \\(p-1\\) values of \\(\\chi(g)\\) and so the group of characters has order at most \\(p-1\\).</p> <p>Define \\(\\lambda(g ^{k}) = e ^{2\\pi ik/(p-1)}\\), note that this is a character. Now we show that \\(\\varepsilon, \\lambda, \\lambda^{2}, \\dots \\lambda^{p-2}\\) are all distinct. Let \\(\\lambda^n =\\varepsilon\\), then \\(\\lambda(g)^n = 1 = e ^{2\\pi in /(p-1)} \\implies p-1 | n\\). Hence, \\(p-1\\) is the smallest integer \\(n\\) such that \\(\\lambda^n = \\varepsilon\\). \\(\\lambda\\) has order \\(p-1\\) and so the group of characters is cyclic of order \\(p-1\\).</p> <p>If \\(a \\in \\mathbb{F}_{p}^{*}\\), then \\(a = g ^{l}\\) for some \\(p-1 \\nmid l\\). Then \\(\\lambda(g^l) = e ^{2\\pi il/(p-1)} \\neq 1\\).</p>"},{"location":"Multiplicative%20Characters/#corollary","title":"Corollary","text":"<pre><code>title:\nIf $a \\in \\mathbb{F}_{p}^{*}$ and $a \\neq 1$, then $\\sum_{\\chi} \\chi(a) = 0$ where the sum is over all characters.\n</code></pre>"},{"location":"Multiplicative%20Characters/#proof_2","title":"Proof:","text":"<p>Let the sum be \\(T\\). Since \\(a\\neq 1\\), there is a character \\(\\rho\\) such that \\(\\rho(a) \\neq 1\\), then \\(\\(\\rho(a) T = \\sum_{\\chi}\\rho \\chi(a) = \\sum_{\\chi}\\chi(a) = T\\)\\) Since \\(\\rho(a)\\neq 1\\), \\(T = 0\\).</p>"},{"location":"Multiplicative%20Characters/#note-characters-are-useful-to-study-equations","title":"Note: Characters are useful to study equations.","text":"<p>Consider \\(x ^{n} = a\\) for \\(a \\in \\mathbb{F}_{p}^{*}\\). By [[nth Power Residues]] we know that this has a solution iff \\(a^{(p-1/d)} = 1\\). where \\(d = (n,p-1)\\) is the the gcd of \\(n,p-1\\), and that if a solution exists then \\(d\\) solutions exist.</p>"},{"location":"Multiplicative%20Characters/#proposition-4","title":"Proposition 4:","text":"<pre><code>title:\nIf $a \\in \\mathbb{F}_p^*$, $n | p-1$ and $x^n=a$ is not solvable then there exists a character such that \n1. $\\chi(a) \\neq 1$\n2. $\\chi^n = \\varepsilon$\n</code></pre>"},{"location":"Multiplicative%20Characters/#proof_3","title":"Proof:","text":"<p>Take \\(\\chi = \\lambda^{(p-1)/n}\\), then it has order \\(n\\). We know that \\(a = g^k\\) such that \\(n \\nmid k\\), since \\(x^n =a\\) is not solvable. Now \\(\\chi(a) = \\lambda(a) ^{p-1/n} = \\lambda(g)^{k(p-1)/n} = e ^{2\\pi ik/n} \\neq 1\\).</p>"},{"location":"Multiplicative%20Characters/#proposition-5","title":"Proposition 5:","text":"<pre><code>title:\nLet $N(x ^{n} = a)$ denote the number of solutions of $x ^{n} = a$, then $$\nN(x ^{n} = a) = \\sum_{\\chi^{n} = \\varepsilon} \\chi(a)$$\nwhere the sum is over all characters of order dividing $n$ (they form a group).\n</code></pre>"},{"location":"Multiplicative%20Characters/#proof_4","title":"Proof:","text":"<p>If \\(x ^{n} = a\\) is not solvable, then there is a \\(\\rho\\) such that \\(\\rho(a) \\neq 1\\) and \\(\\rho^{n} = \\varepsilon\\).  Now $$ \\rho(a) N(x ^{n} = a) = \\sum_{\\chi^{n} = \\varepsilon} \\rho \\chi(a) = \\sum_{\\chi^{n} = \\varepsilon}\\chi(a) = N(x ^{n} = a) $$ Giving \\(N(x ^{n} = a)=0\\) as required.</p> <p>If \\(x ^{n} = a\\) is solvable, then we need to show that the sum is equal to \\((n,p-1) = n\\).  First we show that there are exactly \\(n\\) characters of order dividing \\(n\\). Let \\(\\chi\\) be such a character then \\(\\chi(g)\\) is an n-th root of unity, and since there are at most \\(n\\) such roots, there are at most \\(n\\) such characters. Let \\(\\chi(g) = e ^{2\\pi i/n}\\), then \\(\\varepsilon,\\chi,\\chi^{2},\\dots,\\chi^{n-1}\\) are all distinct characters of order dividing \\(n\\), hence proof.</p> <p>Now there is a \\(\\beta \\in \\mathbb{F}_{p} ^{*}\\) such that \\(\\beta^{n} = a\\), and so, $$ \\sum_{\\chi^{n} = \\varepsilon} \\chi(a) = \\sum_{\\chi^{n} = \\varepsilon} \\chi(\\beta^{n}) = \\sum_{\\chi^{n} = \\varepsilon} \\chi^{n}(\\beta) = \\sum_{\\chi^{n} = \\varepsilon} \\varepsilon(\\beta) = n $$.</p>"},{"location":"Multiplicative%20Characters/#references","title":"References","text":"<p>[[Groups]] Finite Fields [[nth Power Residues]]</p>"},{"location":"Multiplicity%20Codes/","title":"Multiplicity Codes","text":"<p>202310261610</p> <p>Tags : Algorithmic Coding Theory</p>","tags":["Note","Incomplete"]},{"location":"Multiplicity%20Codes/#multiplicity-codes","title":"Multiplicity Codes","text":"<p>Generalisations of RM codes. Improves upon (??) in the high-rate regime degree in RM codes need to be smaller than the field size</p>","tags":["Note","Incomplete"]},{"location":"Multiplicity%20Codes/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Multiset/","title":"Multiset","text":"<p>202306091706</p> <p>Type : #Note Tags : [[Enumerative Combinatorics]]</p>"},{"location":"Multiset/#multiset","title":"Multiset","text":"<p>Intuitively a Multiset is a set with repeated elements, for example $$ {7,3,3,4,4,5} $$. More precisely a Finite Multiset \\(M\\) on a set is a pair \\((S,\\nu)\\) where \\(\\nu:S\\to \\mathbb N\\) such that \\(\\sum\\limits_{x\\in S}\\nu(x)\\) is finite. Where \\(\\nu(x)\\) is the number of repetitions of \\(x\\) and sum of all \\(\\nu(x)\\) is the cardinality of \\(M\\). if \\(S=\\{x_{1}, x_{2}\\dots x_{n}\\}\\) and \\(\\nu(x_{i})=a_{i}\\) then we can write \\(M=\\{x_{1}^{a_{1}}, x_{2}^{a_{2}}\\dots x_{n}^{a_{n}}\\}\\). If \\(|M|=k\\) then we say \\(M\\) is a \\(k\\)-multiset.</p> <p>The total number of multi-subsets of a multiset \\(M\\) is $$ \\prod\\limits_{x\\in S} (\\nu(x)+1) $$ Analogous to the standard set we have a Multi-choose which denotes the number of sub-multisets of a set of order \\(k\\) denoted as  $$ \\left(!!{n\\choose k}!!\\right) $$</p>"},{"location":"Multiset/#references","title":"References","text":""},{"location":"Multivariate%20Polynomial%20Interpolation%20on%20Lines/","title":"Multivariate Polynomial Interpolation on Lines","text":"<p>202310171510</p> <p>Tags : Algorithmic Coding Theory</p>","tags":["Note","Incomplete"]},{"location":"Multivariate%20Polynomial%20Interpolation%20on%20Lines/#multivariate-polynomial-interpolation-on-lines","title":"Multivariate Polynomial Interpolation on Lines","text":"<p>Assume field \\(F\\) is finite. We have a multivariate polynomial \\(p:\\mathbb{F}^{n}\\to\\mathbb{F}\\) of total degree \\(d&lt;|\\mathbb{F}|\\). Fix any line \\(L_{\\mathbf{u},\\mathbf{v}}=\\{\\mathbf{u}+t\\mathbf{v}|t\\in\\mathbb{F}\\}\\subset\\mathbb{F}^{n}\\). Multivariate polynomial restricted to a line, </p>","tags":["Note","Incomplete"]},{"location":"Multivariate%20Polynomial%20Interpolation%20on%20Lines/#simple-fact","title":"Simple fact","text":"<p>We can interpolate \\(p|_{L_{\\mathbf{u},\\mathbf{v}}}\\) given its evaluations at any \\(d+1\\) points on the line \\(L_{\\mathbf{u},\\mathbf{v}}\\).</p>","tags":["Note","Incomplete"]},{"location":"Multivariate%20Polynomial%20Interpolation%20on%20Lines/#much-stronger-fact","title":"Much stronger fact","text":"<p>We can interpolate \\(p\\) on the line by a function that is both - linear (the value of \\(p\\) at any point on the line is a linear function of its values at \\(d+1\\) other points on the line) - \"universal\" (depends neither on \\(\\mathbf{u},\\mathbf{v}\\) nor on \\(p\\))</p>","tags":["Note","Incomplete"]},{"location":"Multivariate%20Polynomial%20Interpolation%20on%20Lines/#interpolating-low-degree-polys-on-lines","title":"Interpolating low-degree polys on lines","text":"<p>Theorem: For \\(d\\in\\mathbb{N}\\), let \\(\\mathbb{F}\\) be a field of size \\(|\\mathbb{F}|&gt;d\\). Then \\(\\exists\\alpha_{1},\\dots,\\alpha_{d+1}\\in\\mathbb{F}\\) s.t. \\(\\forall n\\in\\mathbb{N}\\) and \\(\\forall p:\\mathbb{F}^{n}\\to\\mathbb{F}\\) of total degree \\(d=deg(p)\\) and every \\(\\mathbf{u},\\mathbf{v}\\in\\mathbb{F}^{n}\\), it holds that \\(p(\\mathbf{u})=\\sum\\limits_{i=1}^{d+1}\\alpha_{i}p(\\mathbf{u}+i\\mathbf{v})\\).</p> <p>Why is this surprising? Suppose we fix the polynomial \\(p\\). It is not clear that for a fixed poly \\(p\\) there exists a linear interpolation function that works for every line!</p> <p>Intuition: First consider the case when \\(d=1\\). For some \\(c_{0},\\dots,c_{n}\\in\\mathbb{F}\\), \\(p(\\mathbf{w})=c_{0}+\\sum\\limits_{i\\in[n]}c_{i}\\mathbf{w}_{i}\\). Fix an arbitrary line \\(L_{\\mathbf{u},\\mathbf{v}}\\) and consider two consecutive points in the line, \\(\\mathbf{u}+t\\mathbf{v}\\), \\(\\mathbf{u}+(t+1)\\mathbf{v}\\). eg, \\(p(\\mathbf{u})=2p(\\mathbf{u}+\\mathbf{v})-p(\\mathbf{u}+2\\mathbf{v})\\)</p> <p>The directional derivative of \\(p\\) in the direction of \\(\\mathbf{v}\\) of the line is constant. \\(\\Delta_{\\mathbf{v}}p(\\mathbf{w})=p(\\mathbf{w}+\\mathbf{v})-p(\\mathbf{w})\\) It reduces the degree of the polynomial by \\(1\\). For a pair of points \\(\\mathbf{w},\\mathbf{w}'\\), (delta is the same for both)</p>","tags":["Note","Incomplete"]},{"location":"Multivariate%20Polynomial%20Interpolation%20on%20Lines/#references","title":"References","text":"<p>[[Reed-Muller Codes]] Local Correcting and Decoding of RM codes Local Decoding</p>","tags":["Note","Incomplete"]},{"location":"NP%20Complete/","title":"NP Complete","text":"<p>202301051201</p> <p>Type : #Note Tags : [[Complexity Theory]]</p>"},{"location":"NP%20Complete/#np-complete","title":"NP Complete","text":"<p>A problem is said to be NP Hard if \\(\\forall A\\in NP\\) \\(\\(A\\le_PL\\)\\) If \\(L\\in NP\\) and \\(L\\) is NP Hard then \\(L\\) is called NP Complete</p> <p>The Cook-Levin Theorem states that an SAT is NP Complete Proof: - Idea: Let \\(M\\) be a nondeterminisitc turing machine with run time \\(n^k\\) and \\(L=\\mathbb L(M)\\), we want to show that \\(L \\le_P SAT\\). To do this, we will show that for every language in NP, we can make a non deterministic turing machine such that it will take a polynomial time computational path, and for every corresponding path we can encode it in a boolean expression such that if the computational path is valid and is accepted then the boolean expression would be satisfied.  - </p>"},{"location":"NP%20Complete/#related-problems","title":"Related Problems","text":""},{"location":"NP%20Complete/#references","title":"References","text":"<p>NP Complexity Class Reduction</p>"},{"location":"NP%20Complexity%20Class/","title":"NP Complexity Class","text":"<p>202301022001</p> <p>Type : #Note Tags : [[Complexity Theory]]</p>"},{"location":"NP%20Complexity%20Class/#np-complexity-class","title":"NP Complexity Class","text":""},{"location":"NP%20Complexity%20Class/#np","title":"NP","text":"<pre><code>title:\nClass of decision problems that can be verified in a [deterministic turing machine](&lt;./Turing Machines.md&gt;) in polynomial time, or the the problems that can be decided by a non deterministic turing machine in polynomial time.\n</code></pre>"},{"location":"NP%20Complexity%20Class/#examples","title":"Examples","text":"<ul> <li>SAT - Given a boolean expressions is there an assignment of variables such that the expression evaluates to TRUE </li> <li>3-COLOUR- Given a graph, is there a way to colour the vertices using three colours such that no two neighbouring vertices have the same colour</li> <li>HAMILTONIAN- Given a graph, does it contain a Hamiltonian path.</li> </ul> <p>If P \\(\\ne\\) NP, then there are infinitely many complexity classes between P and NP Complete. Problems like Factoring and Graph Isomorphism are problems which are thought to be neither NP Complete nor P</p>"},{"location":"NP%20Complexity%20Class/#related-problems","title":"Related Problems","text":""},{"location":"NP%20Complexity%20Class/#references","title":"References","text":"<p>Complexity Classes</p>"},{"location":"NTIME%28f%29%20Complexity%20Class/","title":"NTIME(f) Complexity Class","text":""},{"location":"NTIME%28f%29%20Complexity%20Class/#ntimef","title":"NTIME(f)","text":"<pre><code>title:\nClass of decision problems that can be decided by a Nondeterministic Turing Machine in $O(f(n))$ time.\n</code></pre> \\[ NP=\\bigcup_{c\\in\\mathbb N}NTIME(n^c) \\]"},{"location":"NTIME%28f%29%20Complexity%20Class/#examples","title":"Examples","text":""},{"location":"NTIME%28f%29%20Complexity%20Class/#related-problems","title":"Related Problems","text":""},{"location":"NTIME%28f%29%20Complexity%20Class/#references","title":"References","text":"<p>Complexity Classes</p>"},{"location":"N_L%20admits%20Quantifier%20Elimination/","title":"N L admits Quantifier Elimination","text":"<p>202311291711</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"N_L%20admits%20Quantifier%20Elimination/#n_l-admits-quantifier-elimination","title":"N_L admits Quantifier Elimination","text":"<p>As discussed in Quanitifier Elimination for Natural Numbers With Successor. It is sufficient to consider formulas of the form  $$ \\exists x(\\alpha_{1}\\land\\alpha_{2}\\dots \\alpha_{n}) $$ such that each \\(\\alpha_i\\) contains \\(x\\). And there is no negation.</p> <p>If an atomic formula is a congruence, Then we replace all the instances of the variable with the other side of the congruence.</p> <p>Otherwise all of the formulae are inequalities. Say it is of the form $$ \\exists x \\Big(\\bigwedge_{i} t_{i}&lt;\\mathbf{S}^{m_{i}}(x)\\quad\\land\\quad \\bigwedge_{j}S^{n_{j}}(x)&lt;u_{j} \\Big) $$</p> <p>[!note] Idea We have a block of lower bounds followed by a block of upper bounds. We need to have a gap between the biggest lower bound and the smallest upper bound, but since we do not have a max and min function, we can check if there is a gap between every lower bound and upper bound. And that every upper bound is positive.</p> <p>We successively rewrite it as $$ \\begin{align} \\exists x \\bigwedge_{i,j} (t_{i}&lt;\\mathbf{S}^{m_{i}}x\\quad\\land\\quad \\mathbf{S}^{n_{j}}&lt;u_{j})\\ \\exists x \\bigwedge_{i,j} (\\mathbf{S}<sup>{n_{j}}t_{i}&lt;\\mathbf{S}</sup>{m_{i}+n_{j}}x&lt;\\mathbf{S}^{m_{i}}u_{j}) \\ \\exists x \\bigwedge_{i,j} (\\mathbf{S}<sup>{n_{j}+1}t_{i}&lt;\\mathbf{S}</sup>{m_{i}}u_{j})\\quad\\land\\quad \\bigwedge_{j}\\mathbf{S}^{n_{j}}\\mathbf{0}&lt;u_{j} \\ \\end{align} $$</p>","tags":["Note","Incomplete"]},{"location":"N_L%20admits%20Quantifier%20Elimination/#references","title":"References","text":"<p>Quanitifier Elimination for Natural Numbers With Successor</p>","tags":["Note","Incomplete"]},{"location":"Naive%20Algorithm%20for%20Reachability%20in%20Zone%20Automata/","title":"Naive Algorithm for Reachability in Zone Automata","text":"<p>202310192010</p> <p>Tags : Timed Automata</p>","tags":["Note"]},{"location":"Naive%20Algorithm%20for%20Reachability%20in%20Zone%20Automata/#naive-algorithm-for-reachability-in-zone-automata","title":"Naive Algorithm for Reachability in Zone Automata","text":"","tags":["Note"]},{"location":"Naive%20Algorithm%20for%20Reachability%20in%20Zone%20Automata/#the-algorithm","title":"The Algorithm","text":"<p>Input: Timed Automata \\(\\mathcal A\\) Output: <code>Yes</code> if any of the accepting states is reachable from the start state and <code>No</code> otherwise</p> <p> ^a9400a</p>","tags":["Note"]},{"location":"Naive%20Algorithm%20for%20Reachability%20in%20Zone%20Automata/#correctness-of-the-algorithm","title":"Correctness of the Algorithm","text":"<p>The above algorithm does not compute the entire zone graph, it does some optimizations to reduce the search space by not considering zones that already covered by a Passed node.</p> <pre><code>title:Soundess\nIf the algorithm finds a path from starting state to one of the final states, then there will be a corresponding path in the timed automata.\n</code></pre> <p>This is because every transition suggest by the algorithm corresponds to a transition in the Zone Automaton, and reachability in Zone Automaton implies reachability in the corresponsing Timed Automata.</p> <pre><code>title:Completeness\nLet $\\rho:=(q_{0},v_{0})\\xrightarrow{\\delta_{0},t_{0}}(q_{1},v_{1})\\xrightarrow{\\delta_{1},t_{1}}\\cdots\\xrightarrow{\\delta_{n-1},t_{n-1}}(q_{n}, v_{n})$  be a run on the Timed Automata to one of the final states where $q_{0}\\dots q_{n-1}$ are not final states and $q_{n}$ might or might not be one. Then for each configuration in the run, there is a a node $(q_{i},Z_{i})$ such that $v_{i}\\in Z_{i}$\n</code></pre> <p>Proof is by Induction on length of run to \\((q_{n}, v_{n})\\)</p> <p>Base Case We know that \\(v_{0}\\in Z_{0}\\). And when the loop is entered for the first time, \\((q_{0}, Z_{0})\\) is added to the \\(\\text{Passed}\\) list.</p> <p>Induction Step: Assuming that for each configuration in run given above upto \\((q_{n-1},v_{n-1})\\) we have a corresponding symbolic state in the \\(\\text{Passed}\\) list.  As \\((q_{n-1}, Z_{n-1})\\) is in \\(\\text{Passed}\\), any transition that can be taken form that zone, will be put in the \\(\\text{Waiting}\\) list. Which includes the \\(\\tau:=(q_{n-1},v_{n-1})\\xrightarrow{\\delta_{n-1},t_{n-1}}(q_{n},v_{n})\\). Since it a possible transition, we have \\(Z_{n}=\\text{Post}_{\\tau}(Z_{n-1})\\ne \\emptyset\\) hence \\((q_{n},Z_{n})\\) will be put in the \\(\\text{Waiting}\\) list. Which will then be passed if there isn't \\((q_{n}, Z_{n}')\\) such that \\(Z_{n}\\subseteq Z_{n}'\\). In which case we will already be done.</p> <p>Although we did show that our algorithm is Complete and Sound, this does not mean that our algorithm will terminate.</p> <pre><code>title: Non-termination\nThere exists a Timed Automata for which the above algorithm might not terminate.\n</code></pre> <p>Consider the following Timed Automata  This has two clocks and no accepting state. The Zone after when the transition is taken from \\(q_{0}\\to q_{1}\\) can be given by \\(x=y\\) which of the form \\(x-y=k\\) with \\(k=0\\). For every Zone of such form, we get that the transition from \\(q_{1}\\) can be taken and it gives the zone \\(x-y=k+1\\). Hence the program does not terminate.</p>","tags":["Note"]},{"location":"Naive%20Algorithm%20for%20Reachability%20in%20Zone%20Automata/#references","title":"References","text":"<p>Simulation for Zone Automata Reachability Algorithm for Zone Automata</p>","tags":["Note"]},{"location":"Natural%20Deduction/","title":"Natural Deduction","text":"<p>202308141408</p> <p>Type : #Note Tags : [[Logic]], [[Type Theory]]</p>"},{"location":"Natural%20Deduction/#natural-deduction","title":"Natural Deduction","text":"<p>Natural Deductions (denoted by \\(\\text{NJ}(\\to,\\land,\\lor,\\perp)\\)) is a simpler syntax for Intuitionistic logic. Natural deduction uses \"Judgments\" to prove statements</p> <pre><code>title:Judgements\nJudgements are statements of the form\n$$\n\\Gamma\\vdash\\phi\n$$\nWhere $Gamma$ is a set of assumptions, $phi$ is the statement we need to prove.\nThe above statement can be read as \n$$\n\\text{$\\Gamma$ proves $\\phi$}\n$$\n</code></pre> <p>Natural deductions has the following rules It has mainly two types of rules - Rules that Introduce an operator (Labelled with \\(I\\)) - Rules that use, or eliminate an operator (Labelled with \\(E\\))</p> <p>The following are the rules for natural deductions in Intuitionistic Logic.</p> <p></p>"},{"location":"Natural%20Deduction/#references","title":"References","text":"<p>P implies not-not P - Intuitionistic Logic Sequent Calculus</p>"},{"location":"Natural%20Numbers%20with%20Successor%20Function%20is%20Complete/","title":"Natural Numbers with Successor Function is Complete","text":"<p>202311291519</p> <p>tags : [[Logic]]</p>","tags":["Example"]},{"location":"Natural%20Numbers%20with%20Successor%20Function%20is%20Complete/#natural-numbers-with-successor-function-is-complete","title":"Natural Numbers with Successor Function is Complete","text":"<p>[!note] Theorem $$ \\text{Cn}\\; A_{S} = \\text{Th}\\; \\mathfrak N_{S} $$</p> <p>We prove the above theorem by considering Arbitrary models for \\(A_S\\) $$ \\mathfrak U = (|\\mathfrak U|; \\mathbf{0}^\\mathfrak U,S^\\mathfrak U) $$ Since we have that every element has a successor and every nonzero element has a unique predecessor. We have the standard set of points  $$ \\mathbf{0}^\\mathfrak U \\to \\mathbf{S}\\mathbf{0}^\\mathfrak U\\to \\mathbf{S}\\mathbf{S}\\mathbf{0}^\\mathfrak U \\to \\dots $$</p> <p>[!info] Z-Chains For any other point \\(a\\) in \\(\\mathfrak U\\), we have its successor, successor to that and so on. We also have its predecessor, and predecessor to that an so on. We also know that there are no finite cycles, so each \\(a\\) belongs to a \"Z-chain\". $$ \\dots \\to * \\to * \\to a^\\mathfrak U \\to \\mathbf{S}a^\\mathfrak U\\to \\mathbf{S}\\mathbf{S}a^\\mathfrak U \\to \\dots $$</p> <p>It is trivial to show that  $$ \\text{card}(|\\mathfrak U|)= \\begin{cases}     \\aleph_{0} &amp; \\text{if \\(\\mathfrak U\\) has countably many Z-chains}\\     \\lambda &amp; \\text{if \\(\\mathfrak U\\) has uncountable number \\(\\lambda\\) many Z-chains} \\end{cases} $$</p> <p>and </p> <p>[!note] Theorem \\(\\mathfrak U_1\\) and \\(\\mathfrak U_2\\) are isomorphic iff they have the same number of Z-chains.</p> <p>This is true because given any 2 models with same number of Z-chains, we can make a bijection between the set of chains. And for each pair of chains liked by the bijection, we can make a bijection between its elements. This respects the successor operator and hence makes it an isomorphism between models. Using this we trivially get the following corollary.</p> <p>[!hint] Corollary  Two uncountable models are isomorphic if they have the same cardinality.</p> <p>Using the above theorem and the Los-Vaught Test we directly get that \\(\\text{Cn}\\; A_S\\) is complete.</p> <p>We have \\(\\text{Cn}\\; A_{S}\\) is complete, \\(\\text{Th}\\; \\mathfrak N_{A}\\) is consistent and \\(\\text{Cn}\\; A_{S}\\subseteq \\text{Th}\\; \\mathfrak N_{A}\\)  This proves the original theorem.</p> <p>Since \\(\\text{Cn}\\; A_{S}\\) is complete and all sentences in \\(\\text {Cn}\\; A_S\\) are decidable(in the set generated by it, trivially). We have \\(\\text{Th}\\; \\mathfrak N_A\\) is decidable. (Just  keep enumerating all sentences until either a statement or its negation is generated.).</p> <p>A realistically practical decision procedure for \\(\\text{Th}\\; \\mathfrak N_A\\) is given by Quanitifier Elimination for Natural Numbers With Successor.</p>","tags":["Example"]},{"location":"Natural%20Numbers%20with%20Successor%20Function%20is%20Complete/#related","title":"Related","text":"<ul> <li>First Order Logic</li> <li>Decidability of Presburger Arithmetic</li> <li>Quanitifier Elimination for Natural Numbers With Successor</li> <li>Los-Vaught Test</li> </ul>","tags":["Example"]},{"location":"Natural%20Numbers%20with%20Successor%20and%20Ordering/","title":"Natural Numbers with Successor and Ordering","text":"<p>202311291711</p> <p>Tags : [[Logic]]</p>","tags":["Note"]},{"location":"Natural%20Numbers%20with%20Successor%20and%20Ordering/#natural-numbers-with-successor-and-ordering","title":"Natural Numbers with Successor and Ordering","text":"<p>We define the following structure $$ \\mathfrak N_{L}\\quad := \\quad (\\mathbb N;\\mathbf{0},\\mathbf{S},&lt;) $$ The theory of this structure is decidable because it also admits quantifier elimination. But it is finitely axiomatizable and not categorical in any infinte cardinality.</p> <p>The following are a set of axioms for the arithmetic - \\(\\forall y (y\\not\\equiv 0\\to \\exists x(y=\\mathbf{S}x))\\) Every non zero number has a predecessor. - \\(\\forall x\\forall y(x&lt;\\mathbf S y\\leftrightarrow x\\le y)\\) - \\(\\forall x (x\\not&lt;\\mathbf 0)\\)  - \\(\\forall x\\forall y(x&lt;y \\lor x\\equiv y\\lor y&lt;x)\\), trichotomy - \\(\\forall x\\forall y (x&lt;y \\to y\\not&lt;x)\\), antisymmetry - \\(\\forall x\\forall y\\forall z (x&lt;y\\to (y&lt;z\\to x&lt;z))\\), transitivity</p> <p>let the set of axioms be called \\(A_L\\), and we still have that all axioms are true in \\((\\mathfrak N_L)\\) so we again have \\(\\text{Cn}\\;(A_L)\\subseteq\\text{Th}\\;\\mathfrak N_L\\). The inclusion in the other direction is true but not obvious.</p> <p>We can prove it by showing N_L admits Quantifier Elimination</p> <p>This gives us that \\(\\text{Cn}\\; A_L\\) is complete, and \\(\\text{Cn}\\;A_L=\\text{Th}\\;\\mathfrak N_L\\).  This \\(\\text{Th}\\;\\mathfrak N_L\\) is also decidable.</p>","tags":["Note"]},{"location":"Natural%20Numbers%20with%20Successor%20and%20Ordering/#references","title":"References","text":"<p>N_L admits Quantifier Elimination Decidability of Presburger Arithmetic</p>","tags":["Note"]},{"location":"Natural%20Numbers%20with%20Successor/","title":"Natural Numbers with Successor","text":"<p>202311291411</p> <p>Tags : [[Logic]]</p>","tags":["Note"]},{"location":"Natural%20Numbers%20with%20Successor/#natural-numbers-with-successor","title":"Natural Numbers with Successor","text":"<p>We define the following language on \\(\\mathbb N\\) $$ \\mathfrak N_{S} = (\\mathbb N;\\mathbf{0}, S) $$ The axioms that will be used in the theory are 1. \\(\\forall x\\mathbf S x \\ne 0\\), A sentence asserting that \\(0\\) does not have a predecessor 2. \\(\\forall x\\forall y(\\mathbf S x\\equiv \\mathbf S y \\to x \\equiv y)\\). The successor function is injective 3. \\(\\forall y (y\\ne \\mathbf 0 \\to \\exists x (y = \\mathbf S x))\\). Every non-zero number has a predecessor 4. Successor applied multiple times never gives back the arguement     -  \\(\\forall x \\mathbf Sx\\ne x\\)      -  \\(\\forall x \\mathbf S^2x\\ne x\\)      -  \\(\\dots\\)     - \\(\\forall x \\mathbf S^n x \\ne x\\)</p> <p>Let the above set of axioms be called \\(A_S\\) and since all of them are trivially true in \\(\\mathfrak N_S\\), we have  $$ \\text{Cn}\\; A_{S}\\subseteq \\text{Th}\\;\\mathfrak N_{S} $$ where  - \\(\\text {Cn}\\) is the set of formulas that can be derived using the axioms (which will  all be sentences as axioms are sentences) - \\(\\text {Th}\\) is the set of sentences that are true in a theory.</p> <p>What isn't easy to show is that \\(\\text{Cn}\\ A_S = \\text{Th}\\ \\mathfrak N_S\\). i.e Natural Numbers with Successor Function is Complete</p>","tags":["Note"]},{"location":"Natural%20Numbers%20with%20Successor/#references","title":"References","text":"<p>Quanitifier Elimination for Natural Numbers With Successor Natural Numbers with Successor Function is Complete Decidability of Presburger Arithmetic Natural Numbers with Successor and Ordering</p>","tags":["Note"]},{"location":"Network%20Flows/","title":"Network Flows","text":"<p>202310181410</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note","Incomplete"]},{"location":"Network%20Flows/#network-flows","title":"Network Flows","text":"<p>\\(G=(V,E)\\) directed graph \\(s,t\\in V\\), \\(s:\\) source, \\(t:\\) sink \\(c(u,v)\\ge 0\\ \\forall(u,v)\\in E\\)</p> <p>Flow: An assignment \\(f(u,v)\\) \\(\\forall (u,v)\\in E\\) s.t. - \\(f(u,v)\\le c(u,v)\\ \\forall (u,v)\\in E\\)  - \\(f(u,v)=-f(v,u)\\) - \\(\\sum\\limits_{u:(u,v)\\in E}f(u,v)=\\sum\\limits_{w:(v,w)\\in E}f(v,w)\\) \\(\\forall v\\in V\\setminus \\{ s,t \\}\\)</p> <p>Value of a flow \\(f=|f|=\\sum\\limits_{v:(s,v)\\in E}\\)</p>","tags":["Note","Incomplete"]},{"location":"Network%20Flows/#algorithm","title":"Algorithm","text":"<p>While \\(\\exists s-t\\) path \\(P\\) in \\(G_{f}\\) { Set up a flow \\(f'\\) along \\(P\\), \\(|f'|\\) as large as possible. Find residual graph \\(G_{f'}\\). \\(G_{f}=G_{f'},f=f+f'\\) } Return \\(f\\)</p> <p>Residual graph \\(G_{f}\\): For each edge \\((u,v)\\in E\\) s.t. \\(f(u,v)&gt;0\\), \\(G_{f}\\) has an edge \\((v,u)\\) and \\(c(v,u)=f(u,v)\\).</p> <p>To prove: 1. A flow \\(f'\\) in \\(G_{f}\\) iff \\(f+f'\\) is a flow (Exercise.) 2. The flow output by the edge is a max-flow</p> <p>\\(s,t\\) cut: A partition of \\(V\\) into \\(S\\) and \\(T\\)</p>","tags":["Note","Incomplete"]},{"location":"Network%20Flows/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Network%20Reliability/","title":"Network Reliability","text":"<p>202311081411</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note","Incomplete"]},{"location":"Network%20Reliability/#network-reliability","title":"Network Reliability","text":"<p>Given: \\(G=(V,E)\\) undirected, each edge can fail with probability \\(p\\), independent of other edges Goal: To compute \\(p_{\\text{fail}}\\), i.e. the probability that \\(G\\) gets disconnected</p> <p>This problem is #P complete. #P complete: Class of functions that count the number of solutions to a problem in NP (harder than NP), eg, #perfect matchings, #SAT</p> <pre><code>title: **FPRAS: (Fully Polytime Randomized Approximation Scheme)**\nGiven $(G,p,\\epsilon)$, an FPRAS is a polytime algorithm that outputs a value $z$ st $Pr[(1-\\epsilon)p_{\\text{fail}}\\leq z\\leq(1+\\epsilon)p_{\\text{fail}}]\\geq \\frac{3}{4}$.\n\n*Note:* There is no FPRAS for the counting version of NP-complete problems.\n</code></pre> <p>Network reliability: Counting the total weight of disconnected subgraphs of \\(G\\), where weight of a subgraph \\(H\\) that has \\(|E|-r\\) edges is \\(p^{r}(1-p)^{|E|-r}\\). How do we do this?</p> <pre><code>title: Inspiration: Unbiased estimator\nThere are regions in space $R\\subseteq U$. We want to estimate $\\frac{|R|}{|U|}$. We will throw darts at the entire region with uniform probability and compute the number of darts that land in $|R|$. But this will require throwing a lott of darts if the region is very small. So we will need to do work to see what values this works for. Which is what we will do now.\n</code></pre> <p>Do \\(i=1,\\dots,t\\) times: Find a random subgraph of \\(G\\) by choosing each edge with \\((1-p)\\). Set \\(X_{i}=1\\) iff the subgraph is disconnected.</p> <p>\\(X= \\dfrac{1}{t}\\sum X_{i}\\) \\(\\mathbb{E}[X_{i}]=Pr[X_{i}=1]=p_{\\text{fail}}\\)(??) \\(\\mathbb{E}[X]=p_{\\text{fail}}=\\mu\\) By Chebyshev's inequality, \\(Pr[]\\)  idk random calculations</p> <p>Idea: \\(p_{\\text{fail}}\\geq p^{c}\\) where \\(c\\) is the size of a minimal cut in \\(G\\).</p> <ul> <li>If \\(p^{c}\\geq \\frac{1}{n^{4}}\\), we pick random subgraphs of \\(G\\) \\(O\\left( \\frac{n^{4}}{\\epsilon^{2}} \\right)\\) times. (Each edge is picked with probability \\((1-p)\\).)</li> <li>If \\(p^{c}&lt; \\frac{1}{n^{4}}\\),</li> <li>Find \\(\\alpha\\) s.t. \\(Pr[\\text{a cut of size }\\geq\\alpha c \\text{ fails}]\\leq\\epsilon p^{2}\\).</li> <li>Number of cuts of size \\(\\leq\\alpha c\\) is at most \\(n^{2\\alpha}\\). Call them 'small' cuts.</li> <li>Enumerate all small cuts with probability \\(\\geq 1-\\delta\\).</li> <li>Estimate failure probability of small cuts.</li> </ul> <p>Let \\(C_{1},C_{2},\\dots,C_{m}\\) be 'small' cuts in \\(G\\). \\(C_{i}=\\{ e_{i_{1}},\\dots,e_{i_{r_{i}}} \\}\\) \\(Pr[\\bigvee\\limits_{i=1}^{m} C_{i}\\text{ fails}]=Pr[\\bigvee\\limits_{i=1}^{m}(x_{e_{i_{1}}}\\land x_{e_{i_{2}}}\\land\\dots \\land x_{e_{i_{r_{i}}}})=1]\\), where each \\(x_{j}=1\\) with probability \\(p\\). We consider the case where \\(p=\\frac{1}{2}\\).</p> <p>For \\(p=\\frac{1}{2}\\), \\(n=\\) number of variables, \\(m=\\) number of terms DNF formula (Disjunction Normal Form) \\(\\phi=\\bigvee\\limits_{i=1}^{m}(x_{e_{i_{1}}}\\land x_{e_{i_{2}}}\\land\\dots \\land x_{e_{i_{r_{i}}}})\\), each 'clause' is called a term here. \\(Pr[\\phi=1]=\\dfrac{\\text{\\# satisfying assignments to }\\phi}{2^{n}}\\)</p> <p>Counting #\\(\\phi\\): \\(S_{i}=\\) set of satisfying assignments for the \\(i^{th}\\) term \\(l_{i}=\\) # of literals in the \\(i^{th}\\) term \\(|S_{i}|=2^{n-l_{i}}\\) #\\(\\phi=|\\bigcup\\limits_{i=1}^{m}S_{i}|\\) </p> <pre><code>title: The numerator is too small compared to the denominator(??)\n</code></pre> <pre><code>title: [Karp-Luby '83'] **Importance Sampling:**\n**Idea:** To choose a smaller sample space and pick many random assignments from it.\nLet $a_{1},\\dots,a_{s}$ be satisfying assignments.\nSample space: $U=\\{ (a,i)\\ | \\text{ assignment }a \\text{ satisfies the }i^{th} \\text{ term} \\}$\nCall $(a,i)\\in U$ *special* if $i=\\min\\{ i'\\ |\\ (a,i)\\in U \\}$. We want to estimate $\\dfrac{\\text{\\# special elements in U}}{|U|}$. Number of special elements $=s=\\#\\phi$\nWe can make an $m\\times s$ grid of $S_{i}\\times a_{j}$.\n\n**Sampling:**\nSelect a term $i$ with probability $\\dfrac{|S_{i}|}{|U|}$.\nSelect a uniformly random assignment that satisfies $i^{th}$ term.\n$|U|=\\sum\\limits_{i=1}^{m}|S_{i}|$.\nNumber of samples required $=\\dfrac{4m}{\\epsilon^{2}}$.\n</code></pre> <ol> <li>Enumeration of small cuts: Execute Karger's algorithm \\(r\\) times. Output the cut if it is a cut of size \\(\\leq\\alpha c\\) different from the previously output cuts.</li> </ol> <pre><code>title: Motivation\nWe will let $r$ be variable. We want to find how many times we need to execute Karger's algorithm to enumerate all small cuts. So we will calculate the probability that a particular cut is not output after $r$ times.\n</code></pre> <p>\\(Pr[\\text{a cut }C,|C|\\leq\\alpha c,\\text{ is output}]\\geq \\dfrac{1}{n \\choose {2\\alpha}}\\approx \\dfrac{1}{n^{2\\alpha}}\\). (\\(\\theta=n^{2\\alpha}\\)) \\(Pr[C\\text{ is never output}]\\leq\\left( 1-\\frac{1}{\\theta} \\right)^{r}\\leq e^{\\frac{-r}{\\theta}}\\) We want \\(e^{\\frac{-r}{\\theta}} \\leq \\frac{\\delta}{\\theta }\\). \\(r\\geq\\theta\\left( \\log\\theta+\\log \\frac{1}{\\delta} \\right)\\).</p> <ol> <li>Find \\(\\alpha\\) s.t. \\(Pr[\\text{a cut of size }\\geq\\alpha c \\text{ fails}]\\leq\\epsilon p^{2}\\). \\(p^{c}=n^{-(4+\\beta)}\\) Let \\(C_{1},C_{2},\\dots\\) be large cuts with sizes \\(c_{1}\\leq c_{2}\\leq\\dots\\). Consider the first \\(n^{2\\alpha}\\) cuts. $$ \\begin{align} Pr[\\text{any of the first }n^{2\\alpha}\\text{ cuts fails}]&amp;\\leq n<sup>{2\\alpha}.p</sup>{\\alpha c}\\ &amp;= n^{2\\alpha-(4+\\beta)\\alpha}\\ &amp;= n^{-(2+\\beta)\\alpha} \\end{align} $$</li> </ol> <p>Number of cuts of size \\(\\leq\\gamma c\\) is at most \\(n^{2\\gamma}\\).</p> <p>stuff</p>","tags":["Note","Incomplete"]},{"location":"Network%20Reliability/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Networks%20of%20Timed%20Automata/","title":"Networks of Timed Automata","text":"<p>202310101210</p> <p>Tags : Timed Automata</p>","tags":["Note"]},{"location":"Networks%20of%20Timed%20Automata/#networks-of-timed-automata","title":"Networks of Timed Automata","text":"<pre><code>title:Motivation\nIn a lot of real world cases. There are multiple systems working together in sync with each other, acting as one organized structure.\n\nThe organized structure can get very complicated, and its often easier to model the smaller parts of the system that are then connected by constraints and relations. \n\nA **Network of Timed Automata** is a collection of Timed Automata, that run simultaneously to model different parts of a larger system. These parts of synced together by letters that are in common between the Automata. I interpret these are the same *stimulus* or *action* to/by multiple automata.\n\nAlthough it is possible to model the entire **Network** with just a single _monolithic_ automaton. But that is often more complicated to work with.\n\nThis also demands the use for a new reachability algorithm (a lot of times to figure out if its possible to reach a problematic state which will be made clear in the examples) as the **Region Automata** idea is \"absolutely infeasible\".\n</code></pre> <p>A Network of Timed Automata is a set of processes \\(\\langle\\mathcal A_{1},\\mathcal A_{2},\\dots\\mathcal A_{k}\\rangle\\) running in parallel, that have alphabets \\(\\Sigma_{1},\\Sigma_{2}\\dots\\Sigma_{k}\\) which need not be disjoint, and a set of local clocks for each. That is \\(X_{i}\\cap X_{j}=\\emptyset\\) whenever \\(i\\ne j\\). Also, given that \\(a\\in\\Sigma_{i}\\) and \\(a\\in \\Sigma_{j}\\). The Automata \\(\\mathcal A_{i}\\) and \\(\\mathcal A_{j}\\) both must accept the letter at the same time (The letter is accept only when guards of both Automata satisfy).</p> <pre><code>title: Example of a Network\n![[Drawing 2023-10-11 01.28.18.excalidraw]]\n\nHere the Automata have the languages $\\{a,c\\}$ and $\\{b, c\\}$ respectively.\n\nThe language accepted by this is either `ab` or `ba` at time stamps $1\\dots n-1$ and accept a $c$ at time $n$\n</code></pre> <p>To clarify on the Synchronization part, another interesting situation is when two automata from the network are in a configuration where only one of the automata can accept a common letter, then the entire network will come to a halt.</p> <pre><code>title: Network which does not accept due to synchronization problems\n\nIn the above example if we let the alphabet of the first automaton to be $\\{a,b,c\\}$ then the network cannot accept $b$, hence the Language of the network will be $\\{c\\}$\n</code></pre> <p>Here is a more non trivial example where a Network would be useful.</p> <p>The Above example also shows the importance of having a fast Algorithm for Reachability for Networks of Timed Automata. One way to do its to it is to construct a Monolithic Timed Automaton and make its Zone Automaton, and check for reachability on it.</p>","tags":["Note"]},{"location":"Networks%20of%20Timed%20Automata/#references","title":"References","text":"<p>Monolithic Timed Automaton for a Network of Timed Automata Zone Automata Train Track Crossing for Timed Automata</p>","tags":["Note"]},{"location":"Newman%27s%20Lemma/","title":"Newman's Lemma","text":"<p>202309070909</p> <p>Type : #Note  Tags : [[Lambda Calculus]], [[Type Theory]]</p>"},{"location":"Newman%27s%20Lemma/#newmans-lemma","title":"Newman's Lemma","text":"<pre><code>title: Lemma\n$$\n\\text{SN + WCR = CR}\n$$\n</code></pre> <p>Newman's Lemma states that Strong Normalization Theorem and Weak Church Rosser Theorem Implies the Church Rosser Theorem</p> <p>The proof is using WCR and \\(\\nu\\) value of lambda terms which reduces strictly on reductions.</p>"},{"location":"Newman%27s%20Lemma/#proof","title":"Proof","text":"<p>The proof is by induction on \\(\\nu\\) value of lambda term.</p> <p>Given a lambda term \\(L\\) and \\(\\nu(L)=k\\). And given that Church Rosser Theorem is valid for all terms \\(L'\\) where \\(\\nu(L')&lt;k\\). Say \\(L\\) reduces to \\(L_{1}\\) and \\(L_{2}\\). We have \\(2\\) cases</p> <p>Case 1: The first reduction in \\(L\\rightsquigarrow L_1\\) and \\(L \\rightsquigarrow L_{2}\\) is the same. We have \\(L\\to_{\\beta}L'\\) and \\(L'\\rightsquigarrow L_1\\), \\(L'\\rightsquigarrow L_2\\). We have by induction \\(L_{1}\\rightsquigarrow F\\) and \\(L_{2}\\rightsquigarrow F\\)(Church Rosser on \\(L'\\)). This proves Church Rosser for \\(L\\). ![[Drawing 2023-09-07 13.20.36.excalidraw|250]]</p> <p>Case 2: The first reduction in \\(L\\rightsquigarrow L_{1}\\) is different from \\(L\\rightsquigarrow L_{2}\\)  we have \\(L\\to_{\\beta}L_{l}\\rightsquigarrow L_{1}\\) and \\(L\\to_{\\beta}L_{r}\\rightsquigarrow L_{2}\\).  Applying Weak Church Rosser Theorem for \\(L\\) we get \\(S\\) such that \\(L_{l}\\rightsquigarrow S\\) and \\(L_{r} \\rightsquigarrow S\\).  Then by induction, applying Church Rosser Theorem for \\(L_{l}\\) and \\(L_{r}\\) we get \\(R_{1}, R_{2}\\)  such that  $$ \\begin{align} L_{1} \\rightsquigarrow R_{1} &amp;\\text{ and } S\\rightsquigarrow R_{1} &amp;\\text{Church Rosser on \\(L_{l}\\)}\\ L_{2} \\rightsquigarrow R_{2} &amp;\\text{ and } S\\rightsquigarrow R_{2} &amp;\\text{Church Rosser on \\(L_{r}\\)}\\ \\end{align} $$ Since \\(\\nu(L_{l})&lt;\\nu(L)&gt;\\nu(L_{r})\\).</p> <p>Then we apply Church Rosser Theorem on \\(S\\) as \\(\\nu(S)&lt;\\nu(L)\\) we get \\(L'\\) such that \\(R_{1}\\rightsquigarrow L'\\) and \\(R_{2}\\rightsquigarrow L'\\). ![[Drawing 2023-09-07 13.22.15.excalidraw|250]]</p> <p>The base cases for our induction are the Normal Terms for which Church Rosser is trivially True. And we will always reach Normal Terms because of Strong Normalization Theorem.</p>"},{"location":"Newman%27s%20Lemma/#references","title":"References","text":"<p>Strong Normalization Theorem</p>"},{"location":"Normal%20Extensions/","title":"Normal Extensions","text":"<p>202304161204</p> <p>Type : #Note Tags :[[Algebra]]</p>"},{"location":"Normal%20Extensions/#normal-extensions","title":"Normal Extensions","text":"<pre><code>title:\nA field extension $L/K$ is called _normal_ if every irreducible poly in $K[X]$ that has a root in $L$ splits completely in $L[X]$.\n</code></pre>"},{"location":"Normal%20Extensions/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nIf $|\\mathrm{Aut}(L /K)| = [L : K]$ then $L/K$ is normal\n</code></pre>"},{"location":"Normal%20Extensions/#proof","title":"Proof:","text":"<p>Look at Theorem 4 in Galois Correspondence for a proof.</p>"},{"location":"Normal%20Extensions/#theorem-2","title":"Theorem 2:","text":"<pre><code>title:\nThe splitting field of a separable polynomial is a normal extension.\n</code></pre>"},{"location":"Normal%20Extensions/#proof_1","title":"Proof:","text":"<p>This is just theorem 3 in Galois Correspondence followed by an application of theorem 1 above.</p>"},{"location":"Normal%20Extensions/#theorem-3","title":"Theorem 3:","text":"<pre><code>title:\nFor a finite extension $L/K$, TFAE:\n1) $L/K$ is normal\n2) $L$ is the splitting field over $K$ of a polynomial in $K[X]$\n</code></pre>"},{"location":"Normal%20Extensions/#proof_2","title":"Proof:","text":"<p>(1) \\(\\implies\\)(2) Write \\(L = K(\\alpha_{1},\\alpha_{2},\\dots,\\alpha_{n})\\), take the minimal polynomial of all the \\(\\alpha_{i}\\)'s, and take their product (get rid of repeated factors in the product). Now \\(L\\) is the splitting field of this polynomial over \\(K\\).</p> <p>(2) \\(\\implies\\) (1) There is a poly \\(f(X) \\in K[X]\\) s.t. \\(L\\) is a splitting field of \\(f\\) over \\(K\\). Let \\(f(X) = (X-\\beta_{1})(X-\\beta_{2})\\dots(X-\\beta_{n})\\), then any \\(\\alpha \\in L\\) is of the form \\(g(\\beta_{1},\\beta_{2},\\dots,\\beta_{n})\\) for some polynomial \\(g \\in K[X_{1},X_{2},\\dots,X_{n}]\\).  Consider  $$ h(X) = \\prod \\limits_{ \\sigma \\in S_{n}} (X - g(\\beta_{\\sigma(1)},\\beta_{\\sigma(2)}\\dots,\\beta_{\\sigma(n)})) $$ Now \\(\\alpha\\) is a root of \\(h\\). Hence its min poly \\(\\pi_{\\alpha} | h\\). But the coefficients of \\(h\\) are symmetric polys in the \\(\\beta_{i}\\)'s, hence they are polynomials in the elementary symmetric polynomials of the \\(\\beta_{i}\\)'s which are just coefficients of \\(f\\), and hence elements of \\(K\\). Thus the coefficients of \\(h\\) are in \\(K\\). Hence \\(h\\) is a poly over \\(K\\) that splits in \\(L\\), hence \\(\\pi_{\\alpha}\\) splits in \\(L\\).</p>"},{"location":"Normal%20Extensions/#examples","title":"Examples","text":"<ol> <li>Any quadratic extension is normal.</li> <li></li> </ol>"},{"location":"Normal%20Extensions/#references","title":"References","text":"<p>Galois Correspondence Extension Field Splitting Fields</p>"},{"location":"Normal%20Spaces/","title":"Normal Spaces","text":"<p>202303071503</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Normal%20Spaces/#normal-spaces","title":"Normal Spaces","text":"<pre><code>title:\nA topological space $X$ is _normal_ if \n1) Every singleton is closed in $X$.\n2) for each pair of disjoint closed subsets $A,B \\subseteq X$, there is a pair of disjoint open subsets $U,V$ of $X$ such that $A \\subset U$ and $B \\subseteq V$.\nWe then say that $U,V$ separate $A$ and $B$.\n</code></pre>"},{"location":"Normal%20Spaces/#lemma","title":"Lemma:","text":"<pre><code>title:\nLet $X$ be $T_{1}$, then $X$ is normal iff for every closed subset $A$ of $X$ and every open subset $U$ containing $A$, there is a open set $V$ containing $A$ and contained in $U$ such that $Cl(V) \\subseteq U$. \n</code></pre>"},{"location":"Normal%20Spaces/#proof","title":"Proof:","text":"<p>Suppose \\(X\\) is normal, then for any \\(A\\) and every open nbhd \\(U\\) of \\(A\\), take \\(U^{c}\\) to be the closed set \\(B\\), then there is a separation of \\(A\\), \\(U^{c}\\) \\(\\implies \\exists V,W\\) s.t. \\(A \\subseteq V, U^{c} \\subseteq W\\), this gives \\(V \\subset Cl(V) \\subset W^{c} \\subset U\\). We are done.</p> <p>Conversely, given \\(A, B\\) disjoint and closed, take \\(U = B^{c}\\) and find \\(V \\subset Cl(V) \\subset U\\). Then \\(V\\) and \\((Cl(V))^{c}\\) gives the separation. </p>"},{"location":"Normal%20Spaces/#note-subspace-and-product-of-normal-spaces-may-not-be-normal","title":"NOTE: Subspace and product of normal spaces may not be normal.","text":""},{"location":"Normal%20Spaces/#lemma_1","title":"Lemma:","text":"<pre><code>title:\nClosed subspace of a normal space is normal.\n</code></pre>"},{"location":"Normal%20Spaces/#lemma_2","title":"Lemma:","text":"<pre><code>title:\nLet $Y$ be a topo space that is not normal. Then there is a topo space $X$ containing $Y$ as a subspace such that $X$ satisfies the second condition in the definition of a normal space.\n</code></pre>"},{"location":"Normal%20Spaces/#proof_1","title":"Proof:","text":"<p>Take \\(X = Y \\cup \\{\\infty\\}\\), \\(\\tau_{X} = \\tau_{Y} \\cup \\{ X \\}\\). This is normal since any non empty closed subset of \\(X\\) contains \\(\\infty\\), hence they are not disjoint, hence it vacuously satisfies the 2nd condition for a normal space.</p>"},{"location":"Normal%20Spaces/#lemma_3","title":"Lemma:","text":"<pre><code>title:\nEvery regular 2nd countable space is normal.\n</code></pre> <p>Suppose \\(X\\) is the space with \\(\\mathcal{B}\\) as the countable basis. Let \\(C,D\\) be the closed disjoint subsets we wish to separate.</p> <p>Then for any point \\(x \\in C\\), construct a separation \\(U_{x}, V_{x}\\) of \\(x,D\\). Since \\(\\mathcal{B}\\) is a basis, there exists some \\(U \\in \\mathcal{B}\\) such that \\(x \\in U \\subseteq U_{x}\\). Take the union of such \\(U's\\), since \\(\\mathcal{ B}\\) is countable, there is a countable collection of them, let it be \\(\\{ U_{n} \\}_{n \\in \\mathbb{N}}\\). Then \\(U := \\bigcup_{n \\in \\mathbb{N}} U_{n}\\) is an open set covering \\(C\\), and not intersecting \\(D\\). Similarly get \\(\\{ V_{n} \\}_{n \\in \\mathbb{N}}\\) such that \\(V := \\bigcup_{n \\in \\mathbb{N}} V_{n}\\) is a similar cover for \\(D\\).</p> <p>The problem is that \\(U,V\\) may intersect. To combat that, consider \\(U_{n}' := U_n \\setminus (\\bigcup Cl(V_{n}))\\) and \\(V_{n} ' := V_{n} \\setminus (\\bigcup Cl(U_{n}))\\).</p> <p>Check that \\(C \\subset U:=\\bigcup_{n \\in \\mathbb{N}} U_{n}', D \\subset V := \\bigcup_{n \\in \\mathbb{N}} V_{n}'\\) and that \\(U,V\\) are disjoint.</p>"},{"location":"Normal%20Spaces/#examples","title":"Examples","text":"<ol> <li>\\(\\mathbb{R}_{\\ell}\\).    It is \\(T_{1}\\), for any element \\(a\\) of \\(A\\), choose \\([a,x_{a})\\) such that it doesn't intersect \\(B\\), similarly do for all elements of \\(B\\). Then \\(U := \\bigcup_{a \\in A} [a,x_{a})\\) and \\(V := \\bigcup_{b \\in B} [b,x_{b})\\) forms a separation.</li> <li>\\(\\mathbb{R}_{\\ell}^{2}\\) is not normal.</li> <li>Take the diagonal, \\(y=-x\\) this is a closed discrete subspace of \\(\\mathbb{R}_{\\ell}^{2}\\). Then the subset \\(A := \\{ (x,-x) | x \\in \\mathbb{Q} \\}\\) is closed in the parent space, similarly \\(B := \\{ (x,-x) | x \\notin \\mathbb{Q} \\}\\) is also closed in the parent space.</li> <li>Take a separation \\(U,V\\) of \\(A,B\\). </li> <li>Let \\(K_{n}\\) be the set of irrationals \\(x \\in \\mathbb{R}\\) such that \\(\\left[ x,x+\\frac{1}{n}\\right) \\times [-x,-x+\\frac{1}{n}) \\subseteq V\\).</li> <li>Then \\(\\mathbb{R}\\) is a union countably many one point sets (\\(\\mathbb{Q}\\)) and \\(K_{n}'s\\).</li> <li>By baire category, \\(\\bar{K}_{n}\\) has an interval \\((a,b)\\).</li> <li>Then the parallelogram \\((x,-x+\\epsilon)\\) for which \\(a&lt; x&lt;b; 0&lt;\\epsilon&lt; \\frac{1}{n}\\) is contained in \\(V\\).</li> <li>Then any rational point on the diagonal in this interval is a limit point of \\(V\\). Contradiction.</li> <li>\\(\\bar{S}_{\\Omega} \\times \\bar{S}_{\\Omega}\\) is normal since it is Compact and Hausdorff.</li> </ol>"},{"location":"Normal%20Spaces/#related-results","title":"Related Results","text":"<p>1) Metrizable \\(\\implies\\) Normal 2) Regular, Lindelof \\(\\implies\\) Normal 3) Regular, 2nd countable \\(\\implies\\) Normal 4) Compact, Hausdorff \\(\\implies\\) Normal 5) Every well ordered set is normal in the order topology</p>"},{"location":"Normal%20Spaces/#related-problems","title":"Related Problems","text":""},{"location":"Normal%20Spaces/#references","title":"References","text":"<p>Metrizable Spaces Regular Spaces Lindelof Space Second Countability Compactness Hausdorff Property Order Topology [[Well Ordering]] [[S_omega]]</p>"},{"location":"Normalization%20Theorem%20for%20G%C3%B6del%27s%20system%20T/","title":"Normalization Theorem for G\u00f6del's system T","text":"<p>202307261607</p> <p>Type : #Note Tags : [[Type Theory]]</p>"},{"location":"Normalization%20Theorem%20for%20G%C3%B6del%27s%20system%20T/#normalization-theorem-for-godels-system-t","title":"Normalization Theorem for G\u00f6del's system T","text":"<p>In \\(\\text{T}\\) , all the reduction sequences are finite and lead to the same normal form.</p>"},{"location":"Normalization%20Theorem%20for%20G%C3%B6del%27s%20system%20T/#proof","title":"Proof","text":"<p>Part of the result in the extension of Church-Rosser; The other part is a strong normalization result, for which Reducibility is well adept(was actually created for \\(\\text T\\))</p> <p>First we extend the notion of Neutrality: A term is normal if it is not of the from \\(\\langle u, v\\rangle,\\ \\lambda x.v,\\ \\text O, \\text S\\ t, \\text T,\\text F\\). Then without changing anythign we show successively: 1. \\(\\text O,\\ \\text T\\) and \\(\\text F\\) are reducible - They are normal terms of atomic type. 2. If \\(t\\) of type \\(\\text{Int}\\) is reducible (i.e. it is strongly normalizable), then \\(\\text S\\ t\\) is also reducible which comes from \\(\\nu(\\text S\\ t)=\\nu(t)\\). 3. If \\(u,\\ v,\\ t\\) are reducible, then \\(\\text D\\ u\\ v\\ t\\) is reducible - \\(u,v,t\\) are strongly normalizable by CR1, and so one can reason by induction on the number \\(\\nu(u)+\\nu(v)+\\nu(t)\\). The neutral term \\(\\text D\\ u\\ v\\ t\\) converts to one of the following terms     1. \\(\\text D\\ u'\\ v'\\ t'\\). In this case we have \\(\\nu(u')+\\nu(v')+\\nu(t')&lt;\\nu(u)+\\nu(v)+\\nu(t)\\), and by induction the terms are reducible.     2. \\(u\\) or \\(v\\) if \\(t\\) is \\(\\text T\\) or \\(\\text F\\); These two terms are reducible     3. so we conclude by CR3 that the term is reducible 4. If \\(u,v,t\\) are reducible, then \\(\\text R\\ u\\ v\\ t\\) is reducible. Here we reason by induction but on \\(\\nu(u)+\\nu(v)+\\nu(t)+l(t)\\), where \\(l(t)\\) is the number of symbols of the normal form of \\(t\\). in one step \\(\\text R\\ u\\ v\\ t\\) converts to     1. \\(\\text R\\ u'\\ v'\\ t'\\) which is reducible by induction.     2. \\(u\\) if \\(t=O\\) which is reducible     3. \\(v\\ (\\text R\\ u\\ v\\ w) w\\), where \\(\\text S\\ w=t\\); since \\(\\nu(w)=\\nu(t)\\) and \\(l(w)&lt;l(t)\\), the induction hypothesis tells us that \\(\\text R\\ u\\ v\\ w\\) is reducible. As \\(v\\) and \\(w\\) are reducible, so is the term.</p> <p>System \\(\\text T\\)'s expressive power is discussed here.</p>"},{"location":"Normalization%20Theorem%20for%20G%C3%B6del%27s%20system%20T/#references","title":"References","text":"<p>Strong Normalization Theorem</p>"},{"location":"Not%20Unification%20Algorithm%20For%20Type%20Inference/","title":"Not Unification Algorithm For Type Inference","text":"<p>202308282208</p> <p>Type : #Note  Tags : [[Type Theory]], [[Lambda Calculus]]</p>"},{"location":"Not%20Unification%20Algorithm%20For%20Type%20Inference/#not-unification-algorithm-for-type-inference","title":"Not Unification Algorithm For Type Inference","text":"<p>Unification is an algorithmic process for solving equations between symbolic expression.</p> <p>It's famous use cases are Logic Programming and in making type systems for programming Languages, especially the Hindley Milner based Type Inference Algorithm. Here it is being used for type inference of simply typed lambda calculus.</p> <p>This algorithm gives the most general type for a give lambda expression</p> <pre><code>This is not the usual [[Unificate Algorithm For Type Inference|Unification Algorithm]]. I have figured it out based on my partial memory from whatever prof did in class.\n</code></pre> <p>The procedure involves assigning types, then finding constraints and solving them. </p> <p>I will use the following example to show the procedure  $$ S::=\\lambda xyz.xz(yz) $$</p>"},{"location":"Not%20Unification%20Algorithm%20For%20Type%20Inference/#example-walk-through","title":"Example walk through","text":"<pre><code>\\xyz.xz(yz)\n</code></pre>"},{"location":"Not%20Unification%20Algorithm%20For%20Type%20Inference/#step-1","title":"Step 1","text":"<p>Explicitly write out all brackets are assign ad-hoc types to everything <pre><code>\\x.(\\y.(\\z.((xz)(yz))))    T\n x                         X\n    \\y.(\\z.((xz)(yz)))     U\n     y                     Y\n        \\z.((xz)(yz))      V\n         z                 Z\n            (xz)(yz)       W\n             xz            P\n             x             Q\n              z            R\n                 yz        S\n                 y         L\n                  z        M\n</code></pre></p>"},{"location":"Not%20Unification%20Algorithm%20For%20Type%20Inference/#step-2","title":"Step 2","text":"<p>Find out all the constraints  <pre><code>X = Q\nY = L\nZ = R = M\n</code></pre></p> <p>and all of the applications give the following  constraints <pre><code>Y = Z -&gt; S   (from yz : S)\nX = Z -&gt; P   (from xz : P)\nP = S -&gt; W   (from (xz)(yz) : W)\n</code></pre></p> <p>The abstractions give the constraints <pre><code>V = Z -&gt; W\nU = Y -&gt; V\nT = X -&gt; U\n</code></pre></p>"},{"location":"Not%20Unification%20Algorithm%20For%20Type%20Inference/#step-3","title":"Step 3","text":"<p>Solve the constraints <pre><code>X = Z -&gt; S -&gt; W\nU = (Z -&gt; S) -&gt; (Z -&gt; W)\nT = (Z -&gt; S -&gt; W) -&gt; (Z -&gt; S) -&gt; Z -&gt; W\n</code></pre></p> <p>And we have no constraints on \\(Z,S\\) and \\(W\\)</p> <p>This gives the type of the Lambda Expression on renaming the type variables $$ S::=\\quad \\lambda xyz.xz(yz)\\quad:\\quad(\\sigma\\to\\tau\\to\\theta)\\to(\\sigma\\to\\theta)\\to\\sigma\\to\\theta $$</p>"},{"location":"Not%20Unification%20Algorithm%20For%20Type%20Inference/#references","title":"References","text":"<p>Simply Typed Lambda Calculus Syntax Curry-Style typing Northeastern University Notes On Type Inference Cornell University Notes on Type Inference Wikipedia for Type Inference Wikipedia for Unification Algorithm</p>"},{"location":"Number%20Field/","title":"Number Field","text":"<p>202306021206</p> <p>Type : #Note Tags :[[Number Theory]]</p>"},{"location":"Number%20Field/#number-field","title":"Number Field","text":"<p><pre><code>title:\nA number field is a finite extension of $\\mathbb{Q}$.\n</code></pre> - A number field is an algebraic extension of \\(\\mathbb{Q}\\). - Every such field has the form \\(\\mathbb{Q}[\\alpha]\\) for some algebraic number \\(\\alpha \\in \\mathbb{C}\\) (Primitive Element Theorem) - Examples are Cyclotomic Fields.</p>"},{"location":"Number%20Field/#proposition-1","title":"Proposition 1:","text":"<pre><code>title:\nLet $K$ be a degree $n$ extension of $L$. Then there are exactly $n$ extensions to $K$ of any embedding of $L$ in $\\mathbb{C}$.\n</code></pre>"},{"location":"Number%20Field/#proof","title":"Proof:","text":"<p>Consider \\(\\sigma : L \\to \\mathbb{C}\\) an embedding, and let \\(K = L(\\alpha_{1},\\alpha_{2},\\dots,\\alpha_{n})\\). Let \\(f_{1}\\) be the minimal poly of \\(\\alpha_{1}\\) over \\(L\\). Now \\(f_{1}(\\alpha_{1}) = 0 \\implies (\\sigma f_{1})(\\sigma\\alpha_{1}) = 0 \\implies\\) \\(\\alpha_{1}\\) is sent to a root of \\(\\sigma f_{1}\\). Now any extension of \\(\\sigma\\) to \\(L(\\alpha_{1})\\) is given by the image of \\(\\alpha_{1}\\), there are \\(deg(\\sigma f) = deg(f)\\) choices for this. And any choice leads to a valid extension. Thus there are \\([L(\\alpha_{1}) : L]\\) extensions \\(\\widetilde{\\sigma}\\) of \\(\\sigma\\) to \\(L(\\alpha_{1})\\).</p> <p>Putting \\(L(\\alpha_{1},\\alpha_{2})\\) and \\(L(\\alpha_{1})\\) inplace of \\(L(\\alpha_{1})\\) and \\(L\\), we get there are \\([L(\\alpha_{1},\\alpha_{2}):L(\\alpha_{1})]\\) extensions of \\(\\widetilde{\\sigma}\\) to \\(L(\\alpha_{1},\\alpha_{2})\\), hence \\([L(\\alpha_{1},\\alpha_{2}):L]\\) extensions of \\(\\sigma\\) to \\(L(\\alpha_{1},\\alpha_{2})\\). Thus, by induction, there are \\([K:L]\\) extensions of \\(\\sigma\\) to \\(K\\).</p>"},{"location":"Number%20Field/#corollary","title":"Corollary:","text":"<pre><code>title:\nThere are exactly $n$ embeddings of $K$ in $\\mathbb{C}$, given that $[K:\\mathbb{Q}] = n$.\n</code></pre>"},{"location":"Number%20Field/#proposition-2","title":"Proposition 2:","text":""},{"location":"Number%20Field/#references","title":"References","text":"<p>Primitive Element Theorem</p>"},{"location":"Number%20Of%20Roots%20and%20Poles%20of%20a%20Rational%20Function/","title":"Number Of Roots and Poles of a Rational Function","text":"<p>202301181655</p> <p>type : #Example tags :  [[Complex Analysis]]</p>"},{"location":"Number%20Of%20Roots%20and%20Poles%20of%20a%20Rational%20Function/#number-of-roots-and-poles-of-a-rational-function","title":"Number Of Roots and Poles of a Rational Function","text":"<p>### Question: From the above, show that the number of zeroes (with order) of \\(R(z)\\) including those at \\(\\infty\\) is \\(\\max(m,n)\\), also show that the number of poles (with order) = \\(\\max(m,n) = p\\). The number \\(p\\) is called the order of the rational function.</p>"},{"location":"Number%20Of%20Roots%20and%20Poles%20of%20a%20Rational%20Function/#answer","title":"Answer:","text":"<p>First let \\(n&gt;m\\) then, the set of zeroes of \\(R\\) = set of zeroes of numerator of \\(R\\), which has size \\(n\\).  Also since \\(n&gt;m\\), \\(R\\) does not have roots at \\(\\infty\\). Number of poles of \\(R\\) = number of zeroes of denominator + \\(n-m = m + n-m = n\\).</p> <p>Similarly handle the case of \\(m&gt;n\\). </p>"},{"location":"Number%20Of%20Roots%20and%20Poles%20of%20a%20Rational%20Function/#related","title":"Related","text":""},{"location":"Number%20Rings/","title":"Number Rings","text":"<p>202306021406</p> <p>Type : #Note Tags : [[Number Theory]]</p>"},{"location":"Number%20Rings/#number-rings","title":"Number Rings","text":"<pre><code>title:\nA number ring is the ring of integers of a number field, in other words, the set of algebraic integers in the number field.\n</code></pre> <ul> <li>An element of a number ring is a unit iff it has norm \\(\\pm 1\\).</li> <li>An element of a number ring is irreducible if its norm is a prime in \\(\\mathbb{Z}\\).</li> </ul>"},{"location":"Number%20Rings/#additive-structure-of-number-rings","title":"Additive Structure of number rings","text":"<p>We are going to show that the number ring of a number field \\(K\\) of degree \\(n\\) over \\(\\mathbb{Q}\\) is a free group of rank \\(n\\).</p> <p>First note that given any basis of \\(K\\) of \\(\\mathbb{Q}\\), we can multiply it by an integer such that the basis elements all lie in \\(R = \\overline{\\mathbb{Z}} \\cap K\\), this is possible since for any algebraic number there is an integer such that multiplying by it makes it an algebraic integer.</p> <p>Fixing such a basis \\(\\{ \\alpha_{1},\\dots,\\alpha_{n} \\} \\subset R\\) for \\(K\\) over \\(\\mathbb{Q}\\), we have a free abelian group or rank \\(n\\) inside \\(R\\), namely, $$ A = \\mathbb{Z}\\alpha_{1} \\oplus \\mathbb{Z}\\alpha_{2} \\oplus \\dots \\oplus \\mathbb{Z}\\alpha_{n} $$ and this is abelian of rank \\(n\\).</p>"},{"location":"Number%20Rings/#theorem-1","title":"Theorem 1","text":"<pre><code>title:\nLet $\\{ \\alpha_{1},\\dots,\\alpha_{n} \\}$ be a basis for $K$ over $\\mathbb{Q}$ consisting entirely of algebraic integers, and set $d = \\mathrm{disc}(\\alpha_{1},\\dots,\\alpha_{n})$. Then every $\\alpha \\in R$ can be expressed in the form \n$$\n\\frac{m_{1}\\alpha_{1} + \\dots m_{n}\\alpha_{n}}{d}\n$$\nwith all $m_{j} \\in \\mathbb{Z}$ and $d \\mid m_{j}^{2}$.\n</code></pre>"},{"location":"Number%20Rings/#proof","title":"Proof:","text":"<p>Let \\(\\alpha = x_{1}\\alpha_{1} + \\dots + x_{n}\\alpha_{n}\\), \\(x_{i} \\in \\mathbb{Q}\\).  We need to show that \\(dx_{i} \\in \\mathbb{Z}\\) for all \\(i\\), and \\(d | d^{2}x_{i}^{2}\\) or equivalently, \\(dx_{i}^{2} \\in \\mathbb{Z}.\\) \\(\\sigma_{i}(\\alpha) = \\sum_{j=1}^{n} x_{j}\\sigma_{i}(\\alpha_{j})\\) Solving for \\(x_{j}\\)'s via cramer's rule gives $$ x_{j} = \\frac{y_{j}}{|\\sigma_{i}(\\alpha_{j})|} $$ where \\(y_{j}\\) is the determinant of the matrix obtained by replacing the \\(j ^{th}\\) column in \\([\\sigma_{i}(\\alpha_{j})]\\) by \\(\\sigma_{i}(\\alpha)\\).</p> <p>Thus \\(dx_{j} = \\frac{dy_{j}}{|\\sigma_{i}(\\alpha_{j})|} = |\\sigma_{i}(\\alpha_{j})|y_{j} \\in \\overline{\\mathbb{Z}}\\), since \\(y_{j}, |\\sigma_{i}(\\alpha_{j})| \\in \\overline{\\mathbb{Z}}\\). Thus \\(dx_{j} \\in \\mathbb{Q} \\cap \\overline{\\mathbb{Z}} = \\mathbb{Z}\\).  Now \\(dx_{j}^{2} \\in \\mathbb{Q} \\cap \\overline{\\mathbb{Z}} = \\mathbb{Z}\\).</p>"},{"location":"Number%20Rings/#thus-we-get-a-subset-r-subset-mathbbzfracalpha_1d-oplus-mathbbz-fracalpha_2d-oplus-dots-oplus-mathbbz-fracalpha_nd-thus-r-is-sandwiched-between-two-free-groups-of-rank-n-thus-it-has-rank-n-refer-free-group-equivalently-r-has-a-basis-over-mathbbz-this-basis-is-called-an-integral-basis-for-r","title":"Thus, we get \\(A \\subset R \\subset \\mathbb{Z}\\frac{\\alpha_{1}}{d} \\oplus \\mathbb{Z} \\frac{\\alpha_{2}}{d} \\oplus \\dots \\oplus \\mathbb{Z} \\frac{\\alpha_{n}}{d}\\). Thus \\(R\\) is sandwiched between two free groups of rank \\(n\\), thus it has rank \\(n\\) (refer Free Group). Equivalently, \\(R\\) has a basis over \\(\\mathbb{Z}\\), this basis is called an integral basis for \\(R\\).","text":""},{"location":"Number%20Rings/#discriminant-of-a-number-ring","title":"Discriminant of a number ring","text":""},{"location":"Number%20Rings/#theorem-2","title":"Theorem 2:","text":"<pre><code>title:\nLet $\\{ \\beta_{1},\\dots,\\beta_{n} \\}$ and $\\{ \\gamma_{1},\\dots,\\gamma_{n} \\}$ be two integral bases for $R = \\overline{\\mathbb{Z}} \\cap K$. Then $\\mathrm{disc}(\\beta_{1},\\dots,\\beta_{n}) = \\mathrm{disc}(\\gamma_{1},\\dots ,\\gamma_{n}).$\n</code></pre>"},{"location":"Number%20Rings/#proof_1","title":"Proof:","text":"<p>Writing the \\(\\beta\\)'s in terms of the \\(\\gamma\\)'s, we get $$ \\begin{pmatrix} \\beta_{1}  \\ \\beta_{2} \\ \\vdots \\ \\beta_{n} \\end{pmatrix} =  M \\begin{pmatrix} \\gamma_{1} \\ \\gamma_{2} \\ \\vdots \\ \\gamma_{n} \\end{pmatrix} $$ This gives $$ [\\sigma_{i}(\\beta_{j})] = M [\\sigma_{i}(\\gamma_{j})] \\implies \\mathrm{disc}(\\beta) = |M|^{2}\\mathrm{disc}(\\gamma) $$ Thus $$ \\frac{\\mathrm{disc}(\\beta)}{\\mathrm{disc}(\\gamma)}, \\frac{\\mathrm{disc}(\\gamma)}{\\mathrm{disc}(\\beta)} \\in \\mathbb{Z} $$ Thus \\(\\mathrm{disc}(\\beta)=\\mathrm{disc}(\\gamma)\\).</p>"},{"location":"Number%20Rings/#therefore-the-discriminant-of-an-integral-basis-can-be-regarded-as-an-invariant-of-the-ring-r-denote-it-by-mathrmdiscr","title":"Therefore, the discriminant of an integral basis can be regarded as an invariant of the ring \\(R\\). Denote it by \\(\\mathrm{disc}(R)\\).","text":""},{"location":"Number%20Rings/#proposition-1","title":"Proposition 1:","text":"<pre><code>title:\nAssuming $\\alpha_{1},\\dots,\\alpha_{n} \\in R$, they form an integral basis iff $\\mathrm{disc}(\\alpha_{1},\\dots,\\alpha_{n}) = \\mathrm{disc}(R)$.\n</code></pre>"},{"location":"Number%20Rings/#proof_2","title":"Proof:","text":"<p>One direction is clear. For the other direction, write \\(\\alpha_{i}\\)'s in terms of an integral basis, which, following the proof of theorem 2, gives $$ \\mathrm{disc}(\\alpha_{1},\\dots,\\alpha_{n}) = |A|^{2}\\mathrm{disc}(R) $$where \\(A \\in M_{n}(\\mathbb{Z})\\). Since the discriminants are equal by assumption, \\(|A| = \\pm {1}\\), and hence \\(A\\) is invertible, giving the original integral basis in terms of the \\(\\alpha_{i}\\)'s, which means \\(\\alpha_{1},\\dots,\\alpha_{n}\\) forms an integral basis.</p>"},{"location":"Number%20Rings/#theorem-3","title":"Theorem 3:","text":"<pre><code>title:\nGiven 2 number fields $K,L$ and their respective number rings $R,S$. And let $T$ be the number ring of the composite field $KL$, then $RS \\subset T$. \n\nLet $[K:\\mathbb{Q}] = m, [L :\\mathbb{Q}] = n$, $d = \\mathrm{gcd}(\\mathrm{disc} \\ R, \\mathrm{disc} \\ S)$.\nAssume $[KL : \\mathbb{Q}] = mn$ then $T \\subset \\frac{1}{d}RS$.\n</code></pre>"},{"location":"Number%20Rings/#proof_3","title":"Proof:","text":"<p>The first part that \\(RS \\subset T\\) is clear.</p>"},{"location":"Number%20Rings/#lemma-1","title":"Lemma 1:","text":"<pre><code>title:\nGiven an embedding $\\sigma$ of $K$ in $\\mathbb{C}$, and $\\tau$ of $L$ in $\\mathbb{C}$, there is an extension of $\\sigma$ to $KL$, such that its restriction to $L$ is $\\tau$.\n</code></pre>"},{"location":"Number%20Rings/#proof_4","title":"Proof:","text":"<p>Since there are \\(n\\) distinct extensions of \\(\\sigma\\) to \\(KL\\), they must all be distinct on \\(L\\) since any automorphism on \\(KL\\) is determined by its action on \\(K\\) and \\(L\\) separately. Thus there are \\(n\\) possible restrictions to \\(L\\) and one of them must be \\(\\tau\\) since there are exactly \\(n\\) possible embeddings of \\(L\\) in the first place.</p> <p>Continuing with the proof of the theorem, let \\(\\alpha \\in T\\) be any element, then $$ \\alpha = \\sum_{i,j} \\frac{m_{ij}\\beta_{i}\\gamma_{j}}{r} $$ where \\(\\{ \\beta_{1},\\dots,\\beta_{m} \\}\\) is the integral basis for \\(R\\), and \\(\\{ \\gamma_{1},\\dots,\\gamma_{n} \\}\\) is the integral basis for \\(S\\), and \\(m_{ij} \\in \\mathbb{Z}\\), with \\(\\mathrm{gcd}(r,\\mathrm{gcd}(m_{ij})) = 1\\). (This follows from Theorem 1, and the fact that \\(\\{ \\beta_{i}\\gamma_{j} \\}\\) is a basis for \\(KL\\) consisting of algebraic integers.)</p> <p>Now we need to show that \\(d\\alpha \\in RS\\). It suffices to show that \\(r \\mid d\\) which is equivalent to showing that \\(r \\mid \\mathrm{disc}(R), \\mathrm{disc}(S)\\).  We show \\(r \\mid \\mathrm{disc}(R)\\) and the other one is symmetric.</p> <p>Let \\(\\{ \\sigma_{k} \\}_{k=1}^{m}\\) be a set of embeddings of \\(KL\\) in \\(\\mathbb{C}\\) which fixes \\(L\\) pointwise (possible by lemma 1). $$ \\begin{align} \\sigma_{k}(\\alpha) &amp;= \\sum \\frac{m_{ij}}{r}\\sigma(\\beta_{i})\\gamma_{j} \\ \\text{let } x_{i} &amp;= \\sum_{j=1}^{n} \\frac{m_{ij}}{r}\\gamma_{j}  \\ \\text{then } \\sigma_{k}(\\alpha) &amp;= \\sum_{i} \\sigma_{k}(\\beta_{i})x_{i} \\ \\implies \\begin{bmatrix} \\sigma_{1}(\\alpha) \\ \\vdots \\ \\sigma_{m}(\\alpha) \\end{bmatrix} &amp;= \\begin{bmatrix} \\sigma_{i}(\\beta_{j}) \\end{bmatrix} \\begin{bmatrix} x_{1} \\ \\vdots \\ x_{m} \\end{bmatrix} \\end{align} $$ Now applying cramer's rule,  $$ x_{i} = \\frac{y_{i}}{\\delta} $$ where \\(y_{i}\\) is the determinant of the matrix \\([\\sigma_{i}(\\beta_{j})]\\) with the \\(i ^{th}\\) column replaced by \\(\\sigma_{j}(\\alpha)\\)'s. And \\(\\delta = |\\sigma_{i}(\\beta_{j})|\\).</p> <p>We also know \\(\\delta^{2} = \\mathrm{disc}(R)\\), \\(x_{i}\\delta^{2} = y_{i}\\delta \\in \\overline{\\mathbb{Z}}\\) since \\(y_{i}, \\delta \\in \\overline{\\mathbb{Z}}\\). Thus, \\(x_{i}\\mathrm{disc}(R) = \\sum_{j}\\mathrm{disc}(R) \\frac{m_{ij}}{r}\\gamma_{j} \\in \\overline{\\mathbb{Z}} \\cap L = S\\).  Giving \\(r \\mid \\mathrm{disc}(R)m_{ij} \\implies r \\mid \\mathrm{disc}(R)\\).</p>"},{"location":"Number%20Rings/#corollary-1","title":"Corollary 1:","text":"<pre><code>title:\nIf $[KL : \\mathbb{Q}] = mn$ and $d = 1$ then $T = RS$.\n</code></pre>"},{"location":"Number%20Rings/#corollary-2","title":"Corollary 2:","text":"<pre><code>title:\nLet $K = \\mathbb{Q}(\\omega), \\omega = e ^{2\\pi i/m}, R = \\overline{\\mathbb{Z}}\\cap K$. Then $R = \\mathbb{Z}[\\omega]$.\n</code></pre>"},{"location":"Number%20Rings/#proof_5","title":"Proof:","text":"<p>Refer to Cyclotomic Fields, theorem 3.</p>"},{"location":"Number%20Rings/#theorem-4","title":"Theorem 4:","text":"<pre><code>title:\nLet $\\alpha \\in R$ and suppose $\\alpha$ has degree $n$ over $\\mathbb{Q}$. Then there is an integral basis \n$$\n1, \\frac{f_{1}(\\alpha)}{d_{1}}, \\dots \\frac{f_{n-1}(\\alpha)}{d_{n-1}}\n$$\nwhere the $d_{i} \\in \\mathbb{Z}$ and satisfy $d_{1}  \\mid d_{2}\\mid\\dots\\mid d_{n-1}$; the $f_{i}$ are monic polynomials over $\\mathbb{Z}$, and $f_{i}$ has degree $i$. The $d_{i}$ are uniquely determined.\n</code></pre>"},{"location":"Number%20Rings/#proof_6","title":"Proof:","text":"<p>For each \\(k, 1\\le k\\le n\\) let \\(F_{k}\\) be the free abelian group of rank \\(k\\) generated by \\(1/d, \\alpha/d, \\dots \\alpha^{k-1}/d\\), where \\(d = \\mathrm{disc}(\\alpha)\\), and set \\(R_k = R \\cap F_{k}\\).  Thus we have \\(R_1 = \\mathbb{Z}\\) and \\(R_n = R\\).  We will define the \\(d_{i}\\) and the \\(f_{i}\\) so that for each \\(k, 1 \\le k \\le n\\), \\(1, f_1(\\alpha)/d_1 , \\cdots , f_{k\u22121}(\\alpha)/d_{k\u22121}\\) is a basis over \\(\\mathbb{Z}\\) for \\(R_{k}\\). This is certainly true for \\(k = 1\\).  Thus fix \\(k &lt; n\\) and assume that \\(\\{1, f_1(\\alpha)/d_1, \\cdots , f_{k\u22121}(\\alpha)/d_{k\u22121}\\}\\) is a basis over \\(\\mathbb{Z}\\) for \\(R_{k}\\), with the \\(f_i\\) and \\(d_{i}\\) as in the theorem.  We have to define \\(f_{k}\\) and \\(d_{k}\\) and show that we get a basis for \\(R_{k+1}\\) by throwing in \\(f_{k}(\\alpha) /d_{k}\\). </p> <p>Let \\(\\pi\\) be the canonical projection of \\(F_{k+1} = \\mathbb{Z} \\frac{1}{d} \\oplus \\dots \\oplus \\mathbb{Z} \\frac{\\alpha^{k}}{d}\\) on its last factor. That is, \\(\\pi\\) selects the term of degree \\(k\\). </p> <p>Then \\(\\pi(R_{k+1})\\) is a subgroup of the infnite cyclic group  $$ \\mathbb{Z} \\frac{\\alpha^{k}}{d} = \\left{  \\frac{m\\alpha^{k}}{d} : m \\in \\mathbb{Z}  \\right} $$ which implies that \\(\\pi(R_{k+1})\\) is cyclic.  Fixing any \\(\\beta \\in R_{k+1}\\) such that \\(\\pi(\\beta)\\) generates \\(\\pi(R_{k+1})\\), we see that \\(\\{ 1,f_{1}(\\alpha) /d_{1},\\dots, f_{k-1}(\\alpha) /d_{k-1}, \\beta \\}\\) is a basis for \\(R_{k+1}\\). Indeed, given any \\(\\gamma \\in R_{k+1}\\), \\(\\gamma = \\left( \\gamma - \\frac{\\beta\\pi(\\gamma)}{\\pi(\\beta)} \\right) + \\frac{\\beta\\pi(\\gamma)}{\\pi(\\beta)}\\), here \\(\\gamma - \\frac{\\beta\\pi(\\gamma)}{\\pi(\\beta)} \\in R_{k}\\) and \\(\\frac{\\beta \\pi(\\gamma)}{\\pi(\\beta)} \\in \\mathbb{Z}\\beta\\). Thus \\(R_{k+1} = R_{k} \\oplus \\mathbb{Z}\\beta\\).</p> <p>It remains to show that \u03b2 has the right form.  We have \\(\\(\\frac{\\alpha^{k}}{d_{k-1}} = \\pi\\left( \\frac{\\alpha f_{k-1}(\\alpha)}{d_{k-1}} \\right)\\)\\) and this is in \\(\\pi(R_{k+1})\\) since \\(\\frac{\\alpha f_{k-1}(\\alpha)}{d_{k-1}} \\in F_{k+1} \\cap R\\). It follows that \\(\\alpha^{k} /d_{k-1} = m \\pi(\\beta)\\) for some \\(m \\in \\mathbb{Z}\\). Defining \\(d_{k} = md_{k-1}\\), we get \\(\\pi(\\beta) = \\frac{\\alpha^{k}}{d_{k}}\\) and so, \\(\\beta = \\frac{f_{k}(\\alpha)}{d_{k}}\\) for some monic polynomial \\(f_{k}\\) of degree \\(k\\). But why does \\(f_{k}\\) have integer coefficients?</p> <p>We know that \\(df_{k} /d_{k}\\) has integer coefficients, but since \\(f_{k}(\\alpha) /d_{k-1} = m\\beta \\in R\\), we have $$ \\gamma = \\frac{f_{k}(\\alpha) - \\alpha f_{k-1}(\\alpha)}{d_{k-1}} \\in R_k $$ Using our basis for \\(R_{k}\\), we have \\(\\gamma = \\frac{g(\\alpha)}{d_{k-1}}\\), for some \\(g(x)\\in \\mathbb{Z}[x]\\) having degree &lt; \\(k\\). This implies \\(g(x) = f_{k}(x)-xf_{k-1}(x)\\). And hence \\(f_{k} \\in \\mathbb{Z}[x]\\).</p> <p>Now observe that the conditions in the theorem imply that \\(d_{k}\\) is the smallest positive integer \\(m\\) such that \\(mR_{k+1} \\subset \\mathbb{Z}[\\alpha]\\).</p>"},{"location":"Number%20Rings/#properties-of-number-rings","title":"Properties of Number Rings","text":""},{"location":"Number%20Rings/#theorem-5","title":"Theorem 5:","text":"<pre><code>title:\nEvery number ring is a dedekind domain.\n</code></pre>"},{"location":"Number%20Rings/#proof_7","title":"Proof:","text":"<ol> <li>To prove that a number ring is noetherian, note that \\(R\\) is a free abelian group of rank \\(n\\) (say).    Now any ideal \\(I\\) is an additive subgroup of this group, hence is free of rank \\(m \\le n\\), and therefore it is finitely generated.</li> <li>Take any ideal \\(I\\) of \\(R\\), and let \\(\\alpha \\in I\\) be a non unit element.    Now \\(N ^{K}(\\alpha) = \\alpha \\cdot \\alpha' = m\\) where \\(K\\) is the number field corresponding to the number ring \\(R\\) and \\(\\alpha'\\) is the product of all the conjugates of \\(\\alpha\\) in \\(K\\) except \\(\\alpha\\) itself.    Note that \\(\\alpha' \\in K \\cap \\overline{\\mathbb{Z}} = R\\) and so, \\(m \\in I\\).    This gives \\(R/(m) = m ^{n}\\), and \\(|R /I| \\le | R /(m)| = m ^{n}\\).    Thus if \\(I\\) is a prime ideal, then \\(R /I\\) is an finite integral domain, and so it is a field, and thus \\(I\\) is a maximal ideal.</li> <li>Let \\(\\frac{\\alpha}{\\beta} \\in K\\) be a root of a monic poly over \\(R\\). Then \\(\\frac{\\alpha}{\\beta}\\) is an algebraic integer by Theorem 3 of Algebraic Integers.    Thus \\(\\frac{\\alpha}{\\beta} \\in K \\cap \\overline{\\mathbb{Z}} = R\\).</li> </ol>"},{"location":"Number%20Rings/#references","title":"References","text":"<p>Number Field Trace and Norm Free Group Cyclotomic Fields Algebraic Integers Dedekind Domains</p>"},{"location":"ODE/","title":"ODE","text":"<p>202212131712</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"ODE/#ode","title":"ODE","text":"<pre><code>title: Ordinary Differential Equation\nAn _ordinary differential equation_ is a relation containing one independent variable $x\\in\\mathbb R$ and one dependent variable $y$, and some of its derivative $y',y'',\\dots, y^{(n)}$\n</code></pre> <p>Example: \\(x^2y'' -3xy' + 3y = 0\\) </p> <p>The Order of an ODE is defined to be the order of the highest derivative in the equation, hence the above example is a second order ODE.</p> <p>A function between the independent and the dependent variable which satisfies a differential equation is called the Solution of the differential equation.</p> <p>A differential equation is said to be linear if it is linear in \\(y\\) and its derivative. i.e an \\(n^{th}\\)-degree DE can be written as $$ \\mathcal P_n[y] = p_0(x)y+ p_1(x)y'+\\dots+p_n(x)y^{(n)}=r(x) $$ linear differential equation is said to be Homogeneous if \\(r(x)=0\\) otherwise its called Nonhomogeneous.</p>"},{"location":"ODE/#related-problems","title":"Related Problems","text":""},{"location":"ODE/#references","title":"References","text":""},{"location":"Olog/","title":"Olog","text":"<p>202304012304</p> <p>Type : #Note Tags : [[Category Theory]]</p>"},{"location":"Olog/#olog","title":"Olog","text":"<p>Ologs or Ontological Logs is a graphical representation of sets of data and relations between them. Example: <pre><code>\\usepackage{tikz-cd}\n\\begin{document}\n\\begin{tikzcd}\n    \\fbox{\\begin{tabular}{@{}c@{}}\n        An amino acid\\\\\n        found in dairy\n    \\end{tabular}} \\arrow[rd, \"is\"']\n    &amp; \n    \\framebox{argenine} \\arrow[r, \"has\"] \\arrow[l, \"is\"'] \\arrow[d, \"is\"]\n    &amp; \n    \\fbox{\\begin{tabular}{@{}c@{}}\n        an electrically-\\\\\n        charged side\\\\ \n        chain\n    \\end{tabular}} \\arrow[d, \"is\"]\n    \\\\\n    &amp;\n    \\fbox{an amino acid} \\arrow[r, \"has\"] \\arrow[dl, \"has\"'] \\arrow[dr, \"has\"]\n    &amp;\n    \\fbox{a side chain}\n    \\\\\n    \\fbox{an amine group}\n    &amp;\n    &amp;\n    \\fbox{a carboxylic chain}\n\\end{tikzcd}\n\\end{document}\n</code></pre></p> <p>TODO: have to figure out how to put check marks or a symbol to show that the diagram commutes </p> <p>Normally Each arrow can be read by reading the label on its source box, then the label on the arrow, and finally the label on its target box, but sometimes a shorthand version will be used when obvious for example: <pre><code>\\usepackage{tikz-cd}\n\\begin{document}\n\\begin{tikzcd}\n    &amp;\n    \\fbox{\\begin{tabular}{@{}c@{}}\n        A pair $(x,y)$, where\\\\\n        $x$ and $y$ are integers\n    \\end{tabular}} \\arrow[rd, \"y\"] \\arrow[ld, \"x\"']\n    &amp; \n    \\\\\n    \\fbox{an integer}\n    &amp;\n    &amp;\n    \\fbox{an integer}\n\\end{tikzcd}\n\\end{document}\n</code></pre></p>"},{"location":"Olog/#references","title":"References","text":""},{"location":"One%20Parameter%20Group%20Of%20Diffeomorphisms/","title":"One Parameter Group Of Diffeomorphisms","text":"<p>202302121102</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"One%20Parameter%20Group%20Of%20Diffeomorphisms/#one-parameter-group-of-diffeomorphisms","title":"One Parameter Group Of Diffeomorphisms","text":"<p>Let \\(U\\subseteq \\mathbb R^n\\) be the Domain. <pre><code>title:\nA One Parameter Group Of Diffeomorphisms on $U$ is \n$$\n\\begin{matrix}g &amp; : &amp; \\mathbb R \\times U &amp; \\rightarrow &amp;U\\\\&amp;&amp;(t,\\overline x) &amp; \\mapsto &amp; g^t(\\overline x)\\end{matrix}\n$$\nSuch that \n1. $g$ is differentiable. \n2. $g^t$ is a diffeomorphism.\n3. $\\forall t,s\\in\\mathbb R, g^{t+s}(\\overline x)=g^{t}(g^{s}(\\overline x));g^{0}(\\overline x) = \\overline x$\n</code></pre></p> <p>And the pair \\((U, \\{g^{t}\\})\\) is called a Phase Flow.</p>"},{"location":"One%20Parameter%20Group%20Of%20Diffeomorphisms/#references","title":"References","text":""},{"location":"One%20Point%20Compactification/","title":"One Point Compactification","text":"<p>202303021603</p> <p>Type : #Note Tags :</p>"},{"location":"One%20Point%20Compactification/#one-point-compactification","title":"One Point Compactification","text":"<pre><code>title: Compactification\nA compactification of a topological space $X$ is an embedding of $X$ as a dense subspace of a compact topological space.\n\nWe only care about compactifications upto homeomorphisms, so we say that two compactifications $f_1 : X \\to Y_1$ and $f_2 : X\\to Y_2$ are _equivalent_ if there is a homeomorphism $h : Y_1 \\to Y_2$ that fixes embedded elements of $X$, i.e., $h(f_1(x)) = f_2(x)$. \n</code></pre>"},{"location":"One%20Point%20Compactification/#one-point-compactification_1","title":"One point compactification","text":"<pre><code>title:\nLet $X$ be a topological space. Let $Y = X \\sqcup \\{ \\infty \\}$ where $\\infty$ is a formal symbol for an extra point not in $X$. Give $Y$ the topology $\\tau_{\\infty}$ consisting of: \n1) The open subsets $U \\subseteq X$.\n2) Complements $Y \\backslash C$ of closed and compact subsets $C$ of $X$.\nWe call $Y$ the one point compactification of $X$.\n</code></pre>"},{"location":"One%20Point%20Compactification/#proof-of-ys-compactness-and-why-tau_infty-is-a-topology-on-y","title":"Proof of \\(Y\\)'s compactness and why \\(\\tau_{\\infty}\\) is a topology on \\(Y\\):","text":"<p>1) The elements of \\(\\tau_{\\infty}\\) cover \\(Y\\). 2) Take two elements \\(V_{1},V_{2} \\in \\tau_{\\infty}\\) and a point \\(x \\in V_{1}\\cap V_{2}\\),     If both are subsets of \\(X\\) then their intersection contains a nbhd \\(W\\) containing \\(x\\).    If neither are subsets of \\(X\\), then their intersection \\(V_{1}\\cap V_{2} = C_{1}^{c} \\cap C_{2}^{c} = (C_{1} \\cup C_{2})^{c}\\) where \\(C_{1},C_{2}\\) are compact and closed subsets of \\(X\\). Hence, their intersection is open.    If one is a subset and the other is not, then their intersection \\(V_{1} \\cap (Y\\backslash C_{2}) = V_{1} \\cap (X \\backslash C_{2})\\) is a intersection of two open sets of \\(X\\) and so is open in \\(X\\) and hence is open in \\(Y\\). 3) Take a cover of \\(\\{ U_{\\alpha} \\}\\) of \\(Y\\), this has a \\(U_{0}\\) which contains \\(\\infty\\), but since \\(Y \\backslash U_{0}\\) is compact, it has a finite subcover \\(U_{1},\\dots U_{n}\\). Hence, \\(U_{0},U_{1},\\dots U_{n}\\) is a finite subcover of \\(Y\\).</p>"},{"location":"One%20Point%20Compactification/#note","title":"Note:","text":"<p>If \\(X\\) is locally compact and hausdorff, then \\(Y\\) is hausdorff and compact.</p>"},{"location":"One%20Point%20Compactification/#theorem","title":"Theorem","text":"<pre><code>title:\nLet $X \\subseteq Y$ be a subspace of a compact Hausdorff space, such that $Y\\backslash X$ consists of a single point. Then $X$ is locally compact and hausdorff.\n</code></pre>"},{"location":"One%20Point%20Compactification/#proof","title":"Proof:","text":"<p>Since \\(X\\) is a subspace of a hausdorff space then \\(X\\) is hausdorff. Take a point \\(x \\in X\\), then since \\(Y\\) is hausdorff, there exists disjoint open sets \\(V,U\\) such that \\(x \\in V\\) and \\(\\infty \\in U\\), then \\(U^{c}\\) is a compact set containing \\(x\\) and containing \\(V\\) as a neighbourhood around \\(x\\). </p>"},{"location":"One%20Point%20Compactification/#the-following-statement-tells-us-that-a-one-point-compactification-is-unique","title":"The following statement tells us that a One point compactification is unique.","text":"<pre><code>title:\nLet $X$ be a locally compact hausdorff space with one point compactification $Y = X \\sqcup \\{ \\infty \\}$, and suppose $Y'$ is another compact hausdorff space such that $X \\subseteq Y'$ is a subspace with $\\mid Y' \\backslash X\\mid = 1$. Then the unique bijection  $f: Y \\to Y'$ that fixes $X$ is a homeomorphism\n</code></pre>"},{"location":"One%20Point%20Compactification/#proof_1","title":"Proof:","text":"<p>Since \\(Y\\) is compact and \\(Y'\\) is hausdorff, we only need to show that \\(f\\) is continuous. (refer to Homeomorphisms) Take an open set \\(V \\subseteq Y'\\), now if \\(V \\subseteq X\\) then \\(f^{-1}(V)\\) is open in X and we are done. Otherwise \\(\\infty \\in V\\), now \\(f^{-1}(Y'-V)\\) is a compact subset of \\(X\\) and so, \\(f^{-1}(V) = (f^{-1}(Y'-V))^{c}\\) is open in \\(Y\\).</p> <pre><code>title: Corollary\nA space $X$ is homemorphic to an open subspace of a compact hausdorff space $X$ is a locally compact hausdorff space.\n</code></pre>"},{"location":"One%20Point%20Compactification/#related-problems","title":"Related Problems","text":""},{"location":"One%20Point%20Compactification/#references","title":"References","text":""},{"location":"Open%20and%20Closed%20Functions/","title":"Open and Closed Functions","text":"<p>202301201801</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Open%20and%20Closed%20Functions/#open-and-closed-functions","title":"Open and Closed Functions","text":"<p><pre><code>title:\nA function $f$ is called open when $U\\in\\mathcal T_{X}\\implies f(U)\\in\\mathcal T_{Y}$\n</code></pre> <pre><code>title:\nA function $f$ is called closed when $U$ is closed $\\implies f(U)$ is closed.\n</code></pre> These functions are often confused with continuous function by those who are new to the fied.</p> <p>A function is Open when it maps open sets to open sets and a function is Continuous when inverse images of open sets are open</p> <p>Neither are Open functions the same as Closed functions - Constant functions of the type \\(\\mathbb R\\to \\mathbb R\\) with standard topology are continouous and closed but not open - projection functions from \\(\\mathbb R^{2}\\to \\mathbb R\\) withs standard topologies are continuous adn open but not closed.</p>"},{"location":"Open%20and%20Closed%20Functions/#related-problems","title":"Related Problems","text":"<p>1) Show that the projection maps from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}\\) are open but not closed.      \\(A_n\\) := \\([1/n,1-1/n] \\times \\{n\\}\\), the union of these sets project onto \\((0,1)\\). Hence this map is not closed.  </p>"},{"location":"Open%20and%20Closed%20Functions/#references","title":"References","text":"<p>Continuous Functions Homeomorphisms</p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/","title":"Optimal Reachability in Weighted Timed Games","text":"<p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#optimal-reachability-in-weighted-timed-games","title":"Optimal Reachability in Weighted Timed Games","text":"<p> Aditi Muthkhod <p>NOVEMBER 2023  </p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#why-weighted-timed-games","title":"Why Weighted Timed Games?","text":"<ul> <li>want to model systems, hence Timed Automata</li> <li>want to model systems interacting with uncontrollable components, hence Timed Games</li> </ul> <p>[!note] Reachability \\(\\leftrightarrow\\) Strategy to reach a state</p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#plan","title":"Plan","text":"<ol> <li>Weighted timed automata</li> <li>Weighted timed games</li> <li>Main results</li> <li>Proof sketch for bounded cost</li> </ol>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#what-is-a-weighted-timed-automaton","title":"What is a Weighted Timed Automaton?","text":"<p>\\(\\mathcal{A}=(L,X,l_{0},E,F,cost)\\)</p> <p>\\(cost:L\\cup E \\to \\mathbb{N}\\)</p> <p>[!important] Cost does not affect the behaviour of the automaton, only affects the... cost!</p> <p>--</p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#example","title":"Example:","text":""},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#what-is-a-weighted-timed-game","title":"What is a Weighted Timed Game?","text":"<p>A game played on WTA.</p> <p>Edges are partitioned into Controllable edges, played by Controller, and Uncontrollable edges, played by God!</p> <p>Assume final locations are sinks, have cost 0, and have cost 0 self loops.</p> <p>A strategy is a partial function that associates with a finite run, \\((d,e)=\\) (delay,edge).</p> <p>--</p> <ul> <li>run compatible with a strategy: we stick to strategy at every move, except when God plays before we can; gives a set of plays</li> <li>winning strategy: all plays reach final</li> <li>cost of winning strategy = sup (cost of all plays)</li> <li>optimal cost = inf (cost of all winning strategies)</li> </ul>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#the-problems-at-hand","title":"The problems at hand","text":"<p>[!example] Bounded cost problem: Given a threshold \\(c\\in \\mathbb{Q}\\), is there a strategy with cost at most \\(c\\)?</p> <p>[!example] Optimal cost problem: Given a threshold \\(c\\in \\mathbb{Q}\\), is the optimal cost at most \\(c\\)?</p> <p>--</p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#example_1","title":"Example:","text":"<p>note: Dashed and plain arrows Depending on what God chooses, the cost along plays of the game is either 5t + 10(2 \u2212 t) + 1 (through 2) or 5t + (2 \u2212 t) + 7 (through 3) where t is the delay elapsed in location 0. The optimal cost the controller can ensure is thus inf t&lt;=2 max(5t+ 10(2\u2212t) + 1, 5t+ (2\u2212t) + 7) = 14 + 1/3, and the optimal time for firing transition e1 is when t = 4/3 . The controller has an optimal strategy: wait at location 0 until x = 4/3, and enter location 1. Then, the environment chooses to go either to 2 or to 3, and finally when x reaches 2, the controller goes to the final location.</p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#main-results","title":"Main results:","text":"<p>[!bug] The bounded cost problem for WTG with \\(\\geq 3\\) clocks is undecidable.</p> <p>[!bug] The optimal cost problem for WTG with \\(\\geq 4\\) clocks is undecidable.</p> <p>--</p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#thats-a-bummer","title":"That's a bummer! :(","text":"<p>But people have been looking for decidable subclasses of WTG. The first result in that direction is:</p> <p>[!success] The optimal cost in single-clock 'stopwatch' timed games is computable.</p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#proof-sketch-for-undecidability-of-bounded-cost","title":"Proof sketch for undecidability of bounded cost","text":"<p>[!todo] Idea: Reduce halting of 2-counter automaton to existence of a strategy in a particular weighted timed game with 3 clocks.</p> <p>--</p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#machinery","title":"Machinery","text":"<p>{ width=\"1200\" } </p> <ul> <li>spends 1 time unit (ensured by \\(z\\))</li> <li>maintains clock values \\((x,y)\\)</li> <li>increases cost by \\(x_{0}\\), (respectively \\(1-x_{0}\\))</li> </ul> <p>--</p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#more-machinery","title":"More machinery!","text":"<ul> <li> <p>can build modules to increase cost by \\(\\langle x_{0},y_{0},1-x_{0},1-y_{0},1 \\rangle\\)</p> </li> <li> <p>in particular: \\(c_{1}(x_{0},y_{0})=2x_{0}+(1-y_{0})+2\\), \\(c_{2}(x_{0},y_{0})=2(1-x_{0})+y_{0}+1\\)</p> </li> <li> <p>\\(2x_{0}\\ne y_{0} \\implies c_{i}(x_{0},y_{0})&gt;3\\), \\(2x_{0}= y_{0} \\implies c_{i}(x_{0},y_{0})=3\\)</p> </li> <li>get module \\(\\text{Test}_{z}(2x=y)\\)</li> </ul> <p>--</p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#incrementing-one-counter","title":"Incrementing one counter","text":"<ul> <li>store value of counters \\(c,d\\) in clocks \\(x,y\\) with values \\(\\frac{1}{2^{c}}, \\frac{1}{2^{d}}\\)</li> <li>guess the value of \\(z\\) non-deterministically and test if \\(x=2z\\)</li> <li>swap \\(x\\) and \\(z\\)</li> </ul> <p>--</p> <p></p> <ul> <li>spends 1 time unit (ensured by \\(u\\))</li> <li>maintains clock values \\((x,y)\\)</li> <li>if \\(z\\) is guessed incorrectly, God can ensure cost \\(&gt;3\\)</li> <li>decrementing module can be made similarly</li> </ul> <p>--</p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#putting-it-all-together","title":"Putting it all together!","text":"<p>Simulate the 2-counter automaton using these modules. A run either terminates in a final state of the 2-CM, or if God takes the test edge.</p> <p>[!tldr] 2-CM halts \\(\\leftrightarrow\\) Controller has a winning strategy with cost \\(\\leq 3\\)</p> <p>(We used 4 clocks, not 3, but we can get rid of \\(u\\).)</p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#references","title":"References","text":"<p>[!cite] Patricia Bouyer-Decitre. Optimal Reachability in Weighted Timed Automata and Games (Invited Talk). 41st International Symposium on Mathematical Foundations of Computer Science (MFCS 2016).</p> <p>[!cite] P. BOUYER, TH. BRIHAYE, N. MARKEY, Improved Undecidability Results on Weighted Timed Automata. Information Processing Letters 98 (2006) 5, 188\u2013194.</p>"},{"location":"Optimal%20Reachability%20in%20Weighted%20Timed%20Games/#thank-you","title":"Thank you","text":"<p>{ width=\"500\" }</p>"},{"location":"Optimisations%20for%20Expressions%20containing%20FAIL%20and%20I%3E/","title":"Optimisations for Expressions containing FAIL and I>","text":"<p>202310291410</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note","Incomplete"]},{"location":"Optimisations%20for%20Expressions%20containing%20FAIL%20and%20I%3E/#optimisations-for-expressions-containing-textfail-and-triangleright","title":"Optimisations for Expressions containing \\(\\text{FAIL}\\) and \\(\\triangleright\\)","text":"<p>[!warning] Redundancies It is often the case that all occurrences of \\(\\text{FAIL}\\), and its companion \\(\\triangleright\\) can be eliminated. Most of the these optimisations depend on reasoning that \\(\\text{FAIL}\\) can never be returned by an expression \\(E\\). In the above cases, the corresponding occurrences of \\(\\triangleright\\) can  also be removed.</p> <p>If \\(\\text{FAIL}\\) is returned by an expression \\(E\\) then one of the following is necessary 1. \\(\\text{FAIL}\\) is mentioned explicitly in  \\(E\\) 2. \\(E\\) contains a pattern-matching lambda abstraction whose application may fail 3. \\(\\text{FAIL}\\) is the value of one of the free variables of \\(E\\).</p> <p>If the pattern matching compiler is applied throughout then there will be no pattern matching lambda expressions hence 2. will never happen. And presumably since the programmer cannot write \\(\\text{FAIL}\\), 3. will never happen.</p> <p>Hence we focus on all the places where \\(\\text{FAIL}\\) can be introduced by the compiler. There are only 2 such places - In the translation of conditional equations - In the variant of pattern-matching compiler</p> <p>[!todo]  Link Pattern Matching Compiler from the next section of the book</p>","tags":["Note","Incomplete"]},{"location":"Optimisations%20for%20Expressions%20containing%20FAIL%20and%20I%3E/#transformation-rules-for-triangleright-and-textfail","title":"Transformation rules for \\(\\triangleright\\) and \\(\\text{FAIL}\\)","text":"<ul> <li>We can eliminate \\(\\triangleright\\) and \\(\\text{FAIL}\\) if the side cannot produce \\(\\text{FAIL}\\) $$ E_{1} \\triangleright E_{2} \\equiv E_{1} $$   If \\(E_1\\) cannot return \\(\\text{FAIL}\\)</li> <li>We can also eliminate \\(\\triangleright\\) if \\(FAIL\\) is definitely produced on one side $$ E \\triangleright \\text{FAIL} \\equiv E \\quad\\quad \\text{and} \\quad\\quad  \\text{FAIL} \\triangleright E \\equiv E $$</li> </ul> <p>The second point comes from the fact that \\(\\text{FAIL}\\) is the identity element for the operator \\(\\triangleright\\).</p> <p>Using the above rules and the Semantics for the operator we also get the following rule $$ (\\text{IF} E_{1} E_{2} E_{3}) \\triangleright E\\quad \\equiv\\quad \\text{IF} E_{1} E_{2} (E_{3}\\triangleright E) $$ If \\(E_1\\) and \\(E_2\\) cannot return fail.</p>","tags":["Note","Incomplete"]},{"location":"Optimisations%20for%20Expressions%20containing%20FAIL%20and%20I%3E/#eliminating-triangleright-and-textfail-from-conditional-equations","title":"Eliminating \\(\\triangleright\\) and \\(\\text{FAIL}\\) from conditional equations","text":"<p>After the pattern matching compilation is done, then Empty Rule for Match Function gives an expression of the following form. $$ E_{1} \\triangleright E_{2} \\triangleright \\dots E_{m} \\triangleright E $$ Now the \\(E_i\\) might be a set of conditional equations with guards. If there is no <code>otherwise</code> case in the end, then the expression must have translated to <pre><code>IF G1 A1 (IF ... (IF Gg Ag FAIL) ... )\n</code></pre> where <code>g</code> is the number of alternatives of the guards.</p> <p>If there is an <code>otherwise</code> case there is no \\(\\text{FAIL}\\) produced and we get an expression of the form <pre><code>IF G1 A1 (IF ... (IF G(g-1) A(g-1) Ag) ... )\n</code></pre></p> <p>If the right hand side of one of the \\(E_i\\) is of this form, then we can use the third rule followed by the second to give</p> <p>$$ \\begin{matrix} (\\text{IF} G_{1} A_{1} (\\text{IF} \\dots (\\text{IF}  G_{g} A_{g} \\text{FAIL}) \\dots ))\\triangleright E \\ \\equiv \\</p> <p>\\text{IF} G_{1} A_{1} (\\text{IF} \\dots (\\text{IF}  G_{g} A_{g} E) \\dots ) \\ \\end{matrix} $$</p> <p>Application of the three rules eliminates all occurrences of \\(\\text{FAIL}\\) in the expression generated by the empty rule.</p>","tags":["Note","Incomplete"]},{"location":"Optimisations%20for%20Expressions%20containing%20FAIL%20and%20I%3E/#references","title":"References","text":"<ul> <li>Optimisations for Overlapping Patterns</li> <li>Match Function for Enriched Lambda Calculus</li> </ul>","tags":["Note","Incomplete"]},{"location":"Optimisations%20for%20Overlapping%20Patterns/","title":"Optimisations for Overlapping Patterns","text":"<p>202310290210</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note"]},{"location":"Optimisations%20for%20Overlapping%20Patterns/#optimisations-for-overlapping-patterns","title":"Optimisations for Overlapping Patterns","text":"<p>[!warning] Overlapping Patterns If overlapping equations are allowed, then sometimes the pattern-matching compiler may transform a small set of equations in to a case-expression which is much larger.</p>","tags":["Note"]},{"location":"Optimisations%20for%20Overlapping%20Patterns/#redundancies-created-due-to-constructor-rule","title":"Redundancies created due to Constructor Rule","text":"<p>This is a problem that is created by the Constructor Rule for Match Function where expressions of the form</p> <p><pre><code>match (u:us) (qs1 ++ ... ++ qsk) E\n</code></pre> to <pre><code>case u of\n    c1 us1' =&gt; match (us1' ++ us) qs1' E\n    ...\n    ck usk' =&gt; match (usk' ++ us) qsk' E\n</code></pre></p> <p>Normally <code>E</code> would be \\(\\text{ERROR}\\), but if Mixture Rule is used then <code>E</code> can be huge. So we modify the rule in the following way to avoid duplication</p> <pre><code>(case u of\n    c1 us1' =&gt; match (us1' ++ us) qs1' FAIL\n    ...\n    ck usk' =&gt; match (usk' ++ us) qsk' FAIL)\n|&gt; E\n</code></pre> <p>^11a949</p>","tags":["Note"]},{"location":"Optimisations%20for%20Overlapping%20Patterns/#example-problem","title":"Example Problem","text":"<pre><code>unwieldy [] [] = A\nunwieldy xs ys = B xs ys\n</code></pre> <p>Here the pattern matching compiler transforms the above set of equations to</p> <pre><code>unwieldy =\n    \\xs.\\ys. case xs of\n                Nil         =&gt; case ys of\n                                   Nil         =&gt; A\n                                   Case y' ys' =&gt; B xs ys\n                Cons x' xs' =&gt; B xs ys\n</code></pre> <p>where <code>B xs ys</code> is repeated. This would get worse as the number of repetitions or the size of <code>B</code> increases and make the compiled binaries huge.</p> <p>Using the rules above we get <pre><code>unwieldy =\n    \\xs.\\ys. (case xs of\n                Nil         =&gt; (case ys of\n                                   Nil         =&gt; A\n                                   Case y' ys' =&gt; FAIL)\n                               |&gt; Fail\n                Cons x' xs' =&gt; FAIL)\n             |&gt; B xs ys\n</code></pre> this may be bigger, but it has just one occurrence of <code>B</code> hence, if the size of <code>B</code> explodes, the the upper one grows much faster.</p>","tags":["Note"]},{"location":"Optimisations%20for%20Overlapping%20Patterns/#references","title":"References","text":"<ul> <li>Match Function for Enriched Lambda Calculus</li> <li>Patterns</li> <li>Evaluating Pattern Matching in Lambda Calculus</li> </ul>","tags":["Note"]},{"location":"Orbits%20in%20X%20%2A%20X%20being%20strongly%20connected%20under%20transitive%20automorphism/","title":"Orbits in X * X being strongly connected under transitive automorphism","text":"<p>202308211048</p> <p>type : #Example tags : [[Algebraic Graph Theory]]</p>"},{"location":"Orbits%20in%20X%20%2A%20X%20being%20strongly%20connected%20under%20transitive%20automorphism/#orbits-in-xtimes-x-being-strongly-connected-under-transitive-automorphism","title":"Orbits in \\(X\\times X\\) being strongly connected under transitive automorphism","text":""},{"location":"Orbits%20in%20X%20%2A%20X%20being%20strongly%20connected%20under%20transitive%20automorphism/#lemma","title":"Lemma:","text":"<p>Let \\(\\Omega\\) be a non-diagonal orbit in \\(X\\times X\\)(under the action of transitive \\((\\Gamma, X)\\)) . The \\(\\Omega\\) is strongly connected iff it is weakly connected.</p>"},{"location":"Orbits%20in%20X%20%2A%20X%20being%20strongly%20connected%20under%20transitive%20automorphism/#proof","title":"Proof:","text":"<p>Since \\(\\Gamma\\) is transitive, for all \\(x, z\\) in \\(X\\), we have that the  $$ \\begin{align} \\text{in}(x)&amp;= \\text{in}(z)\\ \\text{out}(x)&amp;= \\text{out}(z)\\ \\end{align} $$ Let \\(\\Omega\\) be weakly connected. let \\(S\\) be a strong component such that \\(S\\) is a sink.</p> <p>Clearly for all \\(x\\ne y\\) and \\(\\vec {xy}\\) is an edge in \\(\\Omega\\) then: Given any \\(v\\in\\Omega\\), let \\(k_1=\\text{in}(v)\\) and \\(k_2=\\text{out}(v)\\) and \\(k_{1}= k_{2}\\).</p> <p>Let \\(S\\) be a weak component of \\(\\Omega\\) then every edge in \\(S\\) has** the property that the edge contributes \\(1\\) each to out degree sum and in degree sum.</p>"},{"location":"Orbits%20in%20X%20%2A%20X%20being%20strongly%20connected%20under%20transitive%20automorphism/#related","title":"Related","text":""},{"location":"Orbits%20on%20X%20and%20X%20%2A%20X/","title":"Orbits on X and X * X","text":"<p>202308161104</p> <p>type : #Example tags : [[Algebraic Graph Theory]]</p>"},{"location":"Orbits%20on%20X%20and%20X%20%2A%20X/#orbits-on-x-and-xtimes-x","title":"Orbits on \\(X\\) and \\(X\\times X\\)","text":"<p>The Graph is considered to be transitive here.</p>"},{"location":"Orbits%20on%20X%20and%20X%20%2A%20X/#claim","title":"Claim:","text":"<p>Let \\(\\{O_{1},\\dots,O_{r}\\}\\) be the set of orbits on \\(\\Gamma_{x}\\) and \\(X\\) where \\(O_{1}=\\{x\\}\\) and choose \\(y_{i}\\in O_{i},\\forall i\\in[2..r]\\). Let \\(\\{F_{1},\\dots, F_{r}\\}\\) be the set of  orbits in \\(X\\times X\\) and \\(F_{1}\\) is the diagonal</p>"},{"location":"Orbits%20on%20X%20and%20X%20%2A%20X/#proof","title":"Proof","text":"<p>Consider \\((x,y_{i})\\in F_{i}\\) then given any \\(v\\ne u\\) we can take \\((u, v)\\) to \\((x, v')\\) which can be sent to some \\(x, y_{i}\\) for some \\(i\\) and hence any element is a part of one of \\(F_{i}\\), hence there is a bijection between the set of orbits in \\(X\\) and set of orbits in \\(X\\times X\\).</p> <pre><code>\\usepackage{tikz-cd}\n\\begin{document}\n\\begin{tikzcd}\n\n{(u, v)} \\arrow [d, \"\\alpha\"]&amp;\\\\\n{(x, v')} \\arrow [r, \"\\beta\"]&amp; {(x, y_i)}\n\\end{tikzcd}\n\\end{document}\n</code></pre> <pre><code>title: Number of orbits.\nThe above Claim Shows that given any $x$, the number of orbits of $\\Gamma_{x}$ on $X$ is the same as the number of orbits on $\\Gamma$ on $X\\times X$ hence the number of orbits of $\\Gamma_{x}$ is the same $x$ for all $x\\in X$\n</code></pre>"},{"location":"Orbits%20on%20X%20and%20X%20%2A%20X/#related","title":"Related","text":""},{"location":"Order%20Topology/","title":"Order Topology","text":"<p>202301161201</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Order%20Topology/#order-topology","title":"Order Topology","text":""},{"location":"Order%20Topology/#linear-order","title":"Linear order","text":"<pre><code>title: \nA binary relation $&lt;$ on a set A is a called a linear order if \n1. For every x,y in A x &lt; y or x &gt; y.\n2. $x \\nless x$ for all x in A.\n3. x &lt; y, y&lt;z implies x&lt;z.\n</code></pre>"},{"location":"Order%20Topology/#lemma","title":"Lemma:","text":"<p>The open rays \\((a,\\infty)\\) and \\((-\\infty,b)\\) form a Sub-basis.</p> <p>The topo generated by this sub-basis is called the order topology.</p>"},{"location":"Order%20Topology/#note-subspace-of-order-topology-need-not-be-the-same-as-the-order-topology-on-the-subspace","title":"Note: Subspace of order topology need not be the same as the order topology on the subspace.","text":"<p>Example: X = real numbers Y = \\([0,1) \\cup \\{2\\}\\). In subspace topo of Y, \\(\\{2\\}\\) is open but not in order topo.</p>"},{"location":"Order%20Topology/#theorem","title":"Theorem:","text":"<pre><code>Let X be a linearly ordered set with $\\tau_X$ order topo, $Y$ be a convex subset of X. Then order topo on Y is the same as the subspace topo on Y.\n</code></pre>"},{"location":"Order%20Topology/#proof","title":"Proof:","text":"<p>TO DO</p>"},{"location":"Order%20Topology/#related-results","title":"Related Results","text":"<p>1) Order topology is regular.</p>"},{"location":"Order%20Topology/#related-problems","title":"Related Problems","text":""},{"location":"Order%20Topology/#references","title":"References","text":"<p>Regular Spaces</p>"},{"location":"Ordering%20Equations%20in%20Uniform%20Definitions/","title":"Ordering Equations in Uniform Definitions","text":"<p>202310292137</p> <p>tags : [[Programming Languages]]</p>","tags":["Example"]},{"location":"Ordering%20Equations%20in%20Uniform%20Definitions/#ordering-equations-in-uniform-definitions","title":"Ordering Equations in Uniform Definitions","text":"<p>[!note] Theorem If a definition is uniform, changing the order of the equations does not change the meaning of the definitions.</p> <p>^fd86df</p> <p>Proof: There are 3 rules that are to be applied to simplify the definitions, we induct on the number of variable in the first argument to the match function: - Empty Rule for Match Function     - Empty Rule is only applied on singleton sets of equations, hence any change of ordering trivially does not change the last step. This is the base case for our induction - Variable Rule for Match Function     - Applying variable rule returns a match function with the size of the variable list reduced by one.      - Consider a different ordering of equations, applying the variable rule now gives the above result but in a different, order.      - By induction we can say that these will have these definitions will have the same meaning. - Constructor Rule for Match Function     - For the constructor rule, the ordering between two element that have different constructors for the first variable does not matter, as they will be taken to different branches of the case statement.     - To show that the ordering does not matter even with the same constructor we apply the constructor rule. Since we are looking at equations that use the same constructor, we only need to look at one branch. We then apply the variable rule for the new variables added. And we get a set of equations with strictly less terms.     - We change the original ordering of equations and do the same set of operations. Now by induction we get that changing the order preserves meaning.     - Since we have already proved this for variable rule, we can look at the result before we started applying the variable.  \\(\\square\\)</p>","tags":["Example"]},{"location":"Ordering%20Equations%20in%20Uniform%20Definitions/#related","title":"Related","text":"<p>Uniform Definition of Haskell Functions Match Function for Enriched Lambda Calculus</p>","tags":["Example"]},{"location":"Ordinary%20Generating%20Functions/","title":"Ordinary Generating Functions","text":"<p>202305281205</p> <p>Type : #Note Tags : [[Enumerative Combinatorics]]</p>"},{"location":"Ordinary%20Generating%20Functions/#ordinary-generating-functions","title":"Ordinary Generating Functions","text":"<p>Ordinary Generating Functions are a type of Generating Functions that are of the form  $$ F(x) = \\sum\\limits_{n\\ge0} f(n)x^{n} $$ where \\(f(n)\\) is the Generating Functions.</p>"},{"location":"Ordinary%20Generating%20Functions/#references","title":"References","text":""},{"location":"P%20Complexity%20Class/","title":"P Complexity Class","text":"<p>202301021901</p> <p>Type : #Note Tags : [[Complexity Theory]]</p>"},{"location":"P%20Complexity%20Class/#p-complexity-class","title":"P Complexity Class","text":""},{"location":"P%20Complexity%20Class/#p","title":"P","text":"<pre><code>title:\nClass of decision problems that can be decided by a [determinisitic turing machine](&lt;./Turing Machines.md&gt;) in polynomial time.\n</code></pre>"},{"location":"P%20Complexity%20Class/#examples","title":"Examples","text":"<p>Primality- The language of all Natural numbers which are prime. Reachablility- Given a graph \\(G\\) and vertices \\(v, u\\in G\\), is there a path from \\(u\\) to \\(v\\). Palindrome- The language of all words which don't change on reversal.</p>"},{"location":"P%20Complexity%20Class/#related-problems","title":"Related Problems","text":""},{"location":"P%20Complexity%20Class/#references","title":"References","text":"<p>Complexity Classes</p>"},{"location":"P%20implies%20not-not%20P%20-%20Intuitionistic%20Logic/","title":"P implies not not P   Intuitionistic Logic","text":"<p>202308141458</p> <p>type : #Example tags : </p>"},{"location":"P%20implies%20not-not%20P%20-%20Intuitionistic%20Logic/#p-implies-not-not-p-intuitionistic-logic","title":"P implies not-not P - Intuitionistic Logic","text":"<p>### Question: \\(\\(\\Gamma\\vdash p\\to\\lnot\\lnot p\\)\\)</p>"},{"location":"P%20implies%20not-not%20P%20-%20Intuitionistic%20Logic/#answer","title":"Answer:","text":"<p>The question is a shorthand for \\(p\\to((p\\to\\perp)\\to\\perp)\\) which can be proved in the following way $$ \\frac{} {\\vdash p\\to((p\\to\\perp)\\to\\perp)}(\\to\\mathcal I) $$</p>"},{"location":"P%20implies%20not-not%20P%20-%20Intuitionistic%20Logic/#related","title":"Related","text":""},{"location":"PLC%20Assignment%203/","title":"PLC Assignment 3","text":"<p>Name: Shubh Sharma Roll Number: BMC202170</p> <p>### 1.  ### 2.  ### 3.  ### 4.  ### 5.  ### 6.  ### 7.  ### 8.  ### 9.</p>"},{"location":"PSPACE-Hardness%20of%20LTL%20Model%20Checking/","title":"PSPACE Hardness of LTL Model Checking","text":"<p>202311190011</p> <p>Tags : [[Theory of Computation]], [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"PSPACE-Hardness%20of%20LTL%20Model%20Checking/#pspace-hardness-of-ltl-model-checking","title":"PSPACE-Hardness of LTL Model Checking","text":"","tags":["Note","Incomplete"]},{"location":"PSPACE-Hardness%20of%20LTL%20Model%20Checking/#proof-sketch","title":"Proof sketch","text":"<p>For a TM that is bounded by \\(S(n)\\) which is a polynomial on \\(n\\), then for a know input \\(a=a_{1}\\dots a_{n}\\) we construct an [[R-Structure]] That looks like </p> <p></p> <p>let \\(\\mathcal P = Q\\times\\Sigma\\sqcup\\Sigma\\sqcup\\{\\text{BI},\\text{EI}\\}\\) Where we have \\(S(n)\\) diamonds and for each diamond we have \\(|Q\\times\\Sigma\\sqcup\\Sigma|\\) vertical vertices and on each vertex one of the propositions true. A path from \\(\\text{BI}\\to\\text{EI}\\) encodes an instantaneous discription of a turing machine.</p> <p>We use LTL formulas to represents relations between configurations of the turing machine in each step, accepting conditions are trivial.</p>","tags":["Note","Incomplete"]},{"location":"PSPACE-Hardness%20of%20LTL%20Model%20Checking/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Pairs%20in%20Lambda%20Calculus/","title":"Pairs in Lambda Calculus","text":"<p>202303291503</p> <p>Type : #Note Tags : [[Lambda Calculus]]</p>"},{"location":"Pairs%20in%20Lambda%20Calculus/#pairs-in-lambda-calculus","title":"Pairs in Lambda Calculus","text":"<p>Pair are a collection of two values considered together. To construct a useful definition of pairs, we require a method to pull out components of a pair to use them separately.  A good candidate for a choosing function would be somewhat similar to the Church Booleans as \\(TRUE\\) takes in two inputs and returns the first one and \\(FALSE\\) takes in two inputs and returns the second one. This idea is actually enough to provide a working description of pairs as  $$ PAIR:=\\lambda ab.(\\lambda v.vab) $$ Where \\(v\\) is supposed to be a Church Boolean.</p> <p>The pair function take in two arguments are returns a function which in a sense stores the two inputs, it wait for a selector function when it will later return one of the values </p> <p>So while using the booleans as selector functions, we can use the following aliases</p> \\[ FIRST:= TRUE,\\;\\;\\; SECOND:=FALSE \\] <pre><code>title: infix notation\nwe can write $PAIR(a,b)$ as $(a, b)$\nand we can write the first and second function in following way \n$(a,b).1 = a$\n$(a,b).2 = b$\n</code></pre>"},{"location":"Pairs%20in%20Lambda%20Calculus/#references","title":"References","text":"<p>Church Numerals</p>"},{"location":"Partial%20Functions/","title":"Partial Functions","text":"<p>202212152212</p> <p>Type : #Note Tags : [[PLC]]</p>"},{"location":"Partial%20Functions/#partial-functions","title":"Partial Functions","text":"<p><pre><code>title:\nA _Partial Function_ is a function that has values on some arguments but not on all arguments\n</code></pre> The reason why a function would not return all values are : - Error Termination: Evaluation of a function cannot proceed because of a conflict between the operator and an operand. - Nontermination: Evaluation of a function proceeds indefinitely. An example of the first case would be the expression <code>x:= 3 / 0</code>, because devision by <code>0</code> is undefined. An example of the second scenario can be a function <code>f(x: int) := if x = 0 then 0 else x + f(x-2)</code>, this function does not terminate when <code>3</code> is given as an argument.</p> <p>A partial function is not a function as defined by the mathematical sense.  A mathematical function \\(f:A\\to B\\) is a set of ordered pair \\(f\\subset A\\times B\\) satisfying the following conditions: 1. If \\((x, y) \\in f\\) and \\((x, z) \\in f\\), then \\(y =z\\). 2. For every \\(x \\in A\\), there exists a \\(y\\in B\\) such that \\((x,y) \\in f\\). But a partial function only follows the first condition.</p>"},{"location":"Partial%20Functions/#related-problems","title":"Related Problems","text":""},{"location":"Partial%20Functions/#references","title":"References","text":""},{"location":"Partial%20Isomorphism/","title":"Partial Isomorphism","text":"<p>202311292011</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"Partial%20Isomorphism/#partial-isomorphism","title":"Partial Isomorphism","text":"<p>Let \\(\\mathfrak U\\) and \\(\\mathfrak B\\) be two \\(\\sigma\\)-structures and let \\(\\vec{a}=(a_{1}\\dots a_n)\\) and \\(\\vec{b}=(b_{1}\\dots b_{n})\\) be two tuples in the two structures respectively. Then \\((\\vec a,\\vec b)\\) defines a partial isomorphism if - For every \\(i,j&lt;n\\)    $$   a_{i}\\equiv a_{j} \\iff b_{i}\\equiv b_{j}   $$ - For every \\(i&lt;n\\)   $$   a_{i}\\equiv c^\\mathfrak U \\iff b_{i}\\equiv c^\\mathfrak B   $$ - For every \\(k-\\)ary relation \\(P\\) in \\(\\sigma\\) and every sequence \\(i_{1},i_{2}\\dots i_{k}\\) of non necessarily distinct number from \\([1..n]\\)    $$   (a_{i_{1}},a_{i_{2}}\\dots a_{i_{k}})\\in P^\\mathfrak U \\iff  (b_{i_{1}},b_{i_{2}}\\dots b_{i_{k}})\\in P^\\mathfrak B   $$</p> <p>[!note] Obvious Intuition If we only look at the tuples defined above and ignore the rest of the structure Then the map that looks like an isomorphism.</p>","tags":["Note","Incomplete"]},{"location":"Partial%20Isomorphism/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Path%20Connectedness/","title":"Path Connectedness","text":"<p>202302101302</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Path%20Connectedness/#path-connectedness","title":"Path Connectedness","text":"<pre><code>title: \nGiven points $x,y \\in X$ we say a _path_ in $X$ is a continuous map $f : [a,b] \\to X$ with $f(a) = x, f(b) = y$.\n</code></pre> <pre><code>title:\n$X$ is _path connected_ if between any 2 points $x,y \\in X$ there exists a path in $X$ from $x$ to $y$.\n</code></pre> <pre><code>title: Theorem\nIf $X$ is path connected then $X$ is connected but the converse is not true.\n</code></pre>"},{"location":"Path%20Connectedness/#proof","title":"Proof:","text":"<p>If X were disconnected, then let \\(X = A \\sqcup B\\), let \\(a \\in A, b \\in B\\), then there is a path \\(f : [0,1] \\to X\\) from a to b. Then \\(f([0,1]) = (A \\cap f([0,1])) \\cup (B \\cap f([0,1]))\\) which means \\(f([0,1])\\) is disconnected, which is a contradiction.</p> <p>Converse is not true: S = \\(\\{(x,y) | y = sin 1/x, x&gt;0\\}\\) Let \\(\\bar S = S \\cup \\{(0,0)\\}\\). \\(\\bar S\\) is connected but not path connected. It is connected since \\(S\\) is a cts image of \\((0,\\infty)\\) and \\(\\bar S\\) lies between \\(S\\) and \\(Cl(S)\\).  If it were path connected then there would be a path \\(f:[0,1] \\to \\bar S\\) from \\((1/\\pi,0)\\) to \\((0,0)\\). hence it would take all \\(x\\) values between 0 and \\(1/\\pi\\).  Then the sequence \\(\\{t_n\\}_{n\\in \\mathbb{N}}\\) s.t. \\(f(t_n) = \\left(\\dfrac{1}{2n\\pi+\\pi/2},1\\right)\\). There is a convergent subsequence \\(t_n \\to t_0\\), and since \\(f\\) is continuous, \\(f(t_0) = (0,1) \\notin \\bar S\\). Contradiction.</p> <pre><code>title: Theorem\nIf $A \\subseteq \\mathbb{R}^n$ is open then $A$ connected $\\iff A$ path connected.\n</code></pre>"},{"location":"Path%20Connectedness/#proof_1","title":"Proof:","text":"<p>Claim: If \\(A \\subset \\mathbb{R}^n\\) is open, then the path components (equivalence classes under the relation \\(x\\sim y \\iff\\) there is a path from x to y) of \\(A\\) are also open. Proof: Let \\(x \\in P \\subset A\\), where \\(P\\) is a path component, then \\(B_r(x)\\) is contained in \\(A\\) for some \\(r\\), and hence also in \\(P\\) (since it's path connected). \\(\\square\\)</p> <p>Now suppose \\(A\\) were not path connected, then there is some \\(x\\) in \\(A\\) such that the path component of \\(x\\) is not the whole of \\(A\\), call it \\(U\\). Then \\(A = U \\sqcup V\\) where \\(V\\) is the union of the other path components. But then \\(V\\) = \\(\\phi\\) since \\(U\\) is non empty. Hence \\(A\\) is path connected</p>"},{"location":"Path%20Connectedness/#related-problems","title":"Related Problems","text":""},{"location":"Path%20Connectedness/#references","title":"References","text":"<p>Connectedness</p>"},{"location":"Pattern%20Matching%20to%20Ordinary%20Lambda%20Calculus/","title":"Pattern Matching to Ordinary Lambda Calculus","text":"<p>202311041711</p> <p>Tags : [[Programming Languages]], [[Lambda Calculus]]</p>","tags":["Note","Incomplete"]},{"location":"Pattern%20Matching%20to%20Ordinary%20Lambda%20Calculus/#pattern-matching-to-ordinary-lambda-calculus","title":"Pattern Matching to Ordinary Lambda Calculus","text":"<p>Here we remove the pattern matching used in Enriched Lambda Calculus.</p> <p>For variable pattern we don't have to do anything. For the Other patterns we can do the following</p> <ul> <li>Constant Pattern to Lambda Calculus</li> <li>Product Constructor Pattern Matching to Lambda Calculus</li> <li>Sum Constructor Pattern Matching to Lambda Calculus</li> </ul>","tags":["Note","Incomplete"]},{"location":"Pattern%20Matching%20to%20Ordinary%20Lambda%20Calculus/#reducing-the-number-of-built-in-functions","title":"Reducing the Number of Built-in Functions","text":"<p>The trouble with the above 3 steps is that they introduce a lot of built in functions for each constructor.</p> <p>The idea is to give each constructor a tag, and have a general function that takes in the tag and arity of the constructor as in input to return a function which acts the same. This is the simplification that we get</p> <ul> <li>\\(s\\) (sum constructor) is replaced with \\(\\text{PACK-SUM-d-r}_{s}\\)</li> <li>\\(\\text{UNPACK-SUM-s}\\) is replaced with \\(\\text{UNPACK-SUM-d-r}_{s}\\)</li> <li>\\(t\\) (product constructor) is replace with \\(\\text{PACK-PRODUCT-d-r}_{t}\\)</li> <li>\\(\\text{UNPACK-PRODUCT-t}\\) is replaced with \\(\\text{UNPACK-PRODUCT-d-r}_{t}\\)</li> <li>\\(\\text{SEL-t-i}\\) is replaced with \\(\\text{SEL-r}_{t}\\text{-i}\\) where</li> <li>\\(r_{s}\\) is the arity \\(s\\)</li> <li>\\(d\\) is the structure tag of \\(s\\)</li> <li>\\(r_{t}\\) is the structure tag of \\(t\\)</li> </ul>","tags":["Note","Incomplete"]},{"location":"Pattern%20Matching%20to%20Ordinary%20Lambda%20Calculus/#summary","title":"Summary","text":"<p> ^1331d9</p>","tags":["Note","Incomplete"]},{"location":"Pattern%20Matching%20to%20Ordinary%20Lambda%20Calculus/#references","title":"References","text":"<p>Enriched Lambda Calculus Patterns</p>","tags":["Note","Incomplete"]},{"location":"Patterns/","title":"Patterns","text":"<p>202310221810</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note"]},{"location":"Patterns/#patterns","title":"Patterns","text":"","tags":["Note"]},{"location":"Patterns/#titledefinition-a-pattern-p-is-either-a-variable-v-a-constant-k-a-constructor-pattern-of-the-form-cp_1p_2dotsp_n-where-p_i-are-also-patterns-in-case-of-haskell-all-variables-should-be-distinct","title":"<pre><code>title:Definition\nA pattern $p$ is either\n- a variable $v$\n- a constant $k$\n- a constructor pattern of the form $(c\\;p_1\\;p_{2\\dots}p_n)$ where $p_{i}$ are also patterns\n\nIn case of *Haskell* all variables should be distinct\n</code></pre>","text":"<p>Consider the following Haskell function <pre><code>f p1 = E2\nf p2 = E2\n...\nf p3 = E3\n</code></pre> <pre><code>title:Intuition\nThe intuitive semantics of pattern matching are  \"Try the first pattern, if that fails then try the next, and so on\". This suggests that all of them might fail, in which case an error.\n\nWe introduce values $\\text{FAIL}$ and $\\text{ERROR}$\n</code></pre> The following is the definition of <code>f</code> into Enriched Lambda Calculus.</p> <p><pre><code>f = \\x.(  ((\\p1'.E1') x)\n       |&gt; ((\\p2'.E2') x)\n       ...\n       |&gt; ((\\pn'.En') x)\n       |&gt; ERROR )\n</code></pre> here <code>x</code> is a new variable which is not free in any <code>E</code>.</p>","tags":["Note"]},{"location":"Patterns/#semantics-for-the-triangleright-operator","title":"Semantics for the \\(\\triangleright\\) operator","text":"<pre><code>title: Notation Difference\nThe book uses a symbol called fatbar instead of $\\triangleright$, I cannot find the latex definition for it.\n</code></pre> <p>The function <code>|&gt;</code> is an infix function whose behavior is described by $$ \\begin{align} a &amp;\\triangleright b = a\\ \\text{FAIL} &amp;\\triangleright b = b\\ \\perp &amp;\\triangleright b = \\perp \\end{align} $$ Operationally, <code>|&gt;</code> evaluates the left argument, and if the evaluation terminates and yields anything other than \\(\\text {FAIL}\\) then it returns that, otherwise it returns the second argument. <code>|&gt;</code> is associative and has identity \\(\\text{FAIL}\\).</p> <p>Suppose <code>f</code> is applied to multiple arguments, and one of the initial arguments fails. Then we want the failure to propagate, hence we need the following reduction rule for \\(\\text{FAIL}\\)</p> <p>$$ \\text {FAIL}\\;\\;E\\quad \\to\\quad\\text{FAIL} $$ This property is followed by the fixed point of the \\(K\\) combinator called Eater. </p>","tags":["Note"]},{"location":"Patterns/#semantics-of-pattern-matching","title":"Semantics of Pattern Matching","text":"","tags":["Note"]},{"location":"Patterns/#variables","title":"Variables","text":"<p>The the pattern is a variable \\(v\\), then the pattern matching lambda abstraction is just an ordinary lambda abstraction.</p>","tags":["Note"]},{"location":"Patterns/#constants","title":"Constants","text":"<p>Semantics for constant patterns state that if the argument matches the constant value then we use that value in the expression, otherwise we return fail, this can be written as </p> \\[ \\begin{align*} &amp;\\mathbf{Eval}\\textlbrackdbl\\;\\lambda k.E\\;\\textrbrackdbl\\;a =  \\mathbf{Eval}\\textlbrackdbl\\;E\\;\\textrbrackdbl &amp;&amp;\\text{if }a=\\textbf{Eval}\\textlbrackdbl\\;k\\;\\textrbrackdbl\\\\ &amp;\\mathbf{Eval}\\textlbrackdbl\\;\\lambda k.E\\;\\textrbrackdbl\\;a =\\text{FAIL} &amp;&amp;\\text{if }a\\ne\\textbf{Eval}\\textlbrackdbl\\;k\\;\\textrbrackdbl\\text{ and } a\\ne\\perp\\\\ &amp;\\mathbf{Eval}\\textlbrackdbl\\;\\lambda k.E\\;\\textrbrackdbl\\perp =\\; \\perp \\end{align*} \\] <p>^974443</p>","tags":["Note"]},{"location":"Patterns/#sum-constructors","title":"Sum Constructors","text":"<p>The idea is somewhat similar, we first check if the constructor matches for the pattern, then we recursively pattern match each field of the constructor. We fail if the constructor does not match. $$ \\begin{align} &amp;\\mathbf{Eval}\\textlbrackdbl\\;\\lambda (s\\;p_{1}\\dots p_{k}).E\\;\\textrbrackdbl\\;(s\\;a_{1}\\dots a_{k}) &amp;&amp;= \\mathbf{Eval}\\textlbrackdbl\\;\\lambda p_{1}\\dots \\lambda p_{k}.E\\;\\textrbrackdbl \\; a_{1}\\dots a_{r}\\ &amp;\\mathbf{Eval}\\textlbrackdbl\\;\\lambda (s\\;p_{1}\\dots p_{k}).E\\;\\textrbrackdbl\\;(s' a_{1}\\dots a_{k}) &amp;&amp;=  \\text{FAIL}\\quad\\quad\\text{ if } s\\ne s'\\ &amp;\\mathbf{Eval}\\textlbrackdbl\\;\\lambda (s p_{1}\\dots p_{k}).E\\;\\textrbrackdbl\\perp &amp;&amp;= \\; \\perp \\end{align} $$</p> <p>^b7ccdc</p>","tags":["Note"]},{"location":"Patterns/#product-constructor","title":"Product Constructor","text":"<p>The reason why this is a different section is because we choose to make the product constructors lazy, specifically to handle the case when none of the components are used.</p> \\[ \\begin{align*} \\mathbf{Eval}\\textlbrackdbl\\;\\lambda(t\\ p_{1}\\dots p_{r}).E\\;\\textrbrackdbl\\; a =  \\mathbf{Eval}\\textlbrackdbl\\;\\lambda p_{1}\\dots \\lambda p_{r}.E\\;\\textrbrackdbl\\;&amp;(\\text{SEL-t-}1\\;a)\\\\ &amp;\\dots\\\\ &amp;(\\text{SEL-t-}r\\;a) \\end{align*} \\] <p>^55a864</p> <p>Where \\(\\text{SET-t-}i\\) is a built in function that extracts the \\(i^{th}\\) component of \\(a\\). $$ \\begin{align} &amp;\\text{SET-t-}i (t a_{1}\\dots a_{i}\\dots a_{r})&amp;&amp;= a_i\\ &amp;\\text{SET-t-}i \\perp &amp;&amp;= \\perp \\end{align} $$ This ensure that if none of the components of a product are evaluated, the product is not evaluated either.</p>","tags":["Note"]},{"location":"Patterns/#references","title":"References","text":"<p>Evaluating Pattern Matching in Lambda Calculus</p>","tags":["Note"]},{"location":"Perfect%20Fields/","title":"Perfect Fields","text":"<p>202305251305</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Perfect%20Fields/#perfect-fields","title":"Perfect Fields","text":"<p>```ad-note: title: A field \\(F\\) is perfect if it has characteristic zero or it has characteristic \\(p\\) and every element of \\(F\\) is a \\(p\\)th power. <pre><code>### Theorem:\n```ad-note\ntitle:\nA field F is perfect iff every irreducible polynomial in $F[X]$ is separable.\n</code></pre></p>"},{"location":"Perfect%20Fields/#proof","title":"Proof:","text":"<p>Perfect \\(\\implies\\) irreducible is separable. If \\(char(F) = 0\\), then done. If \\(char(F) = p\\), then take an unseparable polynomial \\(f(X) = a_{0} + a_{1}X ^{p} + \\dots a_{n} X ^{pn} \\in F[X]\\). Since \\(a_{i} \\in F\\) and \\(F\\) is perfect, \\(a_{i} = b_{i} ^{p}\\) for some \\(b_{i} \\in F\\) for all \\(i\\). Hence, \\(f(X) = (b_{0} + b_{1}X + \\dots b_{n}X ^{n}) ^{p}\\) and hence is not irreducible.</p> <p>Hence, irreducible implies separable.</p> <p>Irreducible is separable \\(\\implies\\) Perfect. If char(F) = 0 then done. If char(F) = p, then suppose that \\(a \\in F\\) which is not a \\(p\\)th power.  Then \\(X ^{p} - a\\) is irreducible but not separable.</p>"},{"location":"Perfect%20Fields/#references","title":"References","text":""},{"location":"Permutation%20Matrix%20Theorem/","title":"Permutation Matrix Theorem","text":"<p>202308140236</p> <p>type : #Example tags : [[Algebraic Graph Theory]]</p>"},{"location":"Permutation%20Matrix%20Theorem/#permutation-matrix-theorem","title":"Permutation Matrix Theorem","text":"<p>let \\(\\alpha\\) be an automorphism of \\(G\\). Let \\(\\mathbb P=(P)_{ij}\\) be the permutation matrix corresponding to \\(\\alpha\\). that is \\((P)_{ij}=1\\iff j=\\alpha(i)\\)  Then  $$ \\begin{align} PAP^{t}&amp;= A\\ P^{t}&amp;= P^{-1}\\ (PAP^{t}){ij} &amp;= \\sum\\limits{r,s} (P){ir}(A){rs}(P){js}\\ &amp;= A{kl} \\end{align} $$ where \\(k=\\alpha(i), l=\\alpha(j)\\)</p>"},{"location":"Permutation%20Matrix%20Theorem/#related","title":"Related","text":""},{"location":"Phase%20Flow/","title":"Phase Flow","text":"<p>202302121202</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"Phase%20Flow/#phase-flow","title":"Phase Flow","text":"<p>let \\(U\\subseteq \\mathbb R\\) be the domain and \\(\\{g^t\\}\\) be a One Parameter Group Of Diffeomorphisms. Then the pair \\((U,\\{g^{t}\\})\\) is called a Phase Flow. For each Phase Flow* There exists an associated autonomous first order differential equation which is  $$ \\overline v(\\overline x):=\\frac{d}{dt}(g^{t}(\\overline x))\\bigg|_{t=0} $$ And by fixing \\(\\overline x_0\\), consider the function \\(\\varphi:I\\to U\\) defined as  $$ \\varphi(t)=g^t(\\overline x_0) $$ Which is the unique solution to the IVP with given phase flow \\((U, \\{g^{t}\\})\\) and phase velocity \\(\\overline v\\), and initial value \\(\\overline x_0\\).</p>"},{"location":"Phase%20Flow/#references","title":"References","text":"<p>Phase Space</p>"},{"location":"Phase%20Space/","title":"Phase Space","text":"<p>202301210101</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"Phase%20Space/#phase-space","title":"Phase Space","text":"<p>A Phase Space is the space of all possible states of a given system. Its Dimension is determined by the number of parameter that the space is dependent on. And it is useful to capture the idea of Determinism, Finite Dimentionality, and Differentiability of processes formally.</p> <p>For example Consider two cities joined by two roads, and 2 cars are moving of different roads in different directions simultaneously, then the phase space of the given system can be the following square   We can infer from this that no matter how the cars move, there will come a point where the cars will cross each other.</p> <p>This examples is not related to differential equations but the course of reasoning is on the right track. For example, the state of the processes in classical mechanics consisting of motion of \\(n\\) points can be described by a system of \\(6n\\) parameters which are \\(3\\) parameter for position and \\(3\\) for velocity for each point. A system of \\(n\\) rigid bodies can be described by a phase space of \\(12n\\) dimensions.</p> <p>The motion of the entire system is described by the motion of a point in the phase space. The velocity of the the point in the phase space is defined by the point itself. So each point can be assigned a vector which is called the Phase Velocity Vector. and the set of all phase velocity vector is called the Phase Velocity Vector Field. This field described the differential equation.</p> <p>The concept of phase space reduces the study of evolutionalry process to geometric process of finding a curve defined by a vector field.</p>"},{"location":"Phase%20Space/#related-problems","title":"Related Problems","text":""},{"location":"Phase%20Space/#references","title":"References","text":"<p>Why Differential Equations Integral Curves</p>"},{"location":"Power%20Series/","title":"Power Series","text":"<p>202301181401</p> <p>Type : #Note Tags : [[Complex Analysis]]</p>"},{"location":"Power%20Series/#power-series","title":"Power Series","text":"<p><pre><code>title:\nA **Power Series** with center $z_{0\\in}\\mathbb C$ is a series of functions of the from\n$$\n\\sum\\limits_{n\\ge0}a_{n}(z-z_0)^{n}\n$$\n</code></pre> Fourier Series is an example of power series in case of real functions Example: Geometric Series \\(\\(f(z)=\\sum\\limits_{n\\ge0}z^n\\)\\) We know that for all \\(z\\) such that \\(|z|&lt;1, f(z)\\) converges</p> <pre><code>title: Convention\nThe center of convergence is assumed to be $0$\n</code></pre> <p>Theorem: For every power series there is a radius of convergence, \\(R&gt;0, R\\in\\mathbb R\\) such that 1. For \\(|z|&lt;R,\\sum\\limits a_nz^n\\) converges absolutely and the series converges uniformly. 2. If \\(|z|&gt;R,\\) then \\(\\sum\\limits a_{n}z^{n}\\) is divergent. 3. \\(|z|&lt;R\\), then \\(f(z)=\\sum\\limits a_{n}z^{n}\\) is analytic, and the derivative is a power series whose terms are summation of derivative of terms of the power series. Further, the radius of convergence of the derivative of the power series is also \\(R\\). The circle \\(|z|=R\\) is called the Circle of Convergence. A formula for \\(R\\) by Hadamard is  $$ \\frac1R=\\limsup\\limits_{n\\to\\infty}\\sqrt[n]{|a_n|}  $$</p>"},{"location":"Power%20Series/#related-problems","title":"Related Problems","text":""},{"location":"Power%20Series/#references","title":"References","text":"<p>Analytic Function</p>"},{"location":"Preorder/","title":"Preorder","text":"<p>202301270401</p> <p>Type : #Note Tags :[[Category Theory]]</p>","tags":["Note"]},{"location":"Preorder/#preorder","title":"Preorder","text":"<pre><code>title:\nA **Preorder** relation on a set $X$ is a binary relation on $X$ denoted with $\\le$ such that\n- (A) $x\\le x$\n- (B) if $x\\le y$ and $y\\le z$ then $x\\le z$\n</code></pre> <p>Example: - Discrete Preorder \\((X, =)\\): \\(\\forall x\\in X, x \\le x\\) and if \\(y\\ne x\\) neither \\(x\\le y\\) nor \\(y\\le x\\) holds, essentially no points are comparable - Codiscrete preorder: \\(\\forall x, y\\in X, x\\le y\\) (hence also \\(y\\le x\\)) - The Booleans \\(\\mathbb B=\\{\\text{true, false}\\}\\) have a natural preorder which is \\(\\text{true}\\le\\text{false}\\) - Partial Orders are [[Skeletality|Skeletal]] Preorders - A graph can by used the represent a pre order where nodes represents elements of the sets and an edge from \\(a\\) to \\(b\\) is equivalent to \\(a \\le b\\) - Lexicographic Order : \\(AB \\sqsubseteq_{L} A AB \\sqsubseteq_{L} A A A B\\dots\\) - Prefix Order: \\(A B \\subseteq_{P}  A B A \\subseteq_{P} A B A A\\dots\\) - Subword Order: \\(\\text{HIGMAN}\\preceq \\text{HIGHMOUNTAIN}\\) </p>","tags":["Note"]},{"location":"Preorder/#references","title":"References","text":"<p>Well-Preorder</p>","tags":["Note"]},{"location":"Primitive%20Element%20Theorem/","title":"Primitive Element Theorem","text":"<p>202304142304</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Primitive%20Element%20Theorem/#primitive-element-theorem","title":"Primitive Element Theorem","text":""},{"location":"Primitive%20Element%20Theorem/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nEvery finite separable extension of a field $K$ is of the form $K(\\gamma)$ for some $\\gamma$ in the extension.\n</code></pre>"},{"location":"Primitive%20Element%20Theorem/#proof","title":"Proof:","text":"<p>If \\(K\\) is a finite field, then any finite extension \\(L\\) is a finite field and so, \\(L ^{\\times} = \\langle \\gamma \\rangle\\) for some generator \\(\\gamma \\in L ^{\\times}\\). This would give \\(L = K(\\gamma)\\). (Here we used the fact that \\(L ^{\\times}\\) is a cylic group for any finite field \\(L\\). For proof see Finite Fields.)</p> <p>Now when \\(K\\) is infinite, a finite separable extension \\(L\\) is of the form \\(K(\\alpha_{1},\\alpha_{2},\\dots,\\alpha_{n})\\) where each \\(\\alpha_{i}\\) is separable over \\(K\\). It suffices to show that when \\(K(\\alpha,\\beta)/ K\\) is separable then \\(K(\\alpha,\\beta) = K(\\gamma)\\) for some \\(\\gamma\\).</p> <p>Let \\(L = K(\\alpha,\\beta)\\) and \\(n = [L:K]\\), by Theorem 1 (c) in Separable Extensions, we get that there are \\(n\\) \\(K-\\)homomorphisms from \\(L\\) to a field extension \\(F\\) of \\(K\\). Now we show that \\(L = K(\\alpha + c\\beta)\\) for some \\(c\\), in fact we will show that this is true for all but finitely many \\(c\\). Take \\(K(\\alpha+c\\beta)\\), if it is not equal to \\(L\\), then \\([K(\\alpha+c\\beta) : K] &lt; [L:K]\\), and hence it has less than \\(n\\) \\(K-\\)homomorphisms from itself to \\(F\\). This means that two of the homomorphisms \\(\\tau,\\sigma\\) on \\(L\\) are the same on \\(K(\\alpha+c\\beta)\\).  This is equivalent to \\(\\sigma(\\alpha+c\\beta) = \\tau(\\alpha+c\\beta) \\implies c =  \\frac{\\sigma(\\alpha) - \\tau(\\alpha)}{\\tau(\\beta)-\\sigma(\\beta)}\\). Since there are only finitely many \\(\\sigma,\\tau\\) pairs, there are only finitely many such \\(c\\). This means for all the other \\(c\\)'s, we have \\(L = K(\\alpha+c\\beta)\\).</p>"},{"location":"Primitive%20Element%20Theorem/#we-can-try-finding-primitive-elements-for-galois-extensions","title":"We can try finding primitive elements for galois extensions.","text":""},{"location":"Primitive%20Element%20Theorem/#theorem-2","title":"Theorem 2:","text":"<pre><code>title:\nWhen $L /K$ is a finite galois extension and $\\gamma \\in L$, the degree $[K(\\gamma):K]$ is the size of the galois orbit of $\\gamma$. So $\\gamma$ is a primitive element of $L /K$ iff $|\\{ \\sigma(\\gamma) : \\sigma \\in \\mathrm{Gal}(L /K) \\}| = [L :K]$.\n</code></pre>"},{"location":"Primitive%20Element%20Theorem/#proof_1","title":"Proof:","text":"<p>Since \\(\\gamma\\) is separable over \\(K\\), \\([K(\\gamma) : K]\\) is the number of roots of the min poly of \\(\\gamma\\) over \\(K\\), which is just the conjugates of \\(\\gamma\\) which in turn is just the size of the galois orbit (Theorem 4 Galois Correspondence)</p>"},{"location":"Primitive%20Element%20Theorem/#references","title":"References","text":"<p>[[Field Extensions]] Separable Extensions Finite Fields Galois Extensions Galois Correspondence</p>"},{"location":"Primitive%20Recursion/","title":"Primitive Recursion","text":"<p>202304011804</p> <p>Type : #Note Tags : [[Lambda Calculus]]</p>"},{"location":"Primitive%20Recursion/#primitive-recursion","title":"Primitive Recursion","text":"<p>\\(f:\\mathbb N^{k+1} \\to \\mathbb N\\) is obtained by Primitive Recursion from the total functions \\(g:\\mathbb N^{k}\\to \\mathbb N\\) and \\(h:\\mathbb N^{k+2}\\to \\mathbb N\\) if  $$ \\begin{align} f(0,\\vec n)&amp;= g(\\vec n)\\ f(i+1, \\vec n)&amp;= h(i,f(i,\\vec n), n) \\end{align} $$</p> <p>which is equivalent to the for loop <pre><code>result = g(n1, n2, n3 ... nk) // f(0,n)=g(n)\nfor i in 1..n:\n    result = h(i, result, n1, n2, n3 .. nk) //recursion\nreturn result\n</code></pre></p> <p>Example of a non trivial function which can use primitive recursion in its definition is</p> <p>The above <code>for loop</code> and recursion can sort be expressed in lambda calculus in the following way which isn't correct but can be fixed and gets the idea across</p> <p>Given two lambda expressions for functions \\(h\\) and \\(g\\), we need to construct a lambda expression for a function which is defined in the above manner</p> <p>$$ f(i,n). \\text{if \\(i=0\\) then \\(g(n_1,n_{2} \\dots n_{k})\\) else h(pred(i), f(pred(i)), n )} $$ here we need to make sure we hvae the ability to do a a few things - We need to have booleans, which we do - We need to be able to have if conditions, which we do because of out defintion of booleans, its easy to model that - We need to be able to do recursion....</p> <p>To make it clear how we approach the third one, lets make the expression, closer to lambda calculus,  $$ F:= \\lambda i n_{1}\\dots n_{k}.  \\text{IsZero}\\quad i\\quad      (G\\; n_{1}\\dots n_{k})\\quad      (H\\;          (\\text{Pred}\\quad i)          (F\\quad (\\text{Pred}\\quad i)\\quad n)         n)   $$ The only thing not fixed is that \\(F\\) is used inside its definition. to make things easier, lets pull out \\(F\\) </p> <p>$$ F:= \\lambda f i n_{1}\\dots n_{k}.  (\\text{IsZero}\\quad i\\quad      (G\\; n_{1}\\dots n_{k})\\quad      (H\\;          (\\text{Pred}\\quad i)          (f\\quad (\\text{Pred}\\quad i)\\quad n)         n)) \\quad  F $$ now the part before \\(F\\) has become a perfectly valid lambda expression so we can rewite it as </p> \\[ F:= CF \\]"},{"location":"Primitive%20Recursion/#references","title":"References","text":"<p>Functions Computable in Lambda Calculus</p>"},{"location":"Primitive%20Transitive%20Permutation%20Group/","title":"Primitive Transitive Permutation Group","text":"<p>202308181008</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Primitive%20Transitive%20Permutation%20Group/#primitive-transitive-permutation-group","title":"Primitive Transitive Permutation Group","text":"<pre><code>title:\nIf the only blocks of Transitive permutation group are trivial, then it called **Primitive**.\n</code></pre> <p>Lemma: If \\((\\Gamma,X)\\) is \\(2-\\)transitive, then it is primitive Proof: consider a block \\(B\\), if it is non trivial, then \\(\\exists x, y\\in B\\) and \\(\\exists z\\notin B\\), then consider \\(\\sigma: (x, y)\\to (x, z)\\). This is a contradiction.</p>"},{"location":"Primitive%20Transitive%20Permutation%20Group/#theorem","title":"Theorem","text":"<p>Let \\((\\Gamma, X)\\) be transitive, then \\((\\Gamma, X)\\) is primitive iff \\(\\forall x\\in X,\\Gamma_{x}\\) is a maximal subgroup of \\(\\Gamma\\).</p>"},{"location":"Primitive%20Transitive%20Permutation%20Group/#proof","title":"Proof","text":"<p>Show that the following are equivalent 1. \\(\\Gamma_{x}\\) is not minimal. 2. \\(\\exists\\) a non trivial block (1. \\(\\implies\\) 2.) \\(\\exists H\\) such that \\(\\Gamma_{x}\\subsetneq H\\subsetneq\\Gamma\\)  And let \\(B\\) be the orbit of \\(H\\) which contains \\(x\\) If \\(B\\) is a singleton, then \\(\\Gamma_{x}=H\\) If \\(B=X\\)  then consider some \\(\\alpha\\in\\Gamma\\) such that \\(\\alpha(x)=z\\), but since \\(z\\) is in the orbit of \\(H\\), we have \\(\\beta\\) such that \\(\\beta(x)=z\\), and we have \\(\\beta^{-1}\\circ\\alpha(x)=x\\in H\\) hence \\(\\alpha\\in H\\) which implies \\(H=\\gamma\\) which is also a contradiction.</p> <p>(2. \\(\\implies\\) 1.)</p> <p>\\(\\Gamma_{B}=\\{\\sigma\\in\\Gamma:\\sigma(B)=B\\}\\) then let \\(H=\\Gamma_{B}\\). then \\(\\Gamma_{x}\\subseteq H\\subseteq \\Gamma\\). We show that \\(H\\ne \\Gamma_{x}\\) and \\(H\\ne\\Gamma\\) \\(H\\ne\\Gamma_{x}\\) : consider \\(y\\in B\\ne H\\). then \\(y\\in B\\cap\\sigma (B)\\) then \\(\\sigma\\in H\\). such a \\(\\sigma\\) exists because of transitivity.</p> <p>Suppose \\(H=\\Gamma\\) proof equivalent to the proof in the first part.</p>"},{"location":"Primitive%20Transitive%20Permutation%20Group/#references","title":"References","text":""},{"location":"Probabilistic%20Semantics%20for%20LTL/","title":"Probabilistic Semantics for LTL","text":"<p>202311172111</p> <p>Tags : Timed Automata, [[Probability]]</p>","tags":["Note","Incomplete"]},{"location":"Probabilistic%20Semantics%20for%20LTL/#probabilistic-semantics-for-linear-temporal-logic","title":"Probabilistic Semantics for Linear Temporal Logic","text":"<p>The Interpretation of LTL formulae happen over finite runs of a Timed Automata. The states in a run correspond to worlds used to define the semantics for LTL. And we use the function \\(\\mathcal L\\) in place of \\(\\sigma\\) to give the set of formulas satisfied by \\(s\\).</p> <p>[!note] Intuition Since only the sequence of states corresponds to the Kripke like structures used to give semantics to LTL, we can say that given a formula \\(\\varphi\\) and a path \\(\\pi\\), either all runs of \\(\\pi\\) model satisfy \\(\\varphi\\) or none of them do. Hence we can give a set of symbolic paths that satisfy the formula \\(\\varphi\\) and let the probability that \\(\\varphi\\) is true in the automaton be the probability that the set of paths can be taken.</p> <p>more formally $$ \\mathbb P_{\\mathcal A}(s_{0},\\varphi)=\\mathbb P_{\\mathcal A} {\\varrho\\in \\text{Runs}(\\mathcal A,s_{0})\\; :\\;\\varrho \\models\\mathcal\\varphi} $$ and we say that \\(\\mathcal A\\) almost surely satisfies \\(\\varphi\\) from \\(s_0\\) w.r.t \\(\\mathbb P_\\mathcal A\\), and write \\(\\mathcal A,s_0 \\mid\\!\\approx_{\\mathbb P} \\varphi\\) iff \\(\\mathbb P_\\mathcal A(s_0,\\varphi)=1\\).</p> <p>Out Timed Automata Does not have a Final State, but it is easy to deal with it. We add another \\(\\text{acc}\\) to the set of atomic propositions and assign it to each condition we want and define \\(\\psi = \\diamond \\square \\text{acc}\\). and instead of \\(\\mathcal A,s_0 \\mid\\!\\approx_{\\mathbb P} \\varphi\\), we write \\(\\mathcal A,s_0 \\mid\\!\\approx_{\\mathbb P} \\varphi\\mid\\psi\\)</p> <p>And from Lemma B we directly get that  $$ \\mathcal A,s_0 \\mid!\\approx_{\\mathbb P} \\varphi \\iff \\textbf{R}{\\mathcal A},\\iota (s{0}) \\mid!\\approx_{\\mathbb P}\\varphi $$</p>","tags":["Note","Incomplete"]},{"location":"Probabilistic%20Semantics%20for%20LTL/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/","title":"Scientific Data Science","text":"<p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#almost-certain-model-chekcing","title":"Almost Certain Model Chekcing","text":"<p> Shubh Sharma  <p>NOVEMBER, 2023  </p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#outline","title":"Outline","text":"<ol> <li>Introduction</li> <li>Linear Temporal Logic</li> <li>Timed Automata</li> <li>Probabilistic Semantics</li> <li>Topological Semantics</li> <li>Correspondence of the two Semantics</li> <li>Model Checking Hardness</li> </ol>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#introduction","title":"Introduction","text":""},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#timed-automata-are-really-nice","title":"Timed Automata are really nice (=","text":"<p>They let us describe systems with real-time constraints but still satisfy a lot of nice properties like reachability, safety properties, etc ,etc.</p> <p></p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#but","title":"But","text":""},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#timed-automata-are-too-nice","title":"Timed Automata are too nice )=","text":"<p>Like most mathematical objects, they several assumptions made about them like infinite precision, instantaneous events which are unrealistic</p> <p>So we discuss two semantics for Linear Time Logic using timed automata that let us model check ignoring cases that are too unlikely/extreme a topological and a probabilistic one.</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#linear-temporal-logic","title":"Linear Temporal Logic","text":"<p>Temporal Logics introduce operators that describe how the world changes without explicitly referring to time. This helps us guarantee properties like - Safety: Anything bad will not happen - Liveness: Something good will happen - Fairness: Evolution of subsystems</p> <p>-- Linear Temporal Logic introduces operators with the assumption that there is one execution path in time. That is, we sort of know what how the evolution of the system will be.</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#syntax","title":"Syntax","text":"<p>We have a set of Atomic proposition \\(\\mathcal P\\) and the formulae satisfy the following grammar. { width=\"700\" }</p> <p>-- where the 4 temporal operators are - \\(\\bigcirc f\\;\\;\\) :  \\(f\\) is true in the next state - \\(\\square f\\;\\;\\;\\) : \\(f\\) will be true in all future states  - \\(\\diamond f\\quad\\) : \\(f\\) is true in some future state - \\(f\\cup g\\)  : \\(g\\) will be true in some state in the future, \\(f\\) is true in each state until then  </p> <p>And \\(p\\in\\mathcal P\\)</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#semantics","title":"Semantics","text":"<p>The idea is to model a world, that evolves with time, and at every evolution(discrete) some statements are true. We do logic at a particular time in the world and we need to make relations between statements across time.</p> <p>To give the semantics, we define a satisfactory relation \\(\\models\\) of the type \\((\\mathbb N\\to 2^\\mathcal P)\\times\\mathbb N\\times LTL\\).</p> <p>We define the boolean operators in the usual manner, for the rest of the operators</p> <p>--</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#propositions","title":"Propositions","text":"<p>$$ \\ (\\sigma, j)\\models p\\quad\\text{iff}\\quad p \\in\\sigma(j) $$ </p> <p>--</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#next","title":"Next","text":"<p>$$ (\\sigma, j)\\models \\bigcirc p\\quad\\text{iff}\\quad (\\sigma,j+1) \\models p $$ </p> <p>--</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#henceforth","title":"Henceforth","text":"<p>$$ (\\sigma, j)\\models \\square p\\quad\\text{iff}\\quad \\forall k\\cdot(k \\geq j \\Rightarrow (\\sigma,k)\\models p) $$ </p> <p>--</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#until","title":"Until","text":"<p>$$ (\\sigma, j)\\models f\\cup g\\quad\\text{iff}\\quad \\exists k \\cdot(A \\land B) $$ where  - \\(A = k\\ge j \\Rightarrow (\\sigma,k)\\models g\\)      - \\(g\\) will happen - \\(B=\\forall i \\cdot(j \\leq i &lt; k \\Rightarrow (\\sigma, i) \\models f)\\)     - \\(f\\) keeps happening \\(g\\)</p> <p></p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#timed-automata","title":"Timed Automata","text":"<p>Now we can formally define Timed Automata </p> <p>We define a timed automaton \\(\\mathcal A\\) as the tuple  $$ \\mathcal A = \\langle L, X, E, \\mathcal I, \\mathcal L\\rangle $$</p> <p>-- $$ \\mathcal A = \\langle L, X, E, \\mathcal I, \\mathcal L\\rangle $$ - \\(L\\) : Location / States - \\(X\\) : Clocks - \\(E \\subseteq L \\times \\mathcal G(X) \\times \\mathcal 2^{X} \\times L\\) : Edges     - State + Guard + Resets -&gt; State - \\(\\mathcal I : L\\to \\mathcal G(X)\\) : Invariant  - \\(\\mathcal L : L\\to\\mathcal 2^{\\text{AP}}\\)     - \\(\\text{AP}\\) is a finite set of atomic propositions     - Assignment of a logical proposition to each location.</p> <p>-- Semantics of a Timed Automata \\(\\mathcal A\\) is given by a labelled Transition system \\(T_{\\mathcal A}\\) $$ T_{\\mathcal A} = \\langle S, E \\sqcup \\mathbb R_{+}, \\rightarrow \\rangle $$</p> <ul> <li>\\(S\\) is a set of states/symbolic states \\(\\{s=(l, \\nu) \\in L \\times\\mathbb R^{X}_{+}\\;:\\;\\nu\\vDash \\mathcal I(l)\\}\\)<ul> <li>The clock valuations need to respect the state invariant</li> </ul> </li> <li>We gave 2 kinds of transtions<ul> <li>Discrete transitions </li> <li>Delay transitions</li> <li>And in both cases state invariants should be respected at all times</li> </ul> </li> </ul> <p>-- { width=\"700\" } \\(\\mathcal A\\) is said to be non-blocking if \\(I(s)\\ne\\emptyset\\) for all \\(s\\).</p> <p>We define a symbolic path as a state, followed by a sequence of edges, such that the delay transitions follow a certain constraint { width=\"700\" } --</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#example","title":"Example","text":""},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#region-automata","title":"Region Automata","text":"<p>Here, we will modify the region automaton to be a timed automaton. The goal is to use this, as a more well behaved and easy to work version of the original automaton, that satisfies some strong properties which will be discussed later.</p> <p>-- Let \\(\\mathcal R_\\mathcal A\\) be the set of regions in \\(\\mathcal A\\), then the region automata \\(\\text{R}_\\mathcal A\\) is \\(\\langle Q, X, T, \\kappa,\\lambda\\rangle\\) such that - \\(Q=L\\times \\mathcal R_\\mathcal A\\) - \\(\\kappa((l,\\tau))=\\mathcal I(l)\\) - \\(\\lambda((l,r))=\\mathcal L(l)\\) - \\(X\\) is the same set of clocks - \\(T\\subseteq Q\\times \\text{cell}(R_\\mathcal A) \\times E\\times \\mathcal 2^X\\times Q\\), so \\((l,r)\\xrightarrow{\\text{cell}(r''),e,Y}(l',r')\\) exists if</p> <p>we can recover the usual region automata by getting rid of time constraints and clocks.</p> <p>--</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#important-property","title":"Important Property","text":"<p>Path correspondence: every finite path \\(\\pi((l,\\nu),e_{1}\\dots e_{n})\\) in \\(\\mathcal A\\) has a finite set of corresponding paths \\(\\pi(((l,[\\nu]),\\nu),f_{1}\\dots f_{n})\\) in \\(\\text{R}_\\mathcal A\\). Each path in the set represents the choice of regions and delays taken while traversing the above path in \\(\\mathcal A\\)</p> <p>Also, if \\(A\\) is non-blocking, so is \\(\\text R_\\mathcal A\\)</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#probabilistic-semantics","title":"Probabilistic Semantics","text":"<p>We will now build some machinery to give probabilistic semantics for LTL formulae</p> <p>We define a probability measure for \\(I(s)\\) and a probability measure on the set of enabled transitions. Extending this will give a probability measure on runs.</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#probability-measure-on-is-and-edges","title":"Probability measure on \\(I(s)\\) and edges","text":"<p>Our probability measure \\(\\pi_{s}\\) need to satisfy the following propeties - \\(\\pi_s(I(s))=1\\)  - given the Lebesgue measure \\(\\lambda\\) if \\(I(s)\\) is not finite, \\(\\mu_s\\) should be equivalent to \\(\\lambda\\), otherwise it is uniform over the points - Given any \\(s\\), we also define a probability measure \\(p_s\\) on the set of enabled edges</p> <p>--</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#example_1","title":"Example","text":"<ul> <li>Uniform Distribution over \\(I(s)\\) if it is finite.</li> <li>Lebesgue Measure normalised to have a probability measure if \\(I(s)\\) is a finite set of bounded intervals.</li> <li>Exponential distribution if \\(I(s)\\) contains an unbounded interval.</li> </ul> <p>The exact measure does note matter as the property we desire are qualitative.</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#probability-measure-on-finite-paths","title":"Probability measure on Finite Paths","text":"<p>Given a finite path \\(\\pi(s,e: es)\\) , we can define the probability measure on the path as </p> \\[ \\mathbb P_\\mathcal A(\\pi(s,e:es)) = \\frac{1}{2}\\int_{t\\in I(s,e_{1})} p_{s+t}(e)\\mathbb P_{\\mathcal A}(\\pi(s',es))  d\\mu_{s}(t) \\]"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#this-gives-a-probability-distribution-on-textrunsmathcal-as","title":"This gives a probability distribution on \\(\\text{Runs}(\\mathcal A,s)\\)","text":""},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#sanity-check","title":"Sanity Check!","text":"<ul> <li>Ignore the \\(\\frac{1}{2}\\) for now</li> <li>Its the probability of that \\(e_1\\) is picked and the rest of the path can be traversed, given that \\(t\\) time has elapsed.</li> <li>Integrate that over all \\(t\\in I(s, e_1)\\)</li> <li>For this situation, summing the probabilities of all paths of length \\(n\\) is \\(1\\)</li> <li>\\(\\frac{1}{2}\\) is just the normalisation factor such that now the total probability of a path of any length is \\(1\\)</li> </ul> <p>--</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#example_2","title":"Example","text":""},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#similar-measures-on-mathcal-a-and-textr_mathcal-a","title":"Similar measures on \\(\\mathcal A\\) and \\(\\text{R}_\\mathcal A\\)","text":"<p>We assume that for every state \\(\\mu_s^\\mathcal A = \\mu_{\\iota(s)}^{\\textbf R_\\mathcal A}\\)  this is possible because we have that \\(I(s)=I(\\iota(s))\\)  If \\(p^\\mathcal A\\) is distributed over edges in \\(\\mathcal A\\), we assume that for every state \\(s\\) in \\(\\mathcal A\\) and \\(t\\in \\mathbb R_+\\) \\(p_{s+t}^\\mathcal A=p_{\\iota(s)+t}^{\\textbf R_\\mathcal A}\\) </p> <p>This also gives us that probability of a path in \\(\\mathcal A\\) is the same as probability of its projection in \\(\\text R_\\mathcal A\\).</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#probabilistic-semantics-for-ltl","title":"Probabilistic Semantics for LTL","text":"<p>We consider runs of a timed automaton as sequences of worlds over which we give semantics for our LTL formulae.</p> <p>Here we see \\(\\mathcal L\\) begin used for the very first time. This is like the function which assigns worlds atomic proposition.</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#probability-on-ltl-formulae","title":"Probability on LTL Formulae","text":"<p>Since satisfiability only depends on states, either all, or none of the runs in a path satisfy the formula. Hence we give probability to an LTL formula \\(\\varphi\\) from \\(s\\) as </p> <p>{ width=\"600\" } Which is the total probability of all paths that satisfy \\(\\varphi\\).</p>"},{"location":"Probabilistic%20and%20Topological%20Semantics%20of%20Timed%20Automata%20are%20Equivalent/#satisfying-ltl-formulae","title":"Satisfying LTL formulae","text":"<p>We say that \\(\\mathcal A\\) almost surely satisfies \\(\\varphi\\) from \\(s_0\\) wrt \\(\\mathbb P_{\\mathcal A}\\) and write \\(\\mathcal A,s_0 \\mid\\!\\approx_{\\mathbb P} \\varphi\\) iff \\(\\mathbb P_\\mathcal A(s_0,\\varphi)=1\\).</p> <p>And because of the earlier claim we have.</p> <p>{ width=\"500\" }</p>"},{"location":"Probabilistically%20Checkable%20Proof/","title":"Probabilistically Checkable Proof","text":"<p>202311201611</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note","Incomplete"]},{"location":"Probabilistically%20Checkable%20Proof/#probabilistically-checkable-proof","title":"Probabilistically Checkable Proof","text":"<p>\\(x:\\) \\(n-\\)bit input \\(y:\\) certificate \\(c:\\) completeness \\(s:\\) soundness</p> <p>Verifier \\(V\\) uses \\(r(n)\\) random bits, reads \\(q(n)\\) bits of \\(y\\).</p> <p>If \\(x\\) is a 'yes' instance, then for some \\(y\\), \\(V\\) accepts \\(x\\) with probability \\(\\geq c\\). If \\(x\\) is a 'no' instance, then for any proof \\(y\\), \\(V\\) accepts \\(x\\) with probability \\(\\leq s&lt;c\\).</p> <pre><code>**PCP theorem:** $NP=PCP_{1, \\frac{1}{2}}(O(\\log n),k)$ for some constant $k$.\n</code></pre> <p>\\(PCP_{c,s}(r(n),q(n))\\)</p> <p>Hastad's PCP:  For any \\(\\epsilon,\\delta&gt;0\\), \\(NP=PCP_{1-\\epsilon, \\frac{1}{2}+\\delta}(O(\\log n),3)\\). Moreover, the verifier is restricted to use only the functions \\(odd(x,y,z)\\) or \\(even(x,y,z)\\).</p> <p>Input: \\(x\\), \\(|x|=n\\) bits Verifier picks a random string \\(r\\), \\(|r|=c\\log n\\) bits There are \\(n^{c}\\) such possible strings.</p> <p>If \\(x\\) is a \"yes\" instance, V accepts \\(x\\) with probability \\(\\geq 1-\\epsilon\\), i.e. accepts on \\(\\geq(1-\\epsilon)n^{c}\\) possible settings of \\(r\\). \\(\\geq(1-\\epsilon)n^{c}\\) constraints in the CSP instance are satisfiable</p> <p>If \\(x\\) is a \"no\" instance, by PCP thm, V accepts \\(x\\) with probability \\(\\leq \\frac{1}{2}+\\delta\\) on any \\(y\\), i.e. \\(\\leq\\left( \\frac{1}{2}+\\delta \\right)n^{c}\\) constraints are simultaneously satisfiable.</p> <p>--</p> <p>Say we have an \\(\\alpha-\\)approximation algorithm for MAX-CSP (number of constraints \\(=n^{c}\\)). If the instance is a \"yes\" instance, i.e. if \\(\\geq(1-\\epsilon)n^{c}\\) are satisfiable, then our algorithm satisfies \\(\\geq\\alpha(1-\\epsilon)n^{c}\\) constraints. If \"no\", then our algorithm satisfies \\(\\leq\\left( \\frac{1}{2}+\\delta \\right)n^{c}\\) constraints.</p> <p>If \\(\\alpha(1-\\epsilon)m&gt;\\left( \\frac{1}{2}+\\delta \\right)m\\) then we can solve any problem in NP in polytime, \\(\\implies P=NP\\).</p> <p>Consider \\(\\epsilon,\\delta\\to_{0}\\).</p> <p>[!check] Thus MAX-CSP (on odd/even constraints on 3 bits) has no approximation \\(&gt; \\frac{1}{2}\\) unless \\(P=NP\\).</p>","tags":["Note","Incomplete"]},{"location":"Probabilistically%20Checkable%20Proof/#reduction-from-max-3csp-to-max-3sat","title":"Reduction from Max-3CSP to Max-3SAT","text":"<p>For each of the even and odd functions, we construct four clauses each which have the following property: If the function evaluates to 1, all 4 clauses are satisfied, otherwise only 3 clauses are satisfied.</p> \\[ odd(x_{i},x_{j},x_{k})\\mapsto\\left\\{ \\begin{align*} (x_{1}\\lor x_{j}\\lor x_{k})\\\\ (\\bar{x}_{1}\\lor \\bar{x}_{j}\\lor x_{k})\\\\ (\\bar{x}_{1}\\lor x_{j}\\lor \\bar{x}_{k})\\\\ (x_{1}\\lor \\bar{x}_{j}\\lor \\bar{x}_{k})\\\\ \\end{align*} \\right. \\] \\[ even(x_{i},x_{j},x_{k})\\mapsto\\left\\{ \\begin{align*} (x_{1}\\lor x_{j}\\lor x_{k})\\\\ (x_{1}\\lor x_{j}\\lor x_{k})\\\\ (x_{1}\\lor x_{j}\\lor x_{k})\\\\ (x_{1}\\lor x_{j}\\lor x_{k})\\\\ \\end{align*} \\right. \\] <p>So, a MAX-3CSP instance, \\(\\psi\\) with \\(m\\) constraints \\(\\mapsto\\) a MAX-3SAT instance, \\(\\phi\\) with \\(4m\\) clauses. If there exists an assignment that satisfies \\(k\\) constraints, then it satisfies \\(4k+3(m-k)\\) clauses.</p> <p>\\(\\psi\\) is a yes instance \\(\\implies \\geq(1-\\epsilon)m\\) constraints are satisfiable \\(\\implies\\geq 4(1-\\epsilon)m+3\\epsilon m=(4-\\epsilon)m\\) clauses of \\(\\phi\\) are satisfiable.</p> <p>\\(\\psi\\) is a no instance \\(\\implies\\leq\\left( \\frac{1}{2}+\\delta \\right)m\\) constraints are satisfiable \\(\\implies\\leq 4\\left( \\frac{1}{2}+\\delta \\right)m+3\\left( \\frac{1}{2}-\\delta \\right)m=\\left( \\frac{7}{2}+\\delta \\right)m\\) clauses of \\(\\phi\\) are satisfiable.</p> <p>Unless \\(P=NP\\), there is no \\(\\alpha-\\)approximation for MAX-3SAT, where \\(\\alpha\\) satisfies \\((4-\\epsilon)m\\alpha&gt;\\left( \\frac{7}{2}+\\delta \\right)m\\), or \\(\\alpha&gt; \\dfrac{\\frac{7}{2}+\\delta}{4-\\epsilon}\\approx \\frac{7}{8}+\\frac{\\delta}{4}\\).</p> <p>MAX-3SAT cannot be approximated to \\(&gt; \\frac{7}{8}\\).</p>","tags":["Note","Incomplete"]},{"location":"Probabilistically%20Checkable%20Proof/#references","title":"References","text":"<p>[Williamson-Shmoys]</p>","tags":["Note","Incomplete"]},{"location":"Probability%20Measure%20over%20Finite%20Paths%20in%20a%20Timed%20Automata/","title":"Probability Measure over Finite Paths in a Timed Automata","text":"<p>202311162211</p> <p>Tags : Timed Automata, [[Probability]]</p>","tags":["Note","Incomplete"]},{"location":"Probability%20Measure%20over%20Finite%20Paths%20in%20a%20Timed%20Automata/#probability-measure-over-finite-paths-in-a-timed-automata","title":"Probability Measure over Finite Paths in a Timed Automata","text":"<p>Let \\(\\mathcal A\\) be a Timed Automaton and let \\(\\pi(s, e_1, e_2\\dots e_n)\\) be a symbolic path to be fired. Then we define a probability measure on the sequence of transitions as follows.</p> <p>$$ \\mathbb P_{\\mathcal A}(\\pi(s,e_{1}\\dots e_{n})) =  \\frac{1}{2} \\int_{t\\in I(s, e_{1})} \\,      p_{s+t}(e_{1})     \\mathbb P_{\\mathcal A}(\\pi(s_{t},e_{2}\\dots e_{n}))  d\\mu_{s}(t)  $$ where \\(s\\xrightarrow{t,e_{1}}s_t\\) . We initialise \\(\\mathbb P_\\mathcal A(\\pi(s))=\\frac{1}{2}\\) </p> <p>[!attention] Don't get intimidated by the formula (Sanity Check) Ignore the \\(\\frac{1}{2}\\) for now Its the probability of that \\(e_1\\) is picked and the rest of the path can be traversed, given that \\(t\\) time has elapsed. Integrate that over all \\(t\\in I(s, e_1)\\) For this situation, summing the probabilities of all paths of length \\(n\\) is \\(1\\) \\(\\frac{1}{2}\\) is just the normalisation factor such that now the total probability of a path of any length is \\(1\\)</p> <p>[!example] </p> <p>The above formula gives us </p> <p>[!note] Lemma A Statement: For every state \\(s\\), \\(\\mathbb P_{\\mathcal A}\\) is a probability measure over \\(\\text{Runs}(\\mathbb A, s)\\) Proof Sketch: Induction  - Base case: 0 len paths - For each \\(e\\), consider paths that start with \\(e\\). Probability that any of such paths will be taken is probability \\(e\\) will be taken - Sum up over all \\(e\\) to get 1</p>","tags":["Note","Incomplete"]},{"location":"Probability%20Measure%20over%20Finite%20Paths%20in%20a%20Timed%20Automata/#similar-measures-on-mathcal-a-and-textbfr_mathcal-a","title":"Similar Measures on \\(\\mathcal A\\) and \\(\\textbf{R}_\\mathcal A\\)","text":"<p>We assume that for every state \\(\\mu_s^\\mathcal A = \\mu_{\\iota(s)}^{\\textbf R_\\mathcal A}\\)  this is possible because we have that \\(I(s)=I(\\iota(s))\\)  If \\(p^\\mathcal A\\) is distributed over edges in \\(\\mathcal A\\), we assume that for every state \\(s\\) in \\(\\mathcal A\\) and \\(t\\in \\mathbb R_+\\) \\(p_{s+t}^\\mathcal A=p_{\\iota(s)+t}^{\\textbf R_\\mathcal A}\\) </p> <p>so we get</p> <p>[!note] Lemma B Statement: Let \\(\\mathcal A\\) be a non-blocking Timed Automata. If measure in \\(\\mathcal A\\) and in \\(\\textbf R_\\mathcal A\\) are similar as explained above and \\(\\pi\\)  be  a path in \\(\\mathcal A\\). Then \\(\\iota(\\pi)\\) is a \\(\\mathbb P_{\\textbf R_\\mathcal A}\\) measurable set  of runs in \\(R_\\mathcal A\\) Proof Sketch: Consider a path in \\(\\mathcal A\\), this corresponds to a set of paths in \\(\\textbf R_\\mathcal A\\), Then restrict the path in \\(\\mathcal A\\) such that it corresponds to a single path in the region automaton.  Find the measure of that, and these will be equivalent </p>","tags":["Note","Incomplete"]},{"location":"Probability%20Measure%20over%20Finite%20Paths%20in%20a%20Timed%20Automata/#references","title":"References","text":"<p>Machinery for Probabilistic Semantics for Timed Automata Timed Automata Alternate Definition Region Automata Alternate Definition</p>","tags":["Note","Incomplete"]},{"location":"Product%20Constructor%20Pattern%20Matching%20to%20Lambda%20Calculus/","title":"Product Constructor Pattern Matching to Lambda Calculus","text":"<p>202311041811</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note","Incomplete"]},{"location":"Product%20Constructor%20Pattern%20Matching%20to%20Lambda%20Calculus/#product-constructor-pattern-matching-to-lambda-calculus","title":"Product Constructor Pattern Matching to Lambda Calculus","text":"<p>This pattern checks if the given input is some product constructor of arity \\(r\\) and is written as</p> \\[ (\\lambda t\\ p_{1}\\ p_{2}\\dots p_{r}) \\] <p>The semantics of the Product Constructor pattern is </p> <p>To implement this, we create a built in function \\(\\text{UNPACK-PRODUCT-t}\\) and use it in the transformation</p> <p>$$ (\\lambda(t p_{1} p_{2} \\dots p_{r}).E)\\quad\\equiv\\quad  \\text{UNPACK-PRODUCT-t}\\; (\\lambda p_{1} p_{2}\\dots p_{r}.E) $$ And the inbuilt function is defined in the following way</p> \\[ \\text{UNPACK-PRODUCT-t}\\; f\\; a\\quad\\equiv\\quad f\\; (\\text{SEL-t-1}\\; a)\\dots(\\text{SEL-t-r}\\; a) \\] <p>[!example] Consider the following Haskell program <pre><code>add_pair (x, y) = x + y\n</code></pre> which gets converted to  <pre><code>add_pair = \\(PAIR x y). + x y\n</code></pre> which will be converted to  <pre><code>add_pair = UNPACK-PRODUCT-PAIR (\\x y. + x y)\n</code></pre></p>","tags":["Note","Incomplete"]},{"location":"Product%20Constructor%20Pattern%20Matching%20to%20Lambda%20Calculus/#references","title":"References","text":"<p>Patterns</p>","tags":["Note","Incomplete"]},{"location":"Product%20Type/","title":"Product Type","text":"<p>202307250007</p> <p>Type : #Note Tags : [[Type Theory]]</p>"},{"location":"Product%20Type/#product-type","title":"Product Type","text":"<p>A term of a product type is reducible iff its projections are 1. CR1 Suppose \\(t\\) of type \\(U\\times V\\), is reducible, then \\(\\pi^{1}t\\) is reducible and by induction hypothesis for \\(U\\), \\(\\pi^{1}t\\) is strongly normalizable. Moreover \\(\\nu(t)\\le\\nu(\\pi^{1}t)\\) since to any reduction sequence \\(t,t_{1},t_{2}\\dots\\) we can apply \\(\\pi_{1}\\) such that the last term of the sequence corresponds to a term where \\(\\pi_{1}\\) is not reduced. 2. CR2 If \\(t\\rightsquigarrow t'\\) then its projections too. As \\(t\\) is reducible by hypothesis, so are \\(\\pi^{1}t\\) and \\(\\pi^{2}t\\) and the induction hypothesis says \\(t'\\) is also reducible. 3. CR3 Let \\(t\\) be neutral and suppose all the \\(t'\\) one step from \\(t\\) are reducible, applying a conversion inside \\(\\pi^{1}t\\) results in \\(\\pi^{1}t'\\), since \\(\\pi^{1}t\\) cannot itself be a redex (\\(t\\) is not a pair), and \\(\\pi^{1}t'\\) is reducible since \\(t'\\) is. But all one step reductions of \\(\\pi^{1}t\\) are reducible, hence \\(\\pi^{1}t\\) is reducible(by induction).</p>"},{"location":"Product%20Type/#references","title":"References","text":"<p>Arrow Type Atomic Types</p>"},{"location":"Product%20in%20Category%20Theory/","title":"Product in Category Theory","text":"<p>202304020304</p> <p>Type : #Note Tags : [[Category Theory]]</p>"},{"location":"Product%20in%20Category%20Theory/#product-in-category-theory","title":"Product in Category Theory","text":"<p>The Universal Property for Products in Category Theory  is </p> <p>Let \\(X\\) and \\(Y\\) be sets, then for any set \\(A\\) and functions \\(f:A\\to X\\) and \\(g:A\\to Y\\), there exists a unique function \\(A\\to X\\times Y\\) such that the following diagram commutes. <pre><code>\\usepackage{tikz-cd}\n\\begin{document}\n\\begin{tikzcd}[column sep=small]\n&amp; \nX\\times Y \\arrow[dl, \"\\pi_1\"'] \\arrow[dr, \"\\pi_2\"]  \n&amp; \n\\\\\nX \n&amp;  \n&amp;\nY \n\\\\\n&amp; \nA \\arrow[ul, \"f\"] \\arrow[ur, \"g\"'] \\arrow[uu, \"\\langle{f,g}\\rangle\", dashed]\n&amp;\n\\end{tikzcd}\n\\end{document}\n</code></pre></p> <p>Since this is a universal property, Any object \\(B\\) such that given two functions from a set \\(A\\) to \\(X\\) and \\(Y\\) gives a unique map to \\(B\\) and \\(B\\) is isomorphic to \\(X\\times Y\\).</p>"},{"location":"Product%20in%20Category%20Theory/#ologging-products","title":"Ologging Products","text":"<p>Given two objects \\(c,d\\) in an olog, there is a canonical label \\(\\langle\\langle c\\times d \\rangle\\rangle\\) for their product, which can be understood as:=     A pair \\((x, y)\\), where \\(x\\) is \\(\\langle\\langle c\\rangle\\rangle\\) and \\(y\\) is \\(\\langle\\langle d\\rangle\\rangle\\)  The projections can be labelled as \"yields, as \\(x\\)\" and \"yields, as \\(y\\)\" respectively</p> <pre><code>\\usepackage{tikz-cd}\n\\begin{document}\n\\begin{tikzcd}[column sep=large, row sep=large]\n    \\fbox{a car owner}\n    \\arrow[rr, \"{\\begin{tabular}{@{}c@{}}\n        yields, insofar\\\\\n        as it is a person\\\\\n        and owns, as\\\\\n        primary, a car\n    \\end{tabular}}\"]\n    \\arrow[dd, \"is\"]\n    \\arrow[rrdd, \"{\\begin{tabular}{@{}c@{}}\n        owns, as\\\\\n        primary\n    \\end{tabular}}\", near start, crossing over]\n    &amp;&amp;\n    \\fbox{\\begin{tabular}{@{}c@{}}\n        a pair $(x,y)$,\\\\\n        where $x$ is a\\\\\n        person and $y$ is\\\\\n        a car\n    \\end{tabular}} \n    \\arrow[rr, \"\\begin{tabular}{@{}c@{}}\n        has as associ-\\\\\n        ated utility\n    \\end{tabular}\"]\n    \\arrow[dd, \"{yields,\\ as\\ y}\"]\n    \\arrow[ddll, \"{yields,\\ as\\ x}\", near end]\n    &amp;&amp;\n    \\fbox{a dollar value}\n    \\\\\\\\\n    \\fbox{a person}\n    &amp;&amp;\n    \\fbox{a car}\n\\end{tikzcd}\n\\end{document}\n</code></pre>"},{"location":"Product%20in%20Category%20Theory/#references","title":"References","text":"<p>Coproducts in Category Theory</p>"},{"location":"Product%20topology/","title":"Product topology","text":"<p>202301161201</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Product%20topology/#product-topology","title":"Product topology","text":""},{"location":"Product%20topology/#definition","title":"Definition","text":"<pre><code>title:\n$(X,\\tau_X); (Y,\\tau_Y)$. Then the product topology on the set $X \\times Y$ is the topology generated by the basis $\\mathcal{B}_{X\\times Y} = \\{U \\times V; U \\in \\tau_X, V \\in \\tau_Y\\}$ \n</code></pre>"},{"location":"Product%20topology/#exercise-check-that-this-is-a-basis","title":"exercise: Check that this is a basis.","text":""},{"location":"Product%20topology/#lemma","title":"Lemma:","text":"<p>\\(\\mathcal{B}_{X\\times Y} = \\{B_1 \\times B_2 : B_1 \\in \\mathcal{B}_X, B_2 \\in \\mathcal{B}_Y\\}\\) is a basis.</p>"},{"location":"Product%20topology/#examples","title":"Examples","text":"<ol> <li>Any finite product of discrete spaces is discrete.</li> <li>\\((\\mathbb{R}_{std})^n =\\mathbb{R}_{std}^n\\)</li> </ol>"},{"location":"Product%20topology/#projections","title":"Projections","text":"<p>Let \\((X_i,\\tau_i)\\) be finitely many topo spaces with the kth projection map:  \\(\\(\\pi_k : \\prod X_i \\to X_i\\)\\) \\(\\(\\pi_k((x_i)) = x_k\\)\\)</p>"},{"location":"Product%20topology/#fact","title":"Fact:","text":"<p>\\(A \\subset X\\), \\(B \\subset Y\\) then \\(\\pi_1^{-1}(A) = A\\times Y\\) and \\(\\pi_2^{-1}(B) = X \\times B\\). </p>"},{"location":"Product%20topology/#theorem","title":"Theorem","text":"<pre><code>title:\nThe collection $S_{X\\times Y} = \\{\\pi_1^{-1}(U); U \\in \\tau_X\\} \\cup \\{\\pi_2^{-1}(V) : V \\in \\tau_Y\\}$ is a [Sub-basis](&lt;./Sub-basis.md&gt;) for $\\tau_{X\\times Y}$.\n</code></pre>"},{"location":"Product%20topology/#proof","title":"Proof","text":"<p>Let \\(\\tau\\) be the topo gen by \\(S := S_{X\\times Y}\\). Every element of S is in \\(\\tau_{X \\times Y}\\) implies that \\(\\tau \\subset \\tau_{X \\times Y}\\). Every basis element \\(U \\times V\\) of \\(\\tau_{X\\times Y}\\) is \\(\\pi_1^{-1}(U) \\cap \\pi_2^{-1}(V) \\subset \\tau\\) This gives that \\(\\tau_{X\\times Y} \\subset \\tau\\). </p> <p>This definition with the sub-basis generalises to more general settings.</p>"},{"location":"Product%20topology/#theorem_1","title":"Theorem","text":"<pre><code>title: Product of subspace is same as subspace of product. \n\nIf A is a subspace of X, B is a subspace of Y. Then $\\tau_{A \\times B}$ = subspace topo on $A\\times B$ from $X \\times Y$. \n</code></pre>"},{"location":"Product%20topology/#proof_1","title":"proof:","text":"<p>\\(\\mathcal{B}_{A\\times B} = \\{M \\times N : M \\in \\tau_A, N \\in \\tau_B\\} = \\{(U \\cap A) \\times (V\\cap B) : U \\in \\tau_X, V\\in \\tau_Y\\} = \\{(U\\times V)\\cap (A \\times B)\\} =\\) basis for subspace topo on \\(A\\times B\\).</p>"},{"location":"Product%20topology/#products-and-continuity","title":"Products and continuity","text":"<p>1) Let \\(X, Y\\) be topological spaces and give \\(X \\times Y\\) the product topology. We have the two maps \\(\\pi_1 : X \\times Y \\to X\\) and \\(\\pi_2 : X \\times Y \\to Y\\) . Then \\(\\pi_1\\) and \\(\\pi_2\\) are continuous. 2) Let Z be any topological space. A function \\(f : Z \\to X \\times Y\\) is continuous if and only if both of its components \\(f_1 = \\pi_1 \\circ f : Z \\to X\\) and \\(f_2 = \\pi_2 \\circ f : Z \\to Y\\) are continuous. 3) The product topology is the coarsest topology on \\(X \\times Y\\) such that both the projection maps are continuous.</p> <pre><code>title: Idea\nThere is a natural bijection between $\\mathrm{Func}(X\\times Y, Z)$ and $\\mathrm{Func}(X, \\mathrm{Func}(Y,Z))$. Here Func is the set of all set theoretic funtions between the two sets in consideration.\nBut such a thing does not happen if we consider continuous functions. \n- Suppose $f : X \\times Y \\to Z$ is a continuous function. Now for each $x$, we have $i_x : Y \\to X \\times Y$ which is just an inclusion $i_x(y) = (x,y)$. Now the map $H_f(x) := f \\circ i_x : Y\\to Z$ is a continuous map.\n- So we get a map $\\mathrm{Cont}(X \\times Y,Z) \\to \\mathrm{Func}(X,\\mathrm{Cont}(Y,Z))$. But this is not surjective since we can have functions $f$ that are continuous in one direction for all fixed $x$, but is not continuous. So to fix this we need to talk about continuity of $H_f$ but then we want a topology on $\\mathrm{Cont}(Y,Z)$.\n- We can think of $Func(Y,Z) = \\prod\\limits_{y \\in Y} Z$, so we need to look at product topologies for arbitrary products. \n</code></pre>"},{"location":"Product%20topology/#product-topology-on-arbitrary-products","title":"Product topology on arbitrary products","text":"<p><pre><code>title: \nThe product topology on $\\prod\\limits_{\\alpha \\in J} X_{\\alpha}$ is the topology generated by the sub-basis \n$$S = \\{ \\pi_{\\beta}^{-1}(U_{\\beta}): \\beta \\in J, U_{\\beta} \\text{ open in } X_{\\beta}\\}$$\n</code></pre> - Note that this is the coarsest topology which makes all the projection maps continuous.</p>"},{"location":"Product%20topology/#topology-on-function-spaces","title":"Topology on function spaces","text":"<p>Given the space of functions \\(f : X \\to Y\\), for topo spaces X and Y, we can put a topology on Func(X,Y) which is the product space \\(\\prod\\limits_{x\\in X} Y\\), with the product topology as defined above. Convergence of sequences in this topology is the same as pointwise convergence.</p>"},{"location":"Product%20topology/#lemma_1","title":"Lemma","text":"<pre><code>title:\nLet $(f_n)_{n=1}^{\\infty}$ be a sequence of functions $f_n : Y \\to Z$ and let $f : Y \\to Z$ be another such function. Then $f_n \\to f$ as $n \\to \\infty$ in Func(Y, Z) if and only if the functions $f_n$ converge pointwise to f , i.e. if and only if for each y $\\in$ Y , we have $f_n(y) \\to f(y)$ as $n \\to \\infty$ in Z.\n</code></pre>"},{"location":"Product%20topology/#some-properties-of-products","title":"Some properties of products:","text":"<p>1) Let \\(A_{\\alpha}\\) be a subspace of \\(X_{\\alpha}\\) for each \\(\\alpha\\). Then the product topology on \\(\\prod\\limits_{\\alpha} A_{\\alpha}\\) equals the subspace topology induced from \\(\\prod_{\\alpha}X_{\\alpha}\\). 2) If each \\(X_{\\alpha}\\) is hausdorff then the product is hausdorff. 3) If each \\(X_{\\alpha}\\) has at least two points and the product is hausdorff then each \\(X_{\\alpha}\\) is hausdorff. 4)  Let \\(A_{\\alpha}\\) be a subspace of \\(X_{\\alpha}\\) for each \\(\\alpha\\). Then closure of the product of \\(A_{\\alpha}\\) is equal to the product of the closures.</p>"},{"location":"Product%20topology/#related-problems","title":"Related Problems","text":"<p>1) Show that \\(\\mathbb{R}^\\mathbb{R}\\) is separable in the product topology.    Proof:    Look at each element in R^R as a graph in R^2, then partition the plane into a grid of resolution 1/n at the nth step and pick heights for the function in the first n^2 columns on each side of the origin. This gives a countable collection of functions which can be used to approximate any function in \\(\\mathbb{R}^\\mathbb{R}\\).  </p>"},{"location":"Product%20topology/#references","title":"References","text":"<p>Sub-basis Bases for a topology Hausdorff Property Continuous Functions</p>"},{"location":"Proof%20that%20L2%20cannot%20be%20accepted%20by%20a%201%20Clock%20Timed%20Automata/","title":"Proof that L2 cannot be accepted by a 1 Clock Timed Automata","text":"<p>202311041509</p> <p>tags : Timed Automata</p>","tags":["Example"]},{"location":"Proof%20that%20L2%20cannot%20be%20accepted%20by%20a%201%20Clock%20Timed%20Automata/#proof-that-l_2-cannot-be-accepted-by-a-1-clock-timed-automata","title":"Proof that \\(L_2\\) cannot be accepted by a 1 Clock Timed Automata","text":"<p>[!tldr] Plan We show that a part of the run can of our ATA can be done on an AFA, and we use a pumping lemma type argument on that. </p> <p>[!info] DFA Analogy Consider a DFA \\(\\mathcal B =\\langle Q, q_0, \\delta, F\\rangle\\) over the alphabet \\(\\{a\\}\\). Let \\(f_{\\mathcal B}= q\\mapsto\\delta(q, a):Q\\to Q\\)  Since the number of function of type \\(Q\\to Q\\) are finite, the sequence \\(\\{f_{\\mathcal B}^i\\}_{i\\in\\mathbb N}\\) will have \\(l,m\\) such that \\(f_{\\mathcal B}^m=f_{\\mathcal B}^{m+l}\\) and \\(f_{\\mathcal B}^{m+i}=f_{\\mathcal B}^{m+l+i}\\) for all \\(i&gt;0\\)</p>","tags":["Example"]},{"location":"Proof%20that%20L2%20cannot%20be%20accepted%20by%20a%201%20Clock%20Timed%20Automata/#lemma-1","title":"Lemma 1","text":"<p>Consider an ATA \\(\\mathcal A = \\langle Q,q_0, \\delta, F\\rangle\\) over the alphabet \\(\\{a\\}\\) then let \\(f_\\mathcal A= x\\mapsto \\delta(x, a) : 2^{2^Q}\\to 2^{2^Q}\\). We have that the number of functions of the above type are finite, hence the sequence \\(\\{f_{\\mathcal A}^i\\}_{i\\in\\mathbb N}\\) will have \\(l,m\\) such that \\(f_{\\mathcal A}^m=f_{\\mathcal A}^{m+l}\\) and \\(f_{\\mathcal A}^{m+i}=f_{\\mathcal A}^{m+l+i}\\) for all \\(i&gt;0\\)</p> <p>Consider all ATA with \\(n\\) states. and find corresponding \\(m, l\\) for them. let \\(M= \\max(m)\\) and let \\(L=\\prod l\\).</p>","tags":["Example"]},{"location":"Proof%20that%20L2%20cannot%20be%20accepted%20by%20a%201%20Clock%20Timed%20Automata/#untiming-ata","title":"Untiming ATA","text":"<p>Let $$ \\begin{align} L_{2}\\quad:=\\quad{(a^k,\\tau)\\;&amp;:\\; (\\tau_{1}&lt;\\tau_{2}&lt;1) \\ &amp;\\land(\\tau_{1}+1&lt;\\tau_{k}&lt;\\tau_{2}+1\\text{ for exactly 1 }k) \\ &amp;\\land(\\tau_{i}&lt;\\tau_{i+1}\\text{ for all } i&lt;k)} \\end{align} $$ FTSOC, let \\(\\mathcal A\\) be an ATA which recognises the language that has \\(n\\) states. Pick \\(M\\) and \\(L\\) according to Lemma 1. </p> <p>We consider the word \\(a^{2+M+1+M}\\) and we show that \\(a^{2+M+1+L+M}\\) is also accepted by the language.</p>","tags":["Example"]},{"location":"Proof%20that%20L2%20cannot%20be%20accepted%20by%20a%201%20Clock%20Timed%20Automata/#no-reset-at-a_1-and-a_2","title":"No reset at \\(a_1\\) and \\(a_2\\)","text":"<p>Since the entire run is in \\(t\\in (1,2)\\) and we have that \\(\\tau_i&gt;\\tau_{i+1}\\) we have that the only useful guards are - \\(1&lt;x&lt;2\\) - \\(0&lt;x&lt;1\\) And we only ever go the second case if there is a reset. also, since the guards are between consecutive integer, if we can deal with the reset, then clocks are useless. So we make 2 copies of the automata, where we keep track of whether \\(x\\) has been reset or not and enable/disable guards accordingly. We send the control to the copy if a reset is done and hence, after \\(t=1\\) the clocks do not make a difference. Hence the second word is accepted.</p>","tags":["Example"]},{"location":"Proof%20that%20L2%20cannot%20be%20accepted%20by%20a%201%20Clock%20Timed%20Automata/#reset-at-a_2","title":"Reset at \\(a_2\\)","text":"<p>In this case the value of \\(x\\) from \\(\\tau_2\\) to \\(\\tau_2+1\\) will be in \\((0,1)\\) hence all of the guards will always or never be satisfied. Hence we can drop the transitions where the guards fail and remove the guards from where they were valid. Now we can accept the new word at any time stamps and continue after that from the same state.</p>","tags":["Example"]},{"location":"Proof%20that%20L2%20cannot%20be%20accepted%20by%20a%201%20Clock%20Timed%20Automata/#reset-at-a_1-but-not-at-a_2","title":"Reset at \\(a_1\\) but not at \\(a_2\\)","text":"<p>We do a similar procedure in this case. We make 2 copies and we untime the first one considering that there will not be any other resets until \\(\\tau_1+1\\). Then we can accept multiple letters in the region \\((\\tau_1+1,\\tau_2+2)\\), if there is a reset, we move to the other copy as then the result of the guards will not change as \\(x\\) will always be &lt;1 after that.</p>","tags":["Example"]},{"location":"Proof%20that%20L2%20cannot%20be%20accepted%20by%20a%201%20Clock%20Timed%20Automata/#related","title":"Related","text":"<ul> <li>Expressability of One-Clock Alternating Timed Automata</li> <li>Alternating Timed Automata</li> <li>Alternating Finite Automaton</li> </ul>","tags":["Example"]},{"location":"Push%20Down%20Automata/","title":"Push Down Automata","text":"<p>202210092210</p> <p>Type : #Note Tags : [[ Theory of Computation]]</p>"},{"location":"Push%20Down%20Automata/#push-down-automata","title":"Push Down Automata","text":"<p>Pushdown Automata (Accepting by final states) \\((Q,\\Sigma, T, T_0, Q_0, \\Delta,F)\\) Pushdown Automata (Accepting by empty stack) \\((Q,\\Sigma, T, T_0, Q_0, \\Delta)\\)</p> <p>\\(Q\\)- set of all states</p> <p>\\(\\Sigma\\)- The alphabet used by the Finite State Auotmata</p> <p>\\(T\\)- The alphabet used by the Stack</p> <p>\\(T_0: T\\)- First letter in the Stack</p> <p>\\(Q_0: Q\\)- Starting State</p> <p>\\(\\Delta: Q\\times \\Sigma\\times T\\times T^*\\times Q\\) A transition which will have an initial state, a letter the automata accepts, a letter it reads from the stack, a word it adds to the stack adn the final state</p> <p>\\(F: 2^Q\\)- Final States</p> <p>\\(L = \\{w \\mid \\textbar w\\textbar_a = \\textbar w \\textbar_b \\}\\)</p>"},{"location":"Push%20Down%20Automata/#related-problems","title":"Related Problems","text":""},{"location":"Push%20Down%20Automata/#references","title":"References","text":""},{"location":"Quadratic%20Reciprocity/","title":"Quadratic Reciprocity","text":"<p>202305271605</p> <p>Type : #Note Tags : [[Number Theory]] </p>"},{"location":"Quadratic%20Reciprocity/#quadratic-reciprocity","title":"Quadratic Reciprocity","text":"<pre><code>title:\nGiven two odd primes $p,q$, $$\n\\left(\\frac{q}{p}\\right)\\left(\\frac{p}{q}\\right) = (-1)^{((p-1)/2)((q-1)/2)}\n$$\n</code></pre>"},{"location":"Quadratic%20Reciprocity/#proof-1","title":"Proof 1:","text":"<p>Let \\(\\chi\\) be a character of order \\(2\\) on \\(\\mathbb{F}_{p}\\). Then by Corollary 1 to Theorem 3 in Jacobi Sums, $$ g(\\chi)^{q+1} = \\chi(-1)pJ(\\chi,\\chi, \\dots , \\chi) $$ Where there are \\(q\\) \\(\\chi's\\) in the right hand side of the equation. This gives $$ J(\\chi,\\chi, \\dots ,\\chi) = p <sup>{-1}(g(\\chi)</sup>{2})^{(q+1)/2}\\chi(-1)=p ^{-1}(p \\chi(-1))^{(q+1)/2} \\chi(-1) = p <sup>{(q-1)/2}(-1)</sup>{((p-1)/2)((q-1)/2)} $$ Now \\(J(\\chi,\\chi, \\dots,\\chi) = \\sum\\chi(t_{1})\\chi(t_{2})\\dots \\chi(t_{q})\\). If \\(t_{1} = t_{2} = \\dots = t_{q} = \\frac{1}{q}\\), we get \\(\\chi\\left( \\frac{1}{q} \\right)^{q} = \\chi(q)^{-q} = \\chi(q) = \\left(\\frac{q}{p}\\right)\\). When not all \\(t_{i}\\) are equal, we can cyclicly rotate them to get \\(q\\) equal terms, and so, mod q they vanish.</p> <p>Thus $$\\begin{align} J(\\chi, \\dots ,\\chi) &amp;= \\left( \\frac{q}{p} \\right)  (\\mathrm{mo d}  q) \\ &amp;= p <sup>{(q-1)/2}(-1)</sup>{((p-1)/2)((q-1)/2)} (\\mathrm{mo d}  q) \\ \\implies \\left( \\frac{q}{p} \\right) \\left( \\frac{p}{q} \\right) &amp;= (-1)^{((p-1)/2)((q-1)/2)} (\\mathrm{mo d}  q) \\end{align} $$ and hence, we can drop the mod q since both sides are absolute value 1.</p>"},{"location":"Quadratic%20Reciprocity/#references","title":"References","text":"<p>Legendre Symbol Multiplicative Characters Jacobi Sums Equations over Finite Fields</p>"},{"location":"Quanitifier%20Elimination%20for%20Natural%20Numbers%20With%20Successor/","title":"Quanitifier Elimination for Natural Numbers With Successor","text":"<p>202311291611</p> <p>Tags : [[Logic]]</p>","tags":["Note"]},{"location":"Quanitifier%20Elimination%20for%20Natural%20Numbers%20With%20Successor/#quanitifier-elimination-for-natural-numbers-with-successor","title":"Quanitifier Elimination for Natural Numbers with Successor","text":"<p>[!note] Definition A theory \\(T\\) admits quantifier elimination iff for every formula \\(\\varphi\\), there is a quantifier free formula \\(\\psi\\) such that  $$ T\\models (\\varphi \\leftrightarrow \\psi) $$</p> <p>^411d8b</p> <p>We can assume that every formula \\(\\varphi\\) is of the form $$ \\exists x(\\alpha_{1}\\land \\alpha_{2}\\dots\\alpha_{n}) $$ where each \\(\\alpha_i\\) is an atomic formula or negation of one.</p> <p>We can also assume that each \\(\\alpha_i\\) contain \\(x\\) otherwise we have  $$ \\exists x(\\alpha \\land \\beta) \\models  =!!!| \\alpha \\land \\exists x \\beta $$ if \\(\\alpha\\) does not contain \\(x\\).</p> <p>[!hint] Idea If all of the formulas are negations of equality. Then we always have a solution (the formula discard finitely many options from \\(\\mathbb N\\)) If any of the formulas is not a negation of equality. We replace the variable with the term everywhere and make sure that the term is satisfied by a non-negative value of \\(x\\).</p>","tags":["Note"]},{"location":"Quanitifier%20Elimination%20for%20Natural%20Numbers%20With%20Successor/#negation-elimination","title":"Negation Elimination","text":"<p>So we have that each \\(\\alpha_i\\) has the form \\(\\mathbf{S}^n(x)\\equiv \\mathbf S^m(u)\\) where \\(u\\) us \\(\\mathbf{0}\\) or another variable.</p> <p>If \\(u=x\\) then it is trivial. So we assume that \\(u\\ne x\\) Then we have the terms of the following type</p>","tags":["Note"]},{"location":"Quanitifier%20Elimination%20for%20Natural%20Numbers%20With%20Successor/#all-atomic-formulas-are-negated","title":"All atomic formulas are negated","text":"<p>\\(\\mathbf{S}^nx\\equiv t\\) where \\(t\\) does not contain \\(x\\). If every single atomic formula negated. Then we replace it by \\(\\mathbf{0} \\equiv \\mathbf{0}\\)</p>","tags":["Note"]},{"location":"Quanitifier%20Elimination%20for%20Natural%20Numbers%20With%20Successor/#alpha_0-is-mathbf-snxequiv-t","title":"\\(\\alpha_{0}\\) is \\(\\mathbf S^n(x)\\equiv t\\)","text":"<p>The value for \\(x\\) needs to be non-negative, so we replace the other formula by $$ t\\not \\equiv \\mathbf{0}\\land t\\not \\equiv \\mathbf{S}(\\mathbf{0})\\land\\dots \\land t\\not\\equiv \\mathbf{S}^{m-1}(\\mathbf{0}) $$ if \\(m&gt;0\\) otherwise \\(\\mathbf 0 \\equiv \\mathbf 0\\)  and for every other \\(\\alpha_j\\) of the form \\(\\mathbf S^k x\\equiv  u\\), we replace it with \\(\\mathbf S^k t \\equiv \\mathbf S^m u\\).</p> <p>Since we have eliminated \\(x\\), we can get rid of the quantifier.</p>","tags":["Note"]},{"location":"Quanitifier%20Elimination%20for%20Natural%20Numbers%20With%20Successor/#references","title":"References","text":"<ul> <li>Natural Numbers with Successor</li> <li>Natural Numbers with Successor and Ordering</li> <li>Decidability of Presburger Arithmetic</li> </ul>","tags":["Note"]},{"location":"Quantifier%20rank/","title":"Quantifier rank","text":"<p>202311292211</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"Quantifier%20rank/#quantifier-rank","title":"Quantifier rank","text":"<p>Quantifier rank is just the number of layers deep quantifiers go in a formula. - If \\(\\varphi\\) is atomic then \\(\\text{qr}(\\varphi)=0\\) - \\(\\text{qr}(\\varphi_1\\lor\\varphi_2)=\\text{qr}(\\varphi_1\\land\\varphi_2)\\) which is \\(\\max(\\text{qr}(\\varphi_1),\\text{qr}(\\varphi_2))\\)  - \\(\\text{qr}(\\varphi)=\\text{qr}(\\lnot\\varphi)\\) - \\(\\text{qr}(\\exists x \\varphi) = \\text{qr}(\\varphi)+1\\)</p> <p>We use the notion \\(\\text{FO[k]}\\) to denote the set of first order formulas of rank up to \\(k\\)</p>","tags":["Note","Incomplete"]},{"location":"Quantifier%20rank/#references","title":"References","text":"<ul> <li>Ehrenfeucht-Fra\u00efss\u00e9 Game</li> <li>Rank-k Types</li> </ul>","tags":["Note","Incomplete"]},{"location":"Quotient%20Topology/","title":"Quotient Topology","text":"<p>202302081802</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Quotient%20Topology/#quotient-topology","title":"Quotient Topology","text":"<pre><code>title: IDEA\nAn injective function $f :X \\to Y$ can be seen as a composition of $g:X \\to f(X) = A, i:A\\hookrightarrow Y$. Here $g$ is a bijection and $i$ is an injection. \n\nNow suppose $Y$ were a topo space, what topology can we give to $X$ so that $f$ becomes continuous?\nCertainly we can just give it the discrete topology, but that won't give us any info about $Y$. \nSo we talk about giving it the _coarsest topology_ that makes $f$ continuous. \nThat we can do by giving $A$ the subspace topology of $Y$, and then pulling that topology back to $X$ (this is unique). \n\n**This idea is about pulling back a topology to make the map continuous.**\n\nAnother idea, would be to _PUSH_ a topology onto another space. Suppose $f:X \\to Y$ were onto, and $X$ a topo space, what topology can we give to $Y$ to make $f$ continuous? Certainly we can give it the trivial topology  but again we get no info about $X$ then. So we ask what's the _finest_ topology we can give $Y$ to make $f$ [continuous](&lt;./Continuous Functions.md&gt;).\n</code></pre>"},{"location":"Quotient%20Topology/#quotient-topology_1","title":"Quotient Topology","text":"<pre><code>title: \nLet $f: X \\to Y$ be surjective (here X is a topo space, Y is just a set). \n\nThe _quotient topology_ in Y (induced from X) is the collection of subsets $V \\subseteq Y$ s.t. $f^{-1}(V)$ is open in X.\n</code></pre>"},{"location":"Quotient%20Topology/#quotient-map","title":"Quotient Map","text":"<pre><code>title:\nLet $f:X\\to Y$ be a surjective map between topo spaces.\nThen $f$ is called a _quotient map_ if $Y$ has the quotient topology from $X$, i.e., $V\\subseteq \\tau_Y$ iff $f^{-1}(V) \\in \\tau_X$.\n\nAn equivalent characterisation would be to that $V$ is closed in $Y$ iff $f^{-1}(V)$ is closed in $X$.\n</code></pre>"},{"location":"Quotient%20Topology/#a-quotient-map-is-a-continuous-map","title":"A quotient map is a continuous map.","text":"<p>For any subset \\(C \\subseteq X\\), C is saturated wrt a surjective function \\(p : X\\to Y\\) if \\(C\\) contains every set \\(p^{-1}(Y)\\) that it intersects  \\(\\therefore\\) \\(C\\) = union of fibers of \\(p\\).</p>"},{"location":"Quotient%20Topology/#so-p-is-a-quotient-map-iff-p-is-continuous-and-p-maps-saturated-open-sets-to-open-sets","title":"So, \\(p\\) is a quotient map iff \\(p\\) is continuous and \\(p\\) maps saturated open sets to open sets.","text":"<pre><code>title:\n1) Each surjective, cts, open map is a quotient map.\n2) Each surjective, cts, closed map is a quotient map.\n</code></pre>"},{"location":"Quotient%20Topology/#exercise","title":"Exercise:","text":"<p>Take \\(\\mathbb{R}\\) with std topology. Let \\(x \\sim y \\iff x-y \\in \\mathbb{Z}\\). Then \\(\\mathbb{R}/\\sim\\) is homeomorphic to \\(S^1\\).</p>"},{"location":"Quotient%20Topology/#theorem","title":"Theorem","text":"<pre><code>title:\nLet $f:X\\to Y$ be a quotient map.\nLet $B\\subset Y$,$A = f^{-1}(B)$ let $g : A \\to B$ be the restriction of $f$.\n1) If $A$ is open or closed, then $g$ is a quotient map.\n2) If $f$ is an open or a closed map, then $g$ is a quotient map.\n</code></pre>"},{"location":"Quotient%20Topology/#proof","title":"Proof:","text":"<p>1) Easy to check that \\(g\\) is cts.    Suppose \\(A\\) is open, then take a set \\(V \\subset B\\), such that \\(f^{-1}(V)\\) is open in \\(A\\). This means that \\(f^{-1}(V)\\) is open in \\(X\\) as well and so, \\(V\\) is open in \\(Y\\) since \\(f\\) is a quotient map. But since \\(V\\subset B\\), \\(V\\) is open in \\(B\\).     If \\(A\\) were closed, then \\(f^{-1}(V)\\)  3) Easy to check that \\(g\\) is cts, and open or closed according to whether \\(f\\) is open or closed. Let \\(U\\) be a saturated open set in \\(A\\), then \\(g(U)\\) is open in \\(B\\), and we are done by the previous characterisation of quotient maps.</p>"},{"location":"Quotient%20Topology/#theorem_1","title":"Theorem","text":"<pre><code>title: \nLet $f:X \\to Y$ be a quotient map. $Z$ be any space. Let $f:X \\to Z$ be a function s.t. $h(x) = h(x')$ whenever $f(x) = f(x')$. $g:Y\\to Z$ is such that $h = g \\circ f$. \n\n$g$ is cts iff $h$ is. $g$ is a quotient map iff $h$ is.\n</code></pre>"},{"location":"Quotient%20Topology/#proof_1","title":"Proof:","text":"<p>1) \\(g\\) is cts \\(\\implies\\) \\(g \\circ f\\) is cts since \\(f\\) is cts    \\(h\\) is cts \\(\\implies\\) \\(g^{-1}(V) = f(h^{-1}(V))\\) is open for open \\(V \\implies g\\) is cts. 2) \\(g\\) is quotient map     Then \\(h\\) is surjective, since \\(g\\) is.    Take an \\(h\\)-saturated open set in \\(X\\), \\(h^{-1}(V) = f^{-1}(g^{-1}(V))\\). Suppose this is open in \\(X\\). Then \\(g^{-1}(V)\\) is open in \\(Y\\). and then \\(V\\) is open in \\(Z\\). hence \\(h\\) is quotient map.</p> <p>\\(h\\) is quotient map    Take g-saturated open set in Y, \\(g^{-1}(V)\\) open in Y, but \\(g^{-1}(V) = f(h^{-1}(V))\\). Therefore, \\(f^{-1}(g^{-1}(V)) = h^{-1}(V)\\) is open in X, and so V is open in Z. Hence g is quotient map.</p>"},{"location":"Quotient%20Topology/#references","title":"References","text":"<p>Continuous Functions Open and Closed Functions</p>"},{"location":"R%20with%20Lower%20Limit%20topology/","title":"R with Lower Limit topology","text":"<p>202303192303</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"R%20with%20Lower%20Limit%20topology/#mathbbr-with-lower-limit-topology","title":"\\(\\mathbb{R}\\) with Lower Limit topology","text":"<pre><code>title:\nThe basic open sets in this topology are the sets of the form $[a,b); a,b \\in \\mathbb{R}$. \n</code></pre>"},{"location":"R%20with%20Lower%20Limit%20topology/#properties","title":"Properties","text":"<ol> <li>\\(\\mathbb{R}_{\\ell}\\) admits a basis of clopen sets.</li> <li>\\(\\mathbb{R}_{\\ell}\\) is regular.</li> <li>\\(\\mathbb{R}_{\\ell}\\) is Lindelof.</li> <li>\\(\\mathbb{R}_{\\ell}\\) is normal.</li> </ol>"},{"location":"R%20with%20Lower%20Limit%20topology/#references","title":"References","text":"<p>Regular Spaces Lindelof Space Normal Spaces</p>"},{"location":"R%5Ew/","title":"R^w","text":"<p>202303071003</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"R%5Ew/#mathbbromega-with-the-product-topology","title":"\\(\\mathbb{R}^{\\omega}\\) with the product topology","text":"<pre><code>title:\n$\\mathbb{R}^{\\omega} = \\prod \\limits_{n=1}^{\\infty}\\mathbb{R}$. \n\nThe topology on $\\mathbb{R}^{\\omega}$ is given by the basic open sets of the form: $\\prod\\limits_{n=1}^{\\infty}U_{n}$ where each $U_{n} = (a,b) \\subset \\mathbb{R}$ or $\\mathbb{R}$ itself, such that all except finitely many $U_{n}$'s are $\\mathbb{R}$.\n</code></pre>"},{"location":"R%5Ew/#this-space-has-the-following-properties","title":"This space has the following properties:","text":"<ul> <li>Hausdorff</li> <li>Locally compact</li> <li>Normal</li> </ul>"},{"location":"R%5Ew/#mathbbromega-with-uniform-topology","title":"\\(\\mathbb{R}^{\\omega}\\) with uniform topology","text":"<pre><code>title:\nThe uniform topology is the topology given by the uniform metric.\n\nThe **uniform metric** is defined as follows:\n$$\nd(x,y) = \\sup\\limits_{n\\in \\mathbb{N}} \\left\\{ \\min(1,|x_{n}-y_{n}|) \\right\\} \n$$\n</code></pre>"},{"location":"R%5Ew/#this-space-has-the-following-properties_1","title":"This space has the following properties:","text":"<ul> <li>Hausdorff</li> <li>Metrizable</li> <li>Normal</li> </ul>"},{"location":"R%5Ew/#mathbbromega-with-the-box-topology","title":"\\(\\mathbb{R}^{\\omega}\\) with the box topology","text":"<pre><code>title:\nBox topo is the topo where the basic open sets are $\\prod U_{\\alpha}$ with each $U_\\alpha$ open in the $X_\\alpha$.\n</code></pre>"},{"location":"R%5Ew/#this-space-has-the-following-properties_2","title":"This space has the following properties:","text":"<ul> <li>Hausdorff</li> <li>Not metrizable (show that it doesn't satisfy sequence lemma)</li> </ul>"},{"location":"R%5Ew/#related-problems","title":"Related Problems","text":""},{"location":"R%5Ew/#references","title":"References","text":"<p>Hausdorff Property Normal Spaces Metrizable Spaces Local Compactness</p>"},{"location":"R_K/","title":"R K","text":"<p>202303192303</p> <p>Type : #Note Tags :[[Topology]]</p>"},{"location":"R_K/#r_k","title":"R_K","text":"<pre><code>title:\nTopology on $\\mathbb{R}$ generated by the basis $(a,b)$ and $(a,b) \\setminus K$, where $K = \\{1/n : n \\in \\mathbb{N}\\}$\n</code></pre>"},{"location":"R_K/#properties","title":"Properties","text":"<ol> <li>Hausdorff</li> <li>Not Regular.    Note that \\(K\\) is closed in \\(\\mathbb{R}_{K}\\), we can't separate \\(0\\) and \\(K\\).</li> </ol>"},{"location":"R_K/#references","title":"References","text":"<p>Hausdorff Property Regular Spaces</p>"},{"location":"Rank%20of%20a%20module/","title":"Rank of a module","text":"<p>202302151102</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Rank%20of%20a%20module/#rank-of-a-module","title":"Rank of a module","text":"<pre><code>title:\nFor any integral domain R, the rank of an R-module M is the maximum number of R-linearly independent elements of M.\n</code></pre> <ul> <li>A free R module isomorphic to \\(R^n\\) has rank n.</li> <li>For any submodule N of a free R-module M, rank(N) is atmost rank(M). (follows from the proposition given below)</li> <li>If R is a field, then the rank of the module is just the dimension as a vector space over F.</li> <li>In general, an R module need not have a basis, even if it is torsion free.</li> </ul> <pre><code>title: Proposition\nLet R be an integral domain, and let M be a free R-module of rank $n &lt; \\infty$. Then any n+1 elements of M are R-linearly dependent.\n</code></pre>"},{"location":"Rank%20of%20a%20module/#related-problems","title":"Related Problems","text":""},{"location":"Rank%20of%20a%20module/#references","title":"References","text":"<p>Module Free Module</p>"},{"location":"Rank-k%20Types/","title":"Rank k Types","text":"<p>202311292311</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"Rank-k%20Types/#rank-k-types","title":"Rank-k Types","text":"<p>[!info] Intuition/Theorem In a relational vocabulary.</p> <p>If we are allowed to use precisely \\(m\\) variables, and we have no function symbols, then the number of atomic formulas are finite. \\(\\text{FO}[0]\\) is precisely the boolean formulas of finite variables. Hence \\(\\text{FO}[0]\\) is also finite, up to logical equivalence. \\(\\text{FO}[k]\\) with \\(m\\) variables is just the boolean combinations of \\(\\text{FO}[k-1]\\) with \\(m\\) variables and \\(\\exists x \\varphi\\) where \\(\\varphi\\in \\text{FO}[k-1]\\) with \\(m+1\\) variables.  This shows that \\(\\text{FO}[k]\\) with \\(m\\) variables is a finite set of formulas up to equivalence for all \\(k\\) and \\(m\\)</p> <p>That was a rather concrete motivation. This is because once we give a structure and assign values to the variables, we would finally be able to make sense of it as giving extra constants  to the language and replacing all the variables with it.</p> <p>[!note] Definition A rank-\\(k\\) \\(m\\)-type of a tuple \\(\\vec{a}\\) of size \\(m\\) over the structure \\(\\mathfrak A\\)  is the set of all \\(k\\)-rank formulae with \\(m\\) variable that are satisfied by \\(\\vec{a}\\). $$ \\text{tp}_{k}(\\mathfrak A,\\vec{a})={\\;\\varphi\\in \\text{FO}[k]\\quad|\\quad\\mathfrak A\\models\\varphi(\\vec{a})\\;} $$</p> <p>From the Intuition/Theorem above, we get that there are only finitely many rank-\\(k\\) \\(m\\)-types. Since we have finitely  many formulas upto equivalence, a type can be identified with the maximal subset of \\(\\text{FO}[k]\\) formulas that it satisfies. so each type has a representative element which is: $$ \\alpha_{K}(\\vec{x})=\\bigwedge_{i\\in K}\\varphi_{i}\\land\\bigwedge_{j\\not\\in K}\\lnot\\varphi $$ where \\(K\\) represents the indices of elemets in a subset of \\(\\text{FO}[k]\\).</p> <p>[!note] Theorem 1. For any finite relational vocabulary, the number of differnt rank-\\(k\\) \\(m\\)-types is finite. 2. For all types, we have representative formulae \\(\\alpha_1(\\vec{x})\\dots\\alpha_r(\\vec{x})\\)    1. For all \\(\\mathfrak A\\) and \\(\\vec a\\) we have a representative formula which would be one of the above.   2. Every formula is a equivalent to a disjunction of \\(\\alpha_i\\)'s</p> <p>Part 2.2 lets us prove that properties that satisfy structures are formulas as they are a union of types. Ehrenfeucht-Fra\u00efss\u00e9 Game Also note \\(\\alpha_K(\\vec{x})\\) is also \\(\\text{FO}[k]\\) formula.</p> <p>From Ehrenfeucht-Fra\u00efss\u00e9 Game and the above theorem, we directly have </p> <p>[!note] Theorem The equivalence relation \\(\\equiv_k\\) has a finite index.</p>","tags":["Note","Incomplete"]},{"location":"Rank-k%20Types/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Reachability%20Algorithm%20for%20Zone%20Automata/","title":"Reachability Algorithm for Zone Automata","text":"<p>202310190110</p> <p>Tags : Timed Automata</p>","tags":["Note"]},{"location":"Reachability%20Algorithm%20for%20Zone%20Automata/#reachability-algorithm-for-zone-automata","title":"Reachability Algorithm for Zone Automata","text":"<p>This algorithm takes in a Timed Automaton as in put and check if the final states are reachable from the starting states.</p> <p>It does this by Lazily generating Symbolic States as it traverses the timed automata. Building a Zone Automata alongside it. The process if either the algorithm cannot traverse any further in the initial automata, or if new new Zones can be generated.</p>","tags":["Note"]},{"location":"Reachability%20Algorithm%20for%20Zone%20Automata/#the-naive-algorithm","title":"The Naive Algorithm","text":"<p>We can use a subset relation between Zones to construct the following algorithm</p> <p></p> <p>The correctness of the Algorithm is discussed in the note attached to the Image.</p> <p>The problem with this algorithm is that it might not terminate on all inputs.</p>","tags":["Note"]},{"location":"Reachability%20Algorithm%20for%20Zone%20Automata/#finite-abstraction-of-zone-graph","title":"Finite Abstraction of Zone Graph","text":"<p>The problem is that there needs to be a relation that does not let the automata produce an infinite sequence of zones that cannot be quotiented together into a finite set.</p> <p>One way to do that is a simulation relation. The modified version of the naive algorithm that is correct and does terminate on all inputs, built using  a finite simulation relation instead of a subset relation is</p> <p></p> <p>It's correctness proof is almost identical to that of the naive algorithm and the proof of termination is given in Simulation for Zone Automata.</p>","tags":["Note"]},{"location":"Reachability%20Algorithm%20for%20Zone%20Automata/#references","title":"References","text":"<p>Zone Automata Naive Algorithm for Reachability in Zone Automata Simulation for Zone Automata</p>","tags":["Note"]},{"location":"Reachability%20in%20Lossy%20Channel%20Systems/","title":"Reachability in Lossy Channel Systems","text":"<p>202310241510</p> <p>Tags : [[Theory of Computation]], Timed Automata</p>","tags":["Note"]},{"location":"Reachability%20in%20Lossy%20Channel%20Systems/#reachability-in-lossy-channel-systems","title":"Reachability in Lossy Channel Systems","text":"<p>[!note] Lossy Channel Systems  A Lossy Channel System is like a Channel System but on each transition, the channel can lose some of its letters(from any part of the work) and become a subword.</p> <p>[!tldr] Plan We Encode the Channel runs as Timed Words, and we reduce the problem of reachability in lossy channel systems to universality of one clock timed automata which  is known to be non-primitive recursive.</p>","tags":["Note"]},{"location":"Reachability%20in%20Lossy%20Channel%20Systems/#encoding-channel-runs-as-timed-words","title":"Encoding Channel runs as Timed Words","text":"<p>We use a one clock Timed Automaton to simulate a Channel Systems with one Channel. Consider a run on a channel system, it consists of a sequence of configurations and transitions between them. We represent it in the following way.</p> <p>I let the alphabet of the automata be \\(\\Sigma\\cup Q\\cup(\\Sigma^2\\times \\{?,!\\})\\) where \\(\\Sigma\\) is the alphabet of the channel system and \\(Q\\) is its control.  The the timed word representing a run of the system would have  - \\((q_n,a,x)\\) at \\(t=0\\) where \\(q_n\\) is the last state in the run which was reached by accepting the letter \\(a\\) and \\(x=!\\) if \\(a\\) was written and \\(x=?\\) if \\(a\\) was dropped. - The contents \\(c_{n-1}\\) of the tape are written letter by letter in order in time \\((0,1)\\) - At times \\(t=i\\) where \\(i\\in\\mathbb N\\) we have the \\((q_{n-i},a,x)\\) and in time \\((i,i+1)\\) we have \\(c_{n-i}\\) written in a similar format. - At time \\(t=n\\) we have \\(q_0\\)  - We also enforce the condition that the corresponding letters in \\(c_i\\) and \\(c_{i-1}\\) are exactly \\(1\\) time unit apart.</p> <p>Now have a timed automata that accepts if either adjacent states and transitions are incompatible, or if there isn't a letter for channel configuration exactly 1 unit away.</p> <p>Now we have reduced the problem of Reachability of channel systems to universality in a one clock timed automata. Which is known to be Non-Primitive Recursive.</p>","tags":["Note"]},{"location":"Reachability%20in%20Lossy%20Channel%20Systems/#references","title":"References","text":"<ul> <li>Universality is Decidable for One-Clock Timed Automata</li> <li>Recursive and Recursively Eumberable Sets</li> <li>Primitive Recursion</li> </ul>","tags":["Note"]},{"location":"Recursive%20and%20Recursively%20Eumberable%20Sets/","title":"Recursive and Recursively Eumberable Sets","text":"<p>202211151011</p> <p>Type : #Note Tags : [[Theory of Computation]]</p>"},{"location":"Recursive%20and%20Recursively%20Eumberable%20Sets/#recursive-and-recursively-eumberable-language","title":"Recursive and Recursively Eumberable Language","text":"<p>A Language is called recursively Enumerable if is accepted by a Turing Machine and Recursive if it is accepted by a Total Turing Machine.</p> <p>recursive languages are closed under complementation but recursively enumerable languages are not. This is because for Turing Machines which are not total, not accpeting and rejecting are not synonymous. We can obtain a Total Turing machine which accpets the complement of a Recursive language by replacing the accepting and rejecting states.</p> <p>If both the language and its complement is recursively enumerable then the language is recursive. Proof: Let \\(M\\) and \\(M'\\) be turing machines such that \\(L(M)=A\\) and \\(L(M') =\\ \\sim A\\). Build a Turing Machine \\(N\\) which runs the input \\(x\\) on both \\(M\\) and \\(M'\\) simultaneously on different tracks of its tape, hence \\(N\\) contains alphabets of the form { width=\"400\" } where \\(a\\) is the tape letter for \\(M\\) and \\(c\\) is the tape letter for \\(M'\\) If the Machine \\(M\\) ever accepts, then \\(N\\) immediately accepts and if the machine \\(M'\\) ever accetps then \\(N\\) immediately rejects. Exactly one of the 2 cases must eventually occur as \\(x\\in A\\) or \\(x\\in \\sim A\\) which are both recursively enumerable. Hence \\(L(N)\\) is \\(A\\) and \\(N\\) is a total turing machine, Hence \\(A\\) is recursive.</p>"},{"location":"Recursive%20and%20Recursively%20Eumberable%20Sets/#decidable-an-semidecidable","title":"Decidable an Semidecidable","text":"<p>A property \\(P\\) of a language is called decidable if the set of all the strings having that property is recursive. Simlarly if all the strings having the property \\(P\\) is called semidecidable if the set of all the strings having the property is enumerably recursive. We can also say that if a set of strings has a semidecidable property, then there exits a turing machine which accepts them and loops or rejects on all other strings. </p>"},{"location":"Recursive%20and%20Recursively%20Eumberable%20Sets/#related-problems","title":"Related Problems","text":""},{"location":"Recursive%20and%20Recursively%20Eumberable%20Sets/#references","title":"References","text":"<p>Turing Machines</p>"},{"location":"Reducibility%20Theorems/","title":"Reducibility Theorems","text":"<p>202307250007 ^0b2b53</p> <p>Type : #Note Tags : [[Type Theory]]</p>"},{"location":"Reducibility%20Theorems/#reducibility-theorems","title":"Reducibility Theorems","text":""},{"location":"Reducibility%20Theorems/#pairing","title":"Pairing","text":"<p>If \\(u\\) and \\(v\\) are reducible, so is \\(\\langle u, v\\rangle\\).</p> <p>Because of CR 1, we can reason by induction of \\(\\nu(u)+\\nu(v)\\) to show that \\(\\pi^{1}\\langle u,v\\rangle\\) is reducible. This term converts to: - \\(u\\), which is reducible. - \\(\\pi_{1}\\langle u', v\\rangle\\), with \\(u'\\) one step from \\(u\\). \\(u'\\) is reducible by CR 2, and we have \\(\\nu(u)&lt;\\nu(u')\\); so the induction hypothesis tells us this term is reducible - same with \\(\\pi^{1}\\langle u, v'\\rangle\\) This shows \\(\\pi_{1}\\langle u,v \\rangle\\) is reducible, same argument works for \\(\\pi^{2}\\langle u, v \\rangle\\)</p>"},{"location":"Reducibility%20Theorems/#abstraction","title":"Abstraction","text":"<p>If for all reducible \\(u\\) of type \\(U\\), \\(v[u/x]\\) is reducible then so is \\(\\lambda x.v\\). We again reason on \\(\\nu(u)+\\nu(v)\\)</p> <p>The term \\((\\lambda x.v)u\\) converts to - \\(v[u/x]\\) is reducible by hypothesis - \\((\\lambda x.v') u\\) with \\(v'\\) being one step away from \\(v\\). So \\(v'\\) is reducible, \\(\\nu(v')&lt;\\nu(v)\\), and then induction tells us this term is reducible - \\((\\lambda x.v) u'\\) with \\(u'\\) being one step away from \\(u\\). So \\(u'\\) is reducible, \\(\\nu(v')&lt;\\nu(v)\\), and then induction tells us this term is reducible In every case the neutral term \\((\\lambda x.v)u\\) converts to reducible terms only, and by CR 3 it is reducible. so \\(\\lambda x.v\\) is reducible</p>"},{"location":"Reducibility%20Theorems/#references","title":"References","text":"<p>Reducibility Strong Normalization Theorem</p>"},{"location":"Reducibility/","title":"Reducibility","text":"<p>202307242307 ^4ec029</p> <p>Type : #Note Tags : [[Type Theory]], [[Lambda Calculus]]</p>"},{"location":"Reducibility/#reducibility","title":"Reducibility","text":""},{"location":"Reducibility/#definition","title":"Definition","text":"<p>We define a set \\(RED_{T}\\) (\"Reducible term of type \\(T\\)\") by induction on the type \\(T\\)</p> <ol> <li>for \\(t\\) of atomic type \\(T\\), \\(t\\) is reducible if it is strongly normalisable. ^cf45af</li> <li>for \\(t\\) of type \\(U\\times V\\), \\(t\\) is reducible if its two projections are reducible ^7b3975</li> <li>for \\(t\\) of type \\(U\\to V\\), \\(t\\) is reducible if for all reducible \\(u\\) of type \\(U\\), \\(t\\ u\\) is reducible of type \\(V\\) ^5083eb</li> </ol> <p>The reason why redicibility works where combinatorial intuition fails is because of the complexity</p> <p>\\(t\\in RED_{U\\to V}\\iff\\forall u(u\\in RED_{U}\\implies t\\ u\\in RED_{V})\\)</p>"},{"location":"Reducibility/#properties","title":"Properties","text":"<pre><code>title:Neutrality\nA term is called _neutral_ if it is not of the form $\\langle u,v\\rangle$ or $\\lambda x.v$. In other words, neutral terms are those which are of the form\n- $x$\n- $\\pi^{1}t$\n- $\\pi^{2}t$\n- $t\\ u$\n</code></pre> <p>^e70af2</p> <p>The conditions that interest are  - CR1: If \\(t\\in RED_{T}\\) then \\(t\\) is strongly normalizable ^CR1 - CR2: If \\(t\\in RED_{T}\\) and \\(t\\rightsquigarrow t'\\) then \\(t'\\in RED_{T}\\) ^CR2 - CR3: If \\(t\\) is neutral, and whenever we convert a redex of \\(t\\) we obtain a term \\(t'\\in RED_{T}\\) then \\(t\\in RED_{T}\\) ^CR3 - CR4: If \\(t\\) is neutral and normal then \\(t\\in RED_{T}\\)  ^CR4</p>"},{"location":"Reducibility/#references","title":"References","text":"<p>Atomic Types Product Type Atomic Types Reducibility Theorems</p>"},{"location":"Reduction/","title":"Reduction","text":"<p>202301051201</p> <p>Type : #Note Tags : [[Theory of Computation]]</p>"},{"location":"Reduction/#reduction","title":"Reduction","text":"<p>\\(f:\\Sigma^*\\to\\Sigma^*\\) is a polynomial time computable function.</p> <p>\\(\\(A\\le_P B\\)\\) is a polynomial time reduction if there exits a polynomial computable function \\(f:\\Sigma^*\\to\\Sigma^*\\) such that \\(\\(x\\in A\\iff f(x)\\in B\\)\\) A Reduction from \\(A\\) to \\(B\\) can be intuitively thought as.     \\(A\\) is not any harder than \\(B\\) Since a subroutine in \\(B\\) can  be used to solve \\(A\\) </p> <p>If \\(L_1\\) and \\(L_2\\) are both reducible to each other  $$ \\begin{equation} \\left. \\begin{matrix} L_1\\le_P L_2\\ L_2 \\le_P L_1 \\end{matrix} \\right}\\iff L_1 \\equiv_P L_2 \\end{equation} $$</p>"},{"location":"Reduction/#related-problems","title":"Related Problems","text":""},{"location":"Reduction/#references","title":"References","text":""},{"location":"Reed-Solomon%20Codes/","title":"Reed Solomon Codes","text":"<p>202308221508</p> <p>Type : #Note #Incomplete  Tags : Algorithmic Coding Theory</p>"},{"location":"Reed-Solomon%20Codes/#reed-solomon-codes","title":"Reed-Solomon Codes","text":""},{"location":"Reed-Solomon%20Codes/#references","title":"References","text":""},{"location":"Region%20Automata%20Alternate%20Definition/","title":"Region Automata Alternate Definition","text":"<p>202311162011</p> <p>Tags : Timed Automata</p>","tags":["Note","Incomplete"]},{"location":"Region%20Automata%20Alternate%20Definition/#region-automata-alternate-definition","title":"Region Automata Alternate Definition","text":"<p>Given a Timed Automata we define a region automata \\(\\textbf{R}_A\\) as  \\(\\textbf{R}_{\\mathcal A} = \\langle Q, X, T, \\kappa,\\lambda\\rangle\\) such that - \\(Q=L\\times R_{\\mathcal A}\\) - \\(\\kappa((l,\\tau))=\\mathcal I(l)\\) - \\(\\lambda((l,r))=\\mathcal L(l)\\) - \\(X\\) is the same set of clocks - \\(T\\subseteq Q\\times \\text{cell}(R_\\mathcal A) \\times E\\times \\mathcal 2^X\\times Q\\), so \\((l,r)\\xrightarrow{\\text{cell}(r''),e,Y}(l',r')\\) exists if     - \\(\\text{cell}(r)\\) is the smallest guard which contains \\(r\\)     - \\((l,r)\\xrightarrow{\\text{cell}(r''),e,Y}(l',r')\\) exists if          - There exists \\(e=l\\xrightarrow{g, Y}l'\\)          - \\((l,\\nu)\\xrightarrow{\\tau,e}(l',\\nu')\\) with              - \\(\\nu\\in r\\)              - \\(\\nu+\\tau\\in r''\\)             - \\(\\nu'\\in r'\\) We recover the usual [[Region Automata]] by replacing \\(\\text{cell}(r''),e,Y\\) with just \\(e\\).</p> <p>For each concrete run \\(\\varrho\\) in \\(\\mathcal A\\), we have \\(\\iota(\\varrho)\\) which is its unique image in \\(\\textbf R_\\mathcal A\\)</p>","tags":["Note","Incomplete"]},{"location":"Region%20Automata%20Alternate%20Definition/#some-important-properties","title":"Some Important Properties","text":"","tags":["Note","Incomplete"]},{"location":"Region%20Automata%20Alternate%20Definition/#path-correspondence","title":"Path Correspondence","text":"<p>Every finite path \\(\\pi((l,\\nu),e_{1}\\dots e_{n})\\) in \\(\\mathcal A\\) there is a corresponding finite set of paths \\(\\pi(((l,[\\nu]),\\nu),f_{1}\\dots f_{n})\\) in \\(\\textbf R_\\mathcal A\\). Each path is decided by the choice of regions traversed.</p>","tags":["Note","Incomplete"]},{"location":"Region%20Automata%20Alternate%20Definition/#there-are-bisimulation-properties","title":"There are bisimulation properties","text":"","tags":["Note","Incomplete"]},{"location":"Region%20Automata%20Alternate%20Definition/#non-blocking","title":"Non Blocking","text":"<p>If \\(\\mathcal A\\) is non blocking, then so is \\(\\textbf R_{\\mathcal A}\\).</p>","tags":["Note","Incomplete"]},{"location":"Region%20Automata%20Alternate%20Definition/#references","title":"References","text":"<ul> <li>Timed Automata Alternate Definition</li> <li>[[Region Automata]]</li> </ul>","tags":["Note","Incomplete"]},{"location":"Regular%20Permutation%20Groups/","title":"Regular Permutation Groups","text":"<p>202308231008</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Regular%20Permutation%20Groups/#regular-permutation-groups","title":"Regular Permutation Groups","text":"<p>Let \\((\\Gamma, X)\\) be a permutation group. \\(\\Gamma\\) is said to be Semiregular on \\(X\\) if \\(\\forall x\\in X,\\Gamma_{x}=\\{1\\}\\). Which is to say that the only permutation that has an fixed point is the identity permutation.  If \\(\\Gamma\\) is also transitive, then it is called regular.</p> <p>In spirit, regular permutation groups mimic cycles. </p> <p>The most common example of Regular permutation groups are  cycles</p>"},{"location":"Regular%20Permutation%20Groups/#references","title":"References","text":""},{"location":"Regular%20Spaces/","title":"Regular Spaces","text":"<p>202303071503</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Regular%20Spaces/#regular-spaces","title":"Regular Spaces","text":"<pre><code>title:\nA topological space $X$ is _regular_ if \n1) Every singleton is closed in $X$.\n2) for each point $x \\in X$ and closed subset $A \\subseteq X$, there is a pair of disjoint open subsets $U,V$ of $X$ such that $x \\in U$ and $A \\subseteq V$.\nWe then say that $U,V$ separate $x$ and $A$.\n</code></pre>"},{"location":"Regular%20Spaces/#lemma","title":"Lemma:","text":"<pre><code>title:\nLet $X$ be $T_{1}$, then $X$ is regular iff for every $x$ and every open nbhd U of $x$, there is a nbhd $V$ of $x$ contained in $U$ such that $Cl(V) \\subseteq U$. \n</code></pre>"},{"location":"Regular%20Spaces/#proof","title":"Proof:","text":"<p>Suppose \\(X\\) is regular, then for any \\(x\\) and every open nbhd \\(U\\) of \\(x\\), take \\(U^{c}\\) to be the closed set, then there is a separation of \\(x\\), \\(U^{c}\\) \\(\\implies \\exists V,W\\) s.t. \\(x \\in V, U^{c} \\subseteq W\\), this gives \\(V \\subset Cl(V) \\subset W^{c} \\subset U\\). We are done.</p> <p>Conversely, given \\(x, A\\) disjoint from \\(x\\) and closed, take \\(U = A^{c}\\) and find \\(V \\subset Cl(V) \\subset U\\). Then \\(V\\) and \\((Cl(V))^{c}\\) gives the separation. </p>"},{"location":"Regular%20Spaces/#lemma_1","title":"Lemma:","text":"<pre><code>title:\nAny subspace of a regular space is regular. An arbitrary product of regular spaces is regular.\n</code></pre>"},{"location":"Regular%20Spaces/#lemma_2","title":"Lemma:","text":"<pre><code>title:\nAny space admitting a basis with clopen sets is regular.\n</code></pre>"},{"location":"Regular%20Spaces/#proof_1","title":"Proof:","text":"<p>Suppose \\(X\\) is a space with all basis sets clopen, then take \\(x\\) and \\(C\\) closed, disjoint from \\(x\\). Then \\(C^{c}\\) is open, so there is a basic clopen set \\(U\\) containing \\(x\\). Then \\(U, U^{c}\\) forms a separation.</p>"},{"location":"Regular%20Spaces/#examples","title":"Examples","text":"<p>1) \\(\\mathbb{R}_{\\ell}\\) 2) \\(\\mathbb{R}^{n}\\)</p>"},{"location":"Regular%20Spaces/#related-results","title":"Related Results","text":"<p>1) Regular + 2nd Countable \\(\\implies\\) Normal 2) Regular + Lindelof \\(\\implies\\) Normal 3) Order Topology is Regular 4) Regular + 2nd Countable \\(\\implies\\) Metrizable (Urysohn Metrization Theorem)</p>"},{"location":"Regular%20Spaces/#related-problems","title":"Related Problems","text":""},{"location":"Regular%20Spaces/#references","title":"References","text":"<p>Second Countability Normal Spaces Order Topology Lindelof Space Urysohn Metrization Theorem</p>"},{"location":"Removable%20Singularities%20in%20Analytic%20Functions/","title":"Removable Singularities in Analytic Functions","text":"<p>202303101503</p> <p>Type : #Note Tags : [[Complex Analysis]]</p>"},{"location":"Removable%20Singularities%20in%20Analytic%20Functions/#removable-singularities-in-analytic-functions","title":"Removable Singularities in Analytic Functions","text":"<p>Let \\(\\Omega'\\) be a region obtained by removing a point from the region \\(\\Omega\\), and \\(f\\) is analytic on \\(\\Omega'\\), then <pre><code>title: Theorem\n$\\exists!$ a function that agrees with $f$ on $\\Omega'$ and is analytic on \n$Omega\\iff\\lim\\limits_{z\\to{a}}(z-a)f(z)=0$.\n</code></pre></p> <p>Proof: \\(\\Longrightarrow\\)  Forward direction is trivial as the function in continuous at \\(a\\) \\(\\Longleftarrow\\)  Draw a circle \\(C\\) inside \\(\\Omega\\) which contains \\(a\\). Cauchy's formula is valid  hence we can write  $$     f(z)= \\frac{1}{2i\\pi}\\int_{C} \\frac{f(\\zeta)d\\zeta}{\\zeta-z} $$ for all \\(z\\ne a\\) inside of \\(C\\). But the integral represents and analytic function of \\(z\\) throughout the inside of \\(C\\). Consequently, the functoin which is equal to \\(f(z)\\) for \\(z\\ne a\\) and which has the value  $$     f(z)= \\frac{1}{2i\\pi}\\int_{C} \\frac{f(\\zeta)d\\zeta}{\\zeta-a} $$ for \\(z=a\\) is analytic in \\(\\Omega\\). $$ F(z)= \\frac{f(z)-f(a)}{z-a} $$ which isn't defined for \\(z=a\\) but \\(\\lim\\limits_{z\\to{a}}(z-a)F(z)=0\\) and \\(\\lim\\limits_{z\\to a} F(z)=f'(a)\\) define \\(f_i\\) in the following way $$ \\begin{align} \\frac{f_i(z)-f_i(a)}{z-a}&amp;= f_{i+1}(z)\\ \\frac{f(z)-f(a)}{z-a}&amp;= f_1(z) \\end{align} $$ from these equations we have  $$ f(z)=f(a)+(z-a)f_{1}(a) + (z-a)<sup>{2}f_{2}(a)+\\dots+(z-a)</sup>{n-1}f_{n-1}(a)+(z-a)^{n}f_{n}(z) $$ on differentiating \\(n\\) times, we have \\(\\(f^{(n)}(a)=n!f_{n}(a)\\)\\) This gives us the Taylor's theorem for analytic functions which states $$ f(z) = f(a) + \\frac{f'(a)}{1!}(z-a) + \\frac{f''(a)}{2!}(z-a)^{2}+\\dots+ \\frac{f<sup>{(n-1)}(a)}{(n-1)!}(z-a)</sup>{n-1} +f_{n}(z)(z-a)^{n} $$ where \\(f_{n}(z)\\) is analytic in \\(\\Omega\\)  $$ f_{n}(z) = \\frac{1}{2i\\pi} \\int_{C} \\frac{f_n(\\zeta)d\\zeta}{\\zeta-z} $$ by applying the taylor expansion to \\(F(z)\\) we get \\(\\(F_{\\nu}(a)=\\int_{C} \\frac{d\\zeta}{(\\zeta-a)^{\\nu}(\\zeta-z)}\\)\\) but \\(F_{1}(a)=0\\). Hence $$ f_{n}(z)= \\frac{1}{z-a} \\int_{C} \\frac{f(\\zeta)d\\zeta}{(\\zeta-a)^{n}(\\zeta-a)} $$ which is valid, hence \\(f(z)\\) exists</p>"},{"location":"Removable%20Singularities%20in%20Analytic%20Functions/#related-problems","title":"Related Problems","text":""},{"location":"Removable%20Singularities%20in%20Analytic%20Functions/#references","title":"References","text":""},{"location":"Result%20of%20Expressive%20Power%20of%20G%C3%B6del%27s%20system%20T/","title":"Result of Expressive Power of G\u00f6del's system T","text":"<p>202307261907</p> <p>Type : #Note Tags : [[Type Theory]]</p>"},{"location":"Result%20of%20Expressive%20Power%20of%20G%C3%B6del%27s%20system%20T/#result-of-expressive-power-of-godels-system-t","title":"Result of Expressive Power of G\u00f6del's system T","text":"<p>If \\(t\\) is a closed term of the type \\(\\text{Int}\\to\\text{Int}\\), then it induces a function \\(|t|\\) from \\(\\mathbb N\\to\\mathbb N\\) $$ |t|(m)=n\\iff t \\overline m\\rightsquigarrow\\overline n  $$ And like wise a closed term of  the type \\(\\text{Int}\\to\\text{Bool}\\) induces a predicate on \\(\\mathbb N\\) $$ |t|(m)\\text{ holds} \\iff t \\overline m\\rightsquigarrow \\text T $$ The functions \\(|t|\\) are clearly calculable and normalization theorem gives proof for the termination of all the algorithms obtained from \\(\\text{T}\\) .</p> <p>To prove the reducibility of \\(t\\), we need to be able to the reducibilities for \\(t\\) and its subterms. So we should be able to write finite number of reducibilites, which can be written using Peano Axioms. And to be able to reason on this finite number of reducibilities using induction. Which can also be done using the Peano Axioms, so \\(|t|\\) is totally provable in Peano Arithmetic</p>"},{"location":"Result%20of%20Expressive%20Power%20of%20G%C3%B6del%27s%20system%20T/#references","title":"References","text":"<p>G\u00f6del's system T</p>"},{"location":"Retractions/","title":"Retractions","text":"<p>202304041204</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Retractions/#retractions","title":"Retractions","text":"<pre><code>title:\nFor $A \\subseteq X$, a **retraction** $r : X \\to A$ is a continuous map such that $r(a) = a \\ \\forall \\ a \\in A$. If such an $r$ exists, we say that $A$ is a **retract** of $X$.\n</code></pre>"},{"location":"Retractions/#lemma","title":"Lemma:","text":"<pre><code>title:\nIf $r : X \\to A$ is a retraction, the homomorphism of fundamental groups $i_{*} : \\Pi_{1}(A,a_{0}) \\to \\Pi_{1}(X,a_{0})$ induced by the inclusion $i:A \\hookrightarrow X$ is an injective homomorphism.\n</code></pre>"},{"location":"Retractions/#proof","title":"Proof:","text":"<p>We know that \\(r \\circ i = id_{A}\\) and so, \\(r_{*}\\circ i_{*} = id_{\\Pi_{1}(A,a_{0})}\\). This gives that \\(i_{*}\\) is injective. Also gives that \\(r_{*}\\) is surjective.</p>"},{"location":"Retractions/#lemma_1","title":"Lemma:","text":"<pre><code>title:\nThere is no retraction of $B^{2}$ to $S ^{1}$. Here, $B^{2}$ denotes the closed unit disk $\\{ (x,y) \\mid \\ x ^{2}+y^{2} \\le 1 \\} \\subset \\mathbb{R}^{2}$.\n</code></pre>"},{"location":"Retractions/#proof_1","title":"Proof:","text":"<p>Assume \\(r:B^{2} \\to S ^{1}\\) was a retraction then \\(i_{*}:  \\Pi_{1}(S ^{1},b_{0}) \\to \\Pi_{1}(B^{2},b_{0})\\) must be injective but that's impossible since \\(\\Pi_{1}(S_{1},b_{0}) = \\mathbb{Z}\\) and \\(\\Pi_{1}(B^{2},b_{0}) = \\{ 0 \\}\\).</p>"},{"location":"Retractions/#references","title":"References","text":"<p>Fundamental Group</p>"},{"location":"Ring/","title":"Ring","text":"<p>202211102211</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Ring/#ring","title":"Ring","text":"<pre><code>title: Motivation Behind Ring Theory\nRing Theory was created to number theory of a set that is larger than number, for example take the questions :\nFind integer solution of $x^2+y^2 = z^2$, This is a very tedious problem but instead if we do this.\n$(x - yi)(x + yi) = z^2$ this now becomes a factorization problem in $\\mathbb Z[i]$\nSimilarly $x^3 + y^3 = z^3$ becomes $(x+y)(x+\\omega y)(x+\\omega^2y)=z^3$ in $\\mathbb Z[\\omega]$\n</code></pre> <p>A the set \\((S, *, +)\\) Where \\(S\\) is a set and \\(*, +: (S, S) \\to S\\) such that \\(S\\) is an abelian group under \\(+\\) and its closed under \\(*\\), Such that \\(*\\) is distributive over \\(+\\).</p> <p>A [[Ring Homomorphism]] \\(\\phi:R\\to S\\) , where \\(S\\) and \\(R\\) are rings and  - \\(\\phi(a+b) = \\phi(a) + \\phi(b)\\) - \\(\\phi(ab) = \\phi(a)\\phi(b)\\) \\(\\aleph\\) </p>"},{"location":"Ring/#related-problems","title":"Related Problems","text":""},{"location":"Ring/#references","title":"References","text":""},{"location":"Runtime/","title":"Runtime","text":"<p>202301051201</p> <p>Type : #Note Tags : [[Complexity Theory]]</p>"},{"location":"Runtime/#runtime","title":"Runtime","text":"<p>Let \\(f:\\mathbb N \\to \\mathbb N\\) be a function</p> <ul> <li>Deterministic Turing Machine - A Determinisitic Turing Machine has a runtime of \\(f(n)\\) means that for input \\(x:|x|\\in \\mathbb N\\), The machine Halts in Time \\(O(f(n))\\). </li> <li>Nondeterminisitic Turing Machine - A Nondeterminisitic Turing Machine has a runtime of \\(f(n)\\) means that for input \\(x:|x|\\in \\mathbb N\\), The machine Halts in Time \\(O(f(n))\\) on all paths. </li> </ul>"},{"location":"Runtime/#related-problems","title":"Related Problems","text":""},{"location":"Runtime/#references","title":"References","text":""},{"location":"S%5E1/","title":"S^1","text":"<p>202304041204</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"S%5E1/#s1","title":"S^1","text":"<pre><code>title:\nThis is just a circle.\nCan be identified with $\\{z \\in \\mathbb{C} \\mid \\ |z| = 1\\}$\n</code></pre>"},{"location":"S%5E1/#theorem","title":"Theorem:","text":"<pre><code>title:\nIt has fundamental group = $\\mathbb{Z}$.\n</code></pre>"},{"location":"S%5E1/#proof","title":"Proof:","text":"<p>Need to show \\(\\varphi([f]*[g]) = \\varphi([f])*\\varphi([g])\\) Let \\(\\widetilde{f}(1) = n, \\widetilde{g}(1)=m\\), \\(\\widetilde{f}(0) = \\widetilde{g}(0) = 0\\) Define \\(h(s) := n + \\widetilde{g}(s)\\). This is a lift of \\(g\\) starting at \\(n\\).  Then the path \\(f * \\widetilde{h}\\) is well defined on \\(\\mathbb{R}\\), with endpoints \\(0\\) and \\(n+m\\). Thus it is the unique lifting of \\(f *g\\) starting at \\(0\\). This means \\(\\varphi([f]*[g]) = m+n = \\varphi([f])+\\varphi([g])\\)</p>"},{"location":"S%5E1/#lemma-1","title":"Lemma 1:","text":"<pre><code>title:\nThere is no retraction of $B^{2}$ to $S ^{1}$. Here, $B^{2}$ denotes the closed unit disk $\\{ (x,y) \\mid \\ x ^{2}+y^{2} \\le 1 \\} \\subset \\mathbb{R}^{2}$.\n</code></pre>"},{"location":"S%5E1/#proof_1","title":"Proof:","text":"<p>Look at Retractions.</p>"},{"location":"S%5E1/#lemma-2","title":"Lemma 2:","text":"<pre><code>title:\nLet $h : S ^{1} \\to X$ cts. Then the following are equivalent.\n\n(1) $h$ is null homotopic.\n\n(2) $h$ extends to a cts map $k: B ^{2} \\to X$\n\n(3) $h_{*} : \\Pi_{1}(S ^{1}) \\to \\Pi_{1}(X)$ is the trivial homomorphism.\n</code></pre>"},{"location":"S%5E1/#proof_2","title":"Proof:","text":"<p>(1) \\(\\implies\\) (2) \\(H : S ^{1} \\times I \\to X\\)  - homotopy between \\(h\\) and a constant map.  Define \\(\\Pi : S ^{1} \\times I \\to B ^{2}\\), \\(\\Pi(x,t) = (1-t)x\\) Check that \\(\\Pi\\) is a quotient map. \\(S ^{1} \\times \\{ 1 \\}\\) collapses to the point of origin. Homeomorphism on \\(S ^{1} \\times [0,1) \\to B ^{2} \\setminus \\{ (0,0) \\}\\) Let \\(\\sim\\) denote the equivalence relation given by \\(\\Pi \\implies H\\) is constant on the fibers \\(\\implies \\phi : (S ^{1} \\times I)/\\sim \\to X\\), i.e, \\(\\exists k:B ^{2} \\to X\\) s.t. \\(H = k \\circ\\Pi\\) Since \\(\\Pi\\) maps \\(S ^{1} \\times \\{ 0 \\}\\) to \\(S ^{1} = \\partial B ^{2}\\). \\(k |_{S ^{1}} = H|_{S ^{1} \\times \\{ 0 \\}} = h\\).</p> <p>(2) \\(\\implies\\) (3) If \\(h = k|_{S ^{1}}\\) then \\(h = k \\circ i\\) where \\(i : S ^{1} \\hookrightarrow B ^{2}\\) is the inclusion map. \\(h_{*} = k_{*}\\circ i_{*}\\), but \\(\\Pi_{1}(B ^{2}) = 1\\) then \\(k_{*}\\) trivial \\(\\implies h_{*}\\) is trivial.</p> <p>(3) \\(\\implies\\) (1)  Take the map $$ I \\xrightarrow{f} S ^{1} \\xrightarrow{h}X $$ Where \\(f\\) is just \\(f(x) = (\\cos 2\\pi x,\\sin 2\\pi x)\\), so \\(h_{*}([f]) = [e_{x_{0}}]\\) since \\(h_{*}\\) is trivial. This means that \\(h \\circ f \\simeq_{p} e_{x_{0}}\\) where \\(e_{x_{0}}\\) just denotes the constant path at \\(x_{0}\\). This means there is a homotopy \\(H : I \\times I \\to X\\) such that \\(H(x,0) = h \\circ f (x)\\) and \\(H(x,1) = x_{0}\\). Since \\(H\\) takes equal value on \\(\\{ 0 \\} \\times I\\) and \\(\\{ 1 \\} \\times I\\), I can define \\(\\widetilde{H} : S ^{1} \\times I \\to X\\) with \\(H(x,0) = h \\circ \\widetilde{f}\\) where \\(\\widetilde{f}\\) is just \\(f\\) but on \\(S ^{1}\\) which is actually just \\(id_{S ^{1}}\\), and \\(H(x,1) = x_{0}\\). and so, \\(h \\circ \\widetilde{ f} \\simeq_p x_{0}\\)  But notice that \\(\\widetilde{f}\\) is just \\(id_{S ^{1}}\\). And so, \\(h \\simeq_{p} x_{0}\\).</p>"},{"location":"S%5E1/#corollary","title":"Corollary","text":"<pre><code>title:\n$f: S ^{1} \\to \\mathbb{R}^{2}\\setminus \\{ (0,0) \\}$ is not null homotopic.\n</code></pre>"},{"location":"S%5E1/#proof_3","title":"Proof:","text":"<p>\\(r(x) = \\frac{x}{|x|}\\) a retraction of \\(\\mathbb{R}^{2}\\setminus \\{ (0,0) \\}\\) to \\(S ^{1}\\). \\(r \\circ i = id_{S ^{1}} \\implies r_{*} \\circ i_{*} = id_{\\Pi_{1}(S ^{1})}\\) \\(\\implies i_{*}\\) is non trivial \\(\\implies i\\) is not null homotopic.</p>"},{"location":"S%5E1/#corollary_1","title":"Corollary","text":"<pre><code>title:\n$id: S ^{1} \\to S ^{1}$ is not null homotopic\n</code></pre>"},{"location":"S%5E1/#proof_4","title":"Proof:","text":"<p>\\(id_{*} = id:\\mathbb{Z} \\to \\mathbb{Z}\\) is non trivial.</p>"},{"location":"S%5E1/#theorem_1","title":"Theorem:","text":"<pre><code>title:\nIf $h : S^1 \\to S^1$ is continuous and antipode preserving, then $h$ is not null homotopic.\n</code></pre>"},{"location":"S%5E1/#proof_5","title":"Proof:","text":"<p>We show that \\(h_{*} : \\Pi_{1}(S ^{1}) \\to \\Pi_{1}(S ^{1})\\) is a non trivial homomorphism. Let \\(b_{0} = (1,0)\\) and let \\(h(b_{0}) = a_{0}\\). If \\(a_{0} = b_{0}\\), continue with original \\(h\\), otherwise choose \\(r_{\\theta}: S ^{1}\\to S ^{1}\\) s.t. \\(r_{\\theta}(a_{0})=b_{0}\\), redefine \\(h:= r_{\\theta} \\circ h\\).</p> <p>Consider \\(q : S ^{1} \\to S ^{1}\\), \\(q(z) = z ^{2}\\). \\(q\\) is a closed, continuous surjection, in fact a covering map.</p> <p>\\(\\implies\\) The inverse image of any point \\(w\\) on \\(S ^{1}\\) under \\(g\\) consists of two antipodal points, \\(z\\) and \\(-z\\). \\(\\implies\\) In particular, \\(q \\circ h\\) is a map that is constant on each of the preimages \\(q ^{-1}(w)\\).</p> <pre><code>\\usepackage{tikz-cd} \n\\begin{document} \n\\begin{tikzcd} \u00a0 \u00a0 \nS^1\u00a0 \u00a0 \\arrow[r,\"h\"] \\arrow[dr,\"q \\circ h\"] \\arrow[d,\"q\"] &amp; S ^1\\arrow[d,\"q\"] \\\\\nS^1  \\arrow[r,\"k\"] &amp; S^1\n\\end{tikzcd}\n\\end{document}\n</code></pre> <p>\\(\\implies\\) This induces \\(k : S ^{1}\\to S ^{1}\\). \\(k \\circ q = q\\circ h\\) \\(q(b_{0}) =  h(b_{0}) = b_{0} \\implies k(b_{0}) = b_{0}\\) as well, \\(k(-b_{0}) = -b_{0}\\).</p> <ul> <li>To show \\(k_{*}: \\Pi_{1}(S ^{1}) \\to \\Pi_{1}(S ^{1})\\) is non trivial</li> <li>\\(q\\) is a covering map. Note that if \\(\\widetilde{f}\\) is any path in \\(S ^{1}\\), from \\(b_{0}\\) to \\(-b_{0}\\) then \\(f := q \\circ \\widetilde{f}\\) is a loop that corresponds to a non trivial element in \\(\\Pi_{1}(S ^{1}, b_{0})\\). (end point != beginning point)</li> <li> <p>Claim: \\(k_{*}([f])\\) is non trivial \\(k_{*}([f]) = [k \\circ q\\circ \\widetilde{f}] = [q \\circ h \\circ \\widetilde{f}]\\) \\(h \\circ \\widetilde{f}\\) is a path in \\(S ^{1}\\) from \\(b_{0}\\) to \\(-b_{0}\\) so that \\([q \\circ (h\\circ \\widetilde{f})]\\) is not trivial.</p> </li> <li> <p>\\(k_{*}\\) is injective.   Now \\(q: S ^{1} \\to S ^{1}\\) is a degree 2 cover. \\(q_{*} : \\Pi_{1}(S ^{1}) \\to \\Pi_{1}(S ^{1})\\) is multiplication by 2.   \\(q_{*} \\circ h_{*} = k_{*} \\circ q_{*}\\) is injective, hence \\(h_{*}\\) is injective as well.</p> </li> </ul>"},{"location":"S%5E1/#references","title":"References","text":"<p>Lifts Fundamental Group Retractions</p>"},{"location":"Sankalp%20Talk%20-%20Ax%20Grothediek%20Raw/","title":"Sankalp Talk   Ax Grothediek Raw","text":""},{"location":"Sankalp%20Talk%20-%20Ax%20Grothediek%20Raw/#basics","title":"Basics","text":"<pre><code>title: theory\nA *Theory* is a collection of true *sentences* about certain *structures* that staisfy some axioms.\n</code></pre> <p>[!note] theore A Theory is a collection of true sentences about cretain structures that satisfy some axioms.</p> <p>First Order Logic is a good Language to frame these sentences in.</p>"},{"location":"Sankalp%20Talk%20-%20Ax%20Grothediek%20Raw/#compactness","title":"Compactness","text":"<p>Given a language \\(\\mathcal L\\) , and theory \\(\\mathcal T\\) and a sentence \\(\\phi\\). We say \\(T\\vdash\\phi\\) means that \\(\\phi\\) can be proven using \\(T\\).</p> <p>We say \\(\\phi\\) is a consequenc of \\(T\\) if all models of \\(T\\) satisfy \\(\\phi\\) </p> <p>Godel's completeness theorem states: $$ \\mathcal T \\vDash \\phi \\iff \\mathcal T\\vdash\\phi $$ <pre><code>title: Compactness\nIf $\\mathcal T$ is a theory such that every finite subset is satisfiable, then $T$ is satisfiable\n</code></pre></p> <p>this is called compactness because we can say that the in the power set of \\(T\\), the collections of all subsets of \\(\\mathcal T\\) that contain \\(\\varphi\\) is an open set. Then we just have that the space is compact. This relates to the Stone's Representation Theorem.</p> <p>In the language of rings, the definable sets are varieties.</p>"},{"location":"Sankalp%20Talk%20-%20Ax%20Grothediek%20Raw/#theory-of-algebraically-closed-fields","title":"Theory of Algebraically Closed Fields","text":"<p>The language is the language of Ring theory \\(\\langle \\cdot,+,0,1\\rangle\\)  Axioms - \\(\\exists 0\\) - \\(\\exists 1\\) - \\(\\forall a_0\\dots a_{n} \\exists x (x^n +a_{1}x^n-1 \\dots a_{0}=0)\\)</p> <pre><code>title: Theorem\nAny two uncountable algebraically closed fields of the same characteristic are isomorphic $\\iff$ Their cardinality is the same.\n</code></pre> <p>A theory \\(\\mathcal T\\) is called \\(\\kappa-\\)categorical if all models of size \\(\\kappa\\) are isomorphism</p> <pre><code>title:Theorem\nAlgebraically Closed Fields of a given characteristic are complete.\nThat is all models satisfy sentences, or non of the models satisfy the sentences.\n</code></pre> <p><pre><code>title:theorem\nEvery injective polynomial mapping from $\\mathbb C^n$ to $\\mathbb C^n$ is also surjective.\n</code></pre>  The point is you write the above statement in the language, say \\(\\Phi_{n,d}\\) states that for a fixed \\(n\\) and \\(d\\)  Now  <pre><code>Read the Lecture notes.\n</code></pre></p>"},{"location":"Satisfiability%20in%20First%20Order%20Logic/","title":"Satisfiability in First Order Logic","text":"<p>202309300009</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"Satisfiability%20in%20First%20Order%20Logic/#satisfiability-in-first-order-logic","title":"Satisfiability in First Order Logic","text":"<pre><code>title: Goal\nWhen do we know if set $X\\subseteq\\Phi_{L}$ of sentences in a [First Order Language](&lt;./Syntax of First Order Logic.md#first-order-languages&gt;) have a corresponding [First Order Structure](&lt;./Semantics of First Order Logic.md#first-order-structures&gt;) $\\mathcal M$ such that for each $\\varphi\\in X,\\mathcal M\\models\\varphi$.\nIn other words, when do we say that a set is *Satisfiable*.\n</code></pre> <p>Henkin's Solution to the question reduces the question of [[Satisfiability in Propositional Logic]]</p> <pre><code>title: Approaches that do not work\nJust replacing all the Formulas that cannot be modeled with propositional logic using new propositional variables does not directly work.\n\nFor example\n$$\n\\exists x\\ r(x)\\land\\forall x\\ r'(x) \n$$\nwhere $r'$ is defined to be $\\lnot r$\n\nHere putting two different propositional variables in case of the two quantified terms gives a sentence that is satisfiable, even though the First Order Sentence is clearly not.\n</code></pre> <p>Henkin's approach is to keep expanding the language by adding more constants to it to eventually replace the quantified formulas. This will also blow up the size of \\(X\\) because we need to check for more formulas, but then we would be able to use Propositional Satisfiability Instead.</p> <pre><code>title: Plan\nTo replace all _Prime Formulas_ by propositional variables, but also add extra valid formulas to provide more information about the structure without altering the satifiability.\n\n- Case: Equality\n  For every pair of equality $X\\equiv Y$ and $Y\\equiv Z$ we add $X\\equiv Y\\land Y\\equiv Z\\rightarrow X\\equiv Z$ to add information about transitivity of Equality\n- $\\exists x\\; \\varphi(x)$\n  This formula means that there is a _witnessing term_ $t$ that satisfies $\\varphi$, so we can add the formula $\\exists x\\; \\varphi(x)\\rightarrow\\varphi(t)$\n- $\\lnot\\exists x\\; \\varphi(x)$ means that no valuation of $x$ satisfies $\\varphi$, so we can safely add $\\lnot\\exists x\\;\\varphi(x)\\rightarrow\\lnot\\varphi(t)$ for any arbitrary $t$\n\nThe only place where we need to be careful is case 2, where we need to find a witnessing element, there might not be enough constants in out language to work as witnessing elments. So we need to systematically add elements to our language.\n</code></pre> <p>Let the language be \\(L=\\langle R, F, C\\rangle\\).  We add new constants in steps, and name the sets of constants added \\(C_{0},C_{1}\\dots\\) and change the language along with it as \\(L_{0},L_{1}\\dots\\) - Let \\(C_{0}=\\emptyset\\) and \\(L_0=L\\). - As we add new constants, we can build new formulas, so we need to keep adding more constants. Say we just added a set of constants. Then for all new formulas, where we have \\(1\\) free variable, we add a corresponding constant, so for \\(\\varphi(x)\\) we add \\(c_{\\varphi(x)}\\). The set of new constants generated by \\(\\Phi_{L_{n}}\\setminus\\Phi_{L_{n-1}}\\) is called \\(C_{n+1}\\). Let \\(C_{H}=\\bigcup_{i\\ge 0}C_{i}\\) and let \\(L_{H}=\\langle R, F, C\\cup C_{H}\\rangle\\).</p> <p>With the modified \\(L_{H}\\) we get more machinery to work with - The quantifier axioms which is \\(\\varphi(t)\\rightarrow\\exists x\\;\\varphi(x)\\) which is valid in all languages - The Henkin axioms which are sentences of \\(L_H\\) of the form \\(\\exists x\\; \\varphi(x)\\rightarrow\\varphi(c_{\\varphi(x)})\\) </p> <pre><code>I think the quantifier axioms are important because they let us use non quatifier formula for modus ponens which requires quantifier formulas.\n</code></pre> <p>The Henkin axioms are not automatically true, we need to carefully interpret the witnessing constant for it to be true.</p> <p>Let \\(\\Phi_{H}\\) bet the set of all Henkin Axioms and \\(\\Phi_{Q}\\) be the set of all quantifier axioms.</p> <p>Then we add the equality axioms which are  $$ \\begin{matrix} &amp;&amp;t\\equiv t \\  t\\equiv u &amp; \\rightarrow &amp; u\\equiv t \\  t\\equiv u\\land u\\equiv v &amp; \\rightarrow  &amp; t\\equiv v \\  (t_{1}\\equiv u_{1}\\land t_{2}\\equiv u_{2}\\land\\cdots\\land t_{n}\\equiv u_{n})  &amp; \\rightarrow &amp; f(t_{1},t_{2},\\dots,t_{n})\\equiv f(u_{1},u_{2},\\dots u_{n}) \\  (t_{1}\\equiv u_{1}\\land t_{2}\\equiv u_{2}\\land\\cdots\\land t_{n}\\equiv u_{n}) &amp; \\rightarrow &amp; (r(t_{1},t_{2},\\dots,t_{n})\\rightarrow r(u_{1},u_{2},\\dots,u_{n})) \\end{matrix} $$</p> <p>Let \\(\\Phi_{\\text{Eq}}\\) be the set of all equality axioms. They are valid in all interpretations. </p> <p>Now with all the machinery built, we can prove the following claim.</p> <pre><code>title: First Order Satisfiability\nThe following statements are equivalent:\n1. There is an $L-$ interpretation $\\mathcal I=(\\mathcal M,\\sigma)$ which satifies $X$\n2. There is an $L_{H}-$ interpretation $(\\mathcal M,\\sigma)$ which sastifies $X$\n3. $X\\cup \\Phi_{H}\\cup\\Phi_{Q}\\cup\\Phi_{\\text{Eq}}$ is propositionally satisfiable.\n</code></pre> <p>2 implies 1 is immediate. Given an interpretation which satisfies \\(X\\) in \\(L_{H}\\) we just drop definition of constants that are used in \\(L_{H}\\) which are not in \\(L\\) as \\(X\\) does not use any of those. This gives an interpretation in \\(L\\) which satisfies \\(X\\).</p> <p>1 implies 3. Let \\(\\mathcal I\\) be an \\(L-\\)Interpretation which satisfies \\(X\\). We can now construct an \\(L_{H}-\\)Interpretation for \\(X\\) which agrees with the \\(L-\\)Interpretation and for every formula \\(\\varphi(x)\\) which is satisfiable, we set \\(c_{\\varphi(x)}\\) to be the value when \\(x\\) is replaced with the witnessing element and arbitrarily otherwise. This now also satisfies all formulas in \\(\\Phi_{H}\\), and hence there exists a corresponding propositional valuations which makes it propositionally satisfiable.</p> <p>3 implies 2. Given that \\(X\\cup \\Phi_{H}\\cup\\Phi_{Q}\\cup\\Phi_{\\text{Eq}}\\)'' is propositionally satisfiable, we need to construct an \\(L_{H}-\\)Interpretation that satisfies it.</p> <ul> <li>We define the universe to be the set of terms over \\(L_{H}\\) quotiented by the equivalence relation \\(\\equiv\\).</li> <li>We define the relations \\(r\\) by \\(r^{\\mathcal{M}}=\\{\\langle[t_{1}],[t_{2}],\\dots,[t_{n}]\\rangle\\;:\\; v\\models r(t_{1},t_{2},\\dots t_{n})\\}\\).</li> <li>We define all functions \\(f\\) by \\(f^{\\mathcal{M}}([t_{1}],[t_{2}],\\dots [t_{n}])=[f(t_{1},t_{2},\\dots,t_{n})]\\).</li> <li>For all constants \\(c\\) we do \\(c^{\\mathcal{M}}=[c]\\).</li> <li>All variables are also terms, we do \\(\\sigma(x)=[x]\\).</li> </ul> <p>where \\([x]\\) represents the corresponding element in the universe.</p> <p>This proves satisfiability for all atomic formulas. For the rest of the formulae, we do induction. Trivial for \\(\\varphi=\\lnot\\psi\\) and \\(\\varphi=\\psi_{1}\\lor\\psi_{2}\\). If the formula is of the type \\(\\varphi=\\exists x\\;\\psi(x)\\) then there is a term \\(t_s\\) which corresponds to the witnessing terms of \\(\\varphi\\). therefore if \\((\\mathcal M,\\sigma)\\models\\varphi\\) then \\((\\mathcal M,\\sigma)\\models \\psi(t_{s})\\). By induction we have \\(\\nu\\models \\psi(t_{s})\\) and we have the quantifier axiom, we get \\(\\nu \\models\\varphi\\). Conversly if \\(v\\models \\varphi\\) we have the henkin axiom which gives us \\(\\varphi\\rightarrow \\psi(c_{\\psi(x)})\\) and then we get \\(\\nu\\models\\psi(c_{\\psi(x)})\\) and from the sematics of quantifier \\(\\exists\\), we get \\(\\mathcal I\\models \\exists x\\;\\psi(x)\\).</p>","tags":["Note","Incomplete"]},{"location":"Satisfiability%20in%20First%20Order%20Logic/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Second%20Countability/","title":"Second Countability","text":"<p>202302082202</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Second%20Countability/#second-countability","title":"Second Countability","text":""},{"location":"Second%20Countability/#title-a-topo-space-x-is-called-second-countable-if-exists-a-ctble-basis-mathcalb-on-x-that-generates-tau_x","title":"<pre><code>title:\nA topo space $X$ is called second countable if $\\exists$ a ctble basis $\\mathcal{B}$ on X that generates $\\tau_X$.\n</code></pre>","text":""},{"location":"Second%20Countability/#related-results","title":"Related Results","text":"<p>1) Regular + 2nd Countable \\(\\implies\\) Normal 2) Metrizable + Separable \\(\\implies\\) 2nd Countable 3) Metrizable + Lindelof \\(\\implies\\) 2nd Countable 4) 2nd countable \\(\\implies\\) Lindelof</p>"},{"location":"Second%20Countability/#related-problems","title":"Related Problems","text":"<p>1) Find a 2nd ctble space which has a quotient that's not 2nd ctble.    #### Example:    Take the equiv relation \\(x \\sim y\\) iff \\(x,y \\in \\mathbb{Z}\\) or \\(x=y\\). This is not first countable hence not 2nd countable. Not first countable around the point \\([0]\\) in \\(\\mathbb{R}/\\sim\\). </p>"},{"location":"Second%20Countability/#references","title":"References","text":""},{"location":"Semantics%20for%20LTL/","title":"Semantics for LTL","text":"<p>202311172211</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"Semantics%20for%20LTL/#semantics-for-ltl","title":"Semantics for LTL","text":"<p>The semantics of an LTL formula is defined by a satisfaction relation  $$ \\models\\; \\subseteq(\\mathbb N\\to 2^\\mathcal P)\\times\\mathbb N\\times LTL $$</p> <p>To say\\((\\sigma, j, f) \\in \\models\\) we write \\((\\sigma, j)\\models f\\) </p> <p>We can think of this as a sequence of worlds like a kripke structure where \\(\\sigma\\) defines which world models which proposition and \\(j\\) fixes a world to talk about \\(f\\).</p>","tags":["Note","Incomplete"]},{"location":"Semantics%20for%20LTL/#propositions","title":"Propositions","text":"<p>$$ (\\sigma, j)\\models p\\quad\\text{iff}\\quad p \\in\\sigma(j) $$ </p>","tags":["Note","Incomplete"]},{"location":"Semantics%20for%20LTL/#next","title":"Next","text":"<p>$$ (\\sigma, j)\\models \\bigcirc p\\quad\\text{iff}\\quad (\\sigma,j+1) \\models p $$ </p>","tags":["Note","Incomplete"]},{"location":"Semantics%20for%20LTL/#henceforth","title":"Henceforth","text":"<p>$$ (\\sigma, j)\\models \\square p\\quad\\text{iff}\\quad \\forall k\\cdot(k \\geq j \\Rightarrow (\\sigma,k)\\models p) $$ </p>","tags":["Note","Incomplete"]},{"location":"Semantics%20for%20LTL/#eventually","title":"Eventually","text":"<p>$$ (\\sigma, j)\\models \\diamond p\\quad\\text{iff}\\quad \\exists k\\cdot(k \\geq j \\Rightarrow (\\sigma,k)\\models p) $$ </p>","tags":["Note","Incomplete"]},{"location":"Semantics%20for%20LTL/#until","title":"Until","text":"<p>$$ (\\sigma, j)\\models f\\cup g\\quad\\text{iff}\\quad \\exists k \\cdot(k\\ge j \\Rightarrow(\\sigma,k)\\models g\\quad \\land\\quad \\forall i \\cdot(j \\leq i &lt; k \\Rightarrow (\\sigma, i) \\models f) $$ </p>","tags":["Note","Incomplete"]},{"location":"Semantics%20for%20LTL/#boolean-operators","title":"Boolean operators","text":"<p>These are defined in the usual manner - \\((\\sigma, j)\\models\\lnot f\\iff(\\sigma, j)\\not\\models f\\) - \\((\\sigma, j)\\models f \\lor g\\iff(\\sigma, j)\\models f \\lor(\\sigma,j)\\models g\\) - \\((\\sigma, j)\\models f \\land g\\iff(\\sigma, j)\\models f \\land(\\sigma,j)\\models g\\) - \\((\\sigma, j)\\models f \\Rightarrow g\\iff(\\sigma, j)\\models f \\Rightarrow(\\sigma,j)\\models g\\)</p>","tags":["Note","Incomplete"]},{"location":"Semantics%20for%20LTL/#references","title":"References","text":"<ul> <li>Linear Temporal Logic</li> <li>Probabilistic Semantics for LTL</li> <li>Topological Semantics for LTL</li> <li>Topology over Finite Paths in a Timed Automata</li> </ul>","tags":["Note","Incomplete"]},{"location":"Semantics%20of%20First%20Order%20Logic/","title":"Semantics of First Order Logic","text":"<p>202309211509</p> <p>Tags : [[Logic]]</p>","tags":["Note"]},{"location":"Semantics%20of%20First%20Order%20Logic/#semantics-of-first-order-logic","title":"Semantics of First Order Logic","text":"<p>To give meaning to our formulas, we first fix a set \\(S\\), which we call the Universe. And we then interpret our formulas in terms what we have in the universe. To do that we have an interpretation function \\(\\iota\\). Together they make a First Order Structure.</p> <pre><code>title:History: Tarskian Semantics\nIn 1935, Alfred Tarski attempeted for formulate a new theory of _Truth_ in order to resolve the Liar's paradox.\n\nThe idea to formulate linguistic theoreis without paradoxes like the Liar's paradox, it is generally necessary to distinguish the language that one is talking about (Semantics/object language) with the language one is using to talk (Syntax/ metalanguage).\n\nTarski's thoery of truth demanded that the object language must be contained in the meta language. For example \n- 'Snow is white' is true if and only if snow is white.\nseem trivial because the object language is the same as the metalanguage and is called a 'T' sentence.\n- 'Schnee ist wei\u00df' is true if and only if snow is white.\nhas its object language is German, and metalanguage is English. \n\nThe original formulation involved in inductive definition and only applied to formal languages, like **First Order Logic**.\n</code></pre>","tags":["Note"]},{"location":"Semantics%20of%20First%20Order%20Logic/#first-order-structures","title":"First Order Structures","text":"<p>Given a First Order Language \\(L\\), A First Order Structure \\(L\\) is a pair \\(\\mathcal M=(S,\\iota)\\) where \\(S\\) is a non-empty set and \\(\\iota\\) a function defined over \\(R\\sqcup F\\sqcup C\\) such that - For each relation symbol \\(r\\in R\\) with arity \\(n\\), we have \\(\\iota(r)\\subseteq S^{n}\\) - For each function symbol \\(f\\in F\\) with arity \\(n\\), we have \\(\\iota(f):S^{n}\\to S\\) - For each constant \\(c\\in C\\) we have \\(\\iota(c)\\in S\\).</p> <p>For more readability we denote \\(\\iota(x)\\) as \\(x^{\\mathcal M}\\). That we have enough structure for our First Order Formulas we can build an interpretation for it. This corresponds to Evaluations for propositional logic</p>","tags":["Note"]},{"location":"Semantics%20of%20First%20Order%20Logic/#interpretation","title":"Interpretation","text":"<p>An Interpretation of \\(L\\) is the tuple \\(\\mathcal I=(\\mathcal M,\\sigma)\\) where \\(\\mathcal M\\) is a First Order Structure and \\(\\sigma:Vars\\to S\\) is an assignment of elements of \\(S\\) to variables.</p> <p>Given any \\(\\sigma\\), we denote by \\(\\sigma[x_{1}\\to s_{1},\\dots x_{n}\\to s_{n}]\\) the modified assignment \\(\\sigma'\\) where  $$ \\sigma'(x) = \\left{ \\begin{align} \\text{corresponding }s_{i}&amp;&amp;x\\in{x_{1},x_{2}\\dots x_{n}}\\ \\sigma(x) &amp;&amp;\\text{otherwise} \\end{align} \\right. $$ Similarly \\(\\mathcal I[x\\to s]=(\\mathcal M,\\sigma[x\\to s])=I'\\)</p> <p>Now, Given an interpretation \\(\\mathcal I\\), each term \\(t\\) over \\(L\\) maps to a unique element in \\(S\\). - If \\(t\\) is a constant \\(c\\in C\\), \\(t^{\\mathcal I}=c^{\\mathcal M}\\). - If \\(t\\) is a variable \\(x\\in Var\\), \\(t^{\\mathcal I}=\\sigma(x)\\) - If \\(t\\) is of the form \\(f(t_{1}, t_{2}\\dots t_{n})\\) where \\(f\\in F\\), then \\(t^{\\mathcal I}=f^{\\mathcal M}(t_{1}^{\\mathcal I},t_{2}^{\\mathcal I}\\dots t_{n}^{\\mathcal I})\\)</p>","tags":["Note"]},{"location":"Semantics%20of%20First%20Order%20Logic/#satisfactory-interpretation","title":"Satisfactory Interpretation","text":"<p>A Satisfactory Interpretation corresponds to Satisfying valuation for propositional logic. We say \\(\\mathcal I\\models \\varphi\\) (\\(\\mathcal I\\) satisfies \\(\\varphi\\)) if - \\(\\mathcal I\\models t_{1}\\equiv t_{2}\\) if \\(t_{1}^{\\mathcal I} = t_{2}^{\\mathcal I}\\) - \\(\\mathcal I\\models r(t_{1}\\dots t_{n})\\) if \\((t_{1}^{\\mathcal I}\\dots t_{n}^{\\mathcal I})\\in r^{\\mathcal M}\\) - \\(\\mathcal I\\models\\lnot\\varphi\\) if \\(\\mathcal I\\not\\models\\varphi\\) - \\(\\mathcal I\\models \\varphi\\lor\\psi\\) if \\(\\mathcal I\\models\\varphi\\) or \\(\\mathcal I\\models\\psi\\) - \\(\\mathcal I\\models \\exists x\\ \\varphi\\) if there is an element \\(s\\in S\\) such that \\(\\mathcal I[x\\to s]\\models\\varphi\\) </p> <p>And as usual, we say a formula is satisfiable if there exists a satisfactory interpretation for it. And a formula is valid if all Interpretations satisfy it.</p>","tags":["Note"]},{"location":"Semantics%20of%20First%20Order%20Logic/#free-variables","title":"Free Variables","text":"<p>The Quantifiers change the behavior of variables they interact with. For both \\(\\exists\\) and \\(\\forall\\) the variables they \"bind\" become independent of the Interpretation. Hence for any Interpretation \\((\\mathcal M, \\sigma)\\), \\(\\sigma\\) needs to only assign to the free variables. We build a function \\(FV\\) that gives the set of free variables in a formula, it can be defined as - If \\(\\varphi\\) is the atomic formula \\(r(t_{1}\\dots t_{n})\\) \\(FV(\\varphi)\\) is the set of variables in \\(\\{t_{1}\\dots t_{n})\\}\\) - If \\(\\varphi\\) is \\(t_{1}\\equiv t_{2}\\) then \\(FV(\\varphi)\\) is the set of variables in \\(\\{t_{1}, t_{2}\\}\\) - \\(FV(\\varphi) = FV(\\lnot\\varphi)\\) - \\(FV(\\varphi\\lor\\psi)=FV(\\varphi)\\cup FV(\\psi)\\) - \\(FV(\\exists x\\ \\varphi)=FV(\\varphi)\\setminus \\{x\\}\\)</p> <p>And Considering them we have the following: If \\(\\sigma\\) and \\(\\sigma'\\) agree on \\(FV(\\varphi)\\) then \\((\\mathcal M,\\sigma)\\models \\varphi\\) iff \\((\\mathcal M,\\sigma')\\models \\varphi\\)</p>","tags":["Note"]},{"location":"Semantics%20of%20First%20Order%20Logic/#logical-consequence","title":"Logical Consequence","text":"<p>Logical Consequence has the same meaning as that in Propositional logic. Given a set \\(X\\) of first order equations. We say \\(X\\models \\varphi\\) if for every \\(\\mathcal I\\) such that \\(\\mathcal I\\models X\\) we have \\(\\mathcal I\\models \\varphi\\).</p>","tags":["Note"]},{"location":"Semantics%20of%20First%20Order%20Logic/#references","title":"References","text":"<p>First Order Logic Syntax of First Order Logic</p>","tags":["Note"]},{"location":"Separable%20Extensions/","title":"Separable Extensions","text":"<p>202303071403</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Separable%20Extensions/#separable-elements","title":"Separable elements","text":"<pre><code>title:\nIf $\\alpha$ is algebraic over $K$, it is called _separable_ over $K$ if its minimal polynomial over $K$ is separable.\n</code></pre>"},{"location":"Separable%20Extensions/#separable-extensions","title":"Separable Extensions","text":"<pre><code>title:\nA finite extension $L/K$ is called separable if every element of $L$ is separable over $K$. Otherwise it is called inseparable.\n</code></pre>"},{"location":"Separable%20Extensions/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nLet $L /K$ be a finite extension with $[L:K] = n$ and $\\sigma : K \\to F$ be a field embedding.\n\n(a) The number of extensions of $\\sigma$ to an embedding $L \\to F$ is at most $n$.\n\n(b) If $L /K$ is inseparable, the number of extensions is strictly less than $n$.\n\n(c) If $L /K$ is separable, then there is a field $F' \\supset F$ such that the number of extensions of $\\sigma$ to an embedding $L \\to F'$ is equal to $n$.\n</code></pre>"},{"location":"Separable%20Extensions/#proof","title":"Proof:","text":"<p>(a) Proof proceeds by induction on \\(n\\). Let \\(\\alpha\\) be an element in \\(L \\setminus K\\), then \\(K(\\alpha)\\) is an intermediate field between \\(L\\) and \\(K\\). We show that the number of extensions of \\(\\sigma\\) to an embedding \\(K(\\alpha) \\to F\\) is at most \\([K(\\alpha) : K]\\), then by induction we will get that the number of extensions of the resulting embedding is at most \\([L:K(\\alpha)]\\) which will give the number of extensions of \\(\\sigma\\) to \\(L \\to F\\) is at most \\([L:K]\\) as desired.</p> <p>Let \\(\\pi_{\\alpha}\\) be the minimal poly of \\(\\alpha\\) over \\(K\\). Then \\(\\pi_{\\alpha}(\\alpha) = 0\\), this means that \\(\\sigma \\pi_{\\alpha}(\\sigma(\\alpha)) = 0\\) which implies that \\(\\sigma(\\alpha)\\) is a zero of \\(\\sigma \\pi_{\\alpha}\\) of which there are at most \\(\\mathrm{deg}(\\sigma \\pi_{\\alpha}) = \\mathrm{deg}(\\pi_{\\alpha}) = [K(\\alpha) : K]\\) many. And since the extension of \\(\\sigma\\) to \\(K(\\alpha)\\) is determined by its image on \\(\\alpha\\), we get that there are at most \\([K(\\alpha):K]\\) many extensions.</p> <p>(b) If \\(L/K\\) is inseparable, there is some element \\(\\alpha\\) of \\(L\\) whose minimal poly \\(\\pi_{\\alpha}(X)\\) has repeated roots, hence \\(\\sigma \\pi_{\\alpha}\\) has repeated roots, which means that the number of extensions of \\(\\sigma\\) are strictly less than the degree of \\(\\pi_{\\alpha}\\) and hence \\([K(\\alpha):K]\\).</p> <p>(c)</p>"},{"location":"Separable%20Extensions/#claim","title":"Claim:","text":"<p><pre><code>title:\nLet $L = K(\\alpha_{1},\\alpha_{2},\\dots,\\alpha_{n})$, and let $\\pi_{i}(X)$ be the minimal polynomial for $\\alpha_{i}$ in $K$ for each $i$.\n\nLet $F' /F$ be a field extension in which each $\\sigma \\pi_{i}$ splits completely. Then we claim that $\\sigma$ has $[L:K]$ extensions to embeddings of $L$ into $F'$.\n</code></pre> Look at \\(\\sigma\\) as an embedding of \\(K\\) into \\(F'\\) now. Consider \\(K(\\alpha_{1}) /K\\), the number of extensions of \\(\\sigma\\) to \\(K(\\alpha_{1})\\) = \\(\\mathrm{deg}(\\pi_{1}) = [K(\\alpha_{1}):K]\\). If \\(L = K(\\alpha_{1})\\) then we are done. Otherwise, \\(L = K(\\alpha_{1})(\\alpha_{2},\\dots,\\alpha_{n})\\) and pick an embedding \\(\\tau : K(\\alpha_{1}) \\to F'\\) out of the \\(\\mathrm{deg}(\\pi_{1})\\) extensions of \\(\\sigma\\). Now take \\(m_{i}(X)\\) to be the minimal polys of \\(\\alpha_{i}\\) over \\(K(\\alpha_{1})\\) for \\(i = 2,\\dots ,n\\). We know that \\(m_i | \\pi_{i}\\) and so, \\(\\tau m_{i} | \\tau \\pi_i = \\sigma \\pi_{i}\\). Since \\(\\sigma \\pi_{i}\\) is separable and splits completely in \\(F'\\), so does \\(\\tau m_{i}\\)</p> <p>This means that \\(L , K(\\alpha_{1})\\) and the \\(m_{i}\\)'s satisfy the condition in the claim and hence we get by induction, that \\(\\sigma\\) has \\([L : K(\\alpha_{1})]\\) extensions to \\(L\\). Hence we get there are \\([L:K(\\alpha_{1})][K(\\alpha_{1}):K]\\) extensions to \\(\\sigma\\).</p>"},{"location":"Separable%20Extensions/#theorem-2","title":"Theorem 2:","text":"<pre><code>title:\nLet $L /K$ be a finite extension and let $L = K(\\alpha_{1},\\alpha_{2},\\dots ,\\alpha_{n})$. Then $L/K$ is separable iff all the $a_{i}$'s are.\n</code></pre>"},{"location":"Separable%20Extensions/#proof_1","title":"Proof:","text":"<p>If all the \\(a_{i}\\)'s are separable over \\(K\\), then we can follow the proof of theorem 1 (c) with \\(F = K\\) and \\(\\sigma = id_{K}\\). So we get that there are \\([L:K]\\) extensions of \\(\\sigma\\) to embeddings of \\(L\\) into a field extension of \\(K\\). And so by Theorem 1 (b), we get that \\(L /K\\) cannot be inseparable. Hence \\(L /K\\) is separable.</p>"},{"location":"Separable%20Extensions/#corollary-21","title":"Corollary 2.1:","text":"<pre><code>title:\nIf $f(X) \\in K[X]$ is separable then a splitting field for $f$ over $K$ is separable over $K$.\n</code></pre>"},{"location":"Separable%20Extensions/#proof_2","title":"Proof:","text":"<p>Let \\(L /K\\) be a splitting field for \\(f\\). Then \\(L = K(\\gamma_{1},\\gamma_{2},\\dots,\\gamma_{n})\\) where \\(\\gamma_{i}\\) are the roots of \\(f\\). Therefore the \\(\\gamma_{i}\\)'s are separable over \\(K\\) and hence so is \\(L\\).</p>"},{"location":"Separable%20Extensions/#corollary-22","title":"Corollary 2.2:","text":"<pre><code>title:\nGiven any finite extension $L /K$, the set of all elements of $L$ that are separable over $K$ form a subfield.\n</code></pre>"},{"location":"Separable%20Extensions/#proof_3","title":"Proof:","text":"<p>Take two elements \\(\\alpha, \\beta\\) in \\(L\\) that are separable over \\(K\\), then \\(K(\\alpha,\\beta)\\) is a separable over \\(K\\). Thus \\(\\alpha \\pm \\beta\\), \\(\\alpha\\beta\\), \\(\\alpha^{-1}\\) are all separable. Hence the set of separable elements form a subfield.</p>"},{"location":"Separable%20Extensions/#theorem-3","title":"Theorem 3:","text":"<pre><code>title:\nIf $E/L/K$ is a tower of finite extensions, then $E/K$ is separable iff $E/L$ and $L/K$ are separable.\n</code></pre>"},{"location":"Separable%20Extensions/#proof_4","title":"Proof:","text":"<p>One direction is easy. Now suppose \\(E/L\\) and \\(L /K\\) are separable. Take a field embedding \\(\\sigma : K\\to F\\), then there is an extension \\(F' /F\\) s.t. \\(\\sigma\\) extends to \\([L : K]\\) embeddings of \\(L\\) to \\(F'\\). Now take any one of these, and that will extend to \\([E:L]\\) embeddings of \\(E\\) into an extension \\(F''/F'\\). This means that \\(\\sigma\\) extends to \\([E:K]\\) embeddings of \\(E\\) into \\(F'' /F\\). Thus \\(E/K\\) is separable by Theorem 1.</p>"},{"location":"Separable%20Extensions/#related-problems","title":"Related Problems","text":"<p>Primitive Element Theorem</p>"},{"location":"Separable%20Extensions/#references","title":"References","text":"<p>Extension Field Splitting Fields Separable Polynomials</p>"},{"location":"Separable%20Polynomials/","title":"Separable Polynomials","text":"<p>202303071403</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Separable%20Polynomials/#separable-polynomials","title":"Separable Polynomials","text":"<pre><code>title:\nA polynomial $f(x) \\in K[x]$ is called **separable** when it has distinct roots in a splitting field over $K$. If $f(x)$ has repeated roots in the splitting field, then it is called **inseparable**.\n</code></pre>"},{"location":"Separable%20Polynomials/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nA non zero polynomial in $K[X]$ is separable iff it is relatively prime to its derivative in $K[X]$.\n</code></pre>"},{"location":"Separable%20Polynomials/#proof","title":"Proof:","text":"<p>Let \\(f(X)\\) be a separable poly in \\(K[X]\\). Let \\(\\alpha\\) be a root of \\(f(X)\\), then \\(f(X) = (X-\\alpha)g(X)\\). Differentiating this we get \\(f'(X) = g(X) + (X-\\alpha)g'(X)\\). But since \\(g(\\alpha) \\neq 0\\) (\\(f\\) is separable and \\(\\alpha\\) is a root of \\(f\\)), we get that \\(f'(\\alpha) \\neq 0\\). Therefore, \\(f'\\) and \\(f\\) have no common roots and hence are relatively prime.</p> <p>Now suppose \\(f(X)\\) is NOT separable, then by definition of separability, it has a repeated root say \\(\\alpha\\). Then \\(f(X) = (X - \\alpha)^{2}g(X)\\), differentiating this we get \\(f'(X) = 2(X-\\alpha)g(X) + (X-\\alpha)^{2}g(X)\\), this has \\(\\alpha\\) as a root, and so \\(f\\) and \\(f'\\) are not relatively prime.</p>"},{"location":"Separable%20Polynomials/#lemma-1","title":"Lemma 1:","text":"<pre><code>title:\n$f(X) = X ^{n}-a$ is separable over $K$ ($a \\in K ^{\\times}$) iff $n\\neq 0$ in $K$.\n</code></pre>"},{"location":"Separable%20Polynomials/#proof_1","title":"Proof:","text":"<p>Let \\(f(X) = X ^{n}-a\\) where \\(a \\in K ^{\\times}\\). The derivative \\(f'(X) = nX ^{n-1}\\). This is zero if \\(n = 0\\) in \\(K\\), in which case \\((f,f') = f\\) and hence \\(f\\) is not separable. If \\(n \\neq 0\\), then \\((X ^{n}-a,nX ^{n-1}) = 1\\) since \\(X\\) does not divide \\(X ^{n}-a\\). This establishes the lemma.</p>"},{"location":"Separable%20Polynomials/#lemma-2","title":"Lemma 2:","text":"<pre><code>title:\nIf $K$ has characteristic $p$, and $g(X ^{p}) \\in K[X ^{p}]$ is an arbitrary polynomial in $X ^{p}$ and $c \\in K ^{\\times}$ then $g(X ^{p}) + cX$ is separable in $K[X]$.\n</code></pre>"},{"location":"Separable%20Polynomials/#corollary-1-to-theorem-1","title":"Corollary 1 to Theorem 1:","text":"<pre><code>title:\nIf $f(X) \\in K[X]$ is separable and $L \\supset K$ then every factor of $f(X)$ in $L[X]$ is also separable. An element in an extension of $L$ that is separable over $K$ is also separable over $L$.\n</code></pre>"},{"location":"Separable%20Polynomials/#proof_2","title":"Proof:","text":"<p>Let \\(g(X)\\) be a factor of \\(f(X)\\), say \\(f(X) = g(X)h(X)\\) in \\(L[X]\\).  Since \\(f(X)\\) is separable we can write \\(1 = f(X)u(X) + f'(X)v(X)\\) for some polynomials \\(u(X)\\) and \\(v(X)\\) in \\(K[X]\\).  Then \\(1 = (g(X)h(X))u(X) + (g(X)h'(X) + g'(X)h(X))v(X) = g(X)(h(X)u(X) + h'(X)v(X)) + g'(X)(h(X)v(X))\\).  The last expression shows a polynomial-linear combination of \\(g(X)\\) and \\(g'(X)\\) equals 1, so \\(g(X)\\) is separable. </p> <p>Suppose \\(\\alpha\\) is in an extension of \\(L\\) and it is separable over \\(K\\). Since its minimal polynomial in \\(L[X]\\) divides its minimal polynomial in \\(K[X]\\), separability of \\(\\alpha\\) over \\(K\\) implies separability of \\(\\alpha\\) over \\(L\\) by what we just showed above.</p>"},{"location":"Separable%20Polynomials/#corollary-2-to-theorem-1","title":"Corollary 2 to Theorem 1:","text":"<pre><code>title:\nLet $K$ and $L$ be fields. If $\\sigma : K\\to L$ is a field embedding, then a polynomial $f(X) \\in K[X]$ is separable iff $(\\sigma f)(X)$ is separable in $L[X]$.\n</code></pre>"},{"location":"Separable%20Polynomials/#proof_3","title":"Proof:","text":"<p>Let \\(f\\) be separable in \\(K[X]\\), then we have \\(u,v \\in K[X]\\) such that  $$ f(X)u(X) + f'(X)v(X) = 1 $$ This gives  $$ \\sigma f(X) \\sigma u(X) + \\sigma f'(X) \\sigma v(X) = \\sigma(1) = 1_{L} $$ This means that \\(\\sigma f\\) is separable in \\(L[X]\\).</p> <p>Now assume \\(f(X)\\) is inseparable, then \\(d = (f,f')\\) hence \\(\\sigma d = (\\sigma f,(\\sigma f)')\\) hence \\(\\sigma f\\) is inseparable.</p>"},{"location":"Separable%20Polynomials/#theorem-2","title":"Theorem 2:","text":"<pre><code>title:\nFor every field $K$, an irreducible polynomial in $K[X]$ is separable iff its derivative is non zero in $K[X]$.\n</code></pre>"},{"location":"Separable%20Polynomials/#note-this-means-that-if-mathrmchar-k-0-then-every-irreducible-poly-is-separable-and-if-mathrmchar-k-p-then-an-irreducible-poly-is-separable-iff-it-is-not-a-poly-in-x-p","title":"Note: This means that if \\(\\mathrm{char}\\ K = 0\\) then every irreducible poly is separable, and if \\(\\mathrm{char} \\ K = p\\), then an irreducible poly is separable iff it is NOT a poly in \\(X ^{p}\\).","text":""},{"location":"Separable%20Polynomials/#proof_4","title":"Proof:","text":"<p>Let \\(\\pi(X)\\) be irreducible in \\(K[X]\\), now \\(\\pi(X)\\) is inseparable iff \\((\\pi(X),\\pi'(X)) \\neq 1\\) which is equivalent to saying that \\(\\pi(X) | \\pi'(X)\\) since \\(\\pi\\) is irreducible. But since \\(\\mathrm{deg}(\\pi') &lt; \\mathrm{deg}(\\pi)\\), we get that \\(\\pi'(X) \\equiv 0 \\iff \\pi(X)\\) is inseparable.</p>"},{"location":"Separable%20Polynomials/#theorem-3","title":"Theorem 3:","text":"<pre><code>title:\nThe roots of an inseparable irreducible poly in characteristic $p$, all have multiplicity a common power of $p$.\n</code></pre>"},{"location":"Separable%20Polynomials/#related-problems","title":"Related Problems","text":"<p>Separable Extensions</p>"},{"location":"Separable%20Polynomials/#references","title":"References","text":"<p>Splitting Fields</p>"},{"location":"Separable%20Spaces/","title":"Separable Spaces","text":"<p>202303071503</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Separable%20Spaces/#separable-spaces","title":"Separable Spaces","text":""},{"location":"Separable%20Spaces/#related-results","title":"Related Results","text":"<p>1) Metrizable + Separable \\(\\implies\\) 2nd Countable</p>"},{"location":"Separable%20Spaces/#references","title":"References","text":""},{"location":"Separation%20Axioms/","title":"Separation Axioms","text":"<p>202301181801</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Separation%20Axioms/#separation-axioms","title":"Separation Axioms","text":""},{"location":"Separation%20Axioms/#motivation-quirky-behaviour-of-limits","title":"Motivation- Quirky Behaviour of Limits","text":"<p>Consider the topology \\(\\{\\emptyset, \\{a\\}, X\\}\\) on \\(X=\\{a, b\\}\\)  Here the smallest neighbourhood of \\(a\\) is \\(\\{a\\}\\), hence it a limit  point of the sequence \\(a,a,a,a,a\\dots\\) duh.</p> <p>But the only open neighbourhood of \\(b\\) is \\(\\{a, b\\}\\), so \\(b\\) is also a limit point of the sequence \\(a, a, a,a,a\\dots\\)</p> <p>To obtain unique limits for convergent sequences, we may assume that a topology is \"fine enough\" to separate points. Such additional hypothesis are called Separating axioms or Trennungsaxiome (German)</p> <p>Some of them being: - \\(T_0\\) or Kolmogorov  - \\(T_1\\) or Frechet - \\(T_2\\) or Housdorff </p>"},{"location":"Separation%20Axioms/#related-problems","title":"Related Problems","text":""},{"location":"Separation%20Axioms/#references","title":"References","text":""},{"location":"Sequent%20Calculus/","title":"Sequent Calculus","text":"<p>202310120910</p> <p>Tags : [[Logic]]</p>","tags":["Note"]},{"location":"Sequent%20Calculus/#sequent-calculus","title":"Sequent Calculus","text":"<pre><code>title: Gerhard Gentzen\nBig Induction Thingy\n\nThe guy introduced Sequent Calculus as a method to study [Natural Deduction](&lt;./Natural Deduction.md&gt;)\n\n[Hilbert Style Proofs](&lt;./Hilbert Style Proofs.md&gt;) are a mathematically simple model, while [Natural Deduction](&lt;./Natural Deduction.md&gt;) proofs are closer in spriti to teh proofs written by humans.\nThe goal of Sequent Calculus is to be a more adequate tool for proof construction than natural deductions by getting rid of elimination rules. This makes sure that if you read the proof bottom-up, the more complex formulas are always derived from more simpler ones.\n</code></pre> <pre><code>title:Definition\nA _sequent_ is a pair $(\\Gamma,\\Delta)$ where $\\Gamma$ and $\\Delta$ are finite sequences for formulae called _antecedent_ and _succedent_ respectively, we also write it as $\\Gamma\\vdash\\Delta$\n</code></pre> <p>The intuitive meaning of \\(\\Gamma\\vdash\\Delta\\) is that the conjunctions of all the formulas in \\(\\Gamma\\) implies the disjunction of all formulas in \\(\\Delta\\).</p> <p>The rules for Sequent Calculus are  <pre><code>title: Logics that can be expressed using Sequent Calculus\nAdding restriction or getting rid of the rules changes the logic that is being used by the calculus, for example\n- Enforcing that $|\\Delta|\\le 1$ makes this _intuitionistic_ logic\n- Enforcing that $|\\Delta|=1$ makes it _minimal_ logic\n- Enforcing that $|\\Delta|\\ge 1$ makes it _Pierce Type_ logic\n- Getting rid of the contraction rules makes it _linear_ logic and so on\n</code></pre></p>","tags":["Note"]},{"location":"Sequent%20Calculus/#references","title":"References","text":"","tags":["Note"]},{"location":"Sequentially%20Compact/","title":"Sequentially Compact","text":"<p>202303011803</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Sequentially%20Compact/#sequentially-compact","title":"Sequentially Compact","text":"<pre><code>title:\nA space $X$ is called _sequentially compact_ when every sequence in $X$ has a convergent subsequence.\n</code></pre>"},{"location":"Sequentially%20Compact/#proposition","title":"Proposition","text":"<pre><code>title:\nA first countable space is countably compact iff it is sequentially compact.\n</code></pre>"},{"location":"Sequentially%20Compact/#proof","title":"Proof:","text":"<p>TODO</p>"},{"location":"Sequentially%20Compact/#theorem","title":"Theorem","text":"<pre><code>title:\nLet $X$ be a metrizable space. Then $X$ is compact iff it is sequentially compact.\n</code></pre>"},{"location":"Sequentially%20Compact/#proof_1","title":"Proof:","text":"<ol> <li>First show that a sequential compact space satisfies the Lebesgue Number Lemma.</li> <li>Then show that a sequentially compact space is totally bounded.</li> <li>Now consider an open cover of a sequentially compact space, it has lebesgue number \\(\\delta\\). Let \\(\\epsilon=\\frac{\\delta}{3}\\). Choose a finite covering of \\(X\\) by \\(\\epsilon\\) balls. Since each such ball has diameter less than \\(\\delta\\), there is some element of the open cover which covers this ball. This gives a finite subcover of our open cover. </li> </ol>"},{"location":"Sequentially%20Compact/#related-problems","title":"Related Problems","text":""},{"location":"Sequentially%20Compact/#references","title":"References","text":""},{"location":"Set%20Cover%20Problem/","title":"Set Cover Problem","text":"<p>202309281909</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note"]},{"location":"Set%20Cover%20Problem/#set-cover-problem","title":"Set Cover Problem","text":"<p>Universe \\(U=\\{e_{1},\\dots,e_{n}\\}\\) Family of subsets of \\(U\\), \\(\\mathcal{F}=\\{S_{1},\\dots,S_{i}\\}\\), \\(S_{i}\\subset U\\ \\forall i\\), \\(\\bigcup\\limits_{S_{i}\\in\\mathcal{F}}S_{i}=U\\). Cost \\(c: \\mathcal{F}\\to Q^{+}\\) Goal: Pick a minimum size subcollection \\(\\mathcal{F'}\\subset\\mathcal{F}\\) s.t. union of the collection is \\(U\\). - NP hardness follows because of reduction to this from vertex cover.</p> <pre><code>title: Greedy Algorithm\n$R:$ Set of elements covered so far\n$R=\\phi,\\mathcal{F'}=\\phi$\nwhile $R\\neq U$,\nFind the most cost-effective set $S$. ($S$ has min value of $\\frac{c(S)}{|S\\setminus R|}=\\alpha_{S}$), add to $\\mathcal{F}'$. (Resolve ties arbitrarily.) $R=S\\cup R$.\noutput $\\mathcal{F'}$.\n</code></pre> <p>Analysis: (For a specific rum) \\(S\\setminus R:\\) uncovered elements so far For each element in \\(S\\setminus R\\), set \\(\\text{price}(e)=\\alpha_{S}\\).</p> <p>\\(\\text{cost}(\\mathcal{F'})=\\sum\\limits_{i=1}^{n}\\text{price}(e_{i})=\\sum\\limits_{S\\in\\mathcal{F'}}c(S)\\).</p> <p>Arrange elements of \\(V\\) in the order in which they are covered in the algorithm, say \\(e_{1},\\dots,e_{n}\\). Observe: Before the iteration in which \\(e_{k}\\) is covered, there are \\(\\ge n-k+1\\) uncovered elements.</p> <pre><code>title:Price Upper Bound \n**Claim:** \n$$\n\\text{price}(e_{k})\\le\\frac{OPT}{n-k+1}\n$$\n\n*Proof:* $\\text{price}(e_{1})\\le\\frac{OPT}{n}$\n(It is equal if the first set is U. The other sets would have at least as much mean cost as the price of $e_{1}$.)\nAssume that we get the $k-1$ elements for free. This gives the bound using the same argument as above.\nWe can assume that because otherwise we'll have $OPT-x$ on the RHS which is only better!\n</code></pre> \\[ \\begin{align*} \\text{cost}(\\mathcal F') &amp;= \\sum\\limits_{k=1}^{n}\\text{price}(e_k)\\\\ &amp;\\le \\sum\\limits_{k=1}^{n}\\frac{\\text{cost}(OPT)}{n-k+1}\\\\ &amp;= \\text{cost}(OPT)\\sum\\limits_{i=1}^{n}\\frac{1}{i}\\\\ &amp;\\le \\text{cost}(OPT).\\log(n) \\end{align*} \\] <pre><code>title: Tightness example\nConsider $\\mathcal F=\\{S_{i}\\}.S_{1}=\\{1\\},S_{2}=\\{2\\},\\dots,S_{n}=\\{n\\},S_{n+1}=\\{1,2,\\dots,n\\}.$\n$$\nc(S_{i})= \\left\\{\n\\begin{align*}\n\\frac{1}{i}, &amp;&amp; 1\\le i\\le n\\\\\n1+\\epsilon, &amp;&amp; i=n+1\\\\\n\\end{align*}\n\\right.\n$$\n</code></pre> <p>Thus the analysis is tight and we get an \\(H_n-\\)approximation.</p>","tags":["Note"]},{"location":"Set%20Cover%20Problem/#references","title":"References","text":"<p>Approximation Algorithms Vertex Cover Problem</p>","tags":["Note"]},{"location":"Shortest%20Path%20using%20Primal-Dual/","title":"Shortest Path using Primal Dual","text":"<p>202309211309</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note"]},{"location":"Shortest%20Path%20using%20Primal-Dual/#shortest-path-using-primal-dual","title":"Shortest Path using Primal-Dual","text":"<ul> <li>\\(G = (V,E)\\), directed,</li> <li>non negative weight \\(w_{uv} \\forall (u,v) \\in E\\),</li> <li>two vertices \\(s,t\\) Goal: Find a min weight \\(s-t\\) path.</li> </ul>","tags":["Note"]},{"location":"Shortest%20Path%20using%20Primal-Dual/#lp","title":"LP","text":"","tags":["Note"]},{"location":"Shortest%20Path%20using%20Primal-Dual/#primal","title":"Primal","text":"<p>Variables \\(x_{uv}, (u,v) \\in E\\) Minimise \\(\\sum\\limits_{(u,v)\\in E}x_{uv}w_{uv}\\) s.t.:</p> <pre><code>title: One thing that doesn't work\n- $\\sum\\limits_{v|(s,v)\\in E}x_{sv}=1$\n- $\\sum\\limits_{v|(v,t)\\in E}x_{vt}=1$\n- $\\sum\\limits_{v|(u,v)\\in E}x_{uv} - \\sum\\limits_{w|(w,u)\\in E}x_{wu} = 0$ $\\forall u \\in V\\setminus\\{s,t\\}$\n- $x_{uv}\\geq 0$ $\\forall (u,v)\\in E$\n\nBecause for this LP, two disjoint cycles, one with $s$ and the other with $t$ is a valid solution.\n</code></pre> \\[\\sum\\limits_{v|(u,v)\\in E}x_{uv} - \\sum\\limits_{w|(w,u)\\in E}x_{wu} = \\begin{cases}     1, &amp; \\text{if } u=s \\\\     -1, &amp; \\text{if } u=t \\\\     0, &amp; \\text{otherwise.} \\end{cases}\\] <p>\\(x_{uv}\\geq 0\\) \\(\\forall (u,v)\\in E\\).</p>","tags":["Note"]},{"location":"Shortest%20Path%20using%20Primal-Dual/#dual","title":"Dual","text":"<p>Variables \\(y_v\\) \\(\\forall v\\in V\\) Maximise \\(y_s-y_t\\) s.t.: \\(y_u-y_{v}\\leq w_{uv}\\) \\(\\forall (u,v) \\in E\\)</p> <p>Do until \\(t\\) is included in \\(G'\\): 1. Start with \\(y_v=0\\) \\(\\forall v\\in V\\).     \\(x_{uv}=0\\) \\(\\forall (u,v)\\in E\\). 2. Increase \\(y_s\\), until one more dual constraint becomes tight. 3. Increase simultaneously all the dual variables involved in tight constraints.</p> <ul> <li>Primal solution: \\(x_{uv}=1\\) iff \\((u,v) \\in\\) primal solution</li> <li>Pruning: In \\(G'\\), keep one \\(s-t\\) path \\(P\\) and discard the remaining edges.</li> </ul> <pre><code>title: Observe\nThe primal dual algorithm is actually Dijkstra's algorithm.\n</code></pre> <pre><code>title: Combinatorial interpretation of the dual\n**Marble and string analogy-** How far apart can $s$ and $t$ be placed on a number line s.t. $\\text{dist}(u,v) \\leq w_{uv}$?\n</code></pre>","tags":["Note"]},{"location":"Shortest%20Path%20using%20Primal-Dual/#optimality","title":"Optimality","text":"<ul> <li>primal solution = \\(\\sum\\limits_{(u,v)\\in P}w_{uv}=\\sum\\limits_{(u,v)\\in P}(y_u-y_v)=y_s-y_t\\) = dual solution</li> </ul>","tags":["Note"]},{"location":"Shortest%20Path%20using%20Primal-Dual/#references","title":"References","text":"<p>Linear Programming</p>","tags":["Note"]},{"location":"Simple%20let%20expressions%20to%20Ordinary%20Lambda%20Calculus/","title":"Simple let expressions to Ordinary Lambda Calculus","text":"<p>202311072011</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note","Incomplete"]},{"location":"Simple%20let%20expressions%20to%20Ordinary%20Lambda%20Calculus/#simple-let-expressions-to-ordinary-lambda-calculus","title":"Simple let expressions to Ordinary Lambda Calculus","text":"<p>It is trivial to convert Simple let-expressions to Ordinary Lambda Calculus</p> <pre><code>let v = B in E     ===     (\\v.B) E\n</code></pre>","tags":["Note","Incomplete"]},{"location":"Simple%20let%20expressions%20to%20Ordinary%20Lambda%20Calculus/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Simply%20Connected/","title":"Simply Connected","text":"<p>202304031704</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Simply%20Connected/#simply-connected","title":"Simply Connected","text":"<pre><code>title:\nA topological space $X$ is **simply connected** if $X$ is path connected and for every $x_0 \\in X$, $\\Pi_1(X,x_0) = {1}$.\n</code></pre>"},{"location":"Simply%20Connected/#examples","title":"Examples:","text":"<ol> <li>Convex subsets of \\(\\mathbb{R} ^{n}\\)</li> <li>\\(S ^{n}\\) for \\(n \\ge 2\\).</li> </ol>"},{"location":"Simply%20Connected/#references","title":"References","text":""},{"location":"Simply%20Typed%20Lambda%20Calculus%20Syntax/","title":"Simply Typed Lambda Calculus Syntax","text":"<p>202308281408</p> <p>Type : #Note  Tags : [[Type Theory]] [[Lambda Calculus]]</p>"},{"location":"Simply%20Typed%20Lambda%20Calculus%20Syntax/#simply-typed-lambda-calculus-syntax","title":"Simply Typed Lambda Calculus Syntax","text":"<p>The types of the terms are simple propositions and implications,  Context Free Grammar for types is  $$ \\begin{matrix}\\Phi &amp; := &amp; p &amp; | &amp; \\varphi\\longrightarrow\\psi\\end{matrix} $$</p> <p>And there are two styles of typing lambda calculus: - Curry Style: - Church Style: The two styles are equally powerful.</p> <p>And we have the typability rules: $$ \\begin{matrix}(\\text{Ax}) &amp; \\Gamma,x:\\tau\\vdash x:\\tau \\ (\\text{App}) &amp; \\frac{\\Gamma\\vdash M:\\sigma\\to\\tau\\quad\\quad\\Gamma\\vdash N:\\sigma}{\\Gamma\\vdash MN:\\tau} \\ (\\text{Abs}) &amp; \\frac{\\Gamma, x:\\sigma\\vdash M:\\tau}{\\Gamma\\vdash\\lambda x.M:\\sigma\\to\\tau}\\end{matrix} $$</p> <p>^f1c6a3</p> <p>Some important questions related to this are: - Is a term Typable? - Can we compute a principle type, if the term is typable: Type Inference - Given a Type, are there terms that belong to this type: Type Inhabitance</p> <p>Examples: Curry Style: </p> <p>Church Style: </p>"},{"location":"Simply%20Typed%20Lambda%20Calculus%20Syntax/#references","title":"References","text":"<p>Not Unification Algorithm For Type Inference</p>"},{"location":"Simulation%20for%20Zone%20Automata/","title":"Simulation for Zone Automata","text":"<p>202310132310</p> <p>Tags : Timed Automata</p>","tags":["Note"]},{"location":"Simulation%20for%20Zone%20Automata/#simulation-for-zone-automata","title":"Simulation for Zone Automata","text":"<pre><code>In this approach we give a preorder between _symbolic states_ and we modify $\\text{Trans}$ rule to not add an elment $q$ if we have aleady reached an element $p$ such that $p\\succeq q$.\n\nWe the prove that this operation is finite, i.e, for all sequences $(q,Z_{1}), (q,Z_{2})\\dots$, we have $(q,Z_{j})\\preceq(q,Z_{i})$ if $i\\le j$.\nIf the preorder is finite, then the forward analysis always terminates.\n</code></pre> <p>A Simulations between two Symbolic States of the transition system \\(\\mathcal S\\) is a relation \\((q, v)\\preceq(q',v')\\) if - \\(q=q'\\) - If \\((q,v)\\rightarrow^{\\delta}(q,v+\\delta)\\rightarrow^{t}(q_{1},v_{1})\\) then there exists \\(\\delta'\\) such that \\((q,v')\\rightarrow^{\\delta'}(q,v'+\\delta')\\rightarrow^{t}(q_{1},v'_{1})\\) such that \\(v_{1}\\preceq v_{2}\\) Also shown by the diagram { width=\"400\" }</p> <p>The relation says, that if \\(a\\preceq b\\) then any transition that can be taken by \\(a\\) can also be taken by \\(b\\), but they might have to wait a different amount of time.</p> <p>We extend the following definition for sets of valuations \\(W\\) and \\(W'\\) We say \\((q, W)\\preceq(q', W')\\) if for every \\(a\\in W\\) there exist \\(a'\\in W'\\) such that \\((q, a)\\preceq(q, a')\\).</p> <p>Simulation relations are trivially relfexive and transitive.</p> <p>Now, with simulation relations in our toolbelt, we can slightly change the the naive algorithm to get</p> <p></p> <p>where the only change is converting \\(Z\\subseteq Z'\\) to \\((q, Z)\\preceq(q, Z')\\) in line 18 The proofs for Completeness and Soundness are almost identical to the one in the naive case.</p>","tags":["Note"]},{"location":"Simulation%20for%20Zone%20Automata/#finite-simulation","title":"Finite Simulation","text":"<p>Currently we use subset relation as the simulation which does not terminate. We need to find a Finite Simlution <pre><code>title: Finite Simulation\nA _Simulation Relation_ is called *Finite* if there exists a number $K\\in\\mathbb N$ such that in every run of length more than $K$ that is \n$$\ns_{0}\\rightarrow s_{1}\\rightarrow\\dots\\rightarrow s_{k}\n$$\n there will be $i &lt; j\\le K$ such that $s_{j}\\le s_{i}$\n</code></pre> If we do this, then we guarantee that the program terminates!</p>","tags":["Note"]},{"location":"Simulation%20for%20Zone%20Automata/#simulation-from-region-equivalence","title":"Simulation from region equivalence","text":"<p>Here we give a simulation relation that is finite, and which can be checked efficiently.</p> <p>Given an Automaton, Let \\(M\\) be the maximum bounds function and we say \\(v_{1}\\sim_{M}v_{2}\\) if \\(v_{1}\\) and \\(v_{2}\\) are region equivalent, then we define the relation $$ (q, v_{1}) \\preceq_{M} (q, v_{2})\\quad\\text{if}\\quad v_{1}\\sim_{M} v_{2} $$ We know that this relation is finite, because for any given automata, there are only finitely many zones.</p> <p>To extend this for sets of valuations, we define \\(\\textbf{Closure}_{M}\\). Let \\(W\\) be a set of valuations. Then: $$ \\textbf{Closure}{M}(W)\\;:=\\;{v\\;|\\;\\text{exists \\(v'\\in W\\) such that } v\\sim{M}v'} $$ Or, it is the union of regions intersecting \\(W\\).</p> <p>And we define \\(\\sqsubseteq_{M}\\) to be a relation on sets of valuations defined as  $$ (q, W)\\sqsubseteq_{M}(q, W')\\text{ if } W\\in\\textbf{Closure}_{M}(W') $$ The following relation is Finite, because the number of sets of regions in finite for any Automaton.</p>","tags":["Note"]},{"location":"Simulation%20for%20Zone%20Automata/#references","title":"References","text":"<p>Naive Algorithm for Reachability in Zone Automata Reachability Algorithm for Zone Automata</p>","tags":["Note"]},{"location":"Slide%20Plan/","title":"Slide Plan","text":"<ul> <li>Dimension of a path </li> <li>Probability of a path</li> <li>Banach Mazur Games</li> <li>Winning Strategies 2</li> <li>Topological Semantics</li> <li>Probabilistic Semantics</li> <li>Space is Baire</li> </ul>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/","title":"Almost certain model checking","text":"<p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#almost-certain-model-checking","title":"Almost Certain Model Checking","text":"<p> Shubh Sharma  <p>NOVEMBER, 2023  </p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#outline","title":"Outline","text":"<ol> <li>Introduction</li> <li>Linear Temporal Logic</li> <li>Timed Automata</li> <li>Probabilistic Semantics</li> <li>Topological Semantics</li> <li>Correspondence of the two Semantics</li> </ol>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#introduction","title":"Introduction","text":""},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#timed-automata-are-really-nice","title":"Timed Automata are really nice (=","text":"<p>They let us describe systems with real-time constraints but still satisfy a lot of nice properties like reachability, safety properties, etc ,etc.</p> <p></p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#but","title":"But","text":""},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#timed-automata-are-too-nice","title":"Timed Automata are too nice )=","text":"<p>Like most mathematical objects, they several assumptions made about them like infinite precision, instantaneous events which are unrealistic</p> <p>So we discuss two semantics for Linear Time Logic using timed automata that let us model check ignoring cases that are too unlikely/extreme a topological and a probabilistic one.</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#linear-temporal-logic","title":"Linear Temporal Logic","text":"<p>Temporal Logics introduce operators that describe how the world changes without explicitly referring to time. This helps us guarantee properties like - Safety: Anything bad will not happen - Liveness: Something good will happen - Fairness: Evolution of subsystems</p> <p>-- Linear Temporal Logic introduces operators with the assumption that there is one execution path in time. That is, we sort of know what how the evolution of the system will be.</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#syntax","title":"Syntax","text":"<p>We have a set of Atomic proposition \\(\\mathcal P\\) and the formulae satisfy the following grammar. { width=\"700\" }</p> <p>-- where the 4 temporal operators are - \\(\\bigcirc f\\;\\;\\) :  \\(f\\) is true in the next state - \\(\\square f\\;\\;\\;\\) : \\(f\\) will be true in all future states  - \\(\\diamond f\\quad\\) : \\(f\\) is true in some future state - \\(f\\cup g\\)  : \\(g\\) will be true in some state in the future, \\(f\\) is true in each state until then  </p> <p>And \\(p\\in\\mathcal P\\)</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#semantics","title":"Semantics","text":"<p>The idea is to model a world, that evolves with time, and at every evolution(discrete) some statements are true. We can do propositional logic at a particular time in the world and we need to make relations between statements across time.</p> <p>All Temporal Operators are defined to satisfy the previous description.</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#semantics-for-ltl-using-timed-automata","title":"Semantics for LTL using Timed Automata","text":"<p>Runs in a timed automata provide sequences of worlds, on which the semantics of LTL can be defined.</p> <p>We say that a timed automata \\(\\mathcal A\\) satisfies the LTL formula \\(\\varphi\\) from \\(s\\) if every run starting from \\(s\\) satisfies \\(\\varphi\\)</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#timed-automata","title":"Timed Automata","text":"<p>Now we can formally define Timed Automata </p> <p>We define a timed automaton \\(\\mathcal A\\) as the tuple  $$ \\mathcal A = \\langle L, X, E, \\mathcal I, \\mathcal L\\rangle $$</p> <p>We modify the definition of \\(E\\) to respect \\(\\mathcal I\\)</p> <p>--</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#example","title":"Example","text":"<p>note: point out at the x &lt;= 1</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#some-important-things","title":"Some important Things","text":"<p>{ width=\"700\" } \\(\\mathcal A\\) is said to be non-blocking if \\(I(s)\\ne\\emptyset\\) for all \\(s\\).</p> <p>We define a symbolic path as a state, followed by a sequence of edges, such that the delay transitions follow a certain constraint { width=\"700\" }</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#machinery-for-probabilistic-semantics","title":"Machinery for Probabilistic Semantics","text":"<p>We want to define a probability measure on the set of runs, to do that, we give properties for valid probability measures \\(\\mu_s\\) for edges. - Given any state, some edge can  be eventually taken : \\(\\mu_s(I(s))=1\\) - If \\(I(s)\\) is finite, then \\(\\mu_s\\) is uniform distributions between the points, otherwise it is equivalent to the Lebesgue measure \\(\\lambda\\).(volume of the set in \\(\\mathbb R\\)) - We also define \\(p_s(e)\\) to be the probability measure defined on edges enabled from \\(s\\)</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#probability-measure-on-finite-paths","title":"Probability measure on Finite Paths","text":"<p>Given a finite path \\(\\pi(s,e: es)\\) , we can define the probability measure on the path as </p> \\[ \\mathbb P_\\mathcal A(\\pi(s,e:es)) = \\frac{1}{2}\\int_{t\\in I(s,e_{1})} p_{s+t}(e)\\mathbb P_{\\mathcal A}(\\pi(s',es))  d\\mu_{s}(t) \\] <p>This gives a probability distribution on \\(\\text{Runs}(\\mathcal A,s)\\)</p> <p>We can give a similar probability distributions for stuff in \\(\\text{R}_\\mathcal A\\) and get an equivalent probability distribution on \\(\\text{Runs}(\\text R_\\mathcal A, \\iota(s))\\) </p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#probabilistic-semantics-for-ltl","title":"Probabilistic Semantics for LTL","text":"<p>Due to the semantics, LTL formulas only depend on the sequence of states in the run. Hence given an LTL formula \\(\\varphi\\) and path \\(\\pi\\) in a timed automata, either all the runs of \\(\\pi\\) satisfy \\(\\varphi\\) or none of them do.</p> <p>So we can define the probability of a formula as the probability of all of the paths that accept it.</p> <p>We say \\(\\mathcal A\\) almost surely satisfies \\(\\varphi\\) from \\(s_0\\) and write \\(A,s \\mid\\!\\approx_\\mathbb P \\varphi\\) iff \\(\\mathbb P_\\mathcal A(s_0,\\varphi)=1\\)</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#dimension-of-a-path","title":"Dimension of a Path","text":"<p>A path is a set of runs over a fixed sequence of edges, for each edge \\(e\\) from a state \\(s\\), once can pick \\(t\\in I(s, e)\\) to take the edge. This naturally leads to the questions: Can a path be embedded in \\(\\mathbb R^m\\) for some \\(m\\)?</p> <p>Idea: We map each run to the polyhedron \\((\\tau_1\\dots \\tau_n)\\in\\mathbb R^n\\) for a given path.</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#finding-the-dimension","title":"Finding the Dimension","text":"<p>We say that the edges where the guard is a singleton, do not add dimension to the path, and the edges in which the guards are a non-degenerate interval, add 1 to the dimension.</p> <p>We say that the dimension of a path is undefined if there exists \\(s\\) which is reached by the sequence of edges \\(e_1\\dots e_i\\) from \\(\\pi\\) and dimension of \\(\\text{Pol}(\\pi_{\\mathcal C}(s_0, e_1\\dots e_i+1))&lt;\\) dimension of \\(\\text{Pol}(\\pi_{\\mathcal C}(s_{0},e_{1}\\dots e_{i},e))\\) for some outgoing \\(e\\) from \\(s\\). Otherwise it is defined.</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#topology-on-the-space-of-runs","title":"Topology on the Space of Runs","text":"<p>Our goal is to give topological semantics for LTL, where we ignore extremely unlikely events while checking if the model satisfies the formula, those unlikely events intuitively correspond to small sets, which we define to be meager(countable union of nowhere dense sets).</p> <p>We can define the topology to be paths that have a defined dimension. This makes sure that only paths where for some \\(s\\) and an outgoing \\(e\\), \\(I(s, e)\\) is singleton and \\(I(s)\\) is not.</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#topological-semantics-for-ltl","title":"Topological Semantics for LTL","text":"<p>Now that we have defined what counts as small,we consider co-meager sets as large. Then we say that \\(\\mathcal A\\) largely satisfies \\(\\varphi\\) from \\(s\\) and write \\(A,s \\mid\\!\\approx_\\mathcal L \\varphi\\) iff \\([\\![\\varphi]\\!]\\) is large.</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#banach-mazur-games","title":"Banach Mazur Games","text":"<p>A Banach Mazur Game is a 2-player infinite game which involves chasing points in a topological space.  To play the game there must be - Topological Space \\(Y\\) - Target Set \\(X\\subseteq Y\\) - A family of sets \\(\\mathcal W\\) such that     - each \\(W\\in \\mathcal W\\) contains an open set     - Each open set contains a set in \\(\\mathcal W\\)</p> <p>The game proceeds by player 1 and player 2 take turns in choosing nested sets from \\(\\mathcal W\\). If the intersection of these sets with \\(X\\) is non empty, then player 1 wins, otherwise player 2 wins</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#winning-strategies","title":"Winning Strategies","text":"<p>There is a winning strategy for player 2 iff \\(X\\) is small.</p> <p>Since \\(X\\) is a countable union of nowhere dense sets, on each turn, player 2 will decide to avoid of the sets in the union.</p> <p>If \\(Y\\) is a metric space, then there exists a winning strategy for player 1 iff \\(X\\) is large is some open set.</p> <p>Player 1 picks a family member in the open set for her first move, and then steals the player 2 strategy targeting the complement of the target set. While making sure intersection of the chain is nonempty</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#correspondence-between-the-two-semantics","title":"Correspondence Between the two Semantics","text":"<p>\\(A, s\\mid\\!\\approx_\\mathbb P \\varphi\\Leftrightarrow A,s\\mid\\!\\approx_\\mathcal L\\varphi\\)</p> <p>Discovering Winning Strategies in Banach Mazur Games proves that the target set is small.  It is used in the proof that the set of runs that satisfy \\(\\lnot\\varphi\\) is small if \\(\\varphi\\) is almost surely satisfied.</p>"},{"location":"Slides%20for%20Timed%20Automata%20Presentation/#thank-you-3","title":"THANK YOU  &lt;3","text":"<p>Here is a Tomato Tomato for you!</p>"},{"location":"Smallest%20Filter%20Heyting%20Algebras/","title":"Smallest Filter Heyting Algebras","text":"<p>202309121213</p> <p>tags : [[Logic]]</p>","tags":["Example"]},{"location":"Smallest%20Filter%20Heyting%20Algebras/#smallest-filter-heyting-algebras","title":"Smallest Filter Heyting Algebras","text":"<p>Lemma: Let \\(A\\) be the subset of a Heyting Algebra \\(H\\). Then the smallest filter which contains \\(A\\) is  $$ F={ a\\in H : a\\ge a_{1}\\sqcap a_{2}\\dots a_{k}\\text{ where } a_{1},a_{2}\\dots a_{k}\\in A} $$</p> <p>Proof: \\(F\\) contains \\(\\overline a = a_{1}\\sqcap\\dots\\sqcap a_{k}\\), because for any \\(a, b\\in F\\) we have \\(a\\sqcap b\\in F\\). Hence Any \\(a\\ge\\overline a\\in F\\).</p> <p>Thus, if \\(\\overline F\\) is the smallest filter \\(\\overline F\\subseteq F\\). But any Filter containing \\(A\\) must be a superset of \\(F\\) Hence \\(F\\) is the smallest filter which contains \\(A\\).</p>","tags":["Example"]},{"location":"Smallest%20Filter%20Heyting%20Algebras/#related","title":"Related","text":"<p>Heyting Algebra</p>","tags":["Example"]},{"location":"Solution%20of%20a%20differential%20equation/","title":"Solution of a differential equation","text":"<p>202212131812</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"Solution%20of%20a%20differential%20equation/#solution-of-a-differential-equation","title":"Solution of a differential equation","text":"<pre><code>title:\nA function relation between the independent and the dependent variable which satisfies a differential equation is called the _Solution_ of the differential equation.\n</code></pre> <p>Eq: \\(xy'' + 3y = 6x^3\\)</p> <p>The General Solution of a \\(n^{th}\\)-order differential equation is dependent of \\(n\\) arbitrary constants. The general solution for EQ is : \\(y = x^3 + {c \\over x^3}\\) while a \\(y = x^3\\) is a particular solution of EQ which is obtianed by putting \\(c=0\\).</p> <p>A DE of first order can be written as \\(F(x, y, y') = 0\\) and \\(y = \\phi(x)\\) is called an Explicit Solution given \\(F(x, \\phi(x), \\phi'(x))=0\\) </p> <p>A relation of the form \\(\\psi(x, y)=0\\) is said to be an Implicit Solution of \\(F(x, y, y')=0\\) if it determines one or more function \\(y=\\phi(x)\\) which are explicit solutions. We can test an explicit solution by \\(\\(\\psi_x + \\psi_yy'=0\\implies y'=-\\frac{\\psi_x}{\\psi_y}\\)\\) and putting it in the equation $$ F(x, y, -\\frac{\\psi_x}{\\psi_y})=0 $$</p> <p>A pair of equation \\(x=x(t),y=y(t)\\) is called a Parametric Equation if  $$ F\\left(x(t), y(t), \\frac{dy/dy}{dx/dt}\\right)=0 $$</p>"},{"location":"Solution%20of%20a%20differential%20equation/#related-problems","title":"Related Problems","text":"<p>Solving a First Order ODE</p>"},{"location":"Solution%20of%20a%20differential%20equation/#references","title":"References","text":"<p>ODE</p>"},{"location":"Solving%20a%20First%20Order%20ODE/","title":"Solving a First Order ODE","text":"<p>202301041201</p> <p>Type : #Note Tags :[[Differential Equations]]</p>"},{"location":"Solving%20a%20First%20Order%20ODE/#solving-a-first-order-ode","title":"Solving a First Order ODE","text":""},{"location":"Solving%20a%20First%20Order%20ODE/#homogenous-1st-order-linear-ode","title":"Homogenous 1st order Linear ODE","text":"\\[ \\begin{aligned} a(x)y' + b(x)y &amp;= 0\\\\ y' &amp;= -\\frac{a(x)}{b(x)}y\\\\ y' &amp;= -p(x)y\\\\ \\frac{dy}y&amp;=-\\int p(x)dx \\implies \\ln|y| = -\\int p(x)dx + C\\\\ &amp;\\implies |y|=\\kappa e^{P(x)}\\\\ \\text{where } &amp;P(x) = \\int p(x)dx \\end{aligned} $$ ### Inhomogenous 1st order Linear ODE $$ \\begin{aligned} y'&amp;=-y\\cdot p(x) - q(x)\\\\ \\frac d{dx}(ye^{P(x)}) &amp;= e^{P(x)}(y'+y\\cdot p(x))\\\\ &amp;=-e^{P(x)}q(x)\\\\ \\therefore ye^{P(x)}&amp;= c'-\\int\\limits_{i_0}^xq(t)e^{P(t)}dt\\\\ y&amp;= e^{-P(X)}\\left(c'-\\int\\limits_{i_0}^xq(t)e^{P(t)}dt\\right) \\end{aligned} $$ ### Non Linear(Variable Separable) ODE $$ \\begin{aligned} y'&amp;=f(x, y)\\\\ y'&amp;=h(x)\\cdot g(y)&amp;(g,h \\text{ are continuous, $h$ is non vanishing})\\\\ \\end{aligned} \\]"},{"location":"Solving%20a%20First%20Order%20ODE/#related-problems","title":"Related Problems","text":""},{"location":"Solving%20a%20First%20Order%20ODE/#references","title":"References","text":"<p>Solution of a differential equation</p>"},{"location":"Solving%20a%20linear%20differential%20equations/","title":"Solving a linear differential equations","text":"<p>202301091401</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"Solving%20a%20linear%20differential%20equations/#solving-a-linear-differential-equations","title":"Solving a linear differential equations","text":"\\[ \\phi\\left(x, y, y'\\dots y^{(n)}\\right) $$ And $$ y^{(n)} = \\Psi\\left(x, y, y' \\dots y^{(n-1)}\\right) \\]"},{"location":"Solving%20a%20linear%20differential%20equations/#first-order-differential-equations","title":"First Order Differential Equations","text":""},{"location":"Solving%20a%20linear%20differential%20equations/#case-1-ygx","title":"Case 1 \\(y'=g(x)\\)","text":"<p>By Fundamental Theorem of Calculus: \\(g(x)\\) is continuous on \\([a,b]\\) and \\((a, c)\\to f(x) = c+\\int\\limits_a^xg(t)dt\\)</p>"},{"location":"Solving%20a%20linear%20differential%20equations/#case-2","title":"Case 2","text":""},{"location":"Solving%20a%20linear%20differential%20equations/#a-homogenous-y-pxy","title":"A] Homogenous \\(y'=-p(x)y\\)","text":""},{"location":"Solving%20a%20linear%20differential%20equations/#case","title":"Case","text":""},{"location":"Solving%20a%20linear%20differential%20equations/#related-problems","title":"Related Problems","text":""},{"location":"Solving%20a%20linear%20differential%20equations/#references","title":"References","text":"<p>Solution of a differential equation</p>"},{"location":"Sparse%20recovery-compressed%20sensing/","title":"Sparse recovery compressed sensing","text":"<p>202311161511</p> <p>Tags : Algorithmic Coding Theory</p>","tags":["Note","Incomplete"]},{"location":"Sparse%20recovery-compressed%20sensing/#sparse-recovery-compressed-sensing","title":"Sparse recovery-compressed sensing","text":"<pre><code>title: Motivation\n$x \\in\\mathbb{C}^{n}$ vector is $L-$*sparse* (# non zero entries $=L$)\nThe goal is to design a *measurement matrix* $M_{(m\\times n)}$ s.t. given $y=Mx$, we can uniquely reconstruct $x$. $m$ should be as small as possible.\n</code></pre> <p>Obs: A measurement matrix \\(M\\) can distinguish between all \\(L-\\)sparse vectors iff for every subset \\(\\mathcal{L}\\) of upto \\(2L\\) columns the right kernel of the submatrix \\(M|_{\\mathcal{L}}\\) is zero.</p> <p>L-disjunct matrix: An \\(m\\times n\\) binary matrix is \\(L-\\)disjunct if for any choice of \\(L+1\\) columns \\(M_{0},\\dots,M_{L}\\), \\(\\bigcup\\limits_{i\\in[L]}supp(M_{i})\\not\\subseteq supp(M_{0})\\).</p> <p>Restricted Isometry Property (RIP): \\(m\\times n\\) matrix \\(M\\); for every \\(\\mathcal{L}\\subseteq[n]\\), \\(|\\mathcal{L}|\\leq L\\) and every column vector \\(x \\in\\mathbb{C}^{|\\mathcal{L}|}\\), \\((1-\\alpha) \\lvert x \\rvert_{p}\\leq \\lvert M|_{\\mathcal{L}}.x \\rvert_{p}\\leq(1+\\alpha)\\lvert x \\rvert_{p}\\).</p>","tags":["Note","Incomplete"]},{"location":"Sparse%20recovery-compressed%20sensing/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Spectrum%20of%20The%20adjacency%20Matrix/","title":"Spectrum of The adjacency Matrix","text":"<p>202308121508</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Spectrum%20of%20The%20adjacency%20Matrix/#spectrum-of-the-adjacency-matrix","title":"Spectrum of The adjacency Matrix","text":"<p>The Spectrum of the adjacency matrix is the matrix that contains all pairs of eigenvalues and their multiplicity $$ \\text{Spectrum}(A)=\\begin{pmatrix}\\lambda_{1} &amp; \\lambda_{2} &amp; \\dots \\ m_{1} &amp; m_{2} &amp; \\dots \\end{pmatrix} $$</p> <p>For example $$ \\text{Spectrum}(J)=\\begin{pmatrix}4 &amp; 0 \\ 1 &amp; 3\\end{pmatrix} $$ and $$ \\text{Spectrum}(I)=\\begin{pmatrix}1 \\ 4\\end{pmatrix} $$ Using these we can find the eigenvalues of  $$ \\begin{bmatrix}0 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 0 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 0 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 0\\end{bmatrix} $$ by subtracting the eigenvalues of \\(J\\) and \\(I\\).</p>"},{"location":"Spectrum%20of%20The%20adjacency%20Matrix/#references","title":"References","text":"<p>Traces of Powers of Adjacency Matrices Spectrum of a Cube Graph Spectrum of the complement of a graph</p>"},{"location":"Spectrum%20of%20a%20Cube%20Graph/","title":"Spectrum of a Cube Graph","text":"<p>202308211128</p> <p>type : #Example tags : [[Algebraic Graph Theory]]</p>"},{"location":"Spectrum%20of%20a%20Cube%20Graph/#spectrum-of-a-cube-graph","title":"Spectrum of a Cube Graph","text":"<p>\\(\\(V(Q_{k})=\\{(x_{1},x_{2},\\dots,x_{k}):x_{i}\\in\\{0,1\\}\\}\\)\\) and a edge between two vertices exits if they differ in exactly one component. Can also be thought as Hamming Distance being equal to \\(1\\) iff edge exists.</p> <p>We get \\(Q_{k+1}\\) by taking two copies of \\(Q_{k}\\) and add \\(0\\) as a component to vertices of first one and \\(1\\) as a component to the edges of the second one, and making an edge between corresponding points.</p> <p>The adjacency matrix of a cube graph is  $$ A_{k+1}= \\begin{bmatrix}A_{k} &amp; I \\ I &amp; A_{k}\\end{bmatrix} $$ where \\(I=I_{2^{k}}\\)</p> <p>Claim:  if \\((\\lambda, x)\\) is an eigenpair of \\(Q_{k}\\) then  1. \\((\\lambda +1, y)\\) is an eigenpair where \\(y\\) is \\(\\left[\\frac{x}{x}\\right]\\) 2. \\((\\lambda - 1, z)\\) is an eigenpair where \\(z\\) is \\(\\left[\\frac{-x}{x}\\right]\\) Proof: 1. $$ \\begin{align} A_{k+1}y &amp;= \\begin{bmatrix}A_{k}x+x\\ x+A_{k}x\\end{bmatrix}\\ &amp;= \\begin{bmatrix}\\lambda x+x\\ x+\\lambda x\\end{bmatrix}\\ &amp;= \\lambda\\begin{bmatrix}x\\ x\\end{bmatrix}\\ &amp;= \\lambda y \\end{align} $$ Similar argument works for part 2 Using this we can get the spectra of cube graphs as $$ \\begin{align} \\text{spectrum}(Q_{2})&amp;= \\begin{pmatrix}-2 &amp; 0 &amp; 2\\ 1 &amp; 2 &amp; 1\\end{pmatrix}\\ \\text{spectrum}(Q_{3})&amp;= \\begin{pmatrix}-3 &amp; -1 &amp; 1 &amp; 3\\ 1 &amp; 3 &amp; 3 &amp; 1\\end{pmatrix}\\ \\text{spectrum}(Q_{4})&amp;= \\begin{pmatrix}-4 &amp; -2 &amp; 0 &amp; 2 &amp; 4\\ 1 &amp; 4 &amp; 6 &amp; 4 &amp; 1\\end{pmatrix}\\ \\end{align} $$</p>"},{"location":"Spectrum%20of%20a%20Cube%20Graph/#related","title":"Related","text":""},{"location":"Spectrum%20of%20the%20complement%20of%20a%20graph/","title":"Spectrum of the complement of a graph","text":"<p>202308281126</p> <p>type : #Example tags : [[Algebraic Graph Theory]]</p>"},{"location":"Spectrum%20of%20the%20complement%20of%20a%20graph/#spectrum-of-the-complement-of-a-graph","title":"Spectrum of the complement of a graph","text":""},{"location":"Spectrum%20of%20the%20complement%20of%20a%20graph/#question","title":"Question:","text":"<p>Let \\(G\\) be a \\(k-regular\\) connected graph and let \\(\\overline G\\) be the complement of \\(G\\). Let the spectrum of \\(G\\) be \\(k,\\theta_{2},\\dots,\\theta_{n}\\). Then the spectrum of \\(\\overline G\\) is \\(n-1-k,1-1\\theta_{2},\\dots,-1-\\theta_{n}\\).</p>"},{"location":"Spectrum%20of%20the%20complement%20of%20a%20graph/#proof","title":"Proof:","text":"<p>Let \\(A=A(G)\\) Let \\(\\overline A=A(\\overline G)\\) then \\(\\overline A=(J-I)-A\\) hence  $$ \\begin{align} \\bar A \\vec j&amp;= n\\vec j-\\vec j-A \\vec j\\ &amp;= (n-1)\\vec j-k\\vec j\\ &amp;= (n-1-k)\\vec j \\end{align} $$</p>"},{"location":"Spectrum%20of%20the%20complement%20of%20a%20graph/#related","title":"Related","text":""},{"location":"Splitting%20Fields/","title":"Splitting Fields","text":"<p>202303061103</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Splitting%20Fields/#splitting-fields","title":"Splitting Fields","text":"<pre><code>title: \nLet $K$ be a field, and $f \\in K[x]$ be some nonconstant polynomial, there is a field extension $K_{1}/K$ in which $f(x)$ has a root $\\alpha$. Then $f(x) = (x-\\alpha)g(x)$ where $g \\in K_1[x]$ and $\\mathrm{deg}(g) = \\mathrm{deg}(f)-1$. By applying the same process $n-1$ times. We obtain an extension $L$ where $f$ splits into linear factors$$\nf(x) = c(x-\\alpha_{1})(x-\\alpha_{2})\\dots(x-\\alpha_{n})\n$$ in $L[x]$.\n\nWe call the field $K(\\alpha_{1}\\dots \\alpha_{n})$ that is generated by the roots of $f$ in $L$, a **splitting field** of $f$ over $K$.\n\n\nAlso note that $[K(\\alpha_1,\\dots,\\alpha_n) : K] \\le (deg (f))!$.\n</code></pre>"},{"location":"Splitting%20Fields/#the-following-theorem-shows-that-splitting-field-of-a-polynomial-is-unique","title":"The following theorem shows that splitting field of a polynomial is unique.","text":""},{"location":"Splitting%20Fields/#theorem-1","title":"Theorem 1","text":"<pre><code>title:\nLet $K$ be a field and $f(x)$ be nonconstant in $K[x]$. If $L_{1}$ and $L_{2}$ are splitting\nfields of $f(x)$ over $K$ then \n1) $[L_{1}:K] = [L_{2}: K]$ \n2) There is a field isomorphism $\\phi : L_{1} \\to L_{2}$ fixing all of $K$. \n3) The number of such isomorphisms $L_{1} \\to L_{2}$ is at most $[L_1:K]$ \n</code></pre>"},{"location":"Splitting%20Fields/#to-prove-this-we-need-to-prove-a-general-case-of-this-theorem","title":"To prove this, we need to prove a general case of this theorem.","text":""},{"location":"Splitting%20Fields/#theorem-2","title":"Theorem 2","text":"<pre><code>title:\nLet $K_{1},K_{2}$ be fields with $\\sigma : K_{1} \\to K_{2}$ an isomorphism between them. And let $f(x)$ be nonconstant in $K_{1}[x]$. If $L_{1}$ is a splitting field of $f(x)$ over $K_{1}$ and let $L_{2}$ be a splitting field of $(\\sigma f)(x)$ over $K_{2}$ respectively then: \n1) $[L_{1}:K_1] = [L_{2}: K_2]$ \n2) $\\sigma$ extends to an isomorphism $\\sigma_2 : L_{1} \\to L_{2}$ fixing all of $K$. \n3) The number of such extensions $L_{1} \\to L_{2}$ is at most $[L_1:K_1]$ \n</code></pre>"},{"location":"Splitting%20Fields/#proof","title":"Proof:","text":"<p>Induction on \\([L_{1}:K_{1}]\\).  \\([L_{1}:K_{1}] = 1\\) case is trivial. \\([L_{1}:K_{1}] &gt; 1\\): 1) Take a root of \\(\\alpha\\) of \\(f\\) in \\(L_{1}\\) which is not in \\(K_{1}\\). Then, \\(K_{1}(\\alpha)\\) is a subfield of \\(L_{1}\\). 2) Take the minimal poly of \\(\\alpha\\) over \\(K_{1}\\), call this \\(\\pi\\). 3) Now suppose there was an isomorphism \\(\\sigma_{2}\\)  from \\(L_{1}\\) to \\(L_{2}\\) that extended \\(\\sigma\\).    Then \\((\\sigma_{2}\\pi)(\\sigma_{2}\\alpha) = (\\sigma \\pi)(\\sigma_{2}(\\alpha)) = \\sigma(\\pi(\\alpha)) = \\sigma(0) = 0\\). Hence, \\(\\sigma_{2}\\) takes \\(\\alpha\\) to roots of \\(\\sigma \\pi\\). 4) Now we show that \\(\\sigma \\pi\\) has a root in \\(L_{2}\\). Since \\(\\pi(x) | f(x)\\) then \\(\\sigma\\pi(x) | \\sigma f(x)\\). Since \\(\\sigma f\\) splits in \\(L_{2}\\), \\(\\sigma \\pi\\) does too, and hence has a root \\(\\alpha'\\) in \\(L_{2}\\). 5) Hence, we can extend \\(\\sigma\\) to \\(\\sigma_{1} : K_{1}(\\alpha) \\to K_{2}(\\alpha')\\) where \\(\\sigma_{1}\\) is unique once the choice for \\(\\alpha'\\) has been made. 6) Now since \\([L_{1}:K_{1}(\\alpha)] &lt; [L_{1}:K_{1}]\\), we can induct (here \\(K_{1},K_{2}\\) gets replaced by \\(K_{1}(\\alpha), K_{2}(\\alpha')\\) respectively). This gives an extension of \\(\\sigma\\) to \\(\\sigma_{2} : L_{1} \\to L_{2}\\), so we are done for 2. For 1, just see that \\([K_{1}(\\alpha):K_{1}] = [K_{2}(\\alpha) : K_{2}]\\). 7) For 3, we need to work a bit more and see that any extension \\(\\sigma_{2}\\) of \\(\\sigma\\) restricts to an isomorphism \\(K_{1}(\\alpha) \\to K_{2}(\\sigma_{2}(\\alpha))\\).    Where \\(\\sigma_{2}(\\alpha)\\) is a root of \\(\\sigma_{2}\\pi\\) where \\(\\pi\\) is some irreducible factor of \\(f\\). So, the number of extensions to \\(K_{1}(\\alpha)\\) is \\([K_{1}(\\alpha) : K_{1}]\\). And by induction, the number of extensions from \\(K_{1}(\\alpha)\\) to \\(L_{1}\\) are at most \\([L_{1}:K_{1}(\\alpha)]\\).</p> <p>This gives that the number of extensions \\(\\sigma_{2}\\) of \\(\\sigma\\) is at most \\([K_{1}(\\alpha) : K_{1}][L_{1}:K_{1}(\\alpha)] =[L_{1}:K_{1}]\\). </p>"},{"location":"Splitting%20Fields/#now-theorem-1-follows-by-putting-k_1k_2k-sigma-id_k","title":"Now Theorem 1 follows by putting \\(K_{1}=K_{2}=K, \\sigma = id_{K}\\).","text":""},{"location":"Splitting%20Fields/#note","title":"NOTE:","text":"<p>In  the above proof, we can even show that the number of extensions \\(\\sigma_{2}\\), are infact equal to \\([L_{1}:K_{1}]\\). Since, if we assume this to be the induction hypothesis, then we can show that the number of extensions to \\(K_{1}(\\alpha) \\to K_{2}(\\alpha')\\) is exactly equal to \\(\\mathrm{deg}(\\pi)\\) by showing that all roots of \\(\\sigma \\pi\\) are distinct. Since if that's the case then there are \\(\\mathrm{deg}(\\sigma \\pi)\\) many possibilities of \\(\\alpha'\\), each giving a distinct isomorphism (this is possible when \\(\\sigma \\pi\\) is separable). Suppose two choices gave the same isomorphism, that would mean \\(K_{2}(\\alpha_{1}') = K_{2}(\\alpha_{2}')\\), which would mean that between \\(K_{2}(\\alpha_{1}')\\) and \\(K_{2}(\\alpha_{2}')\\) there are two isomorphisms, one is just identity, and the other sends \\(\\alpha_{1}'\\) to \\(\\alpha_{2}'\\). This is a contradiction to the fact that there is unique isomorphism between \\(K(\\alpha)\\) and \\(K[x]/(f)\\) for any irreducible \\(f\\) and \\(\\alpha\\) a root of \\(f\\).</p>"},{"location":"Splitting%20Fields/#theorem-3","title":"Theorem 3:","text":"<pre><code>title:\nLet $K_{1},K_{2}$ be fields with $\\sigma : K_{1} \\to K_{2}$ an isomorphism between them. And let $f(x)$ be nonconstant in $K_{1}[x]$. If $L_{1}$ is a splitting field of $f(x)$ over $K_{1}$ and let $L_{2}$ be a splitting field of $(\\sigma f)(x)$ over $K_{2}$ respectively, then if $f(x)$ is separable then:\n1) $[L_{1}:K_1] = [L_{2}: K_2]$ \n2) $\\sigma$ extends to an isomorphism $\\sigma_2 : L_{1} \\to L_{2}$ fixing all of $K$. \n3) The number of such extensions $L_{1} \\to L_{2}$ is exactly $[L_1:K_1]$ \n</code></pre>"},{"location":"Splitting%20Fields/#proof_1","title":"Proof:","text":"<p>Refer to the note above and Theorem 2's proof to show this.</p>"},{"location":"Splitting%20Fields/#corollary","title":"Corollary:","text":"<pre><code>title:\nIf $L/K$ is the splitting field of a separable polynomial then there are $[L:K]$ automorphisms of $L$ that fix the elements of $K$.\n</code></pre>"},{"location":"Splitting%20Fields/#note_1","title":"Note:","text":"<p>Let \\(F\\) be a field. For a given integer \\(n\\), there may or may not exist polynomials of degree \\(n\\) in \\(F[X]\\) whose splitting field has degree \\(n!\\) \u2014 this depends on \\(F\\).</p> <p>For example, there do not exist such polynomials for \\(n&gt;1\\) if \\(F = \\mathbb{C}\\), nor for \\(n &gt; 2\\) if \\(F = \\mathbb{R}\\) or \\(F = \\mathbb{F}_{p}\\).  - For \\(F = \\mathbb{F}_{p}\\), we can show that any irreducible poly of degree \\(d\\) which has root \\(\\alpha\\) in a field extension of \\(\\mathbb{F}_{p}\\), has as roots \\(\\alpha, \\alpha^{p}, \\alpha^{p^{2}}, \\dots, \\alpha^{p ^{d-1}}\\). So the splitting field of a degree \\(d\\) poly over \\(\\mathbb{F}_{p}\\) will just be \\(\\mathbb{F}_{p}(\\alpha)\\) of degree \\(d\\).</p> <p>However we can write down infinitely many polynomials of degree \\(n\\) in \\(\\mathbb{Q}[X]\\) with splitting fields of degree \\(n!\\). </p> <ul> <li>TO DO</li> </ul>"},{"location":"Splitting%20Fields/#examples","title":"Examples","text":"<p>1) The extension \\(\\mathbb{Q}(\\sqrt{ 2 }, \\sqrt{ 3 })/\\mathbb{Q}\\) is a splitting field of \\((x^{2}-3)(x^{2}-2)\\).     Its degree is 4, so there are 4 automorphisms that fix \\(\\mathbb{Q}\\).    To find the automorphisms, just observe that \\(\\sqrt{ 2 }\\) can only go to \\(\\pm \\sqrt{ 2 }\\), similarly \\(\\sqrt{ 3 }\\) can only go to \\(\\pm \\sqrt{ 3 }\\). Thus there are 4 possibilities, but since we know that there are exactly 4 automorphisms, then all of these should work.</p>"},{"location":"Splitting%20Fields/#references","title":"References","text":"<p>Extension Field Separable Extensions Separable Polynomials Splitting Fields</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/","title":"Splitting of Primes in extensions","text":"<p>202306061206</p> <p>Type : #Note Tags : [[Number Theory]] [[Algebra]]</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#splitting-of-primes-in-extensions","title":"Splitting of Primes in extensions","text":"<pre><code>title: Discussion\nWe know of primes in $\\mathbb{Z}$ which are not irreducible in latger extensions, for example $5 = (2+i)(2-i)$ in $\\mathbb{Z}[i]$ and although 2,3 are primes in $\\mathbb{Z}$, the ideals $(2)$ and $(3)$ split in $\\mathbb{Z}[\\sqrt[]{ -5 }]$.\n$$\n\\begin{align}\n(2) &amp;= (2,1+\\sqrt[]{ -5 })^{2} \\\\\n(3) &amp;= (3,1+\\sqrt[]{ -5 })(3,1-\\sqrt[]{ -5 })\n\\end{align}\n$$\nThis phenomenon is called __splitting__. When we say that 3 splits as a product of two primes in $\\mathbb{Z}[\\sqrt[]{ -5 }]$, we really mean the prime ideal $(3)$ splits in $\\mathbb{Z}[\\sqrt[]{ -5 }]$.\nWe now consider the general problem of a given prime splitting in a given number ring.\n</code></pre> <p>From now on, let \\(K\\) and \\(L\\) be number fields with \\(K \\subset L\\), and \\(R = \\overline{\\mathbb{Z}} \\cap K\\) and \\(S = \\overline{\\mathbb{Z}} \\cap L\\).</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nLet $P$ be a prime ideal of $R$, and $Q$ a prime ideal of $S$. Then the following are equivalent:\n1. $Q \\mid PS$\n2. $Q \\supset PS$\n3. $Q \\supset P$\n4. $Q \\cap R = P$\n5. $Q \\cap K = P$\nIf any of the above conditions hold, $Q$ is said to __lie over__ $P$ or $P$ is said to __lie under__ $Q$.\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof","title":"Proof:","text":"<p>\\((1) \\implies (2) \\implies (3)\\) is easy. \\((3) \\implies (4)\\) We know that \\(Q \\cap R \\supset P\\), and that \\(Q \\cap R\\) is a proper ideal of \\(R\\) (otherwise \\(1 \\in Q \\cap R \\subset Q \\implies Q = S\\)), hence \\(P\\) being a maximal ideal is equal to \\(Q \\cap R\\).  \\((4) \\implies (5)\\) \\(Q \\cap K = Q \\cap \\overline{\\mathbb{Z}} \\cap K = Q \\cap R = P\\). \\((5) \\implies (1)\\), since \\((5) \\implies (4) \\implies (3) \\implies(2) \\implies (1)\\) (easy to check).</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#theorem-2","title":"Theorem 2:","text":"<pre><code>title:\nEvery prime $Q$ of $S$ lies over a unique prime $P$ of $R$; every prime $P$ of $R$ lies under at least one prime $Q$ of $S$.\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof_1","title":"Proof:","text":"<p>Let \\(P = Q \\cap R\\), note that \\(P\\) is a prime ideal of \\(R\\), hence \\(Q\\) lies over a prime ideal of \\(R\\). We just need to show that \\(Q \\cap R \\neq 0\\), for this we take any element \\(\\alpha \\in Q\\), take its norm \\(N ^{L}(\\alpha) \\in \\mathbb{Z} \\subset R\\), we also know that \\(N ^{L}(\\alpha) \\in Q\\), since the product of the conjugates of \\(\\alpha\\) other than \\(\\alpha\\) are contained in \\(S\\). Thus \\(N ^{L}(\\alpha) \\in Q \\cap R \\neq 0\\).</p> <p>If \\(Q\\) lies over another prime \\(P_{1}\\), \\(P_{1} = Q \\cap R = P\\) due to theorem 1, thus the prime \\(P\\) is unique.</p> <p>Now for the second part, given any prime \\(P\\) of \\(R\\), take \\(PS\\) and look at its prime factorisation in \\(S\\), \\(Q\\) is any one of those prime factors. For this we need to show that \\(PS \\neq S\\). For this, refer to lemma 2 of theorem 2 of Dedekind Domains, and we get a \\(\\gamma \\in K - R\\), such that \\(\\gamma P \\subset R \\implies \\gamma PS \\subset RS = S\\), thus if \\(1 \\in PS\\) then \\(\\gamma \\in PS\\), this is a contradiction, therefore \\(PS \\neq S\\).</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#definition","title":"Definition:","text":"<p>The exponent of \\(Q\\) in the prime decomposition of \\(PS\\) is known as its ramification index, denoted by \\(e(Q | P)\\).</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#examples","title":"Examples:","text":"<p>Let \\(R = \\mathbb{Z}\\) and \\(S = \\mathbb{Z}[i]\\), then \\(2S = (1-i)^{2}\\), here \\((1-i)\\) is a prime which can be verified by looking at \\(|S /(1-i)| = 2\\), and so \\(e((1-i)|2) = 2\\). On the other hand, \\(e(Q | p) = 1\\) for all \\(p \\neq 2\\) and \\(Q\\) lying over \\(p\\).</p> <p>Now if \\(S = \\mathbb{Z}[\\omega]; \\omega=e ^{2\\pi i/m}; m = p ^{r}\\), for some prime \\(p \\in \\mathbb{Z}\\), then the principal ideal \\((1-\\omega)\\) in \\(S\\) is a prime lying over \\(p\\), and \\(e((1-\\omega)|p) = \\phi(m)\\). \\((1-\\omega)\\) is prime because \\(p = N(1-\\omega)\\), therefore \\(1-\\omega \\mid p\\). Now \\(1-\\omega \\nmid j\\) for all \\(1 \\le j \\le p\\), since if it did, it would also divide \\(kp + 1\\) for some \\(k\\) (since we could multiply by \\(j\\)'s inverse modulo \\(p\\).), and hence it would divide \\(1\\), so it would be a unit and have norm \\(\\pm 1\\), that's a contradiction. Thus, \\(0,1,\\dots (p-1)\\) lie in distinct congruence classes mod \\((1-\\omega)\\). We can show that any other element reduces to one of these classes. Thus \\(\\mathbb{Z}[\\omega]/(1-\\omega) = \\mathbb{F}_{p}\\).</p> <p>\\(e((1-\\omega) | p) = \\phi(m)\\) because \\(p = u(1-\\omega)^{\\phi(m)}\\) where \\(u\\) is a unit. This happens because \\(p = \\prod_{k}(1-\\omega^{k})\\) where \\(k\\) goes over all numbers \\(1 \\le k \\le m\\) such that \\((k,m) = 1\\), and each \\((1-\\omega^{k}) = \\frac{1-\\omega^{k}}{1-\\omega} \\cdot (1-\\omega)\\). Here \\(\\frac{1-\\omega^{k}}{1-\\omega}\\) is a unit, since its reciprocal \\(\\frac{1-\\omega}{1-\\omega^{k}} = \\frac{1-\\omega^{hk}}{1-\\omega^{k}} = 1 + \\omega ^{k} + \\dots + \\omega^{(h-1)k} \\in \\mathbb{Z}[\\omega]\\). This gives \\((p) = (1-\\omega)^{\\phi(m)}\\)</p> <p>Whereas \\(e(Q | q) = 1\\) whenever \\(q \\neq p\\) and \\(Q\\) lies over \\(q\\).</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#definition_1","title":"Definition:","text":"<p>We know that \\(R/P\\) and \\(S /Q\\) are fields. These are called the residue fields modulo of \\(R\\) and \\(S\\) associated with \\(P\\) and \\(Q\\). Moreover, we have an homomorphism \\(R \\to S \\to S /Q\\) whose kernel is \\(R \\cap Q = P\\), thus there is an embedding \\(R /P \\to S /Q\\). We know that these are finite fields and thus, \\(S /Q\\) is a finite extension over \\(R /P\\), let \\(f\\) be the degree. Then \\(f\\) is called the inertial degree of \\(Q\\) over \\(P\\), denoted by \\(f(Q|P)\\).</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#examples_1","title":"Examples:","text":"<p>Let \\(R = \\mathbb{Z}\\) and \\(S = \\mathbb{Z}[i]\\), we have seen that \\((1-i)\\) lies above \\(2\\). \\(S / (1-i)\\) and \\(R /(2)\\) both have order 2. Hence, \\(R /P\\) and \\(S /Q\\) both are fields of order 2, and thus \\(f = 1\\). Whereas \\(3S\\) is a prime in \\(S\\), and \\(|S /3S| = 9\\). So \\(f(3S | 3) = 2\\).</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#note-e-and-f-are-multiplicative-in-towers","title":"Note: \\(e\\) and \\(f\\) are multiplicative in towers.","text":"<p>If \\(P \\subset Q \\subset U\\) are primes in the number rings \\(R \\subset S \\subset T\\), then $$ \\begin{align} e(U|P) &amp;= e(U|Q)e(Q|P) \\ f(U|P) &amp;= f(U|Q)f(Q|P) \\end{align} $$ We prove two theorems together, </p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#theorem-3","title":"Theorem 3:","text":"<pre><code>title:\nLet $n$ be the degree of $L$ over $K$, ($L,K,R,S$ as before) and let $Q_{1},\\dots Q_r$ be the primes of $S$ lying over a prime $P$ of $R$. Let $e_{1},\\dots e_{r}$ be the corresponding ramification indices. Let $f_{1},\\dots ,f_{r}$ be the corresponding inertial degrees. Then $\\sum_{i}e_{i}f_{i} = n$.\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#theorem-4","title":"Theorem 4:","text":"<pre><code>title:\nLet $R,S,K,L$ as before, and $n = [L:K]$. Let $\\|I\\|$ denote $|R/I|$ for an ideal $I$ of $R$.\n1. For ideals $I,J \\subset R$, $$\n\\|I\\|\\|J\\| = \\|IJ\\|\n$$\n2. For an ideal $I \\subset R$ and the ideal $IS \\subset S$, $$\n\\|IS\\| = \\|I\\| ^{n}\n$$\n1. Let $\\alpha \\in R \\setminus \\{ 0 \\}$, for the principal ideal $(\\alpha)$,$$\n\\|(\\alpha)\\| = |N ^{K}_{\\mathbb{Q}}(\\alpha) |\n$$\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof-of-41","title":"Proof of 4.1:","text":"<p>We first prove this for coprime \\(I,J\\) and then for prime powers, and the general result will follow. If \\(I,J\\) are coprime, then \\(I \\cap J = IJ\\) (in a number ring), and so by chinese remainder theorem, $$ R /I \\times R/J \\simeq R /IJ $$ Giving us \\(\\|I\\|\\|J\\| = \\|IJ\\|\\).</p> <p>Now we will show that \\(|R /P ^{n}| = |R/P| ^{n}\\). We can show that \\(\\|P\\| = |P ^{k} / P ^{k+1} |\\), and we will be done since \\(R /P ^{n} \\simeq R /P \\times P /P^{2} \\times \\dots P ^{n-1} /P ^{n}\\) We will show an isomorphism between $$ R /P \\to P ^{k} /P ^{k+1} $$ Let \\(\\alpha \\in P ^{k} - P ^{k+1}\\) be a non zero element, then there is an obvious isomorphism $$ R /P \\to \\alpha R /\\alpha P $$ Now there is a homomorphism $$ \\alpha R \\hookrightarrow P ^{k} \\to P ^{k} /P ^{k+1} $$ whose kernel is \\(\\alpha R \\cap P ^{k+1}\\) and image is \\((\\alpha R+P ^{k})/ P ^{k+1}\\). Note that \\(\\alpha R + P ^{k} = \\mathrm{gcd}(\\alpha R,P ^{k}) = P ^{k}\\), and \\(\\alpha R \\cap P ^{k+1} = \\mathrm{lcm}(\\alpha R,P ^{k+1}) = \\alpha P\\). Giving us $$ \\alpha R/\\alpha P \\simeq P ^{k} /P ^{k+1} $$ as needed.</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof-3-special-case","title":"Proof 3 (Special case):","text":"<p>For the case when \\(K = \\mathbb{Q}\\). Then \\(P = p\\mathbb{Z}\\) for some prime \\(p\\). We have $$ pS = \\prod_{i=1}<sup>{r}Q_{i}</sup>{e_{i}} $$ hence $$ \\begin{align}  |pS| = \\prod_{i=1}^{r} |Q_{i}| ^{e_{i}} = \\prod_{i=1}^{r}(p <sup>{f_{i}})</sup>{e_{i}}  \\end{align} $$ On the other hand, we know \\(\\|pS\\| = p ^{n}\\), and we are done.</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof-42","title":"Proof 4.2:","text":"<p>See that \\(S /PS\\) is a ring containing \\(R /P\\) and hence is a vector space over it.</p> <p>Let \\(\\alpha_{1},\\dots,\\alpha_{n+1} \\in S\\), we show that the corresponding elements in \\(S /PS\\) are linearly dependent. Since the \\(\\alpha_{i}\\)'s are linearly dependent over \\(K\\), they are dependent over \\(R\\). Thus there are \\(\\beta_{i} \\in P, \\ \\forall i \\in [n+1]\\) s.t. $$ \\sum\\alpha_{i}\\beta_{i} = 0 $$ we want to show that not all of the \\(\\beta_{i}\\)'s are in \\(P\\).</p> <p>By lemma 3 of Dedekind Domains applied to \\(A = P\\) and \\(B = (\\alpha_{1},\\dots,\\alpha_{n+1})\\), we get that there is a \\(\\gamma \\in K\\) such that \\(\\gamma B \\subset R\\), but \\(\\gamma B \\nsubseteq P\\), multiplying the above equation by \\(\\gamma\\), we get the required fact.</p> <p>Now we need to show that the dimension is infact equal to \\(n\\). Let \\(P \\cap \\mathbb{Z} = p\\mathbb{Z}\\) and consider all primes \\(P_{i}\\) of \\(R\\) lying above \\(p\\). We know \\(S /P_{i}S\\) is a vector space over \\(R /P_{i}\\) with dimension \\(n_{i}\\), we will show that each \\(n_{i} = n\\). Set \\(e_{i} = e(P_{i}|p)\\) and \\(f_{i} = f(P_{i}|p)\\). Then we know by the special case of theorem 3 proved above, $$ \\sum e_{i}f_{i} = m $$ where \\(m = [K : \\mathbb{Q}]\\). We have $$ \\begin{align} \\ pS &amp;= \\prod_{i}P_{i}^{e_{i}}S = \\prod_{i}(P_{i}S)^{e_{i}}  \\ \\implies |pS| &amp;= \\prod_{i}|P_{i}S| ^{e_{i}} =  \\prod_{i} |P_{i}| ^{n_{i}e_{i}} = \\prod_{i}p ^{f_{i}n_{i}e_{i}} \\end{align} $$ But we also know \\(\\|pS\\| = p ^{mn}\\). This gives $$ \\begin{align} mn &amp;= \\sum_{i}f_{i}n_{i}e_{i}  \\ \\implies \\sum_{i}n_{i}f_{i}e_{i} = \\sum_{i}e_{i}f_{i}n \\end{align} $$ And since \\(n_{i} \\le n\\), it is infact equal to \\(n\\).</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof-3-general-case","title":"Proof 3 (General case):","text":"<p>We have \\(PS = \\prod_{i}Q_{i}^{e_{i}}\\). $$ \\begin{align} |P| ^{n} = |PS| &amp;= \\prod_{i}|Q_{i}| ^{e_{i}}  \\ \\ &amp;= \\prod_{i}|P| ^{f_{i}e_{i}} \\ \\implies n &amp;= \\sum_{i} f_{i}e_{i} \\end{align} $$</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof-43","title":"Proof 4.3:","text":"<p>Let \\(M\\) be a normal extension of \\(K\\), and let \\(T = \\overline{\\mathbb{Z}} \\cap M\\). For each embedding \\(\\sigma\\) of \\(K\\) in \\(\\mathbb{C}\\), we have $$ |\\alpha T| = |\\sigma(\\alpha)T| $$ This is true because, we can extend \\(\\sigma\\) to \\(M\\), and note that \\(\\sigma (T) = \\sigma(K \\cap \\overline{\\mathbb{Z}}) = \\sigma(K) \\cap \\sigma(\\overline{\\mathbb{Z}}) = K \\cap \\overline{\\mathbb{Z}} = T\\).</p> <p>Now \\(T /\\alpha T \\to \\sigma T /\\sigma(\\alpha T) = T /\\sigma(\\alpha)T\\) is an isomorphism. Where the first arrow is the map that takes \\(x + \\alpha T\\) to \\(\\sigma(x) + \\sigma(\\alpha)T\\).</p> <p>Now let \\(N = N_{\\mathbb{Q}}^{K}(\\alpha)\\) and let \\(m = [M : K]\\)</p> \\[ \\begin{align} \\|NT\\| &amp;= \\prod_{\\sigma}\\|\\sigma(\\alpha)T\\| \\\\ \\implies |N| ^{mn} &amp;= \\prod_{\\sigma} \\|\\alpha T\\| \\\\ &amp;= \\|\\alpha T\\| ^{n} \\\\ &amp;= \\|(\\alpha R)\\| ^{mn} \\\\ \\implies |N| &amp;= \\|(\\alpha)\\| \\end{align} \\]"},{"location":"Splitting%20of%20Primes%20in%20extensions/#applications","title":"Applications:","text":"<ol> <li>\\((1-\\omega)\\) is a prime ideal in \\(\\mathbb{Z}[\\omega]\\), where \\(\\omega = e ^{2\\pi i/p ^{r}}\\) We know that \\(p\\mathbb{Z}[\\omega] = (1-\\omega)^{n}\\) where \\(n = \\phi(p ^{r})\\), since \\(n\\) is the degree of \\(\\mathbb{Q}(\\omega)\\) over \\(\\mathbb{Q}\\), any futher splitting of \\((1-\\omega)\\) would violate the above theorem.</li> <li>This can also be seen by observing \\(\\|(1-\\omega)\\| ^{n} = \\|(1-\\omega) ^{n}\\| = \\|p\\mathbb{Z}[\\omega]\\| = p ^{n} \\implies \\|(1-\\omega)\\| = p\\), now theorem 4.1 gives that \\((1-\\omega)\\) is a prime ideal.</li> </ol>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#theorem-5","title":"Theorem 5:","text":"<pre><code>title:\nIf $L$ is a normal extension of $K$ and $P$ is a prime in $R$, the galois group $\\mathrm{Gal}(L /K)$ permutes the primes lying over $P$ and it does so transitively.\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof_2","title":"Proof:","text":"<p>Fix a prime \\(Q\\) lying over \\(P\\), and \\(\\sigma \\in \\mathrm{Gal}(L /K)\\), then \\(\\sigma(Q)\\) is another prime ideal over \\(P\\), since \\(\\sigma(Q) \\cap K = Q \\cap K = P\\). Now let \\(Q'\\) be a prime over \\(P\\) such that \\(\\sigma(Q) \\neq Q'\\) for all \\(\\sigma \\in G\\). Now choose an element \\(x \\in S\\) such that $$ \\begin{align} x \\equiv 1  (\\mathrm{mo d} \\sigma(Q)) \\ x \\equiv 0  (\\mathrm{m od} Q') \\end{align} $$ this is possible by CRT.</p> <p>Now \\(N ^{L}_{K}(x) \\in R \\cap Q' = P\\). We have \\(x \\notin \\sigma(Q) \\implies \\sigma^{-1}(x) \\notin Q\\) for all \\(\\sigma \\in \\mathrm{Gal}(L /K)\\). But this means \\(N ^{L}_{K}(x) = \\prod_{\\sigma} \\sigma^{-1}(x) \\notin Q\\), this is a contradiction since \\(N_{K}^{L}(x) \\in P \\subset Q\\).</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#corollary","title":"Corollary:","text":"<pre><code>title:\nIf $L$ is normal over $K$ and $Q$ and $Q'$ are two primes above $P$, then $e(Q|P) = e(Q'|P)$ and $f(Q|P) = f(Q'|P)$.\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof_3","title":"Proof:","text":"<p>\\(e(Q|P) = e(Q'|P)\\) follows from unique factorisation. For the second part, note the chain of isomorphisms:$$ S /Q \\to \\sigma S /\\sigma Q \\to S /\\sigma Q $$</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#definition_2","title":"Definition:","text":"<pre><code>title:\nA prime $P$ of $R$ is __ramified__ in $S$ (or in $L$) if $e(Q|P) &gt;1$ for some prime $Q$ of $S$ lying over $P$.\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#theorem-6","title":"Theorem 6:","text":"<pre><code>title:\nLet $p$ be a prime in $\\mathbb{Z}$, and suppose $p$ is ramified in a number ring $R$. Then $p \\mid \\mathrm{disc}(R)$.\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof_4","title":"Proof:","text":"<p>Let \\(P\\) be a prime in \\(R\\) lying over \\(p\\), such that \\(e(P|p) &gt; 1\\). Let \\(pR = PI\\), with \\(I\\) divisible by all primes of \\(R\\) lying over \\(p\\).</p> <p>Let \\(\\sigma_{1},\\dots \\sigma_{n}\\) denote the embeddings of \\(K\\) into \\(\\mathbb{C}\\), and extend these to automorphisms of an extension \\(L\\) of \\(K\\), which is normal over \\(\\mathbb{Q}\\).</p> <p>Let \\(\\alpha_{1},\\dots\\alpha_{n}\\) be an integral basis of \\(R\\). The idea is to find a collection of linearly independent elements whose discriminant is divisible by \\(p\\) and s.t. the subgroup \\(H\\) generated by them satisfies \\(p \\nmid |R /H|\\). (Look at Discriminant of an n-tuple, related problems, Problem 3).</p> <p>Let \\(\\alpha \\in I - pR\\), \\(\\alpha = m_{1}\\alpha_{1} + \\dots + m_{n}\\alpha_{n}\\), since \\(\\alpha \\notin pR\\), one of the \\(m_{i}\\)'s is not divisible by \\(p\\), WLOG let it be \\(m_{1}\\). Then \\(\\mathrm{disc}(\\alpha,\\alpha_{2},\\dots\\alpha_{n}) = m_{1}^{2}\\mathrm{disc}(\\alpha_{1},\\dots,\\alpha_{n})\\).</p> <p>Now note that \\(\\alpha\\) is in every prime of \\(R\\) lying over \\(p\\), and hence in every prime of \\(S\\) lying over \\(p\\). Fix a prime \\(Q\\) of \\(S\\) lying over \\(P\\), then \\(\\sigma^{-1}(Q)\\) is another prime lying over \\(p\\), and so \\(\\alpha \\in \\sigma^{-1}(Q)\\) This means \\(\\sigma_{i}(\\alpha) \\in Q\\) for all \\(i = 1,2,\\dots n\\) implying \\(\\mathrm{disc}(\\alpha,\\alpha_{2},\\dots,\\alpha_{n}) \\in Q \\subset p\\mathbb{Z}\\).</p> <p>Thus we are done.</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#corollary-1","title":"Corollary 1:","text":"<pre><code>title:\nLet $\\alpha \\in R$, $K = \\mathbb{Q}[\\alpha]$ and let $f$ be any monic poly over $\\mathbb{Z}$ s.t. $f(\\alpha) = 0$. If $p$ is a prime such that $p \\nmid N ^{K}(f'(\\alpha))$, then $p$ is unramified in $K$.\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof_5","title":"Proof:","text":"<p>Note that \\(m_{\\alpha}'(\\alpha) \\mid f'(\\alpha)\\) where \\(m_{\\alpha}(x)\\) is the monic minimal poly of \\(\\alpha\\) over \\(\\mathbb{Q}\\). Thus, \\(N ^{K}(m_{\\alpha}'(\\alpha)) \\mid N ^{K}(f'(\\alpha)) \\implies \\mathrm{disc}(\\alpha) \\mid N ^{K}(f'(\\alpha))\\). So if \\(p \\nmid N ^{K}(f'(\\alpha))\\), then \\(p \\nmid \\mathrm{disc}(\\alpha)\\) hence \\(p\\) is unramified.</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#corollary-2","title":"Corollary 2:","text":"<pre><code>title:\nOnly finitely many primes of $\\mathbb{Z}$ are ramified in a number ring $R$.\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof_6","title":"Proof:","text":"<p>Since there are only finitely many prime divisors of \\(\\mathrm{disc}(R)\\).</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#corollary-3","title":"Corollary 3:","text":"<pre><code>title:\nLet $R$ and $S$ be number rings, $R \\subset S$, then only finitely many primes of $R$ are ramified in $S$.\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof_7","title":"Proof:","text":"<p>If \\(P\\) is a prime ramified in \\(S\\), then \\(P \\cap \\mathbb{Z} = p\\mathbb{Z}\\) is ramified in \\(S\\). But there are only finitely many such \\(p's\\), and any such \\(p\\) has only finitely many primes lying over it in \\(R\\), hence only finitely many primes of \\(R\\) ramify in \\(S\\).</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#splitting-primes-in-cyclotomic-extensions","title":"Splitting primes in Cyclotomic extensions","text":"<p>Let \\(\\omega = e ^{2\\pi i/m}\\) and fix a prime \\(p \\in \\mathbb{Z}\\). Since \\(\\mathbb{Q}(\\omega)\\) is a normal extension of \\(\\mathbb{Q}\\), the corollary to theorem 5 shows that $$ p\\mathbb{Z}[\\omega] = (Q_{1}\\dots Q_{r})^{e} $$ where \\(Q_{i}'s\\) are distinct primes in \\(\\mathbb{Z}[\\omega]\\) with same inertial degrees and we have \\(ref = \\phi(m)\\).</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#theorem-7","title":"Theorem 7:","text":"<pre><code>title:\nWrite $m = p ^{k}n$ with $p \\nmid n$. Then we have $e = \\phi(p ^{k})$ and $f =$ the multiplicative order of $p$ mod $n$. \n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof_8","title":"Proof:","text":"<p>Let \\(\\alpha = \\omega^{p ^{k}}\\) and \\(\\beta = \\omega^{n}\\). Then \\(\\alpha\\) is an \\(n ^{th}\\) root of unity and \\(\\beta\\) is a \\((p ^{k})^{th}\\) root of unity. We will first see how \\(p\\) splits in \\(\\mathbb{Q}(\\alpha),\\mathbb{Q}(\\beta)\\) and then combine our knowledge to get the splitting behaviour in \\(\\mathbb{Q}(\\omega)\\).</p> <p>When \\(p \\nmid m\\), then \\(k = 0\\) and \\(n = m\\), thus \\(\\alpha = \\omega\\) and \\(\\beta = 1\\). When \\(p \\mid m\\), we consider how \\(p\\) splits in \\(\\mathbb{Q}(\\beta)\\). We know that \\(p = u(1-\\beta)^{\\phi(p ^{k})}\\), and thus \\(p\\mathbb{Z}[\\beta] = (1-\\beta)^{\\phi(p ^{k})}\\), and since \\(\\phi(p ^{k}) = [\\mathbb{Q}(\\beta) :\\mathbb{Q}]\\), we get that \\((1-\\beta)\\) is a prime ideal in \\(\\mathbb{Z}[\\omega]\\) (by theorem 3).</p> <p>Now let's see how it splits in \\(\\mathbb{Q}(\\alpha)\\). Since \\(p \\nmid n\\), \\(p \\nmid \\mathrm{disc}(\\mathbb{Z}[\\alpha]) \\mid n ^{\\phi(n)}\\) (refer to Discriminant of an n-tuple), hence \\(p\\) is unramified in \\(\\mathbb{Z}[\\alpha]\\) and so $$ p\\mathbb{Z}[\\alpha] = P_{1}P_{2}\\dots P_{r} $$ where \\(P_{i}'s\\) are distinct primes each with the same inertial degree \\(f\\), and \\(rf = \\phi(n)\\).</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#claim","title":"Claim:","text":"<pre><code>title:\n$f$ is the order of $p$ mod $n$.\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof_9","title":"Proof:","text":"<p>Note that the galois group of \\(\\mathbb{Q}(\\alpha)\\) over \\(\\mathbb{Q}\\) is the multiplicative group \\(\\mathbb{Z}_{n}^{*}\\). Take the automorphism \\(\\sigma \\in \\mathrm{Gal}(\\mathbb{Q}(\\alpha)/\\mathbb{Q})\\) corresponding to \\(\\bar{p} \\in \\mathbb{Z}_{n}^{*}\\). The order of \\(\\sigma\\) in \\(\\mathrm{Gal}(\\mathbb{Q}(\\alpha)  / \\mathbb{Q})\\) is the same as the order of \\(p\\) mod \\(n\\). Now fix a \\(P_{i} = P\\), then \\(\\mathbb{Z}[\\alpha] /P\\) has degree \\(f\\) over \\(\\mathbb{Z}_{p}\\) and hence the galois group \\(\\mathrm{Gal}((\\mathbb{Z}[\\alpha] /P) / \\mathbb{Z}_{p})\\) is a cyclic group (since finite fields) of order \\(f\\). Let \\(\\tau\\) be the generator of this group (\\(\\tau\\) is the map that takes an element to its \\(p ^{th}\\) power).</p> <p>We need to show that \\(\\sigma\\) and \\(\\tau\\) have the same order. Now let \\(\\sigma^{a} = 1\\), $$ \\begin{align} \\sigma^{a} = 1 &amp;\\iff \\alpha^{p ^{a}} = \\alpha  \\ &amp;\\iff p ^{a} = 1  (\\mathrm{mo d} n) \\ \\end{align} $$ On the other hand, $$ \\begin{align} \\tau^{a} = 1 &amp;\\iff \\alpha^{p ^{a}} = \\alpha  (\\mathrm{mo d}  P) \\ \\end{align} $$ We must show \\(p ^{a} = 1 \\ (\\mathrm{mo d}\\ n)\\). Let \\(p ^{a} = b \\ (\\mathrm{mo d}\\ n)\\), then \\(\\alpha^{b} = \\alpha \\ (\\mathrm{mo d}\\ P) \\implies \\alpha^{b-1} = 1 \\ (\\mathrm{mo d}\\ P)\\). Now we know $$ (1-\\alpha)(1-\\alpha<sup>{2})\\dots(1-\\alpha</sup>{n-1}) = n $$ If \\(b \\neq 1\\), then \\(n \\in P\\) from the above equation, this is impossible since \\(p \\in P\\) and \\((n,p) = 1\\). Thus \\(b = 1\\).</p> <p>Finally, we can put together \\(\\mathbb{Z}[\\alpha]\\) and \\(\\mathbb{Z}[\\beta]\\). Fix primes \\(Q_{1},\\dots,Q_{r}\\) of \\(\\mathbb{Z}[\\omega]\\) lying above \\(P_{1},P_{2},\\dots P_{r}\\) respectively. Since all \\(Q_{i}\\) lie over \\(p\\), they must lie over \\((1-\\beta)\\) since it's the only prime in \\(\\mathbb{Z}[\\beta]\\) lying over \\(p\\). Now \\(e(Q_{i} | p) \\ge e((1-\\beta)|p) = \\phi(p ^{k})\\) and \\(f(Q_{i}|p) \\ge f(P_{i}|p) = f\\). Thus, \\(\\sum_{i}e(Q_{i}|p)f(Q_{i}|p) \\ge r\\phi(p ^{k})f = \\phi(p ^{k}) \\phi(n) = \\phi(m)\\). Thus, \\(e(Q_{i}|p) = \\phi(p ^{k})\\) and \\(f(Q_{i}|p) = f\\) and the \\(Q_{i}'s\\) are the only primes of \\(\\mathbb{Z}[\\omega]\\) lying over \\(p\\).</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#corollary_1","title":"Corollary:","text":"<pre><code>title:\nIf $p \\nmid m$, then $p$ splits into $\\phi(m) /f$ distinct prime ideals in $\\mathbb{Z}[\\omega]$, where $f$ is the order of the prime $p$ mod $n$.\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#method-to-factorise-a-prime-ideal-in-an-extension","title":"Method to factorise a prime ideal in an extension","text":""},{"location":"Splitting%20of%20Primes%20in%20extensions/#theorem-8","title":"Theorem 8:","text":"<pre><code>title:\nLet $R,S,L,K$ as before, and let $n = [L:K]$. Fix an element $\\alpha \\in S$ of degree $n$ over $K$, so that $L = K[\\alpha]$ ([Primitive Element Theorem](&lt;./Primitive Element Theorem.md&gt;)).\nIn general, $R[\\alpha]$ is a subgroup of $S$, and the factor group $S /R[\\alpha]$ is finite (Problem 3 of related problems, [Discriminant of an n-tuple](&lt;./Discriminant of an n-tuple.md&gt;), and the fact that both are free abelian groups of rank $mn$).\nFix a prime $P$ of $R$.\nNow let $g$ be the monic irreducible poly for $\\alpha$ over $K$ (this has coefficients in $R$).\nNow consider $\\overline{g} \\in (R /P)[x]$, it factors uniquely in $(R /P)[x]$ \n$$\ng = \\overline{g_{1}}^{e_{1}}\\dots \\overline{g_{r}}^{e_{r}}\n$$\nwhere the $g_{i}$ are monic irreducible polynomials over $R$ such that $\\overline{g_{i}}$ are all distinct. \n\nNow assume that $p \\nmid |S /R[\\alpha]|$, where $p$ is a prime of $\\mathbb{Z}$ lying under $P$. Then the prime decomposition of $PS$ is given by \n$$\nQ_{1}^{e_{1}}\\dots Q_{r}^{e_{r}}\n$$\nwhere $Q_{i} = (P,g_{i}(\\alpha)) = PS + (g_{i}(\\alpha))$.\nAlso, $f(Q_{i}|P) = \\mathrm{deg}(g_{i})$.\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof_10","title":"Proof:","text":"<p>Let \\(f_{i} := deg(g_{i})\\). We need to show that:  1) \\(PS \\supset Q_{1}^{e_{1}}\\dots Q_{r} ^{e_{r}}\\) 2) \\(Q_{i}+  Q_{j} = S\\) for all \\(i\\neq j\\). 3) For all \\(i\\), either \\(S = Q_{i}\\) or \\(S /Q_{i}\\) is a field of order \\(|R /P| ^{f_{i}}\\) </p> <p>Once we show these, we can conclude as follows,  (3) implies \\(f_{i} = f(Q_{i}|P)\\) (1) implies that \\(PS = \\prod_{j}Q_{j}^{k_{j}}\\) with \\(k_{j} \\le e_{j}\\), giving \\(\\sum_{j} f(Q_{j}|P)k_{j} = mn \\le \\sum_{j}f_{j}e_{j} = deg(g) = mn\\) This gives \\(k_{j} = e_{j}\\) and that all \\(Q_{i}'s\\) are primes and we are done.</p> <ol> <li>This is easy, $$ \\begin{align} Q_{1}^{e_{1}}\\dots Q_{r} ^{e_{r}} &amp;= \\prod_{i}(PS+(g_{i}(\\alpha))) ^{e_{i}} \\ \\ &amp;\\subset PS + \\left(\\prod_{i}g_{i}^{e_{i}}(\\alpha)\\right) \\ &amp;= PS + (0) = PS \\end{align} $$</li> <li>\\(Q_{i}+Q_{j} = PS + (g_{i}(\\alpha)) + (g_{j}(\\alpha))\\).    Since \\(\\overline{g_{i}}\\) and \\(\\overline{g_{j}}\\) are irreducibles in \\((R /P)[x]\\), there are two polys \\(\\overline{u},\\overline{v} \\in (R /P)[x]\\) such that $$ \\begin{align} &amp;\\overline{ug_{i} + vg_{j}} = \\overline{1}  \\ &amp;\\implies ug_{i} +vg_{j} - 1 \\in P [x] \\ &amp;\\implies u(\\alpha)g_{i}(\\alpha) + v(\\alpha)g_{j}(\\alpha) - 1 \\in P[\\alpha] \\subset R[\\alpha] \\subset S \\ &amp;\\implies  u(\\alpha)g_{i}(\\alpha) + v(\\alpha)g_{j}(\\alpha) - 1 \\in PS \\ &amp;\\implies 1 \\in Q_{i} + Q_{j} = PS + (g_{i}(\\alpha)) + (g_{j}(\\alpha)) \\end{align} $$</li> <li>Let \\(F_{i} := (R /P)[x] /(\\overline{g_{i}})\\), this is a field of size \\(|R /P| ^{f_{i}}\\). We show that this is isomorphic to \\(S /Q_{i}\\).  Take the homomorphism \\(R[x] \\to F_{i}\\) defined in the obvious way, modding by \\(P\\), then by \\((\\overline{g_{i}})\\). This is onto, with kernel \\((P,g_{i})\\).  Now map \\(R[x] \\to R[\\alpha] \\subset S\\), this induces a homomorphism \\(R[x] \\to S /Q_{i}\\). The kernel for this contains \\(P\\) and \\(g_{i}\\), and hence it contains \\((P,g_{i})\\) which is a maximal ideal as seen above. Thus the kernel is either \\((P,g_{i})\\) or \\(S\\). Moreover, this is onto, to show this we need to show that \\(R[\\alpha] + Q_{i} = S\\).</li> </ol> <p>We know that \\(p \\in P \\subset Q_{i}\\), hence \\(pS \\subset Q_{i}\\). Thus, \\(S = R[\\alpha] + pS\\), since \\(|S /(R[\\alpha] + pS)|\\) is a common divisor of \\(|S /R[\\alpha]|\\) and \\(|S /pS| = p ^{mn}\\), which is \\(1\\) (prove this using 3rd isomorphism theorem).</p> <p>Thus, we get that \\(S /Q_{i}\\) is either \\(R[x] /(P,g_{i})\\) or is just zero according to the two choices of the kernel.  This gives the result.</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#splitting-primes-in-quadratic-extensions","title":"Splitting primes in quadratic extensions","text":"<p>Let \\(p\\) be a prime in \\(\\mathbb{Z}\\), then there are three possibilities according to theorem 3: $$ pR = \\begin{cases} P^{2}, &amp; f(P|p) = 1 \\ P, &amp; f(P|p) = 2 \\ P_{1}P_{2}, &amp; f(P_{1}|p) = f(P_{2}|p) = 1 \\end{cases} $$</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#theorem-9","title":"Theorem 9:","text":"<pre><code>title:\nWith notation as above, we have:\nif $p \\mid m$, then \n$$\npR = (p,\\sqrt[]{ m })^{2}\n$$\nIf $m$ is odd, then \n$$\n2R = \\begin{cases}\n(2,1 + \\sqrt[]{ m })^{2} &amp; \\text{if } m \\equiv 3 \\ (\\mathrm{mo d}\\ 4) \\\\\n\\left( 2, \\frac{1 + \\sqrt[]{ m }}{2} \\right) \\left( 2, \\frac{1-\\sqrt[]{ m }}{2} \\right)  &amp; \\text{if } m \\equiv 1 \\ (\\mathrm{mo d}\\ 8) \\\\\n\\mathrm{prime} &amp; \\text{if } m \\equiv 5 \\ (\\mathrm{mo d}\\ 8)\n\\end{cases}\n$$\nIf $p$ is odd, $p \\nmid m$ then \n$$\npR = \\begin{cases}\n(p,n + \\sqrt[]{ m })(p,n-\\sqrt[]{ m }) &amp; \\text{if } m\\equiv n^{2} \\ (\\mathrm{mo d}\\ p) \\\\\n\\mathrm{prime} &amp; \\text{if $m$ is not a square mod $p$}\n\\end{cases}\n$$\n</code></pre>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#proof_11","title":"Proof:","text":"<p>Use theorem 8. Let \\(\\alpha = \\sqrt[]{ m }\\). Now \\(R \\supset = \\mathbb{Z}[\\alpha] = \\mathbb{Z}[\\sqrt[]{ m }]\\). This is a proper inclusion iff \\(m \\equiv 1 \\ (\\mathrm{mo d}\\ 4)\\). Note that in the case when \\(m\\) is 1 mod 4, we have \\(|S /\\mathbb{Z}[\\sqrt[]{ m }]| = 2\\). So, for \\(p = 2\\) and \\(m = 1\\) mod 4, we will have to use \\(\\alpha = \\frac{1+\\sqrt[]{ m }}{2}\\).</p> <p>For the first case,  we have \\(g(x) = x^{2}-m\\), \\(\\overline{g}(x) = x^{2} - \\overline{m}\\). If \\(p \\mid m\\), then \\(\\overline{g}(x) = x^{2} \\implies pR = Q^{2}\\) where \\(Q = pR + (\\sqrt[]{ m }) = (p,\\sqrt[]{ m })\\).</p> <p>If \\(p \\nmid m\\), then \\(\\overline{g}\\) is irreducible if \\(m\\) is not a square mod \\(p\\), and reducible if \\(m\\) is a square mod \\(p\\). \\(\\overline{g}(x) = (x-n)(x+n) \\implies pR = (p,\\sqrt[]{ m } + n)(p,n-\\sqrt[]{ m })\\).</p> <p>This takes care of all cases except when \\(p = 2, m = 1\\) mod 4. For that, take \\(\\alpha = \\frac{1 + \\sqrt[]{ m }}{2}\\). Then \\(g(x) = x^{2} - x + \\frac{1-m}{4}\\), taken mod 2, \\(\\overline{g}(x) = x^{2}+x+\\frac{1-m}{4}\\). If \\(m = 1\\) mod 8, then \\(\\overline{g}(x) = x^{2} + x \\implies pR =(2,\\alpha)(2,\\alpha+1)\\). If \\(m = 5\\) mod 8 then \\(\\overline{g}(x)\\) is irreducible and \\(pR\\) is prime.</p>"},{"location":"Splitting%20of%20Primes%20in%20extensions/#references","title":"References","text":"<p>Number Rings Dedekind Domains Trace and Norm Normal Extensions Galois Correspondence Discriminant of an n-tuple Primitive Element Theorem</p>"},{"location":"Spooky%20Day%20Logic%20stuff/","title":"Spooky Day Logic stuff","text":"<ul> <li>Define a 2-counter Automaton, and prove reachability in Counter Automata is undecidable.</li> <li>Given a 2 counter Automaton and 2 natural numbers, check if there is a run that takes \\(\\langle q_{0},0,0\\rangle\\) to \\(\\langle q,n_{1},n_{2}\\rangle\\).</li> <li>For a given machine, we create a first order formula \\(\\phi\\) such that, the machine will only reach the configuration if the first order formula would be valid<ul> <li>\\(L=(R,F,C)\\)<ul> <li>\\(C=Q \\cup \\{0\\}\\)</li> <li>\\(F=\\{S\\}\\) such that \\(\\#S=1\\)</li> <li>\\(R=\\{\\text{conf}\\}\\) such that \\(\\#\\text{conf}=3\\)</li> </ul> </li> <li>\\(\\varphi_{\\text{init}}= \\text{conf}(q_{0},0,0)\\)</li> <li>\\(\\varphi_{t}=\\forall x\\forall y\\forall z\\ \\text{conf}(q_{1},x,y)\\land (S(z)\\equiv y) \\to \\text{conf}(q_{2},S(x),z)\\) </li> <li>Make bigger formula which is just a huge <code>and</code> block</li> <li>Proof is induction on the number of steps required to reach the thingy</li> <li>Then you write a formula which says that if you can reach a given configuration then you can reach the next configuration, and thne you make a formula that says you can get to the desired state if the transitions allow it.</li> <li>lemma<ul> <li>If the formula is valid, then there are 2 numbers that will imply that a state is reachable</li> <li>proof is consider the model where universe is \\(Q \\cup \\mathbb N\\). The Interpretation is true in this model.</li> </ul> </li> <li>Since First Order Logic can model a counter automaton, things will be undecidable.</li> </ul> </li> <li>How to we get back decidability!<ul> <li>We can try restricting stuff. More restriction makes the above prove more tedious, because even with just a binary relation, validity is still dedidable</li> <li>We can try restricting the universe, apparently there is a huge amount of stuff, sir might study a few papers and give in class,<ul> <li>Example FO formulas with just 2 variable, turns out that this is decidable???? what the fuck</li> </ul> </li> </ul> </li> <li>Multi Sorted First Order Logic, where you can have a collection of universes.</li> </ul> <p>[!warning] LOOK say you Universes are A and B in MSFOL you say you have \\(A \\sqcup B\\), and whenever you write \\(\\exists x.A\\ \\phi(x)\\), just write \\(\\exists x.\\phi(x)\\land \\phi_{A}(x)\\) where we define \\(\\phi_{A}(x)\\) to be \\(A\\subseteq A \\sqcup B\\)</p>"},{"location":"Steiner%20Tree%20Problem/","title":"Steiner Tree Problem","text":"<p>202309211309</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note"]},{"location":"Steiner%20Tree%20Problem/#steiner-tree-problem","title":"Steiner Tree Problem","text":"<p>\\(G=(V,E)\\), costs \\(c_{uv}\\geq 0\\) \\(\\forall(u,v)\\in E\\), two types of vertices: required and Steiner</p> <p>Goal: To find a min-cost subset \\(T \\subset E\\) st \\(T\\) is a tree that includes all required vertices and any subset of Steiner vertices.</p> <ul> <li>Lemma: Any approximation for metric case \\(\\implies\\) the same approximation for non metric case</li> </ul>","tags":["Note"]},{"location":"Steiner%20Tree%20Problem/#algorithm","title":"Algorithm","text":"<p>Find an \\(MST\\) on terminal vertices.</p>","tags":["Note"]},{"location":"Steiner%20Tree%20Problem/#analysis","title":"Analysis","text":"<p>Let \\(T\\) be an optimal Steiner tree. Double all edges of \\(T\\) and find an Euler tour (so that later you can go over an edge at most twice). Obtain a \\(TSP\\) tour by shortcutting and eliminating Steiner vertices.</p> <p>\\(\\text{cost}(MST) \\le \\text{cost}(TSP \\text{ tour}) \\le 2.OPT\\)</p> <p>So we get a \\(2-\\)approximation.</p>","tags":["Note"]},{"location":"Steiner%20Tree%20Problem/#generalised-steiner-treesteiner-forest-problem","title":"Generalised Steiner tree/Steiner forest problem","text":"<p>\\(G=(V,E)\\) costs \\(c_{e}\\ge 0\\) \\(\\forall e \\in E\\) \\(s_{1},t_{1},s_{2},t_{2},\\dots,s_{k},t_{k} \\in V\\)</p> <p>Goal: To find a min cost \\(F \\subset E\\) s.t. \\(F\\) has a path between \\(s_{i},t_{i}\\) \\(\\forall i\\).</p>","tags":["Note"]},{"location":"Steiner%20Tree%20Problem/#lp","title":"LP","text":"","tags":["Note"]},{"location":"Steiner%20Tree%20Problem/#primal","title":"Primal","text":"<p>We want one constraint for each \\(S \\subset V\\) s.t. \\(\\exists i S \\cap \\{s_{i},t_{i}\\}=1\\) \\(\\delta(S):\\) set of edges with exactly one end point in \\(S\\)</p> <p>Minimise \\(\\sum\\limits_{e\\in E}c_{e}x_{e}\\) s.t. - \\(\\sum\\limits_{e\\in\\delta(S)}x_{e}\\ge 1\\) \\(\\forall S\\subset V, \\exists i S\\cap\\{s_{i},t_{i}\\}=1\\) - \\(x_{e}\\ge 0\\) \\(\\forall e\\in E\\)</p>","tags":["Note"]},{"location":"Steiner%20Tree%20Problem/#dual","title":"Dual","text":"<p>Maximise \\(\\sum\\limits_{S}y_{S}\\) s.t. - \\(\\sum\\limits_{S|e\\in\\delta(S)}y_{S}\\le c_e\\) \\(\\forall e\\in E\\) - \\(y_{S}\\ge 0\\) \\(\\forall S\\)</p> <pre><code>title: An algo that doesn't work\n1. $F= \\phi$\nOnly maintain non zero valued dual variables.\n2. Set $y_S=0$ $\\forall S$, $x_{e}=0$ $\\forall e \\in E$.\n3. Increase $y_S$ for some $S$. Let $e$ be the edge for which the dual constraint goes tight.\n$F=F\\cup\\{e\\}$.\n4. Continue until all $s_{i}-t_{i}$ get connected.\n\n$$\n\\begin{align*}\n\\text{cost}(F)&amp;=\\sum\\limits_{e\\in F}c_{e}\\\\\n&amp;= \\sum\\limits_{e\\in F}\\sum\\limits_{S|e\\in\\delta(S)}y_{S}\\\\\n&amp;= \\sum\\limits_{S}|F\\cap\\delta(S)|y_{S}\n\\end{align*}\n$$\n\n$|F\\cap\\delta(S)|$ can be as large as $k$ or even $\\frac{n}{2}$ so we can't bound it.\n</code></pre>","tags":["Note"]},{"location":"Steiner%20Tree%20Problem/#new-algorithm","title":"New algorithm:","text":"<p>Let \\(C\\) be the set of connected components \\(S\\) (s.t. \\(\\exists i\\) s.t. \\(S\\cap\\{s_{i},t_{i}\\}=1\\)) in \\(G'=(V,F).\\) Increase dual variables simultaneously for each \\(S \\in C\\).</p> <p>Observe: At each iteration, edges that are added to \\(F\\) connect two distinct connected components. \\(F\\) is a forest.</p> <p>Reverse deletion step: Examine edges in the reverse order of their inclusion in \\(F\\). If their removal keeps each \\(s_{i}-t_{i}\\) connected, remove them. Get \\(F'\\).</p> <pre><code>title: Lemma\nFor any $C$ in any iteration, $\\sum\\limits_{S\\in C}|\\delta(S)\\cap F'| \\le 2|C|$.\n</code></pre> <pre><code>title: Theorem\n$F'$ is a $2-$approximation i.e. $\\sum\\limits_{S}|F'\\cap\\delta(S)|y_{S} \\le 2\\sum\\limits_{S}y_{S} \\le 2.\\text{primal OPT}$\n**Proof** by induction on the number of iterations\n$$\n\\begin{align*}\n\\text{Increase in LHS at an iteration} &amp;=\\epsilon\\sum\\limits_{S\\in C}|F'\\cap\\delta(S)|\\\\\n&amp;\\le 2\\epsilon|C|\\\\\n&amp;=2\\epsilon.\\text{ no of dual vars increased in this iteration}\\\\\n&amp;=\\text{increase in }RHS\\\\\n\\end{align*}\n$$\n</code></pre>","tags":["Note"]},{"location":"Steiner%20Tree%20Problem/#references","title":"References","text":"<p>Linear Programming</p>","tags":["Note"]},{"location":"Stoke%27s%20Theorem/","title":"Stoke's Theorem","text":"<p>202210141010</p> <p>Type : #Note Tags : [[Calc]]</p>"},{"location":"Stoke%27s%20Theorem/#stokes-theorem","title":"Stoke's Theorem","text":""},{"location":"Stoke%27s%20Theorem/#speical-case-of-stokes-theorem-in-mathbb-r2","title":"Speical case of Stoke's Theorem in \\(\\mathbb R^2\\)","text":"<p>Let \\(U\\subset \\mathbb R^2\\) be a connected, non empty and open. A vector field on \\(U\\) is a map \\(\\vec v: U\\to \\mathbb R^2\\). We can write \\(\\vec v(x, y) = \\big(v_x(x,y), v_y(x,y)\\big)\\) where \\(v_x, v_y\\) are real valued function. We will suppose that \\(\\vec v\\) is \\(C^1\\), equivalently \\(v_x, v_y\\) are continuous.</p> <pre><code>title: Vector Field as Fluids\nThink of a fluid flowing in $U$, the vector field $\\vec v$ can be thought of as the vecocity field for the fluid\n</code></pre> <p><pre><code>title: Curl\n$$\n\\nabla \\times \\vec v = \\frac{\\partial v_y}{\\partial x} - \\frac{\\partial v_x}{\\partial y}\n$$\nCurl can be thought of as the amount of \"swirl\" in the fluid or the amount of \"spin\" in the fluid around a point\n</code></pre> <pre><code>title: Divergence\n$$\n\\nabla \\cdot \\vec v = \\frac{\\partial v_x}{\\partial x} + \\frac{\\partial v_y}{\\partial y}\n$$\nDivergence can be thought of as the volume of fluid created or destroyed at a point.\n</code></pre> Let \\(R\\subset U\\) be a rectangle (Represented by ABCD) then the Stoke's theorem says $$ \\begin{aligned} \\int_R\\nabla\\times\\vec v = \\int_{\\vec\\gamma_{AB}}\\vec v\\cdot d\\vec\\gamma_{AB}+\\int_{\\vec\\gamma_{BC}}\\vec v\\cdot d\\vec\\gamma_{BC}+\\int_{\\vec\\gamma_{CD}}\\vec v\\cdot d\\vec\\gamma_{CD}+\\int_{\\vec\\gamma_{DA}}\\vec v\\cdot d\\vec\\gamma_{DA} \\end{aligned} $$</p> <p>In General, the Stokes theorem can be written as  $$ \\int_R\\nabla\\times\\vec v = \\int_{\\vec\\gamma_R} \\vec v\\cdot d\\vec\\gamma_R $$ where \\(\\int_{\\vec\\gamma_R}\\) is a Line Integral.</p>"},{"location":"Stoke%27s%20Theorem/#related-problems","title":"Related Problems","text":""},{"location":"Stoke%27s%20Theorem/#references","title":"References","text":""},{"location":"Stone%27s%20Representation%20Theorem/","title":"Stone's Representation Theorem","text":"<p>202308161610</p> <p>type : #Example #Incomplete  tags : [[Logic]]</p>"},{"location":"Stone%27s%20Representation%20Theorem/#stones-representation-theorem","title":"Stone's Representation Theorem","text":""},{"location":"Stone%27s%20Representation%20Theorem/#theorem","title":"Theorem:","text":"<p>For every boolean algebra is isomorphic to a field of sets.</p>"},{"location":"Stone%27s%20Representation%20Theorem/#proof","title":"Proof:","text":""},{"location":"Stone%27s%20Representation%20Theorem/#theorem_1","title":"Theorem:","text":"<p>\\(\\varphi\\) is a classical tautology iff \\(\\mathbb B\\models \\varphi\\) for all Boolean Algebras \\(\\mathcal B\\)</p>"},{"location":"Stone%27s%20Representation%20Theorem/#proof_1","title":"Proof:","text":"<p>The Implication from right to left is immediate.</p> <p>For the other direction, FTSOC say \\(\\mathcal{B}\\not\\models\\varphi\\) for some \\(\\mathcal B\\). By Stone's Representation Theorem Every Boolean Algebra is a field of sets over some \\(X\\).</p> <p>Since \\(\\mathcal B\\not\\models\\varphi\\), There is a valuation \\(v\\) in \\(\\mathcal B\\) such that \\(\\textlbrackdbl\\varphi\\textrbrackdbl_{v}\\ne X\\). Thus \\(\\exists x\\in X\\) such that \\(x\\notin\\textlbrackdbl\\varphi\\textrbrackdbl_{v}\\) </p> <p>Construct a valuation \\(w\\) in \\(\\mathbb{B}\\) where \\(w(p)=1\\iff x\\in\\textlbrackdbl p\\textrbrackdbl_{v}\\). Then by induction on the size of \\(\\varphi\\) we get \\(w(\\varphi)=1\\iff x\\in\\textlbrackdbl\\varphi\\textrbrackdbl_{v}\\)</p> <p>Thus \\(\\textlbrackdbl\\varphi\\textrbrackdbl_{w}\\ne 1\\)</p>"},{"location":"Stone%27s%20Representation%20Theorem/#related","title":"Related","text":"<p>Boolean Algebra Bruh, Stone was a Chad: Wiki</p>"},{"location":"Strong%20Normalization%20Theorem/","title":"Strong Normalization Theorem","text":"<p>202307242307</p> <p>Type : #Note Tags : [[Type Theory]]</p>"},{"location":"Strong%20Normalization%20Theorem/#strong-normalization-theorem","title":"Strong Normalization Theorem","text":""},{"location":"Strong%20Normalization%20Theorem/#theorem","title":"Theorem:","text":"<p>All terms are reducible</p> <p>Hence all terms are Strongly Normalizable.</p> <p>In the proof of the theorem, we need a stronger induction hypothesis to handle the case of abstraction. This is the purpose of the following proposition, from which the theorem follows by butting \\(u_{i}=x_{i}\\)</p>"},{"location":"Strong%20Normalization%20Theorem/#proposition","title":"Proposition","text":"<p>Let \\(t\\) be any term, and suppose all the free variables of \\(t\\) are among \\(x_{1}, x_{2}\\dots x_{n}\\) of the types \\(U_{1}, U_{2}\\dots U_{n}\\). if \\(u_{1}, u_{2}\\dots u_{n}\\) are reducible terms of the above types then \\(t[u_{1}/x_{1}, u_{2}/x_{2}, \\dots u_{n}/x_{n}]\\) is reducible</p>"},{"location":"Strong%20Normalization%20Theorem/#proof","title":"Proof","text":"<p>By induction of \\(t\\). We write \\(t[u/x]\\) for the above term 1. \\(t\\) is \\(x_{i}\\) : Trivial 2. \\(t\\) is \\(\\pi^{1}w\\): by induction hypothesis, for every sequence \\(\\underline u\\) of reducible terms, \\(w[u/x]\\) is reducible. That means that \\(\\pi^{1}[u/x]\\) is reducible, but this term is nothing other than \\(\\pi^{1}w[u/x]=t[u/x]\\) 3. \\(t\\) is \\(\\pi^{2}w\\): Same as the above point 4. \\(t\\) is \\(\\langle v, w \\rangle\\): by induction hypothesis both \\(v[u/x]\\) and \\(w[u/x]\\) are reducible.    Reducibility Theorems#Pairing says that \\(t[u/x]=\\langle v[u/x],w[u/x]\\rangle\\) is reducible. 5. \\(t\\) is \\(w\\ v\\): by induction hypothesis \\(w[u/x]\\) and \\(w[v/x]\\) are reducible, anbd so by definition \\(w[u/x](v[u/x])\\) is reducible which is just \\(t[u/x]\\) 6. \\(t\\) is \\(\\lambda y.w\\) of type \\(V\\to W\\): by induction hypothesis, \\(w[u/x,v]\\) is reducible for all \\(v\\) of type \\(V\\). Reducibility Theorems#Abstraction says that \\(t[u/x]=\\lambda y.(w[u/x])\\) is reducible</p> <p>Hence, the proposition and the preceding theorem is also true.</p>"},{"location":"Strong%20Normalization%20Theorem/#references","title":"References","text":"<p>Reducibility</p>"},{"location":"Strongly%20Regular%20Graphs/","title":"Strongly Regular Graphs","text":"<p>202308281108</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Strongly%20Regular%20Graphs/#strongly-regular-graphs","title":"Strongly Regular Graphs","text":"<pre><code>title: intuition\nThe idea of *Strong Regularity*, along with the degree of vertices, also makes the relation between vertices identical, in terms of the number of common vertices\n</code></pre> <p>Let \\(G\\) be a \\(k-\\)regular graphs. Then \\(G\\) is said to be strongly regular if \\(\\exists\\) constants \\(a,c\\) such that - \\(\\forall x\\sim y\\) then the number of \\(z\\) adjacent to both \\(x\\) and \\(y\\) is \\(a\\) - otherwise, the number of adjacent vertices to both vertices is \\(c\\)</p> <p>The Peterson Graph is Strongly regular with \\(c=1, a=0\\).</p> <pre><code>title: Complement\nThe Complement of a *Strongly Regular* Graph is also a *Strongly Regular* Graph with $a'=n-2k+c-2$ and $c'=n-2k+a$\n</code></pre>"},{"location":"Strongly%20Regular%20Graphs/#references","title":"References","text":"<p>Triangular Graphs Lattice Graphs</p>"},{"location":"Structure%20Preserving%20Maps/","title":"Structure Preserving Maps","text":"<p>202301270201</p> <p>Type : #Note Tags : [[Category Theory]]</p>"},{"location":"Structure%20Preserving%20Maps/#structure-preserving-maps","title":"Structure Preserving Maps","text":"<p>A central theme in cateogry theory is the study of structure preserving maps.  A map \\(f:X\\to Y\\) is like an observation of \\(X\\) via a relationship it has with another object, \\(Y\\) amd asking which aspects of \\(X\\) one wants to preserve under the observation \\(f\\) simply becomes the question what category are you working in?.</p> <p>As an example, there are a lot of funtions from \\(\\mathbb R\\) to \\(\\mathbb R\\), and we can think of them as observations: rather than view \\(x\\) directly, we only observe \\(f(x)\\). Out of all functions \\(f;\\mathbb R\\to \\mathbb R\\), only some of them are: - Order preserving(Preorder): if \\(x\\le y\\) implies \\(f(x)\\le f(y)\\), for all \\(x, y\\in \\mathbb R\\); - Metric preserving: if \\(|x-y|=|f(x)-f(y)|\\), for all \\(x, y\\in \\mathbb R\\); - Order preserving: if \\(f(x+y)= f(x) + f(y)\\), for all \\(x, y\\in \\mathbb R\\);</p> <p>For each of the three properties defined above (call it foo), there exists fuctions which preserve foo and are called foo-preserving.</p> <p>The less structure is preserved by our observation, the more surprises occur when we observe its operations. One might call it Generating Effects</p>"},{"location":"Structure%20Preserving%20Maps/#references","title":"References","text":""},{"location":"Sub-basis/","title":"Sub basis","text":"<p>202301071201</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Sub-basis/#sub-basis","title":"Sub-basis","text":"<pre><code>title: Definition\nLet $X$ be a set. Sub-basis $\\mathcal{S}$ on $X$ is a collection of subsets of $X$ whose union equals $X$. Topology generated by $S$, $\\tau_{\\mathcal{S}}$ is the collection of all unions of finite intersections of members of $S$.\n</code></pre>"},{"location":"Sub-basis/#title-lemma-the-collection-mathcalb-of-all-finite-intersections-b-s_1cap-s_2-cap-cdots-cap-s_n-is-a-basis-on-x-proof-covering-property-holds-since-it-holds-for-s-check-that-gluing-property-holds","title":"<pre><code>title: Lemma\nThe collection $\\mathcal{B}$ of all finite intersections $B = S_1\\cap S_2 \\cap \\cdots \\cap S_n$ is a basis on $X$. \n\n#### Proof:\nCovering property holds since it holds for $S$, check that gluing property holds.\n</code></pre>","text":""},{"location":"Sub-basis/#related-problems","title":"Related Problems","text":""},{"location":"Sub-basis/#references","title":"References","text":"<p>Bases for a topology</p>"},{"location":"Subset%20Relation%20between%20Timed%20Regular%20Languages/","title":"Subset Relation between Timed Regular Languages","text":"<p>202309051009</p> <p>Type : #Note Tags : Timed Automata, [[Theory of Computation]]</p>"},{"location":"Subset%20Relation%20between%20Timed%20Regular%20Languages/#subset-relation-between-timed-regular-languages","title":"Subset Relation between Timed Regular Languages","text":"<p>Theorem: Let \\(\\mathcal A\\) be a Timed Automaton. The problem of checking whether \\(\\mathcal L(\\mathcal A)=T\\Sigma^*\\) is Undecidable.</p> <p>Proof: We will prove this by encoding the membership problem of a deterministic [[Counter Automata|2-counter machines]] as a universality problem.</p> <p>We define a state of the counter automata as follows  $$ (q,i,c,d) $$ where \\(q\\) is the state, \\(i\\) is the position of the head on the tape, \\(c\\) is the value of first counter and \\(d\\) is the value of the second counter.</p> <p>Here the first two components both have a finite value, hence a finite alphabet suffices to represent all pairs \\((q, i)\\) which will be read at integer time stamps.</p> <p>The values of the counters are countably infinite. Hence we cannot do the same thing we did before, instead we add 2 new letters to the alphabet, which are \\(a_{c}, a_{d}\\). We accept \\(c_{i}\\) many \\(a_{c}\\) and \\(d_{i}\\) many \\(a_{d}\\) after accepting \\((q_{i},i_{i})\\) and before \\((q_{i+1},i_{i+1})\\). To read the next state, we read \\(a_{c}\\) again exactly after \\(1\\) time unit and read an extra or skip reading one to represent increment and decrement of the counters.</p> <p>A timed word \\((\\omega,\\tau)\\) over \\(\\Sigma_\\text{enc}\\) encodes a run of a counter automata if - Every integral point until the last time stamp contains a \\((q, i)\\) letter. - At each \\((i, i+1)\\) the word is of the form \\(a_{c}^*a_{d}^{*}\\). No two letters come at the same time stamp. - The interval \\([0,1)\\) encodes the initial configuration. - \\([i,i+1)[i+1, i+2)\\) encodes the appropriate transition given at \\(i\\) - The last configuration should be accepting.</p> <p>The idea is, we make a timed automata whose language is the complement of the above language, and checking of universality of the language gives membership of the counter automata which is undecidable.</p> <p>To do that, we just make an automata to fail a condition for every condition, and then take the union of all of them.</p> <pre><code>Make the automata for all the conditions.\n</code></pre>"},{"location":"Subset%20Relation%20between%20Timed%20Regular%20Languages/#references","title":"References","text":"<p>Untiming Timed Automata Undecidability of the Membership Problem</p>"},{"location":"Subspace%20Topology/","title":"Subspace Topology","text":"<p>202301161101</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Subspace%20Topology/#subspace-topology","title":"Subspace Topology","text":""},{"location":"Subspace%20Topology/#definition","title":"Definition","text":"<p><pre><code>title:\nLet $X$ be a topo space, and $Y \\subset X$, $\\tau_Y = \\{U \\cap Y: U \\in \\tau_X\\}.$\nThis topology is said to be _inherited_ from $X$ and is called the **subspace topology**.\n</code></pre> We can check that this is indeed a topology.</p>"},{"location":"Subspace%20Topology/#lemma","title":"Lemma:","text":"<p>If \\(\\mathcal{B}\\) is a basis for \\(\\tau_X\\), then \\(\\mathcal{B}_Y = \\{B \\cap Y : B \\in\\mathcal{B}\\}\\) is a basis on \\(Y\\) for \\(\\tau_Y\\). </p> <p>Proof: 1. Covering : \\(\\bigcup \\limits_{c \\in \\mathcal{B}_Y} c = \\bigcup \\limits_{B \\in\\mathcal{B}} \\cap Y = Y\\) 2. Gluing: \\(C_1,C_2 \\in \\mathcal{B}_Y\\), \\(C_1 \\cap C_2 = B_1 \\cap B_2 \\cap Y\\). For any \\(x \\in B_1 \\cap B_2 \\cap Y\\), there is a \\(B_3 \\in \\mathcal{B}\\) such that \\(x \\in B_3 \\subset B_1 \\cap B_2\\). Therefore, \\(x \\in B_3 \\cap Y = C_3 \\in \\mathcal{B}_Y\\).  </p>"},{"location":"Subspace%20Topology/#examples","title":"Examples","text":"<ol> <li>Any subspace of a discrete space is discrete.</li> <li>\\(Y = (a,b) \\subset \\mathbb{R}\\). \\(\\mathcal{B}_{(a,b)} = \\{(c,d) : a\\le c &lt; d\\le b\\}\\).</li> <li>\\(Y = [a,b) \\subset \\mathbb{R}\\). \\(\\mathcal{B}_Y = \\{[a,d),(c,d) : a&lt;c&lt;d \\le b \\}\\). </li> <li>\\(Y = \\mathbb{Z} \\subset X = \\mathbb{R}\\). For any \\(n\\in \\mathbb{Z}, \\{n\\} = (n-1/2,n+1/2) \\cap \\mathbb{Z}\\). discrete subspace.</li> <li>\\(Y = \\{1/n : n \\in \\mathbb{Z}\\} \\cup \\{0\\} \\subset X = \\mathbb{R}\\)  not discrete topo.</li> <li>\\(X = \\mathbb{R}_{\\ell} \\supset [a,b] = Y\\). \\(\\{b\\}\\)  is open in the subspace topo, no other singleton is open. </li> </ol>"},{"location":"Subspace%20Topology/#question-is-mathbbq-open-in-mathbbr","title":"question: is \\(\\mathbb{Q}\\) open in \\(\\mathbb{R}\\)?","text":""},{"location":"Subspace%20Topology/#lemma-open-in-open-is-open","title":"Lemma: (open in open is open)","text":"<p>Let \\(X\\) be a T.S., \\(Y\\subset X\\). If \\(U\\) is open in Y, Y is open in Xn then U is open in X. (Sufficient but not necessary condition).</p>"},{"location":"Subspace%20Topology/#lemma-subspace-of-a-subspace-is-a-subspace","title":"lemma: (subspace of a subspace is a subspace)","text":"<p>Let X be a T.S., Y is a subspace of X, A is a subset of Y. Then the subset topo on A from Y is the same as that from X.</p>"},{"location":"Subspace%20Topology/#related-problems","title":"Related Problems","text":""},{"location":"Subspace%20Topology/#references","title":"References","text":""},{"location":"Successor%20invariant%20FOL%20on%20finite%20structures/","title":"Successor invariant FOL on finite structures","text":"<p>202311211411</p> <p>Tags : [[Logic]]</p>","tags":["Note","Incomplete"]},{"location":"Successor%20invariant%20FOL%20on%20finite%20structures/#successor-invariant-fol-on-finite-structures","title":"Successor invariant FOL on finite structures","text":"<p>\\(\\sigma:\\{ R,F,C\\}\\), \\(S\\in F\\) (successor function which we will call a relation), \\(\\#(S)=1\\) (the arity is 1) A formula \\(\\varphi\\) is said to be successor invariant on a class of structures \\(\\mathcal{C}\\) if for all \\(\\mathcal{M}\\in \\mathcal{C}\\), for all successor relations \\(S_{1},S_{2}\\) on \\(|\\mathcal{M}|\\) (why mod??), \\(\\mathcal{M},S_{1}\\vDash\\varphi\\) iff\\(\\mathcal{M},S_{2}\\vDash\\varphi\\).</p> <p>Fo-succ inv: Class of structures (??) that can be defined using the successor relation in FO, and are succ inv.</p> <p>[!info] Fo-succ inv \\(\\supsetneq\\) FO on finite structures.</p> <p>[!info] Fo-succ inv = FO on finite bounded degree structures. Finite bounded degree refers to the max degree of an element in the (??) graph, which just counts the number of elements it appears with, together, in some relation.</p>","tags":["Note","Incomplete"]},{"location":"Successor%20invariant%20FOL%20on%20finite%20structures/#order-invariant-fo","title":"Order invariant FO","text":"<p>\\(\\in,&lt;\\) \\((X,\\mathcal{P}(X)):\\) structures over \\(\\{ \\epsilon \\}\\) \\(a\\in b\\) iff \\(b\\in\\mathcal{P}(X)\\) and \\(a\\) is in \\(b\\).</p> <p>\\(\\exists y:S_{2}\\ \\forall x:S_{1}\\quad x \\not \\in y\\) (empty set exists) \\(\\forall x:S_{1}\\ \\exists y:S_{2}\\quad(x \\in y \\land \\forall z:S_{1}\\ z\\in y \\implies z=x)\\) (singletons exists for all)\\(\\forall y_{1},y_{2}:S_{2}\\ \\exists y_{3}:S_{2}\\quad(\\forall x:S_{1}\\ x \\in y_{1}\\implies x \\in y_{3})\\land(\\forall x:S_{1}\\ x \\in y_{2}\\implies x \\in y_{3})\\land(\\forall x:S_{1}\\ x \\in y_{3}\\implies x \\in y_{1}\\lor x \\in y_{2})\\)(closed under union)</p>","tags":["Note","Incomplete"]},{"location":"Successor%20invariant%20FOL%20on%20finite%20structures/#references","title":"References","text":"<p>First Order Logic [Successor invariant FOL on finite structures, Benjamin, Rossman, Journal of symbolic Logic 2007]</p>","tags":["Note","Incomplete"]},{"location":"Sum%20Constructor%20Pattern%20Matching%20to%20Lambda%20Calculus/","title":"Sum Constructor Pattern Matching to Lambda Calculus","text":"<p>202311041911</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note","Incomplete"]},{"location":"Sum%20Constructor%20Pattern%20Matching%20to%20Lambda%20Calculus/#sum-constructor-pattern-matching-to-lambda-calculus","title":"Sum Constructor Pattern Matching to Lambda Calculus","text":"<p>This pattern checks if the given input matches one of the sum constructors of arity \\(r\\) which is written as $$ \\lambda(s p_{1}\\dots p_{r}) $$ and the semantics is given by </p> <p>Once again, we make a new function \\(\\text{UNPACK-SUM-s}\\) and use it to implement the above in the following way $$ (\\lambda(s p_{1} p_{2} \\dots p_{r}).E)\\quad\\equiv\\quad  \\text{UNPACK-SUM-t}\\; (\\lambda p_{1} p_{2}\\dots p_{r}.E) $$ and we define \\(\\text{UNPACK-SUM-s}\\) in the following way $$ \\begin{align} &amp;\\text{UNPACK-SUM-s} f (s a_{1}\\dots a_{r})\\quad&amp;&amp;\\equiv\\quad f a_{1}\\dots a_{r}\\ &amp;\\text{UNPACK-SUM-s} f (s' a_{1}\\dots a_{r})\\quad&amp;&amp;\\equiv\\quad \\text{FAIL} &amp;\\text{if }s'\\ne s \\ &amp;\\text{UNPACK-SUM-s} f \\bot\\quad&amp;&amp;\\equiv\\quad\\bot \\end{align} $$</p> <p>[!example] Consider the following Haskell Program <pre><code>reflect (LEAF n) = LEAF n\nreflect (BRANCH t1 t2) = BRANCH (reflect t2) (reflect t1)\n</code></pre> which gets converted to  <pre><code>reflect = \\t.( ((\\(LEAF n).LEAF n) t)\n             |&gt; ((\\BRANCH t1 t2).BRANCH (reflect t2) (reflect t1) t)\n             |&gt; ERROR)\n</code></pre> and this translates to <pre><code>&gt;reflect = \\t.( UNPACK-SUM-LEAF (\\n.LEAF n) t)\n             |&gt; (UNPACK-SUM-BRANCH (\\t1 t2.BRANCH (reflect t2) (reflect t1)) t)\n             |&gt; ERROR)\n</code></pre></p>","tags":["Note","Incomplete"]},{"location":"Sum%20Constructor%20Pattern%20Matching%20to%20Lambda%20Calculus/#references","title":"References","text":"<p>Patterns</p>","tags":["Note","Incomplete"]},{"location":"Support%20of%20an%20Automorphism%20on%20a%20Graph/","title":"Support of an Automorphism on a Graph","text":"<p>202308141108</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Support%20of%20an%20Automorphism%20on%20a%20Graph/#support-of-an-automorphism-on-a-graph","title":"Support of an Automorphism on a Graph","text":"<pre><code>title:definition\nThe *Support* of an automorphism is the set of objects that are not fixed by the automorphism.\n</code></pre>"},{"location":"Support%20of%20an%20Automorphism%20on%20a%20Graph/#lemma","title":"Lemma","text":"<p>Let \\(m\\) be an even number and \\(|A|=m\\) and \\(A\\subset [n]\\). Then \\(\\sigma'\\) has the largest number of orbits on \\(E[n]\\) iff \\(\\sigma'\\) is an involution that consists of a product of \\(\\frac{m}{2}\\) transposition</p>"},{"location":"Support%20of%20an%20Automorphism%20on%20a%20Graph/#lemma_1","title":"Lemma","text":"<p>Let \\(\\sigma\\in S_{n}\\) such that \\(\\sigma\\) is a product of \\(r\\) transpositions and \\(n-2r\\) fixed points. Then \\(\\text{orb}(\\sigma')={n\\choose 2}-r(n-r-1)\\)</p> <p>For the proof, consider the possible transposition of edges. There are two possible situations $$ \\begin{align} \\sigma': {x, y}\\longmapsto {z, w}\\ \\sigma': {x, y}\\longmapsto {x, w} \\end{align} $$ where \\(x,y,z,w\\) are all distinct For case \\(2\\), there are \\((n-2r)\\times r\\) ways For case \\(1\\), there are \\(\\frac{1}{2}{r\\choose 2}\\times 4=r(r-1)\\) Hence the total number of transposition are \\(r(n-r-1)\\)</p>"},{"location":"Support%20of%20an%20Automorphism%20on%20a%20Graph/#references","title":"References","text":""},{"location":"Syntax%20of%20First%20Order%20Logic/","title":"Syntax of First Order Logic","text":"<p>202309211509</p> <p>Tags : [[Logic]]</p>","tags":["Note"]},{"location":"Syntax%20of%20First%20Order%20Logic/#syntax-of-first-order-logic","title":"Syntax of First Order Logic","text":"","tags":["Note"]},{"location":"Syntax%20of%20First%20Order%20Logic/#syntax","title":"Syntax","text":"","tags":["Note"]},{"location":"Syntax%20of%20First%20Order%20Logic/#first-order-languages","title":"First Order Languages","text":"<p>To define the formulas of First Order Logic we first fix the underlying Language. A First Order Language is a triple \\(L= (R,F,C)\\) where - \\(R\\) is a countable set of Relation Symbols - \\(F\\) is a countable set of Function Symbols - \\(C\\) is a countable set of Constants We also have a countable set \\(Var\\) of variables.</p>","tags":["Note"]},{"location":"Syntax%20of%20First%20Order%20Logic/#terms","title":"Terms","text":"<p>We now build terms, that would then be connected to give formulas. The set of Terms over a First Order Language \\(L\\) is the smallest set satisfying the following condition. - Every constant \\(c\\in C\\) is a term - Every variable \\(x\\in Var\\) is a term - Let \\(t_{1},t_{2}\\dots,t_{n}\\) be terms over \\(L\\) and let \\(f\\in F\\) be a function symbol of arity \\(n\\). Then \\(f(t_{1},t_{2}\\dots t_{n})\\) is a term. A term which does not contain any variables is called closed.</p>","tags":["Note"]},{"location":"Syntax%20of%20First%20Order%20Logic/#atomic-formulas","title":"Atomic Formulas","text":"<p>The atomic formulas of \\(L\\) are defined as follows. - Let \\(r\\in R\\) be a relation symbol and \\(t_{1}\\dots t_{n}\\) are terms over \\(L\\) then \\(r(t_{1},\\dots t_{n})\\) is an atomic formula - Let \\(t_1\\) and \\(t_{2}\\) be terms, then \\(t_{1}\\equiv t_{2}\\) is an atomic formulas</p>","tags":["Note"]},{"location":"Syntax%20of%20First%20Order%20Logic/#formulas","title":"Formulas","text":"<p>Given the above set of atomic formulas, we can define the complete set of formulas \\(\\Phi_{L}\\) is the following way. - Every atomic formula over \\(L\\) belongs to \\(\\Phi_{L}\\) - If \\(\\varphi\\in \\Phi_{L}\\) then \\(\\lnot\\varphi\\in\\Phi_{L}\\) - If \\(\\varphi,\\psi\\in\\Phi_{L}\\) then \\(\\varphi\\lor\\psi\\in\\Phi_L\\) - If \\(\\varphi\\in\\Phi_{L}\\) and \\(x\\in Vars\\), then \\(\\exists x\\ \\varphi\\in\\Phi_{L}\\)  And we use parentheses to disambiguate the formula.</p> <pre><code>The *Terms* and things related to it all, live inside the universe, Hence symbols from $F$ are only used in terms. The *Formulas* are to be interpreted as true of false statements. \n</code></pre>","tags":["Note"]},{"location":"Syntax%20of%20First%20Order%20Logic/#references","title":"References","text":"<p>First Order Logic Semantics of First Order Logic</p>","tags":["Note"]},{"location":"System%20of%20First%20Order%20Differential%20Equations/","title":"System of First Order Differential Equations","text":"<p>202301111201</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"System%20of%20First%20Order%20Differential%20Equations/#system-of-first-order-differential-equations","title":"System of First Order Differential Equations","text":"<p>$$ \\text{x bar raised to dot is fancy v bar(t, xbar)} $$ \\(\\overline x = (x_1,x_2, \\dots, x_n)\\in\\mathbb R^n\\) \\(\\overline v:\\Omega\\to \\mathbb R^n\\)  This is equivalent to solving \\(n\\) differential equation for each dimentions</p>"},{"location":"System%20of%20First%20Order%20Differential%20Equations/#n1","title":"n=1","text":"<p>This just becomes a single first order differential equation. $$ {\\partial x\\over \\partial t} = t $$</p>"},{"location":"System%20of%20First%20Order%20Differential%20Equations/#n2","title":"n=2","text":"<p>$$ \\begin{aligned} {\\partial x_1\\over \\partial t} = x_1\\ {\\partial x_2\\over \\partial t} = x_2\\ \\end{aligned} $$ This is an example of a decoupled system of differnetial eqautions since both the differential equations can be solved independently to get the \\(2\\) components of the answer</p>"},{"location":"System%20of%20First%20Order%20Differential%20Equations/#otherwise","title":"otherwise","text":"<p>Proposition: Any \\(n^{th}\\) order differential equ ation can be reduced to a system of \\(n\\) first order differential equations. Proof: $$ x^{(n)}=F(t,x, x', x'',\\dots,x^{(n-1)}) $$ where the \\(i^{th}\\) derivative can be then written in terms of all \\(j^{th}\\) derivatives where \\(j&gt; i\\). This can be solved by succesively solving the differnetial equations from the last term tot he first term.</p> <p>Systems that are not dependent on \\(t\\) are called autonomous systems</p>"},{"location":"System%20of%20First%20Order%20Differential%20Equations/#related-problems","title":"Related Problems","text":""},{"location":"System%20of%20First%20Order%20Differential%20Equations/#references","title":"References","text":""},{"location":"T-Transitiviy%20of%20Graphs/","title":"T Transitiviy of Graphs","text":"<p>202308161038</p> <p>type : #Example tags : [[Algebraic Graph Theory]]</p>"},{"location":"T-Transitiviy%20of%20Graphs/#t-transitiviy-of-graphs","title":"T-Transitiviy of Graphs","text":"<pre><code>title: defintion\nLet $t&gt;1$ be an integer. Let $(\\Gamma, X)$ be a permutation group. $T$ is said to be $t-$transitive on $X$ if $\\forall$ ordered $t-$tuples $(x_{1},\\dots,x_{n})$ and $(y_{1},\\dots,y_{n})$ of distinct elements, $\\exists \\alpha\\in\\Gamma$ such that $\\alpha(x_{i})=y_{i}$ for all $i$. For $t=1$ we call $\\Gamma$ tranistive\n</code></pre> <ul> <li>\\(m-\\)transitive implies \\(n-\\)transitive for all \\(n\\le m\\)</li> <li>Let \\(x\\in X\\) and \\((\\Gamma, X)\\) be transitive and \\(t&gt;1\\). T\\((t-1)-\\)transitive on \\(X\\setminus \\{x\\}\\) and conversely.</li> </ul> <p>proof of converse: <pre><code>\\usepackage{tikz-cd}\n\\begin{document}\n\\begin{tikzcd}\n{(z_1, z_2,\\dots, z_r)} \\arrow[rr, \"\\alpha\"] \\arrow[dd, \"\\beta^{-1}\\circ\\gamma\\circ\\alpha\"', dashed] &amp;  &amp; {(x, z_2',\\dots,z_r`)} \\arrow[d, \"\\gamma\"]                   \\\\\n                                                                                                     &amp;  &amp; {(x,w_2',\\dots,w_r')} \\arrow[lld, \"\\beta^{-1}\"', bend right] \\\\\n{(w_1, w_2,\\dots, w_r)} \\arrow[rr, \"\\eta\"'] \\arrow[rru, \"\\beta=\\delta\\circ\\eta\"']                    &amp;  &amp; {(x, w_2'',\\dots, w_r'')} \\arrow[u, \"\\delta\"']              \n\\end{tikzcd}\n\\end{document}\n</code></pre></p> <pre><code>The second property above is useful for finding transitivity of graphs.\n</code></pre>"},{"location":"T-Transitiviy%20of%20Graphs/#related","title":"Related","text":"<p>Orbits on X and X * X Generously Transitive Kneser Graph Edge-Transitive, Non Transitive is Bipartite</p>"},{"location":"TOC%20Problem%20Set%201%20%28not%20graded%29/","title":"TOC Problem Set 1 (not graded)","text":"<p>TOC Problem Set 1 Not to be graded. </p>"},{"location":"TOC%20Problem%20Set%201%20%28not%20graded%29/#1","title":"1.","text":"<p>Design DFAs for the following language in \\(\\{0,1\\}^*\\) - \\(L=\\{w\\ |\\ \\text{Number of }0\\text{s and } 1\\text{s are both even} \\}\\) - \\(L=\\{w\\ |\\ w\\ \\text{read as a binary number is divisible by }3\\}\\) </p>"},{"location":"TOC%20Problem%20Set%201%20%28not%20graded%29/#2-subsequence","title":"2. Subsequence","text":"<p>For any word \\(w\\in {0,1}^*\\), let \\(L_{w} = \\{y\\ |\\ w\\text{ is a subsequence of }y\\}\\); any string \\(s'\\) which can be formed by dropping letters from \\(s\\) while maintaining ordering is subsequence of \\(s\\).</p> <p>Design a DFA when: - \\(w=1101\\) - \\(w=010001\\)</p> <p>Show that a similar construction works for any \\(w\\in \\{0,1\\}^*\\). </p>"},{"location":"TOC%20Problem%20Set%201%20%28not%20graded%29/#3","title":"3.","text":"<p>Is the language \\(L=\\{a^{m}+ a^{n}=a^{m+n}\\}\\) over the alphabet \\(\\{a,+,=\\}\\) regular? (Here \\(a^{m}\\) is \\({aa\\dots a}\\) repeated \\(m\\) times.) </p>"},{"location":"TOC%20Problem%20Set%201%20%28not%20graded%29/#4","title":"4.","text":"<p>Given a language on a singleton alphabet, give a criterion that would determine if it is a regular language. </p>"},{"location":"TOC%20Problem%20Set%201%20%28not%20graded%29/#5","title":"5.","text":"<p>For any languages \\(L_{1}\\) and \\(L_{2}\\) on alphabets \\(\\Sigma_{1}\\) and \\(\\Sigma_{2}\\), let \\(\\Sigma=\\Sigma_{1}\\cup\\Sigma_{2}\\).  Given a word \\(w\\) over \\(\\Sigma\\), let:  1. \\(\\pi_{1}(w)\\) be the word obtained from \\(w\\) by deleting all the letter that are not in \\(\\Sigma_{1}\\) 1.  \\(\\pi_{2}(w)\\) be the word obtained from \\(w\\) by deleting all the letter that are not in \\(\\Sigma_{2}\\)</p> <p>If \\(L_{1}\\) and \\(L_{2}\\) are regular then: - Given the DFAs for \\(L_1\\) and \\(L_{2}\\), if \\(\\Sigma_{1}\\cap \\Sigma_{2}=\\emptyset\\) then design a DFA for \\(L=\\{w\\ |\\ \\pi_{1}(w)\\in L_{1}, \\pi_{2}(w)\\in L_{2} \\}\\). - Given the NFAs for \\(L_1\\) and \\(L_{2}\\) design an NFA for the above \\(L\\). </p>"},{"location":"TOC%20Problem%20Set%201%20%28not%20graded%29/#6","title":"6.","text":"<p>Give an algorithm to check if a given NFA accepts any words at all (i.e. its language is not empty).  </p>"},{"location":"TOC%20Problem%20Set%201%20%28not%20graded%29/#7","title":"7.","text":"<p>Give an algorithm to check if a given NFA  accepts a word of even length. </p>"},{"location":"TOC%20Problem%20Set%201%20%28not%20graded%29/#8","title":"8.","text":"<p>Given a regular language \\(L\\) and its DFA: - Find a DFA for \\(\\text{Rev}(L)=\\{w^{R}\\ |\\ w\\in L\\}\\). - Find a DFA for \\(\\text{Pal}(L)=\\{ww^{R}\\ |\\ w\\in L\\}\\), or show that the language is not always regular.</p>"},{"location":"TUE%207%20Lec%20Notes/","title":"TUE 7 Lec Notes","text":"<p>[!quote] Srivathsan See It is possible to do it, because I will tell you how to do it.</p>"},{"location":"TUE%207%20Lec%20Notes/#automata-learning","title":"Automata Learning","text":"<ul> <li>Valid Questions<ul> <li>Give a word and get a yes/no : Membership</li> <li>Give a guess for the language and a get yes / counterexample: Equivalence</li> </ul> </li> <li>Idea: use strings to identity the states.</li> <li>If we have a language with a minimal DFA with \\(n\\) states, then we will need atleast \\(n\\) equivalence queries. </li> <li>Story: Student ask a Teacher.</li> <li>Main Ideas:<ul> <li>States Identified using Strings</li> <li>Start Building Automata based on the tiny information we have at all points along the way and keep building a table with membership queries of the words and their 1 letter extensions.</li> <li>DANA Anguling Paper</li> </ul> </li> <li>Applications:<ul> <li>testing of telecommunication systems at Seimens</li> <li>Integration testing at France Telecom</li> <li>Automatic testing of an online conference service at Springer-Verlag</li> <li>Testing requirements of a brake-by-wire system from Volvo</li> <li>Models of a smart card reader for internet banking</li> <li>Learning Models of Legacy Software</li> <li>Models of network protocols</li> </ul> </li> </ul>"},{"location":"TUE%207%20Lec%20Notes/#ifpl-graph-reduction","title":"IFPL - Graph Reduction","text":"<ul> <li>Represent Application, lambda abstraction, constants and inbuilt functions as graphs.</li> <li>For situations were arguments need to be copied  <p>[!quote] Suresh Arunava: Sir, do we give presentation in Pairs Suresh: I don't know, in general we will first do baby steps by attending classes</p> </li> </ul>"},{"location":"Tensors/","title":"Tensors","text":"<p>202210062110</p> <p>Type : #Note   Tags : [[Calc]]</p>"},{"location":"Tensors/#tensor-algebra","title":"Tensor Algebra","text":"<p>[!note] Tensor Algebra</p> <p>To any Vector space \\(V\\), we can associate a Tensor Algebra \\(\\mathcal{T}(V)\\), called the Tensor Algebra \"over\" \\(V\\)</p> <p>We set \\(\\mathcal{T}^0(V) = \\mathbb{R}, \\mathcal{T}^1(V) = V^*,\\) similarly $$ \\mathcal{T}^k(V) =\\text{set of all multilinear maps }T: \\underbrace{V\\times\\dots\\times V}_\\text{k times}\\to\\mathbb{R} $$ we set  $$ \\mathcal{T}^*(V) = \\oplus_k\\mathcal{T}^k(V) \\text{  (algebraic direct sum of vector spaces)} $$ and the elements above are of the form \\((T^0,T^1\\dots)\\) where all but finitely many are zero.</p> <p>Elements of \\(\\mathcal{T}^0\\) multiply to k-tensors as scalars, otherwise given \\(S\\in\\mathcal{T}^l(V),\\ T\\in\\mathcal{T}^m(V)\\) we define the Tensor product \\(S\\otimes T\\) as</p> \\[S(v_1,v_2\\dots v_l)\\cdot T(w_1,w_2\\dots w_m) = S\\otimes T(v_1,v_2\\dots v_l,w_1\\dots w_l) $$ OR $$     S\\otimes T =_{def} (S_0T_0,\\ S_0T_1+S_1T_0,\\ S_0T_2+S_1T_1+S_2T_0\\dots) \\] <p>Thus \\(\\mathcal{T}^k(V)\\times \\mathcal{T}^l(V) \\to \\mathcal{T}^{k+l}(V)\\) is a bilinear map which is associative and noncommutative. Hence \\(\\mathcal{T}^*(V)\\) is an \\(\\mathbb{R}\\)-algebra(\\(\\mathbb{R}\\) is a subring).</p>"},{"location":"Tensors/#related-problems","title":"Related Problems","text":"<p>Calc - Problem Session - 7 Oct</p>"},{"location":"Tensors/#references","title":"References","text":"<p>Ramdas's Notes Exterior Algebra</p>"},{"location":"Textbook%20review%208%20math/","title":"Textbook review 8 math","text":"<p>202310041310</p> <p>Tags : [[NCERT]] [[Education]]</p>","tags":["Note","Incomplete"]},{"location":"Textbook%20review%208%20math/#textbook-review-8-math","title":"Textbook review 8 math","text":"","tags":["Note","Incomplete"]},{"location":"Textbook%20review%208%20math/#contents","title":"Contents","text":"<ul> <li> <p>Squares and square roots and cubes and cube roots - why two different things?</p> </li> <li> <p>The prelims part is titled \"Prilims\"..... smh</p> </li> <li>Reads like an undergrad book<ul> <li>Goes top down instead of building things up from bottom to top</li> </ul> </li> <li>Intimidating because of fancy words (Closure)<ul> <li>When should such words be introduced, if at all? Definitely not used to introduce things, words can maybe be mentioned at the end of a section (by the way, this is called closure) and until kids are comfortable using only the word (which is expected to take a while), whenever the idea is referred to, refer to it both by name and by the idea itself</li> </ul> </li> <li>How necessary is learning abstraction at the age of 13?</li> <li>Can the art be slightly more appealing</li> <li>Highlight that there is a revision / start with a revision, because if someone doesn't remember the terms / ideas that are being mentioned right at the start of the chapter, they might not get as far as the revision before getting discouraged [pg 15]</li> <li>Can linear equations be motivated again in class 8</li> </ul>","tags":["Note","Incomplete"]},{"location":"Textbook%20review%208%20math/#ignore","title":"Ignore :)","text":"<ul> <li>'Rationalized' vs 'rationalization'</li> </ul>","tags":["Note","Incomplete"]},{"location":"Textbook%20review%208%20math/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"The%20Riemann%20Sphere/","title":"The Riemann Sphere","text":"<p>202301111401</p> <p>Type : #Note Tags :[[Complex Analysis]]</p>"},{"location":"The%20Riemann%20Sphere/#the-riemann-sphere","title":"The Riemann Sphere","text":"<p>Idea: Construct an extended compelx plane with \\(\\infty\\) Let \\(\\(S=\\{(x_1,x_2,x_3):x_1^2+x^2_2+x_3^2=1\\}\\subseteq\\mathbb R^3\\)\\) Define $$ \\begin{aligned} \\tau:S&amp;\\longrightarrow \\mathbb C\\ (x_1,x_2,x_3)&amp;\\longmapsto z={x_1+ix_2\\over 1-x_3} \\end{aligned} $$ The inverse map is given by \\(\\sigma: \\mathbb C \\to S\\) and is called the sterographic projection <pre><code>$$\n\\begin{aligned}\n|z|={1+x_3\\over 1-x_3}&amp;\\implies x_3={|z|^2-1\\over|z|^2+1}\\\\\nx_1={z+\\overline z\\over 1+|z|^2}&amp;,x_2={z-\\overline z\\over 1+|z|^2}&amp;\n\\end{aligned}\n$$\n</code></pre> Thus \\(\\sigma:\\mathbb C\\longrightarrow S\\setminus (0,0,1)\\) is a diffeomorphism, infact it is a one point compaction.</p>"},{"location":"The%20Riemann%20Sphere/#alternative-view-point","title":"Alternative View Point","text":"<p>Let \\(X=\\mathbb C, Y=\\mathbb C\\) Let \\(\\phi:X\\setminus \\{0\\}\\to Y\\setminus \\{0\\}\\) \\(z\\longmapsto \\frac1z=w\\)  Define $$ S={X\\amalg Y\\over x\\rightsquigarrow\\phi(x)} $$ As \\(\\phi\\) is analytic, \\(S\\) is a complex manifold. This is equivalent to circle inversion.</p>"},{"location":"The%20Riemann%20Sphere/#related-problems","title":"Related Problems","text":""},{"location":"The%20Riemann%20Sphere/#exercise","title":"Exercise:","text":"<p>Let \\(d(p,q)\\) denote the distance in \\(\\mathbb{R}^3\\). Show that: 1. \\(d(\\sigma(z),\\sigma(z')) = \\dfrac{2|z-z'|}{\\sqrt{1+|z|^2}\\sqrt{1+|z'|^2}}\\)  2. \\(d(\\sigma(z),0) = \\dfrac{2}{\\sqrt{1+|z|^2}}\\)</p>"},{"location":"The%20Riemann%20Sphere/#exercise_1","title":"Exercise:","text":"<p>Prove that \\(\\theta: \\mathbb{C} \\to S\\) defined by \\(\\theta(z) = \\sigma(1/z)\\) is continuous.</p> <p>Proof: \\(\\|\\theta(z+h) - \\theta(z)\\| = \\|\\sigma(1/(z+h)) - \\sigma(1/z)\\| = \\dfrac{2|z+h-z|}{\\sqrt{1+|z+h|^2}\\sqrt{1+|z|^2}} \\to 0\\) as \\(h \\to 0\\).  Note that this transformation is the same as stereographic projection from the south pole.</p>"},{"location":"The%20Riemann%20Sphere/#references","title":"References","text":""},{"location":"The%20fundamental%20theorem%20of%20Galois%20Theory/","title":"The fundamental theorem of Galois Theory","text":"<p>202304171104</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"The%20fundamental%20theorem%20of%20Galois%20Theory/#the-fundamental-theorem-of-galois-theory","title":"The fundamental theorem of Galois Theory","text":""},{"location":"The%20fundamental%20theorem%20of%20Galois%20Theory/#theorem-1-artin","title":"Theorem 1 (Artin):","text":"<pre><code>title:\nLet $E$ be a field and $H$ be a finite group of automorphisms of $E$. If $[E : E^H]$ is finite then $E /E^H$ is a galois extension and $\\mathrm{Gal}(E/E^H) = H$\n</code></pre>"},{"location":"The%20fundamental%20theorem%20of%20Galois%20Theory/#proof","title":"Proof:","text":"<p>Take \\(\\alpha \\in E\\), let \\(S_{\\alpha} := \\{ \\sigma(\\alpha) : \\sigma \\in H \\}\\). Let \\(h_{\\alpha}(X) = \\prod \\limits_{\\beta \\in S_{\\alpha}} (X-\\beta)\\). The coefficients of \\(h_{\\alpha}\\) are in \\(E^H\\), since applying any \\(\\sigma \\in H\\) to \\(h_{\\alpha}\\), we get \\(h_{\\alpha}\\) itself. Now take any \\(f \\in E ^{H}[X]\\) with \\(\\alpha\\) as a root, we can see that \\(\\sigma(\\alpha)\\) is also a root of \\(f\\) for all \\(\\sigma\\), hence \\(h_{\\alpha} | f\\). Thus, \\(h_{\\alpha}\\) is the min poly of \\(\\alpha\\) over \\(E ^{H}\\), with \\(deg(h_{\\alpha})\\le |H|\\). Thus \\(E /E ^{H}\\) is a separable and normal extension. Thus, \\(E /E ^{H}\\) is galois (since it is finite as well).</p> <p>Now since \\(E /E ^{H}\\) is separable, by Primitive element theorem, \\(E = E ^{H}(\\alpha)\\). Now, \\(|\\mathrm{Gal}(E /E ^{H})| = [E : E ^{H}] =[E ^{H}(\\alpha): E ^{H}] = \\mathrm{deg}(h_{\\alpha}) = |S_{\\alpha}| \\le |H|\\). But \\(H \\le \\mathrm{Gal}(E /E ^{H})\\), hence \\(H = \\mathrm{Gal}(E /E ^{H})\\).</p>"},{"location":"The%20fundamental%20theorem%20of%20Galois%20Theory/#theorem-2-artin","title":"Theorem 2 (Artin):","text":"<pre><code>title:\nLet $E$ be a field and $H$ be a finite group of automorphisms of $E$. Then $[E:E^H]$ is finite.\n</code></pre>"},{"location":"The%20fundamental%20theorem%20of%20Galois%20Theory/#proof_1","title":"Proof:","text":"<p>From the proof of theorem 1 above, we know that \\(\\alpha\\) is separable over \\(E ^{H}\\) for all \\(\\alpha \\in E\\), with degree bounded by \\(|H|\\). Let \\(\\alpha\\) be such that \\([E^H(\\alpha) : E^H]\\) be maximum. We claim that \\(E^H(\\alpha) = E\\), this will finish the proof. Let \\(\\beta \\in E\\), then \\(E ^H(\\alpha,\\beta)\\) is separable over \\(E^H\\) since both \\(\\alpha\\) and \\(\\beta\\) are. Hence by primitive element theorem, \\(E^H(\\alpha,\\beta) = E^H(\\gamma)\\) whose degree over \\(E^H\\) is at most \\([E^H(\\alpha): E^H]\\). Which gives \\([E^H(\\alpha ,\\beta) : E^H] = [E^H(\\alpha) : E ^{H}] \\implies E ^{H}(\\alpha ,\\beta) = E^H(\\alpha)\\). Thus, \\(\\beta \\in E^H(\\alpha)\\) for all \\(\\beta \\in E\\), hence \\(E = E^H(\\alpha)\\).</p>"},{"location":"The%20fundamental%20theorem%20of%20Galois%20Theory/#theorem-3-galois","title":"Theorem 3 (Galois):","text":"<pre><code>title:\nLet $L /K$ be a finite Galois extension with $G = \\mathrm{Gal}(L /K)$. Then the mappings $F \\mapsto \\mathrm{Gal}(L /F)$ and $H \\mapsto L^H$ are inverses of each other and satisfy the following properties when $H$ and $F$ correspond:\n1. $|H| = [L:F]$ and $[F:K] = [G:H]$\n2. two intermediate fields $F$ and $F'$ with corresponding subgroups $H$ and $H'$ are isomorphic over $K$ iff $H$ and $H'$ are conjugate subgroups of $G$.\n3. $F /K$ is galois iff $H$ is a normal subgroup of $G$, and $\\mathrm{Gal}(F/K) \\cong G/H$.\n</code></pre>"},{"location":"The%20fundamental%20theorem%20of%20Galois%20Theory/#proof_2","title":"Proof:","text":"<p>To show that the mappings in question are inverses of each other, we need to show that \\(F ^{\\mathrm{Gal}(L /F)} = F\\) and \\(\\mathrm{Gal}(L /L ^{H}) = H\\). The first equality follows from the theorem 3 and theorem 1 of Galois Extensions. The second equality follows from theorem 1 and 2 above.</p> <ol> <li>\\([L : L ^{H}] = |\\mathrm{Gal}(L /L ^{H})| = |H|\\) (By theorem 1 above).     \\([G:H] = |G| /|H| = [L : K] /[L:L ^{H}] = [L ^{H} : K] = [F : K]\\).</li> <li>First we show that \\(F\\) and \\(F'\\) are \\(K-\\)isomorphic iff \\(F' = \\sigma(F)\\) for some \\(\\sigma \\in G\\).    Note that if \\(F' = \\sigma(F)\\) then the restriction of \\(\\sigma\\) to \\(F\\) is the required isomorphism between them.    Now suppose \\(\\phi : F \\to F'\\) is a \\(K\\) isomorphism. Since \\(F /K\\) is separable, by primitive element theorem, we get \\(F = K(\\gamma)\\). Since \\(\\phi\\) fixes \\(K\\), \\(\\phi(\\gamma)\\) is a conjugate of \\(\\gamma\\) over \\(K\\). So by theorem 4 of Galois Correspondence, we get \\(\\phi(\\gamma) = \\sigma(\\gamma)\\). Since \\(\\sigma\\) and \\(\\phi\\) agree on \\(K\\) and \\(\\gamma\\), they agree on \\(F\\), and hence \\(\\phi = \\sigma|_{F}\\) so \\(F' = \\phi(F) = \\sigma(F)\\).</li> </ol> <p>Now for all \\(\\tau \\in G\\), we get  $$ \\begin{align} \\tau \\in \\mathrm{Gal}(L /\\sigma(F)) &amp;\\iff \\tau(\\sigma(a)) = \\sigma(a)  \\forall  a \\in F \\  &amp;\\iff \\sigma^{-1}(\\tau(\\sigma(a))) = a  \\forall  a \\in F \\ &amp;\\iff \\sigma^{-1}\\tau \\sigma \\in \\mathrm{Gal}(L /F) \\ &amp;\\iff \\tau \\in \\sigma H \\sigma^{-1} \\end{align} $$ This gives us \\(H' = \\sigma H \\sigma^{-1}\\).</p> <ol> <li>Since \\(F /K\\) is separable, it is galois iff it is normal. But the \\(K-\\)conjugates of any element in \\(F\\) are its orbit under \\(\\mathrm{Gal}(L /K)\\), hence \\(F /K\\) is normal iff \\(\\sigma(F) \\subset F\\) for all \\(\\sigma \\in G\\). Since \\(\\sigma(F)\\) and \\(F\\) have the same degree over \\(K\\), the inclusion holds iff equality holds. So $$ \\begin{align} F /K \\text{ is normal} &amp;\\iff \\sigma(F) = F  \\forall  \\sigma \\in G \\  &amp; \\iff \\sigma H \\sigma^{-1} = H  \\forall  \\sigma \\in G\\  &amp; \\iff H \\triangleleft G \\end{align} $$    Now restricting elements in \\(\\mathrm{Gal}(L /K)\\) to \\(F\\) defines a map from \\(\\mathrm{Gal}(L /K) \\to \\mathrm{Gal}(F /K)\\). The kernel of this map is obviously \\(\\mathrm{Gal}(L /F)\\). This gives an injection \\(\\mathrm{Gal}(L /K) /\\mathrm{Gal}(L /F) \\hookrightarrow \\mathrm{Gal}(F /K)\\), the domain has size \\([G : H] = [F : K] = |\\mathrm{Gal}(F /K)| =\\) size of codomain. Hence, the above injection is a surjection too and \\(\\mathrm{Gal}(F /K) \\cong \\mathrm{Gal}(L /K) / \\mathrm{Gal}(L /F)\\).</li> </ol>"},{"location":"The%20fundamental%20theorem%20of%20Galois%20Theory/#theorem","title":"Theorem:","text":"<pre><code>title:\nLet $L/K$ be finite Galois and $F$ and $F'$ be intermediate fields with corresponding subgroups $H$ and $H'$.\n(a) $\\mathrm{Gal}(L /FF') = H \\cap H'$ and $\\mathrm{Gal}(L/F \\cap F') = \\langle H, H'\\rangle$, where $\\langle H, H'\\rangle$ denotes the subgroup of $\\mathrm{Gal}(L/K)$ generated by $H$ and $H'$. \n\n(b) $F \\subset F'$ if and only if $H' \\subset H$, in which case $[F' : F] = [H : H']$. \n</code></pre>"},{"location":"The%20fundamental%20theorem%20of%20Galois%20Theory/#proof_3","title":"Proof.","text":"<p>For (a), we use the inclusion-reversing nature of the Galois correspondence. The composite field \\(FF'\\) is the smallest field containing both \\(F\\) and \\(F'\\) in \\(L\\), so its corresponding subgroup \\(\\mathrm{Gal}(L/FF ')\\) is the largest subgroup of \\(\\mathrm{Gal}(L/K)\\) contained in \\(H\\) and \\(H'\\), so it is \\(H \\cap H'\\). </p> <p>The argument for the other equation is similar. For (b), the equivalence of the inclusions comes from the Galois correspondence. Moreover, we then have \\([F': F] = [L : F]/[L : F'] = |H|/|H'| = [H : H']\\). </p>"},{"location":"The%20fundamental%20theorem%20of%20Galois%20Theory/#references","title":"References","text":"<p>Primitive Element Theorem Separable Extensions Normal Extensions Galois Extensions Galois Correspondence</p>"},{"location":"Theorem-%20Bipartite%20Graph%20Eigen%20Value%20Pairs/","title":"Theorem  Bipartite Graph Eigen Value Pairs","text":"<p>202308091040</p> <p>type : #Example tags : [[Algebraic Graph Theory]]</p>"},{"location":"Theorem-%20Bipartite%20Graph%20Eigen%20Value%20Pairs/#theorem-bipartite-graph-eigen-value-pairs","title":"Theorem- Bipartite Graph Eigen Value Pairs","text":""},{"location":"Theorem-%20Bipartite%20Graph%20Eigen%20Value%20Pairs/#theorem","title":"Theorem:","text":"<p>Let \\(G\\) be bipartite. Then if \\(\\lambda\\) is an eigenvalue, so is \\(-\\lambda\\) with the same multiplicity</p>"},{"location":"Theorem-%20Bipartite%20Graph%20Eigen%20Value%20Pairs/#proof","title":"Proof:","text":"<p>Let \\(A=\\mathcal A(G)\\). Then we can write \\(A\\) in the form $$ A = \\begin{bmatrix}O_{n_{1}\\times n_{1}} &amp; B_{n_{1}\\times n_{2}} \\ B^{t} &amp; O_{n_{2}\\times n_{2}}\\end{bmatrix} $$ Let \\((\\lambda, z)\\) be an eigenpair. let \\(z=\\begin{bmatrix}x \\\\ y\\end{bmatrix}\\) \\(Az = \\begin{bmatrix}By\\\\ B^{t}x \\end{bmatrix}=\\lambda z=\\lambda \\begin{bmatrix}x \\\\ y\\end{bmatrix}\\)</p> <p>So, \\(By = \\lambda x\\) and \\(B^{t}x = \\lambda y\\)</p> <p>We claim that \\(z'=\\begin{bmatrix}-x \\\\ y\\end{bmatrix}\\) is an eigenvector for \\(-\\lambda\\) \\(Az' = \\lambda \\begin{bmatrix}x \\\\ -y\\end{bmatrix}=-\\lambda z\\)</p> <p>The same process can be done for every single eigenvector for the eigenvalue \\(\\lambda\\) which gives the same multiplicity of \\(-\\lambda\\)</p>"},{"location":"Theorem-%20Bipartite%20Graph%20Eigen%20Value%20Pairs/#related","title":"Related","text":""},{"location":"Tietze%20Extension%20Theorem/","title":"Tietze Extension Theorem","text":"<p>202303302303</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Tietze%20Extension%20Theorem/#tietze-extension-theorem","title":"Tietze Extension Theorem","text":"<pre><code>title:\nLet $X$ be a normal space, let $A \\subseteq X$ be a closed subspace, and let $f:A \\to [a,b]$ be a cts function for some $[a,b] \\subseteq \\mathbb{R}$. There exists a cts function $\\bar{f}:X \\to [a,b]$ such that $\\bar{f}|_{A} = f$.\n</code></pre>"},{"location":"Tietze%20Extension%20Theorem/#lemma","title":"Lemma:","text":"<pre><code>title:\nLet $X$ be a normal space, $A \\subseteq X$ be a closed subspace, and let $f : A \\to \\mathbb{R}$ be a cts function such that for some $C &gt; 0$ we have $|f(x)| \\le C$ for all $x \\in A$. There exists a cts function $g : X \\to \\mathbb{R}$ such that $|g(x)| \\le \\frac{1}{3}C$ for all $x \\in X$ and $|f(x) - g(x)| \\le \\frac{2}{3}C$ for all $x \\in A$.\n</code></pre>"},{"location":"Tietze%20Extension%20Theorem/#proof","title":"Proof:","text":"<p>Take \\(Y = f ^{-1}\\left( \\left[ -C,-\\frac{C}{3} \\right] \\right), Z = f ^{-1}\\left( \\left[ \\frac{C}{3}, C \\right] \\right)\\). Since \\(f : A \\to \\mathbb{R}\\) is a cts function, these are closed subsets of \\(A\\) and hence of \\(X\\). So by Urysohn's Lemma,  there is a function \\(h : X \\to \\mathbb{R}\\) such that \\(h(Y) \\subset \\{ 0 \\}, h(Z) \\subset \\{ 1 \\}\\). Then take the function \\(g(x) = \\frac{2C}{3}\\left( h(x)-\\frac{1}{2} \\right)\\). Then for \\(x\\) in each of \\(Y,Z,X \\setminus (Y \\cup Z)\\), we get that \\(f(x),g(x)\\) belong to the same interval \\(\\left[ -C,-\\frac{C}{3} \\right]\\), \\(\\left[ \\frac{C}{3},C \\right]\\), \\(\\left[ -\\frac{C}{3}, \\frac{C}{3} \\right]\\). Hence done.</p>"},{"location":"Tietze%20Extension%20Theorem/#proof-of-tietzes-theorem","title":"Proof of Tietze's theorem:","text":"<p>WLOG take \\([a,b] = [0,1]\\) We construct a sequence of functions \\(g_{n} : X \\to [0,1]\\) such that  1) \\(\\displaystyle |g_{n}(x)| \\le \\frac{1}{3}\\left( \\frac{2}{3} \\right)^{n-1}\\) 2) \\(\\displaystyle |f(x) - \\sum \\limits_{ i=1}^{ n }g_{i}(x)| \\le \\left( \\frac{2}{3} \\right)^{n}\\) for all \\(x \\in A\\).</p> <p>This follows by induction and the previous lemma. Then take \\(\\bar{f} := \\sum \\limits_{ i=1}^{ \\infty }g_{i}(x)\\). This is uniformly converging because of condition \\((1)\\). And \\(\\bar{f}|_{A} = f\\) because of condition \\((2)\\).</p>"},{"location":"Tietze%20Extension%20Theorem/#another-formulation-of-tietzes-theorem","title":"Another formulation of Tietze's Theorem","text":"<pre><code>title:\nLet $X$ be a normal space, let $A \\subset X$ be a closed subset, and let $f : A \\to \\mathbb{R}$ be a cts function. There exists a cts function $\\bar{f} : X \\to \\mathbb{R}$ such that $\\bar{f}|_{A} = f$.\n</code></pre>"},{"location":"Tietze%20Extension%20Theorem/#proof_1","title":"Proof:","text":"<p>It is enough to show that for any cts function \\(f : A \\to (-1,1)\\), there is a cts function \\(\\bar{f} : X \\to (-1,1)\\) since \\((-1,1)\\) is homeomorphic to \\(\\mathbb{R}\\).</p> <p>Assume then \\(f:A \\to (-1,1)\\) is cts, then by Tietze's theorem, there is \\(f' : A \\to [-1,1]\\) cts. Let \\(B = f' ^{-1}(\\{ -1,1 \\})\\). The set \\(B\\) is closed in \\(X\\), disjoint from \\(A\\), hence by Urysohn's lemma, there is a function \\(k : X \\to [0,1]\\) such that \\(k(B) \\subset \\{ 0 \\}\\), and \\(k(A)\\subset \\{ 1 \\}\\). Then define \\(\\bar{f} := k(x)\\cdot f'(x)\\). This is the required function.</p>"},{"location":"Tietze%20Extension%20Theorem/#references","title":"References","text":"<p>Urysohn Lemma Continuous Functions Closed sets Subspace Topology Normal Spaces</p>"},{"location":"Time%20Table%20Planning/","title":"Time Table Planning","text":"<ul> <li>Shubh<ul> <li>Study for courses</li> <li>Non course Studies</li> <li>Guitar</li> <li>Keyboard</li> <li>Physical Activities</li> <li>Annoying People</li> <li>Chilling</li> <li>Completing bucket list</li> </ul> </li> <li>Aditi<ul> <li>Course Work</li> <li>Yoga`</li> <li>Annoying Profs (pun intended)</li> <li>ReLaX</li> <li>Swimming</li> <li>Cycling</li> <li>Ukulele</li> <li>Annoying People</li> <li>TIFR application</li> <li>Timepass</li> </ul> </li> </ul>"},{"location":"Timed%20Automata%20Alternate%20Definition/","title":"Timed Automata Alternate Definition","text":"<p>202311161611</p> <p>Tags : Timed Automata</p>","tags":["Note","Incomplete"]},{"location":"Timed%20Automata%20Alternate%20Definition/#timed-automata-alternate-definition","title":"Timed Automata Alternate Definition","text":"","tags":["Note","Incomplete"]},{"location":"Timed%20Automata%20Alternate%20Definition/#syntax-of-timed-automata","title":"Syntax of Timed Automata","text":"<p>A Timed Automaton \\(\\mathcal A\\) is the tuple $$ \\mathcal A = \\langle L, X, E, \\mathcal I, \\mathcal L\\rangle $$ - \\(L\\) : Location / States - \\(X\\) : Clocks - \\(E \\subseteq L \\times \\mathcal G(X) \\times \\mathcal 2^X \\times L\\) : Edges     - State + Guard + Resets -&gt; State - \\(\\mathcal I : L\\to \\mathcal G(X)\\) : Invariant  - \\(\\mathcal L : L\\to\\mathcal 2^{\\text{AP}}\\)     - \\(\\text{AP}\\) is a finite set of atomic propositions     - Assignment of a logical proposition to each location.</p>","tags":["Note","Incomplete"]},{"location":"Timed%20Automata%20Alternate%20Definition/#semantics-of-timed-automata","title":"Semantics of Timed Automata","text":"<p>Semantics of a Timed Automata \\(\\mathcal A\\) is given by a labelled Transition system \\(T_{\\mathcal A}\\) $$ T_{\\mathcal A} = \\langle S, E \\sqcup \\mathbb R_{+}, \\rightarrow \\rangle $$ - \\(S\\) is a set of states/symbolic states \\(\\{s=(l, \\nu) \\in L \\times\\mathbb R^{X}_{+}\\;:\\;\\nu\\vDash \\mathcal I(l)\\}\\) - \\(\\to\\) is a transition function of type \\(S\\times E \\sqcup \\mathbb R \\times S\\) composed of     - Delay transition: \\((l, \\nu)\\xrightarrow{\\tau}(l, \\nu+\\tau)\\) if \\(\\tau\\in \\mathbb R_+\\) and for all \\(0\\le \\tau'\\le\\tau\\) we have \\(\\nu + \\tau'\\vDash\\mathcal I(l)\\). Basically time is elapsed preserving the invariant.     - Discrete transitions: \\((l, \\nu)\\xrightarrow{e}(l',\\nu')\\) if \\(e=(l,g,T,l')\\in E\\) such that         - \\(\\nu \\vDash \\mathcal I(l) \\land g\\)         - \\(\\nu' = [Y\\leftarrow 0]\\nu\\)         - \\(\\nu'\\vDash \\mathcal I(l')\\)</p> <p>Also we define  $$ \\begin{align} I(s,e) &amp;= {\\tau\\in \\mathbb R_{+}\\;:\\;s\\xrightarrow{\\tau,e}s'} &amp;&amp;\\text{for some }s' \\ I(s)&amp;= \\bigcup_{e}I(s,e) \\end{align} $$ \\(\\mathcal A\\) is said to be non-blocking if \\(I(s)\\ne\\emptyset\\) for all \\(s\\) ^15073a</p>","tags":["Note","Incomplete"]},{"location":"Timed%20Automata%20Alternate%20Definition/#runs","title":"Runs","text":"<p>A run in a timed automaton is an alternating sequence of delay and discrete transitions written as  $$ \\varrho\\quad:\\quad s_{0}\\xrightarrow{\\tau_{1}} s_{1}'\\xrightarrow{e_{1}}s_{1}\\xrightarrow{\\tau_{2}}s_{2}'\\cdots\\xrightarrow{e_{n}}s_{n} $$ which can be written compactly as $$ \\varrho\\quad:\\quad s_{0}\\xrightarrow{\\tau_{1},e_{1}}s_{1}\\cdots\\xrightarrow{\\tau_{n},e_{n}}s_{n} $$ The set of runs in an automata \\(\\mathcal A\\) and starting at \\(s\\) is written as \\(\\text{Runs}(\\mathcal A, s)\\)</p>","tags":["Note","Incomplete"]},{"location":"Timed%20Automata%20Alternate%20Definition/#symbolic-paths","title":"Symbolic Paths","text":"<p>A Symbolic Path is a set of runs starting at a symbolic state followed by a sequence of edges and restricted under some constraint \\(\\mathcal C\\). It is defined as $$ \\pi_{\\mathcal C}(s,e_{1}\\dots e_{n})={\\varrho=s\\xrightarrow {\\tau_{1},e_{1}}\\cdots\\xrightarrow{\\tau_{n},e_{n}}s_{n}\\;:\\;\\varrho\\in \\text{Runs}(\\mathcal A, s)\\text{ and } (\\tau_{i})\\vDash\\mathcal C} $$</p>","tags":["Note","Incomplete"]},{"location":"Timed%20Automata%20Alternate%20Definition/#references","title":"References","text":"<p>Region Automata Alternate Definition</p>","tags":["Note","Incomplete"]},{"location":"Timed%20Automata%20Homework%201/","title":"Timed Automata Homework 1","text":"<p>Name: Shubh Sharma Roll Number: BMC202170</p>","tags":["Assignment"]},{"location":"Timed%20Automata%20Homework%201/#1","title":"1.","text":"<p>![[Drawing 2023-09-16 13.47.36.excalidraw|600]]</p>","tags":["Assignment"]},{"location":"Timed%20Automata%20Homework%201/#2","title":"2.","text":"<p>Let  - \\(Q = S\\cup A\\cup B\\cup C\\cup D\\) where     - \\(S = \\{s\\}\\)     - \\(A=\\{a_{0}\\}\\)     - \\(B = \\{b_{0}, \\dots,b_{3}\\}\\)     - \\(C = \\{c_{(i,j)}\\}\\) where \\(i,j\\in\\{0..3\\}\\)     - \\(D = \\{d_{3}, \\dots d_{6}\\}\\) - \\(\\Sigma=\\{a,b,c,d\\}\\) - \\(X=\\{*\\}\\) - \\(Q_{0}= S\\) - \\(F = D\\)</p> <p>And for the transitions, we have a transition from \\(s\\) to \\(a_{0}\\) accepting \\(a\\) and resetting the clock.</p> <p>We have transitions \\(a_{0}\\) to \\(b_{i}\\) for \\(i\\in \\{0..3\\}\\) accepting \\(b\\) if clock reads \\(i\\). We reset the clock</p> <p>We have transitions \\(b_{i}\\in B\\) to \\(c_{(i,i+t)}\\) if \\(c_{(i,i+t)}\\in C\\) which accept \\(c\\) if clock reads \\(t\\in\\{0..3\\}\\). We reset the clock. \\(i+t\\) is when \\(c\\) is accepted after \\(a\\) so first condition checks</p> <p>We have transitions \\(c_{(i,j)}\\in C\\) to \\(d_{j+t}\\) if \\(d_{j+t}\\in D\\) and \\(j+t-i\\ge 3\\) which accept \\(d\\) if the clock reads \\(t\\in\\mathbb N\\).   \\(j+t\\) is when \\(d\\) is accepted and \\(i\\) is when \\(b\\) was accepted, so second conditions also checks</p>","tags":["Assignment"]},{"location":"Timed%20Automata%20Homework%201/#3","title":"3.","text":"<p>Consider a language \\(L\\), we can construct a timed automaton \\(\\mathcal A\\) that accepts \\(L\\), and because of question 4, without loss of generality, assume \\(\\mathcal A\\) has just \\(1\\) clock and let \\(l\\) be one more than the largest number in any guard. And that the clock is reset on every transition.</p> <p>Since the clock resets on every transition. If there is a transition, from a state whose guard is satisfiable. Then we keep the transition otherwise we remove it. As, given a state and such a transition, we can always wait for a suitable amount of time and take the transition.</p> <p>Construct a new timed automaton \\(\\mathcal A'\\) where we remove everything related to clocks, it becomes a standard finite state automaton. This automata accepts \\(\\text{Untime(L)}\\) as for any run in \\(\\mathcal A\\), there is a corresponding run in \\(\\mathcal A'\\) accepting the same word. And for any run in \\(\\mathcal A'\\), we can look at the corresponding transitions in \\(\\mathcal A\\), which can always be taken because the clock always resets.</p>","tags":["Assignment"]},{"location":"Timed%20Automata%20Homework%201/#4","title":"4.","text":"<p>Suppose we are given a Discrete Timed Autotmaton \\(\\mathcal A\\) such that \\(|X|=n\\). let \\(l\\) be one more than the largest number occurring in any guard.</p> <p>We construct a new automata \\(\\mathcal A'\\) such that   - \\(Q'=Q\\times\\{0,1,2\\dots l\\}^{n}\\) - \\(\\Sigma' = \\Sigma\\) - \\(X' = \\{*\\}\\) - \\(Q_{0}' = \\{(q, \\underbrace{0,\\dots 0}_{n \\text{ 0s}})\\ :\\ q\\in Q_{0}\\}\\)  - \\(F_{0}' = \\{q' :\\ \\text{ such that } q \\text{ is the first component of } q' \\text{ and } q\\in F_{0}\\}\\)  - \\(T'\\) = all tuples \\((q', g', a, R', q_{1}')\\) for all \\(t\\in \\{0..l\\}\\) where     - \\(q' = (q, a_{1}, a_{2},\\dots a_{n})\\)     - \\(R' = \\{*\\}\\)     - \\(q_{1}''=(q_{1},b_{1},b_{2}\\dots b_{n})\\) where \\(b_{i}=\\min\\{l,a_{i}+ t\\}\\)     -  We get \\(g'\\) by replacing clock \\(i\\) with \\(b_{i}\\) in \\(g\\)     - if replacing clock \\(i\\) with \\(b_{i}\\) in \\(g\\) satisfies it     - \\(q' = (q_{1}, c_{1}, \\dots c_{n})\\) where \\(c_{i}= 0\\) if clock \\(i\\) is in \\(R\\) and \\(b'\\) otherwise.</p> <p>Here we have constructed a discrete timed automaton with just \\(1\\) clock, that is \\(*\\). Since none of the guards can differentiate between number \\(\\ge l\\) we represent all of them by \\(l\\). This gives a finite amount of configurations represented by clocks.</p>","tags":["Assignment"]},{"location":"Timed%20Automata%20Plan/","title":"Timed Automata Course Plan","text":"<ul> <li>7 weeks are left</li> <li>Last week will be reserved for project (Nov 21)<ul> <li>Sir will give some paper, we have to pick one or find our own</li> </ul> </li> <li>6 weeks of lectures <ul> <li>Which is 18 lectures</li> </ul> </li> <li>Midsem (1st/2nd) week of November</li> <li>Will do Basic Topics until midsem</li> <li>Last 10 lectures for advanced topics which will not be graded<ul> <li>Homework will be given on those topics</li> </ul> </li> </ul>"},{"location":"Timed%20Automata%20with%20Diagonal%20Constraints/","title":"Timed Automata with Diagonal Constraints","text":"<p>202310282010</p> <p>Tags : Timed Automata</p>","tags":["Note"]},{"location":"Timed%20Automata%20with%20Diagonal%20Constraints/#timed-automata-with-diagonal-constraints","title":"Timed Automata with Diagonal Constraints","text":"<p>[!note] Definition A timed automata with diagonal constraints is a normal timed automata whose guards come from the set defined below. I will also call them d-timed automata.</p>","tags":["Note"]},{"location":"Timed%20Automata%20with%20Diagonal%20Constraints/#diagonal-constraints","title":"Diagonal Constraints","text":"<p>let \\(X\\) be a set of clocks. The set of guards with diagonal constraints \\(\\Phi(X)\\) is given the following grammar: $$ \\Phi(X)\\quad:=\\quad x \\diamond c \\; |\\;x-y\\diamond c\\;|\\; \\Phi(X) \\land\\Phi(X)  $$ where \\(x, y\\in X\\) and \\(\\diamond : \\{\\le, &lt;, =, &gt;, \\geq\\}\\) and \\(c \\in Z\\) </p> <p>There are two main theorem for d-timed automata - Diagonal Constraints do not add any power. That is because there is a construction from a d-timed automata of timed automata. - Diagonal Constraints Offer Exponential Succinctness.</p>","tags":["Note"]},{"location":"Timed%20Automata%20with%20Diagonal%20Constraints/#references","title":"References","text":"<ul> <li>Srivathsan's Notes</li> </ul>","tags":["Note"]},{"location":"Topological%20Groups/","title":"Topological Groups","text":"<p>202303310003</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Topological%20Groups/#topological-groups","title":"Topological Groups","text":"<pre><code>title:\nA topological group $G$ is a group endowed with a topology $\\tau_{G}$ such that multiplication and inversion are cts functions in this topology.\n</code></pre> <ul> <li>If \\(G\\) is a topo group, and \\(X\\) is a topo space, then a topological group action of \\(G\\) on \\(X\\) is a cts map \\((g,x) \\mapsto gx : G \\times X \\to X\\).</li> </ul> <pre><code>title:\nLet $H$ be a topological group satisfying $T_{1}$ condition, then $H$ is a topological group iff the map $H \\times H \\to H$ sending $(x,y) \\mapsto xy ^{-1}$ is cts.\n</code></pre>"},{"location":"Topological%20Groups/#proof","title":"Proof:","text":"<p>If \\(H\\) is a topo group then the given map is a composition of two continuous maps hence cts.</p> <p>Conversely, given that this is cts, then it is cts in both components, so that function \\(f((1,y)) = y ^{-1}\\) is cts. Hence \\(f(x,f((1,y))) = xy\\) is continuous.</p>"},{"location":"Topological%20Groups/#properties","title":"Properties","text":""},{"location":"Topological%20Groups/#lemma","title":"Lemma:","text":"<pre><code>title:\nOn a topological group, the following maps are homeomorphisms:\n- Left multiplication or right multiplication by an element.\n- inversion\n- conjugation by an element\n</code></pre>"},{"location":"Topological%20Groups/#note-this-statement-tells-us-that-to-look-at-open-nbhds-of-g-it-is-the-same-as-looking-at-open-nbhds-of-e","title":"NOTE: This statement tells us that to look at open nbhds of \\(g\\), it is the same as looking at open nbhds of \\(e\\).","text":""},{"location":"Topological%20Groups/#definition","title":"Definition:","text":"<p>```ad-note: title: Homogenous space A topo space \\(X\\) where for any pair of points \\(x,y\\) there is a homeomorphism from \\(X\\) to \\(X\\) taking \\(x\\) to \\(y\\) is called a homogenous space. <pre><code>### Lemma:\n```ad-note\ntitle:\nLet $H$ be a subspace of a topo group $G$. Then if $H$ is a subgroup of $G$, then both $H$ and $Cl(H)$ are topo groups.\n</code></pre></p>"},{"location":"Topological%20Groups/#proof_1","title":"Proof:","text":"<p>Take the function \\(H \\times H \\to H\\) that takes \\((x,y)\\) to \\(xy ^{-1}\\). This is continuous since it is the restriction of a cts function on \\(G \\times G\\) to a subspace.</p> <p>Similarly, we need to show that if \\(f : G\\times G \\to G\\) where \\(f(x,y) = xy ^{-1}\\), then \\(f(Cl(H) \\times Cl(H)) \\subseteq Cl(H)\\).  But \\(f(Cl(H) \\times Cl(H)) = f(Cl(H \\times H)) \\subset Cl(f(H \\times H)) \\subset Cl(H)\\) Since \\(f\\) is continous on \\(G \\times G\\).</p>"},{"location":"Topological%20Groups/#proposition","title":"Proposition","text":"<pre><code>title:\nGiven $G$, a topo group, and $H$ a subgroup,\n1. $G/H$ is a homogenous space.\n2. If $H$ is closed, then $G /H$ is $T_{1}$.\n3. Show that the quotient map $p : G \\to G /H$ is open.\n4. if $H$ is closed, and $H \\lhd G$ then $G /H$ is topo group.\n</code></pre>"},{"location":"Topological%20Groups/#proof_2","title":"Proof:","text":"<ol> <li>Take two elements \\(\\alpha H, \\beta H \\in G /H\\). Then there is a homeomorphism \\(f : G \\to G\\) which is multiplication by \\(\\beta \\alpha^{-1}\\). But then \\(g = p \\circ f\\) is a quotient map. This maps \\(x\\) to \\(\\beta \\alpha^{-1}x H\\) but then \\(g(x) = g(y)\\) iff \\(y ^{-1}x \\in H\\). So this induces a map \\(G /H \\to G /H\\) which takes \\(xH\\) to \\(\\beta\\alpha^{-1}xH\\), and so \\(\\alpha H\\) goes to \\(\\beta H\\).</li> <li>We know \\(H\\) is closed and that \\(G /H\\) is homogenous, hence any singleton is closed.</li> <li>Let \\(U\\) be open in \\(G\\), then \\(p ^{-1}(p(U)) = UH = \\bigcup \\limits_{ h \\in H} U\\cdot h\\) is open. Hence \\(p(U)\\) is open.</li> <li>\\(H \\lhd G\\) gives that \\(G /H\\) is a group, \\(H\\) being closed gives this is \\(T_{1}\\). <pre><code>\\usepackage{tikz-cd} \n\\begin{document} \n\\begin{tikzcd} \u00a0 \u00a0 \nG\u00a0 \u00a0 \\arrow[r,\"f\"] \\arrow[d,\"p\"]&amp; G \\arrow[d,\"p\"] \\\\\nG/H  \\arrow[r,\"g\"] &amp; G/H\n\\end{tikzcd}\n\\end{document}\n</code></pre> Take the map \\(g : G /H\\) to \\(G/H\\), \\(g(xH) = axH\\). This lifts to a map \\(f : G \\to G\\); \\(f(x) = ax\\), with the above diagram commuting. So take an open set \\(U\\) in \\(G /H\\) and then \\(g ^{-1}(U) = p(f ^{-1}(p ^{-1}(U)))\\) which is open since \\(p\\) is an open map.</li> </ol> <p>Similar argument holds when \\(g(xH) = x ^{-1}H\\).</p>"},{"location":"Topological%20Groups/#lemma_1","title":"Lemma:","text":"<pre><code>title:\n1) If $H$ is a normal subgroup of $G$ then so is $Cl(H)$.\n2) Every open subgroup of $G$ is closed.\n3) If $A$ and $B$ are compact sets in $G$ then so is $AB$.\n</code></pre>"},{"location":"Topological%20Groups/#proof_3","title":"Proof:","text":"<p>1) \\(Hg = gH \\subseteq g \\cdot Cl(H) \\implies Cl(H)\\cdot g \\subseteq g\\cdot Cl(H)\\). Similarly the other inclusion holds. 2) Let \\(H\\) be an open subgroup of \\(G\\), then \\(G \\setminus H = \\bigcup \\limits_{ g \\notin H} g \\cdot H\\) which is a union of open sets and hence open. Thus \\(H\\) is closed. 3) \\(A \\times B\\) is compact subset of \\(G \\times G\\), the multiplication map is continuous, and \\(AB\\) is the image of \\(A \\times B\\) under this map.</p>"},{"location":"Topological%20Groups/#definition_1","title":"Definition:","text":"<pre><code>title:\nLet $G,H$ be topo groups. A map of topo groups $G \\to H$ is a continuous map that is also a group homomorphism.\n</code></pre>"},{"location":"Topological%20Groups/#examples","title":"Examples:","text":"<ol> <li>Any group with discrete topology.</li> <li>\\((\\mathbb{Z},+)\\), (\\(S ^{1}\\), \\(\\cdot\\) )</li> <li>(\\(\\mathbb{R}\\),+)</li> <li>(\\(\\mathbb{R}_{+}\\), \\(\\cdot\\) )</li> <li>\\(GL_{n}(\\mathbb{C}), GL_{n}(\\mathbb{R})\\)</li> <li>\\(SL_{n}(\\mathbb{R}), SL_{n}(\\mathbb{C})\\).</li> </ol>"},{"location":"Topological%20Groups/#references","title":"References","text":"<p>Topological Spaces [[Groups]] Continuous Functions Quotient Topology</p>"},{"location":"Topological%20Invariants/","title":"Topological Invariants","text":"<p>202301201801</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Topological%20Invariants/#topological-invariants","title":"Topological Invariants","text":"<p>A property \\(\\mathcal P\\) of  a topological space \\(X\\) is called a Topological Invariant if whenever \\((X, \\mathcal T_Y)\\) and \\((Y, \\mathcal T_Y)\\) are two homeomorphic topological spaces, one has property \\(\\mathcal P\\) if and only if the other has the property \\(\\mathcal P\\) . Invariants give us a quick series of checks on wether two spaces can be homeomorphic: if we can identify a topological invariant that one space has and another does not, they cannot be homeomorphic.</p> <p>The following properties are topological invariants: 1. \\(T_0\\) or Kolmogorov Property. 2. \\(T_1\\) or Frechet Property. 3. \\(T_2\\) or Hausdorff Property. 4. Having a particular Cardinality. 5. Seperability.</p>"},{"location":"Topological%20Invariants/#related-problems","title":"Related Problems","text":""},{"location":"Topological%20Invariants/#references","title":"References","text":"<p>Homeomorphisms</p>"},{"location":"Topological%20Semantics%20for%20LTL/","title":"Topological Semantics for LTL","text":"<p>202311181511</p> <p>Tags : Timed Automata</p>","tags":["Note","Incomplete"]},{"location":"Topological%20Semantics%20for%20LTL/#topological-semantics-for-ltl","title":"Topological Semantics for LTL","text":"<p>The interpretation of LTL formulae happens over finite runs of Timed Automata. The states in a run correspond to the worlds used to define the semantics of LTL. Ans we use the function \\(\\mathcal L\\) in place of \\(\\sigma\\) to give the set of formulas satisfied by \\(s\\).</p> <p>[!note] Intuition Since only the sequence of states corresponds to the Kripke like structures used to give semantics to LTL, we can say that given a formula \\(\\varphi\\) and a path \\(\\pi\\), either all runs of \\(\\pi\\) model satisfy \\(\\varphi\\) or none of them do. Similar to Probabilistic Semantics for LTL, we want to be able to ignore paths and runs that depend on precision of the Timed Automata. For a probabilistic case, we ignore unlikely events, an analogous option would be meager sets. </p> <p>So we say that \\(\\mathcal A\\) largely satisfies \\(\\varphi\\) from \\(s\\), or \\(\\mathcal A, s\\mid\\!\\approx_{L} \\varphi\\) if the set of runs is large.</p> <p>But we also have </p> <p>[!note] Lemma Let \\(\\iota : \\text{Runs}(\\mathcal A,s)\\to\\text{Runs}(\\text R(\\mathcal A),\\iota(s))\\) be the projection map of finite runs from \\(\\mathcal A\\)  to \\(\\text{R}(\\mathcal A)\\). Then \\(\\iota\\) is continuous. And for every \\(O\\in\\mathcal T_\\mathcal A\\) we have \\(\\text{Int}(\\iota(O))\\) is non empty.</p> <p>Using the above lemma we get that $$     \\mathcal A, s \\mid!\\approx \\varphi \\iff \\text{R}(\\mathcal A),\\iota (s)\\mid!\\approx\\varphi $$</p>","tags":["Note","Incomplete"]},{"location":"Topological%20Semantics%20for%20LTL/#references","title":"References","text":"<p>Correspondence of Topological and Probabilistic Semantics for LTL</p>","tags":["Note","Incomplete"]},{"location":"Topological%20Space%20of%20Paths%20in%20a%20Timed%20Automata%20is%20Baire/","title":"Topological Space of Paths in a Timed Automata is Baire","text":"<p>202311181411</p> <p>Tags : Timed Automata, [[Topology]]</p>","tags":["Note","Incomplete"]},{"location":"Topological%20Space%20of%20Paths%20in%20a%20Timed%20Automata%20is%20Baire/#topological-space-of-paths-in-a-timed-automata-is-baire","title":"Topological Space of Paths in a Timed Automata is Baire","text":"<p>Let \\(\\mathcal A\\) be a timed automaton and \\(s\\) be a state of \\(\\mathcal A\\) and let \\(\\mathcal T_\\mathcal A\\) be a topology on \\(\\text{Runs}(A,s)\\). Then the topological space \\(\\mathbb T=(\\text{Runs}(\\mathcal A, s),\\mathcal T_\\mathcal A)\\) is a [[Baire Spaces|Baire Space]] </p>","tags":["Note","Incomplete"]},{"location":"Topological%20Space%20of%20Paths%20in%20a%20Timed%20Automata%20is%20Baire/#proof","title":"Proof","text":"<p>We proceed with proving that if we play the Banach-Mazur game on the above topological space with \\(\\mathcal T_\\mathcal A\\) as the family sets, and keeping a basic open set \\(\\pi_\\mathcal C = \\pi_\\mathcal C(s,e_1\\dots e_n)\\) as the target set(let \\(\\pi\\) be unconstrained version of \\(\\pi_\\mathcal c\\)).</p> <p>That is we play \\(MB(\\pi_\\mathcal C,\\text{Runs}(\\mathcal A, s), \\mathcal T_\\mathcal A )\\).</p> <p>We show that Player 2 does not have a winning strategy for any open set as target and using   this would show that none of the basic open sets of the space are meager. Hence the space is Baire.</p> <p>We do that by showing a winning strategy exists for player 1.</p> <p>We restrict the space to the path \\(\\pi\\). Because of how we have defined our topology, there is an embedding of \\(\\pi\\) in \\(\\mathbb R^m\\) for some \\(m\\le n\\).</p> <p>The strategy for player 1 is: - player one picks \\(\\pi_1=\\pi_{\\mathcal C}\\)  Then we consider the embedding of \\(\\pi_1\\) in \\(\\mathbb R^m\\).</p> <p>Now the game is equivalent to one where the space is \\(\\mathbb R^n\\), The target is a basic open set and the basis is the family of sets the gamed is played on. Also we have </p> <p> Thus, Player 1 has a winning strategy.</p>","tags":["Note","Incomplete"]},{"location":"Topological%20Space%20of%20Paths%20in%20a%20Timed%20Automata%20is%20Baire/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Topological%20Spaces/","title":"Topological Spaces","text":"<p>202301031101</p> <p>Type : #Note Tags :[[Topology]]</p>"},{"location":"Topological%20Spaces/#topological-spaces","title":"Topological Spaces","text":"<p>Let \\(X\\) be a set. A Topology on \\(X\\) isa  collection \\(\\tau\\) of subsets of \\(X\\) such that they follow the following properties 1. \\(\\emptyset, X\\) are both in \\(\\tau\\) 2. (Arbitrary unions)    \\(\\{U_\\alpha\\}_{\\alpha\\in\\tau}\\) such that each \\(U_\\alpha\\in\\tau\\) then \\(\\bigcup\\limits_{\\alpha\\in\\tau}\\in\\tau\\)  3. (Finite intrersection)    \\(\\{U_1,U_2\\dots U_n\\}\\in\\tau\\implies\\bigcap\\limits_{i=1}^nU_i\\in\\tau\\) Members of \\(\\tau\\) are defined as open sets</p>"},{"location":"Topological%20Spaces/#example","title":"Example","text":"<ul> <li>This is the Standard topology on \\(\\mathbb R^n\\) $$ \\begin{aligned} x&amp;\\in\\mathbb R^n\\ B_\\varepsilon(x)&amp;={y\\in\\mathbb R^n|d(x,y)&lt;\\varepsilon}\\ \\tau_{\\mathbb R^n}&amp;={U\\subseteq X|\\forall x\\in U,\\exists \\text{ such that } B_\\varepsilon(x)\\subseteq U} \\end{aligned} $$</li> </ul> <p>Usually a topology can be specified with a, smaller subcollection. - The topolgy created by all subsets of a set is called the discrete topology - let \\(X\\) bet a set, the topology \\(\\tau_X=\\{\\emptyset,X\\}\\) is called the trivial topology - let \\(X=\\{a,b\\}\\), for this set, some of the possible topologies would be     - Trival Topology     - Discrete Topology     - \\(\\tau_a=\\{\\emptyset,\\{a\\}, X\\}\\)     - \\(\\tau_b=\\{\\emptyset,\\{b\\}, X\\}\\)     In case of \\(\\tau_a\\), the element \\(a\\) is separated away from the other point \\(b\\) as there is an open set which contains \\(a\\) but no open set that contains \\(b\\), which means \\(a\\) can get arbitrarily close to \\(b\\) but \\(b\\) cannot get arbitrarily close to \\(a\\) .     There does not exists a metric on this topology - let \\(X\\) be a set. let \\(\\tau_X\\) = collection of sets \\(U\\subset X\\) such that \\(U\\setminus X\\) is finite or \\(X\\)     - \\(\\emptyset\\in\\tau_X, X\\in\\tau_X\\)     - \\(\\{U_\\alpha\\}\\) if each \\(U_\\alpha\\) is empty \\(\\implies\\) \\(\\bigcup U_\\alpha\\in \\tau_X\\), otherwise \\(U_\\beta\\) is no empty for some \\(\\beta\\) and \\(X\\setminus U_\\beta\\) is finite,        if \\(V=\\bigcup\\limits_{\\alpha\\in\\tau}X_\\alpha, X\\setminus V\\subset X\\setminus U_\\beta\\)     - \\(\\{U_1,U_2\\dots U_n\\}\\) if some \\(U\\) is empty, \\(W\\) is empty where \\(W=\\bigcap\\limits_{i=1}^n U_\\alpha\\), if none of \\(U\\) are empty then \\(X\\setminus W=(X\\setminus U_1)\\cup(X\\setminus U_2)\\cup\\dots\\cup(X\\setminus U_n)\\) - \\(X=\\mathbb R\\) and \\(\\tau_X=\\{(a, \\infty) | a\\in\\mathbb R\\}\\), This is called Ray Topology. - Subspace Topology - Product topology - Order Topology</p> <p>For some \\(X\\) , \\(\\tau_X,\\tau'_X\\) are two topologies on \\(X\\) and if \\(\\tau\\subset \\tau'\\) then \\(\\tau\\) is said to be coarser than \\(\\tau'\\).</p>"},{"location":"Topological%20Spaces/#related-problems","title":"Related Problems","text":""},{"location":"Topological%20Spaces/#references","title":"References","text":"<p>What is Topology</p>"},{"location":"Topology%20over%20Finite%20Paths%20in%20a%20Timed%20Automata/","title":"Topology over Finite Paths in a Timed Automata","text":"<p>202311181311</p> <p>Tags : Timed Automata, [[Topology]]</p>","tags":["Note","Incomplete"]},{"location":"Topology%20over%20Finite%20Paths%20in%20a%20Timed%20Automata/#topology-over-finite-paths-in-a-timed-automata","title":"Topology over Finite Paths in a Timed Automata","text":"<p>For a timed automaton \\(\\mathcal A\\), we define a basic open set as a constrained symbolic path \\(\\pi_{\\mathcal C}=\\pi_{\\mathcal C}(s,e_{1}\\dots e_{n})\\) such that the dimension of \\(\\pi_{\\mathcal C}\\) is defined and \\(\\text{Pol}(\\pi_\\mathcal C)\\) is open in \\(\\text{Pol}(\\pi)\\) for the topology induced on it, where \\(\\pi\\) is the unconstrained version of the same path.</p> <p>[!note] Informal Idea We say that a path, i.e a set of runs is open, if its dimension is defined (sort of like treating other paths as discrete points or something, making them not open) and, its open in the polygon corresponding to it.</p> <p>We write \\(\\mathcal T_{\\mathcal A}\\) for the topology of \\(\\text{Runs}(\\mathcal A,s)\\) induced by these sets. i.e The sets defined above, along with the entire space form a base for \\(\\mathcal T_\\mathcal A\\).</p> <p>[!example]  Here, let \\(s_0 = (l_0,0)\\) be the initial state. The basic (unconstrained) open sets here are of the form  \\(\\pi(s_{0},(e_{1}e_{2})^*)\\) and \\(\\pi(s_{0},e_{1}(e_{2}e_{1})^*)\\)  An example of a basic constrained open set would be \\(\\pi_{\\mathcal c}(s_{0},e_{1}e_{2})\\) where \\(\\mathcal c = \\left\\{  \\frac{1}{3}&lt;t_{1}&lt; \\frac{1}{2} \\; ;\\;t_{1}+t_{2}&gt;5  \\right\\}\\) It is also easy to check that the set of paths of the form \\(\\pi(s_{0},(e_{1}e_{2})^*e_{1}e_{3}e_{4}^*)\\) is meager.</p>","tags":["Note","Incomplete"]},{"location":"Topology%20over%20Finite%20Paths%20in%20a%20Timed%20Automata/#references","title":"References","text":"<ul> <li>Dimension of a Path</li> <li>Timed Automata Alternate Definition</li> <li>Probabilistic Semantics for LTL</li> <li>Topological Space of Paths in a Timed Automata is Baire</li> </ul>","tags":["Note","Incomplete"]},{"location":"Torsion%20submodule/","title":"Torsion submodule","text":"<p>202302151102</p> <p>Type : #Note Tags : [[Algebra]]</p>"},{"location":"Torsion%20submodule/#torsion-submodule","title":"Torsion submodule","text":""},{"location":"Torsion%20submodule/#related-problems","title":"Related Problems","text":""},{"location":"Torsion%20submodule/#references","title":"References","text":"<p>Module</p>"},{"location":"Trace%20and%20Norm/","title":"Trace and Norm","text":"<p>202306021206</p> <p>Type : #Note Tags : [[Number Theory]]</p>"},{"location":"Trace%20and%20Norm/#trace-and-norm","title":"Trace and Norm","text":"<p><pre><code>title:\nLet $K$ be a number field of degree $n$, we define two function $T ^{K}$ and $N ^{K}$ on $K$ as follows: Let $\\sigma_{1},\\sigma_{2}, \\dots,\\sigma_{n}$ be the $n$ embeddings of $K$ into $\\mathbb{C}$.\nFor each $\\alpha \\in K$, set $$\n\\begin{align}\nT^K(\\alpha) = \\sum_{i=1}^{n}\\sigma_{i}(\\alpha)  \\\\\nN^K(\\alpha) = \\prod_{i=1}^{n}\\sigma_{i}(\\alpha)\n\\end{align}\n$$\n</code></pre> - Write \\(T,N\\) instead of \\(T ^{K}, N ^{K}\\) if context is clear. - \\(T(\\alpha+\\beta) = T(\\alpha) + T(\\beta)\\) - \\(N(\\alpha\\beta) = N(\\alpha)N(\\beta)\\) - For \\(r \\in \\mathbb{Q}\\), \\(T(r) = nr\\), \\(N(r) = r ^{n}\\). - For \\(r \\in \\mathbb{Q}, \\alpha \\in K\\) \\(T(r\\alpha)= rT(\\alpha)\\), \\(N(r\\alpha) = r ^{n} N(\\alpha)\\)</p>"},{"location":"Trace%20and%20Norm/#theorem-1","title":"Theorem 1:","text":"<pre><code>title:\nLet $\\alpha$ have degree $d$ over $\\mathbb{Q}$. Let $t(\\alpha), n(\\alpha)$ denote the sum and product of its conjugates respectively.\nThen \n$$\n\\begin{align}\nT(\\alpha) = \\frac{n}{d}t(\\alpha) \\\\\nN(\\alpha) = (n(\\alpha))^{n/d}\n\\end{align}\n$$\n\nwhere $n = [K:\\mathbb{Q}]$.\n</code></pre>"},{"location":"Trace%20and%20Norm/#proof","title":"Proof:","text":"<p>Every embedding of \\(\\mathbb{Q}(\\alpha)\\) into \\(\\mathbb{C}\\) extends to \\(\\frac{n}{d}\\) embeddings of \\(K\\) into \\(\\mathbb{C}\\). (refer to Number Field)</p>"},{"location":"Trace%20and%20Norm/#corollary-1","title":"Corollary 1:","text":"<pre><code>title:\n$T(\\alpha),N(\\alpha) \\in \\mathbb{Q}$\n</code></pre>"},{"location":"Trace%20and%20Norm/#proof_1","title":"Proof:","text":"<p>\\(t(\\alpha),n(\\alpha) \\in \\mathbb{Q}\\) since they are coefficients of \\(\\alpha's\\) minimal polynomial, and the result follows.</p>"},{"location":"Trace%20and%20Norm/#corollary-2","title":"Corollary 2:","text":"<pre><code>title:\nIf $\\alpha$ is an algebraic integer, $T(\\alpha),N(\\alpha) \\in \\mathbb{Z}$.\n</code></pre>"},{"location":"Trace%20and%20Norm/#generalisations","title":"Generalisations","text":"<p><pre><code>title:\nLet $K,L$ be number fields with $K \\subset L$. Denote by $\\sigma_{1},\\sigma_{2},\\dots,\\sigma_{n}$ the $n$ embeddings of $L$ in $\\mathbb{C}$ which fix $K$ pointwise. Here $n = [K:L]$.\nFor $\\alpha \\in L$, the relative trace and relative norm are defined by:\n$$\n\\begin{align}\nT ^{L}_{K}(\\alpha) = \\sum_{i=1}^{n}\\sigma_{i}(\\alpha) \\\\\nN ^{L}_{K}(\\alpha) = \\prod_{i=1}^{n}\\sigma_{i}(\\alpha)\n\\end{align}\n$$\n</code></pre> - \\(T ^{L}_{K}(\\alpha+\\beta) = T ^{L}_{K}(\\alpha) + T ^{L}_{K}(\\beta)\\) - \\(N ^{L}_{K}(\\alpha\\beta) = N ^{L}_{K}(\\alpha)N ^{L}_{K}(\\beta)\\) - For \\(r \\in K\\), \\(T ^{L}_{K}(r) = nr\\), \\(N ^{L}_{K}(r) = r ^{n}\\). - For \\(r \\in K, \\alpha \\in L\\) \\(T_{K}^{L}(r\\alpha)= rT_{K}^{L}(\\alpha)\\), \\(N ^{L}_{K}(r\\alpha) = r ^{n} N ^{L}_{K}(\\alpha)\\)</p>"},{"location":"Trace%20and%20Norm/#theorem-2","title":"Theorem 2:","text":"<pre><code>title:\nLet $\\alpha$ have degree $d$ over $K$. Let $t(\\alpha), n(\\alpha)$ denote the sum and product of its conjugates over $K$ respectively.\nThen \n$$\n\\begin{align}\nT^L_K(\\alpha) = \\frac{n}{d}t(\\alpha) \\\\\nN^L_K(\\alpha) = (n(\\alpha))^{n/d}\n\\end{align}\n$$\n\nwhere $n = [K:\\mathbb{Q}]$.\n</code></pre>"},{"location":"Trace%20and%20Norm/#proof_2","title":"Proof:","text":"<p>Every embedding of \\(\\mathbb{Q}(\\alpha)\\) into \\(\\mathbb{C}\\) extends to \\(\\frac{n}{d}\\) embeddings of \\(K\\) into \\(\\mathbb{C}\\). (refer to Number Field)</p>"},{"location":"Trace%20and%20Norm/#corollary","title":"Corollary:","text":"<pre><code>title:\n$T_{K}^{L}(\\alpha)$ and $N_{K}^{L}(\\alpha)$ are in $K$. If $\\alpha \\in \\overline{\\mathbb{Z}} \\cap L$ then they are in $\\overline{\\mathbb{Z}} \\cap K$.\n</code></pre>"},{"location":"Trace%20and%20Norm/#theorem-3","title":"Theorem 3:","text":"<pre><code>title:\nLet $K,L,M$ be number fields with $K \\subset L \\subset M$. Then for all $\\alpha \\in M$, we have $$\n\\begin{align}\nT_{K}^{L}(T_{L}^{M}(\\alpha)) = T_{K}^{M}(\\alpha)\\\\\nN_{K}^{L}(N_{L}^{M}(\\alpha)) = N_{K}^{M}(\\alpha)\n\\end{align}\n$$\n</code></pre>"},{"location":"Trace%20and%20Norm/#proof_3","title":"Proof:","text":"<p>Let \\(\\sigma_{1},\\dots,\\sigma_{n}\\) and \\(\\tau_{1},\\dots,\\tau_{m}\\) be the embeddings of \\(L\\) and \\(M\\) which fix \\(K\\) and \\(L\\) pointwise, respectively. Now we want to compose the \\(\\sigma'\\)s with the \\(\\tau'\\)s but for that we need to extend them to a normal extension \\(N\\) of \\(M\\). So choose any extension of each \\(\\sigma_{i}\\) and \\(\\tau_j\\), and relabel those automorphisms of \\(N\\) as \\(\\sigma_{i}\\) and \\(\\tau_{j}\\).</p> <p>Now, $$ \\begin{align} T_{K}<sup>{L}(T_{L}</sup>{M}(\\alpha)) = T_{K}^{L}\\left( \\sum_{i=1}^{m}\\tau_{i}(\\alpha) \\right) &amp;= \\sum_{j=1}^{n}\\sigma_{j}\\left( \\sum_{i=1}^{m}\\tau_{i}(\\alpha) \\right)  \\ &amp;= \\sum_{i,j} \\sigma_{j}(\\tau_{i}(\\alpha)) \\ \\end{align} $$ Now note that each \\(\\sigma_{j}\\circ \\tau_{i}\\) is an embedding of \\(M\\) which fixes \\(K\\). We know that there are \\(mn\\) embeddings of \\(M\\) fixing \\(K\\), if we can show that the \\(\\sigma_{i} \\circ \\tau_{j}'s\\) are distinct, we will be done. If \\(\\sigma_{i}\\circ\\tau_{j} = \\sigma_{k}\\circ\\tau_{\\ell}\\), then applying both sides to an element of \\(L\\), we get \\(\\sigma_{i}=\\sigma_{k}\\) are equal on the image of \\(L\\) under the embeddings \\(\\tau_{j},\\tau_{\\ell}\\). This gives \\(i = k\\). Since \\(\\sigma_{i}=\\sigma_{k}\\) is an automorphism, we can multiply by its inverse to get \\(\\tau_{j} = \\tau_{\\ell}\\), and hence \\(j = \\ell\\).</p>"},{"location":"Trace%20and%20Norm/#references","title":"References","text":"<p>Algebraic Integers Number Field</p>"},{"location":"Traces%20of%20Powers%20of%20Adjacency%20Matrices/","title":"Traces of Powers of Adjacency Matrices","text":"<p>202308121508</p> <p>Type : #Note Tags :[[Algebraic Graph Theory]]</p>"},{"location":"Traces%20of%20Powers%20of%20Adjacency%20Matrices/#traces-of-powers-of-adjacency-matrices","title":"Traces of Powers of Adjacency Matrices","text":"<p>The traces of powers of Adjacency Matrices denote the number of closed paths of a given length</p> <p>like \\(\\text{trace}(A^{n})\\) would be the number of all closed paths of length \\(n\\).</p> <p>and trace of square and cube have a natural understanding  \\(\\text{trace}(A^{2})\\) is the total number of edges in the graph. \\(\\text{trace}(A^{3})\\) is the total number of triangles in the graph.</p> <p>Also given the characteristic polynomial of the form \\(\\sum\\limits_{i=1}^{n}c_{i}x^{i}\\)  then \\(c_{i}=(-1)^{i}\\frac{\\text{trace}(A^{i})}{i}\\) </p>"},{"location":"Traces%20of%20Powers%20of%20Adjacency%20Matrices/#references","title":"References","text":"<p>Automorphism Group of a Graph</p>"},{"location":"Train%20Track%20Crossing%20for%20Timed%20Automata/","title":"Train Track Crossing for Timed Automata","text":"<p>202310101458</p> <p>tags : Timed Automata</p>","tags":["Example"]},{"location":"Train%20Track%20Crossing%20for%20Timed%20Automata/#train-track-crossing-for-timed-automata","title":"Train Track Crossing for Timed Automata","text":"<pre><code>There is a *Road* and *Train Tracks* that cut through it.\nOn either side of the *Train Tracks* there are gates which close when a train is passing to prevent vehicles and pedestrians from trying to cross at the same time.\n\nDesign a system to automate the functioning of the gates.\n</code></pre> <p>A possible system to model the above situation can be to construct a Networks of Timed Automata where we have a Timed Automaton for each of the following. - The Train - The Gate - The Controller which is there to help the gate and the train communicate.</p> <p>![[Railway Crossing Timed Automata.excalidraw|1000]]</p> <p>In the above model, we see that whenever the train approaches in the left automata, it also triggers the transition in the middle one, for the controller, which then signals the gates to close within 2 seconds.  Similarly when the train is leaving, the controller triggers the opening of the gates after a certain time interval.</p> <p>The starting state of the system is \\(\\langle \\text{Far}, \\text{waiting for train}, \\text{open}\\rangle\\)</p> <pre><code>title: Motivation for reachability algorithms\nHere it is important to notice that even though the state space has $4^{3}=64$ states, there are certain states that should not be reachable, like the state\n$\\langle\\text{near}, -, \\text{open}\\rangle$.\nHence it is important to have reachability algorithms for model checking.\n</code></pre>","tags":["Example"]},{"location":"Train%20Track%20Crossing%20for%20Timed%20Automata/#related","title":"Related","text":"<p>Zone Automata</p>","tags":["Example"]},{"location":"Translating%20Haskell%20Programs%20to%20Lambda%20Calculus/","title":"Translating Haskell Programs to Lambda Calculus","text":"<p>202310210310</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note","Incomplete"]},{"location":"Translating%20Haskell%20Programs%20to%20Lambda%20Calculus/#translating-haskell-programs-to-lambda-calculus","title":"Translating Haskell Programs to Lambda Calculus","text":"<p>A Haskell program can be consists of a set of definitions with an expression to be evaluated.  I will layout haskell programs in the following way to keep the components separate <pre><code>set of definitions\n---\nExpressions to be evaluated\n</code></pre> for example  <pre><code>square n = n * n\n---\n2 * (square 5)\n</code></pre> and the translation to Enriched Lambda Calculus will be <pre><code>let square = \\n. * n n\nin  (* 2 (square 5))\n</code></pre></p> <p>To help describe the process, we talk about two translation schemes, one for the final expression called TE and one for all the definitions called TD. We also make TR to talk about more complicated right hand side only features of the language.</p> <p>These can be summarized as  </p> <p>With these, given a Haskell program <pre><code>Definition 1\nDefinition 2\n...\nDefinition n\n---\nExpression\n</code></pre> we generate the following lambda expression <pre><code>letrec \n    TD[[ Definition 1 ]]\n    TD[[ Definition 2 ]]\n    ...\n    TD[[ Definition n ]]\nin \n    TE[[  Expression  ]]\n</code></pre></p> <p><code>let</code> was used in the previous example instead of <code>letrec</code>, but Haskell programs can have recursive definitions, so we use <code>letrec</code> in general. </p>","tags":["Note","Incomplete"]},{"location":"Translating%20Haskell%20Programs%20to%20Lambda%20Calculus/#example","title":"Example","text":"<pre><code>average a b = (a+b)/2\n---\naverage 2 (3+5)\n</code></pre> <p>This will be transformed to <pre><code>letrec\n    TD[average a b = (a+b)/2](&lt;average a b = (a+b)/2.md&gt;)\nin\n    TE[[ average 2 (3+5) ]]\n</code></pre></p> <p>The application for <code>TE</code> gives us <pre><code>   TE[[ average 2 (3+5)]]\n-&gt; TE[[ average ]]  TE[[ 2 ]] TE[[ 3+5 ]]\n-&gt; average 2 (TE[[ + ]] TE[[ 3 ]] TE[[ 5 ]] )\n-&gt; average 2 (+ 3 5)\n</code></pre></p> <p>Similarly, application of <code>TD</code> gives us <pre><code>   TD[average a b = (a+b)/2](&lt;average a b = (a+b)/2.md&gt;)\n-&gt; average = \\a.\\b. TE[(a+b)/2](&lt;(a+b)/2.md&gt;)\n-&gt; average = \\a.\\b.( TE[/](&lt;../../../../../../.md&gt;) TE[[ (a+b) ]] TE[[ 2 ]])\n-&gt; average\n      = \\a.\\b.(/ (TE[[ + ]] TE[[ a ]] TE[[ b ]]) 2)\n-&gt; average = \\a.\\b.(/ (+a b) 2)\n</code></pre></p> <p>putting it all together we get <pre><code>letrec\n    average = \\a.\\b.(/ (+ a b) 2)\nin\n    average 2 (+ 3 5)\n</code></pre></p> <p>To complete the example, this is the above expression converted to ordinary lambda calculus</p> \\[ (\\lambda \\text{average}.(\\text{average}\\; 2\\;(+\\;3\\;5)))\\quad(\\lambda a.\\lambda b.(/ (+\\;a\\;b)\\;2)) \\] <pre><code>- Defining Types\n- Type Checking\n</code></pre>","tags":["Note","Incomplete"]},{"location":"Translating%20Haskell%20Programs%20to%20Lambda%20Calculus/#references","title":"References","text":"<ul> <li>Enriched Lambda Calculus</li> <li>Translation Scheme from Haskell to Lambda Calculus for Expressions</li> <li>Translation Scheme from Haskell to Lambda Calculus for Definitions</li> <li>Translation Scheme from Haskell to Lambda Calculus for Some RHS only options</li> <li>Evaluating Pattern Matching in Lambda Calculus</li> </ul>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Definitions/","title":"Translation Scheme from Haskell to Lambda Calculus for Definitions","text":"<p>202310210410</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Definitions/#translation-scheme-from-haskell-to-lambda-calculus-for-definitions","title":"Translation Scheme from Haskell to Lambda Calculus for Definitions","text":"<p>The \\(\\mathbf{TD}\\) scheme takes a Haskell definition as its argument and produces a \\(\\text{letrec}\\) definition as its result.</p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Definitions/#variable-definition","title":"Variable Definition","text":"<p>Consider the following definition <pre><code>v = 5 * 7\n</code></pre> which can be translated to  <pre><code>v = * 5 7\n</code></pre></p> <p>In general we get $$ \\mathbf{TD}\\textlbrackdbl\\;v=E\\;\\textrbrackdbl\\quad\\equiv\\quad v=\\mathbf{TE}\\textlbrackdbl\\;E\\;\\textrbrackdbl $$ where \\(v\\) is a variable and \\(E\\) is an expression.</p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Definitions/#simple-function-definition","title":"Simple Function Definition","text":"<p>Consider the following definition <pre><code>square n = n * n\n</code></pre> which translates to  <pre><code>square = \\n. * n n\n</code></pre></p> <p>We can generalize this as follows $$ \\mathbf{TD}\\textlbrackdbl\\;f\\;v_{1}\\;\\dots\\;v_{n}=E\\;\\textrbrackdbl\\quad\\equiv\\quad f=\\lambda v_{1}\\dots\\lambda v_{n}.\\mathbf{TE}\\textlbrackdbl\\; E\\;\\textrbrackdbl $$</p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Definitions/#function-definitions-with-pattern-matching","title":"Function Definitions with Pattern  Matching","text":"<p>Consider the following definition <pre><code>head [x:_] = x\n</code></pre> It is not plausible to translate us using the above rule but the syntax of pattern matching is a subset of expressions, so we can do the following. $$ \\mathbf{TD}\\textlbrackdbl\\;f\\;p_{1}\\dots p_{n}=E\\;\\textrbrackdbl\\quad\\equiv\\quad f=\\lambda v_{1}\\dots\\lambda v_{n}.(((\\lambda p_{1}' \\dots\\lambda p_{n}'. E')v_{1}\\dots v_{n}) \\triangleright \\text{ERROR}) $$</p> <p>More in-depth analysis of the following topics is done in Patterns.</p> <p></p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Definitions/#summary","title":"Summary","text":"","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Definitions/#ab1963","title":"^ab1963","text":"","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Definitions/#references","title":"References","text":"<p>Translation Scheme from Haskell to Lambda Calculus for Definitions Translation Scheme from Haskell to Lambda Calculus for Some RHS only options Translating Haskell Programs to Lambda Calculus</p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Expressions/","title":"Translation Scheme from Haskell to Lambda Calculus for Expressions","text":"<p>202310210310</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Expressions/#translation-scheme-from-haskell-to-lambda-calculus-for-expressions","title":"Translation Scheme from Haskell to Lambda Calculus for Expressions","text":"<p>We describe \\(\\mathbf{TE}\\) by case analysis of forms of Haskell expressions.</p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Expressions/#constants","title":"Constants","text":"<p>The constants or built-in functions are all the same set of constants for Enriched Lambda Calculus, so the following rule is enough $$ \\mathbf{TE}\\textlbrackdbl\\; k\\; \\textrbrackdbl = k' $$ where \\(k'\\) refers to the constant defined in Lambda Calculus which corresponds to \\(k\\) defined in Haskell.</p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Expressions/#variables","title":"Variables","text":"<p>Similar to the above translation, this is enough $$ \\mathbf{TE}\\textlbrackdbl\\; v\\;\\textrbrackdbl= v $$ where \\(v\\) is a variable (including names of user defined functions and constructors)</p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Expressions/#function-application","title":"Function Application","text":"<p>Function application in Haskell is denoted by juxtaposition, which is the same as the syntax used in Lambda Calculus $$ \\mathbf{TE}\\textlbrackdbl\\;E_{1}\\;E_{2}\\;\\textrbrackdbl = \\mathbf{TE}\\textlbrackdbl\\;E_{1}\\;\\textrbrackdbl \\quad \\mathbf{TE}\\textlbrackdbl\\;E_{2}\\;\\textrbrackdbl $$ But Haskell also provides an option to use Infix(operator between the operands) notation. This translates to  $$ \\mathbf{TE}\\textlbrackdbl\\;E_{1}\\;\\text{infix}\\;E_{2}\\;\\textrbrackdbl =  \\mathbf{TE}\\textlbrackdbl\\;\\text{infix} \\;\\textrbrackdbl \\quad \\mathbf{TE}\\textlbrackdbl\\;E_{1}\\;\\textrbrackdbl \\quad  \\mathbf{TE}\\textlbrackdbl\\;E_{2}\\;\\textrbrackdbl $$ And it allows the user defined operators to infix by putting them in ` (back-tick)</p> \\[ \\mathbf{TE}\\textlbrackdbl\\;E_{1}\\;\\text{`v`}\\;E_{2}\\;\\textrbrackdbl =  \\mathbf{TE}\\textlbrackdbl\\;\\text{`v`} \\;\\textrbrackdbl \\quad   \\mathbf{TE}\\textlbrackdbl\\;E_{1}\\;\\textrbrackdbl \\quad  \\]","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Expressions/#summary","title":"Summary","text":"<p> ^76f158</p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Expressions/#references","title":"References","text":"<p>Translation Scheme from Haskell to Lambda Calculus for Expressions Translation Scheme from Haskell to Lambda Calculus for Definitions Translating Haskell Programs to Lambda Calculus</p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Some%20RHS%20only%20options/","title":"Translation Scheme from Haskell to Lambda Calculus for Some RHS only options","text":"<p>202310222010</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Some%20RHS%20only%20options/#translation-scheme-from-haskell-to-lambda-calculus-for-some-rhs-only-options","title":"Translation Scheme from Haskell to Lambda Calculus for Some RHS only options","text":"","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Some%20RHS%20only%20options/#for-more-complex-definitions","title":"For More Complex Definitions","text":"<p>Haskell has some more options that can be used to simplify the syntax of the definition by adding things like Conditional Equations and Where Clauses. These things combine with Pattern matching but only affect the Right side of the definition, hence we define TR to only deal with the right hand side of the definition.</p> <p>Considering that the following modification is required to define \\(\\mathbf{TD}\\) $$ \\mathbf{TD}\\textlbrackdbl\\;p=R\\;\\textrbrackdbl\\quad\\equiv\\quad\\mathbf{TE}\\textlbrackdbl\\;p\\;\\textrbrackdbl=\\mathbf{TR}\\textlbrackdbl\\;R\\;\\textrbrackdbl $$</p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Some%20RHS%20only%20options/#conditional-equations","title":"Conditional Equations","text":"<p>Consider the following definition <pre><code>gcd a b | a &gt; b  = gcd (a-b) b\n        | a &lt; b  = gcd a (b-a)\n        | a == b = a\n</code></pre> It is easy to see that the translation of the part after the <code>|</code> would be <pre><code>(IF (&gt;  a b) (gcd (- a b) b)\n(IF (&lt;  a b) (gcd a (- b a))\n(IF (== a b) a FAIL)))\n</code></pre> Note that if all the guards fail, <code>FAIL</code> is returned, we can optimize to ignore that case if the final condition is the constant <code>True</code>.</p> <p>This can be summarized by  </p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Some%20RHS%20only%20options/#where-clauses","title":"Where Clauses","text":"<p>Haskell allows the right side of the definition to be qualified by a \\(\\text{Where}\\) clause. Like <pre><code>gcd a b | a &gt; b  = gcd diff b\n        | a &lt; b  = gcd a (-diff)\n        | a == b = a\n    where\n         diff = a - b\n</code></pre> \\(\\text{where-}\\)clauses is very similar to \\(\\text{let-}\\)expressions, the only difference being that \\(\\text{where-}\\)clauses provide a nice syntactic sugar over \\(\\text{let-}\\)expressions when used with conditional definitions The obvious translation would be <pre><code>gcd a b = \n    letrec\n        diff = a - b\n    in\n        if (a &lt; b)  then gcd diff b else\n        if (a &gt; b)  then gcd a (-diff) else\n        if (a == b) then a else FAIL\n</code></pre></p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Some%20RHS%20only%20options/#summary","title":"Summary","text":"<p> ^d83623</p>","tags":["Note","Incomplete"]},{"location":"Translation%20Scheme%20from%20Haskell%20to%20Lambda%20Calculus%20for%20Some%20RHS%20only%20options/#references","title":"References","text":"<p>Translation Scheme from Haskell to Lambda Calculus for Expressions Translation Scheme from Haskell to Lambda Calculus for Some RHS only options Translating Haskell Programs to Lambda Calculus</p>","tags":["Note","Incomplete"]},{"location":"Travelling%20Salesman%20Problem/","title":"Travelling Salesman Problem","text":"<p>202310021510</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note","Incomplete"]},{"location":"Travelling%20Salesman%20Problem/#travelling-salesman-problem","title":"Travelling Salesman Problem","text":"<p>Given: \\(n\\) cities \\(a_1,\\dots,a_n\\), pairwise distances \\((a_{i},a_{j})\\ge 0\\ \\forall i,j\\) Goal: Find a tour that visits all cities and incurs min cost (all cities visited exactly once).</p> <pre><code>title: No approximation for the general case\nTake $G$, and assume all weights of edges of $G$ are of cost $1$ and cost of non edges is $k$ for $k$ as large as desired.\nThus, assume metric property.\n</code></pre> <p>\\(\\alpha-\\)approximation of general \\(TSP \\implies\\) Hamiltonian cycle \\(\\in P\\) where \\(\\alpha\\in\\text{Poly}(n)\\).</p>","tags":["Note","Incomplete"]},{"location":"Travelling%20Salesman%20Problem/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Triangular%20Graphs/","title":"Triangular Graphs","text":"<p>202308301008</p> <p>Type : #Note Tags : [[Algebraic Graph Theory]]</p>"},{"location":"Triangular%20Graphs/#triangular-graphs","title":"Triangular Graphs","text":"<p>Let \\(m\\) be a positive integer. The Triangular Graph \\(T_{m}\\) is the graph where \\(V={M\\choose2}\\) where \\(M=[m]\\) with \\(A\\sim B\\iff |A\\cap B|=1\\) for \\(A, B\\in V\\) </p> <pre><code>The **Triangular Graphs** is just the [Line Graph](&lt;./Line Graph.md&gt;) of the **Complete Graph**\n</code></pre> <p>It is strongly regular with $$ \\begin{align} n&amp;= {m\\choose 2}\\ k&amp;= 2(m-2)\\ a&amp;= m-2\\ c&amp;= 4 \\end{align} $$ <pre><code>title:Peterson Creep\nThe Complement of the Traigular Grapth $T_{5}$ is a graph which is stronlgy regular with parameters $(10,3,0,1)$ Which is the _Peterson Graph_!\n</code></pre></p>"},{"location":"Triangular%20Graphs/#references","title":"References","text":"<p>Line Graph Automorphism Group of a Graph</p>"},{"location":"Trigonometric%20Functions/","title":"Trigonometric Functions","text":"<p>202212171912</p> <p>Type : #Note Tags : [[Analysis]]</p>"},{"location":"Trigonometric%20Functions/#trigonometric-functions","title":"Trigonometric Functions","text":"<p>We define $$ \\begin{aligned} &amp;C(x) = \\frac12[E(ix) + E(-ix)],&amp;S(x) = \\frac{1}{2i}[E(ix)-E(-ix)] \\end{aligned} $$ where  $$ E(z) = \\sum_{n=0}^\\infty \\frac{z^n}{n!} $$ Where \\(E(x)\\) is the [[Exponential Function]]. We have $$ \\begin{aligned} &amp;C'(x) = -S(x), &amp;S'(x) = C(x) \\end{aligned} $$</p> <p>There exists a positive number \\(x\\) such that \\(C(x) = 0\\), this is because if there does not exists an \\(x\\) then \\(C(x)&gt;0\\implies S'(x)&gt;0\\) which implies \\(S(x)\\) is monotonically increasing, since \\(S(0) = 0\\), \\(S\\ge 0\\) , this implies \\(C\\) is monotonically decreasing but for some \\(0&lt;x&lt;y\\) we have $$ S(x)(y-x)&lt;\\int_x^yS(t)dt=C(x)-C(y)\\le 2 $$  which is a contradiction, Hence there exists numbers such that \\(C\\) vanishes on them. Let \\(x_0\\) be the smallest such number, this exists as the set of zeros of continuous functions is closed, and we define \\(\\pi\\) by $$ \\pi = 2x_0$$  Thus we have  $$  E(\\frac{\\pi\\iota}2) = i  $$  and the addition formula gives us   $$  E(\\pi\\iota) = -1  $$  hence   $$  E(z+2\\pi\\iota) = E(z)  $$</p>"},{"location":"Trigonometric%20Functions/#related-problems","title":"Related Problems","text":""},{"location":"Trigonometric%20Functions/#references","title":"References","text":""},{"location":"Tube%20Lemma/","title":"Tube Lemma","text":"<p>202302121802</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Tube%20Lemma/#tube-lemma","title":"Tube Lemma","text":"<pre><code>title: Theorem\n$X$ and $Y$ topological spaces with $Y$ [compact](&lt;./Compactness.md&gt;). Let $p \\in X$. If $N \\subset X \\times Y$ is open s.t. $\\{p\\} \\times Y \\subset N$ then there is a nbhd $U\\subset X$ of $p$ s.t. $U \\times Y \\subset N$.\n</code></pre>"},{"location":"Tube%20Lemma/#proof","title":"Proof:","text":"<p>For each \\((p,q)\\in X \\times Y\\) we have \\((p,q) \\in \\{p\\}\\times Y \\subset N\\). Since \\(N\\) is open, we have a basis element \\(V_q\\) s.t. \\((p,q) \\in U_q\\times V_q\\subset N\\). This is a cover of \\(\\{p\\}\\times Y\\) and since \\(Y\\) is cpt, this has a finite subcover. Hence \\(\\bigcap U_{q_i}\\times Y\\) contains \\(\\{p\\}\\times Y\\). So \\(U = \\bigcap U_{q_i}.\\square\\)</p>"},{"location":"Tube%20Lemma/#related-problems","title":"Related Problems","text":""},{"location":"Tube%20Lemma/#references","title":"References","text":""},{"location":"Turing%20Machine%20Configuration/","title":"Turing Machine Configuration","text":"<p>202211150011</p> <p>Type : #Note Tags : [[Theory of Computation]]</p>"},{"location":"Turing%20Machine%20Configuration/#turing-machine-configuration","title":"Turing Machine Configuration","text":"<p>The configuration of the turing machine is the triple \\(Q\\times\\{y\\sqcup^\\omega|y\\in\\Gamma^*\\}\\times\\mathbb N\\) which is the current state, the current content of the tape and the position of the pointer. It is generally represented by \\((p, z, n)\\) </p> <p>The start configuration on the input \\(x\\in\\Sigma^*\\) is the configuration \\((s, \\dashv x\\sqcup^\\omega, 0)\\).</p> <p>We can define the next configuration relation \\(\\xrightarrow[M]{1}\\) as $$ \\begin{equation} (p, z, n) \\xrightarrow[M]{1} \\begin{cases} (q, s^n_b(z), n-1)&amp;\\text {if }\\delta(p,z_n)=(q, b, L)\\ (q, s^n_b(z), n+1)&amp;\\text {if }\\delta(p,z_n)=(q, b, R)\\ \\end{cases} \\end{equation} $$</p> <p>and we can define the transitive closure of \\(\\xrightarrow[M]{*}\\) of \\(\\xrightarrow[M]{1}\\) as - \\(\\alpha \\xrightarrow[M]{0}\\alpha\\) - \\(\\alpha \\xrightarrow[M]{n+1}\\beta\\) if \\(\\alpha\\xrightarrow[M]{n}\\gamma\\xrightarrow[M]{1}\\beta\\) for some \\(\\gamma\\) - \\(\\alpha \\xrightarrow[M]{*}\\beta\\) if \\(\\alpha\\xrightarrow[M]{n}\\beta\\) for some \\(n\\ge0\\)</p> <p>The machine accepts an input \\(x\\in \\Sigma^*\\) if  $$ (s, \\dashv x\\sqcup^\\omega, 0)\\xrightarrow[M]{*}(t, y, n) $$</p> <p>The machine rejects an input \\(x\\in \\Sigma^*\\) if  $$ (s, \\dashv x\\sqcup^\\omega, 0)\\xrightarrow[M]{*}(r, y, n) $$</p> <p>If a machine rejects or accepts an input, it is said to halt on the input, as with a PDA, it might not halt in which case it is said to loop. If a Turing Machine halts on all its inputs it is called total.</p> <p>\\(L(M) = \\{x\\in\\Sigma^*| M \\text { accepts } x \\}\\)  A set of strings is called  - Recursivly Enumerable (R.E.) if it is \\(L(M)\\) for some turing machine \\(M\\) - Co R.E. if its complement is (R.E.) - Recursive, if it is \\(L(M)\\) for a total turing machine.</p> <p>For example \\(\\{ww|w\\in \\{a, b\\}^*\\}\\) is a recurisve set.</p>"},{"location":"Turing%20Machine%20Configuration/#related-problems","title":"Related Problems","text":""},{"location":"Turing%20Machine%20Configuration/#references","title":"References","text":"<p>Turing Machines</p>"},{"location":"Turing%20Machines%20with%20multiple%20tapes/","title":"Turing Machines with multiple tapes","text":"<p>202211171711</p> <p>Type : #Note Tags : [[Theory of Computation]]</p>"},{"location":"Turing%20Machines%20with%20multiple%20tapes/#turing-machines-with-multiple-tapes","title":"Turing Machines with multiple tapes","text":"<p>A Turing Machine with mutliple tapes intuitively hs 3 semi-infinte tapes and 3 heads which all point to different tapes and can move independently of each other. on each step, all 3 cells that the heads point to are read and then changed. It's transition functions would be of the type  \\(\\delta: Q\\times\\Gamma^3 \\to Q\\times\\Gamma^3\\times\\{L,R\\}^3\\) </p> <p>A Turing Machine with 3 tapes can be simulated by a turing machine with a single tape.</p> <p>The idea is that in the tape, each cell will store 3 letters in differnet \"tracks\" and on each track one symbol has a special mark to indicate where the head points to in each stack. { width=\"400\" } The tape symbols in \\(N\\) belong to the set \\(\\{\\vdash\\}\\cup(\\Gamma\\cup\\Gamma')^3\\)  where \\(\\Gamma'\\stackrel{\\mathclap{def}}{=}\\{\\hat c|c\\in \\Gamma\\}\\) and { width=\"30\" } is the blank symbol. { width=\"400\" }</p> <p>This is the initial configuration of the tape with the input copied on the first track</p> <p>In each step the turing machine, we start from the left and move right till we read all the marked letters, the depending on the transition function of the multi tape turing machine we will change the tape, ad move the head back to the leftmost element for the next step.</p> <p>Hence, a turing machine with multiple tapes is not more powerful than a regular turing machine.</p>"},{"location":"Turing%20Machines%20with%20multiple%20tapes/#related-problems","title":"Related Problems","text":"<p>Two-Way Infinite Tape Turing Machine</p>"},{"location":"Turing%20Machines%20with%20multiple%20tapes/#references","title":"References","text":"<p>Turing Machines</p>"},{"location":"Turing%20Machines/","title":"Turing Machines","text":"<p>202211141611</p> <p>Type : #Note Tags : [[Theory of Computation]]</p>"},{"location":"Turing%20Machines/#turing-machines","title":"Turing Machines","text":""},{"location":"Turing%20Machines/#informal-definition","title":"Informal Definition","text":"<p>A TM has a set of finite states and a semi-infinite tape whose left is marked with the \\(\\vdash\\) symbol, the input word, which is a finite word is written on the tap after the \\(\\vdash\\) symbol and the rest of the tape is filled with \\(\\sqcup\\), there is also a head(pointer), and we can read/write the cell it points to.  The Machine starts on a state \\(S\\) and ends on the states \\(t\\) if it accepts the word or ends at state \\(r\\) if it rejects a word.</p>"},{"location":"Turing%20Machines/#formal-definition","title":"Formal Definition","text":"<p>A Turing Machine is a 9-tuple $$ M = (Q, \\Sigma, \\Gamma, \\vdash, \\sqcup, \\delta, s, t, r) $$ where  - \\(Q\\) is a finite set (the states). - \\(\\Sigma\\) is a finite set (the input alphabet). - \\(\\Gamma\\) is a finite set (tape alphabet) which contains \\(\\Sigma\\). - \\(\\vdash\\in \\Gamma-\\Sigma\\) , the left end marker. - \\(\\sqcup \\in \\Gamma - \\Sigma\\), the blank symbol. - \\(\\delta: Q\\times \\Gamma \\to Q\\times\\Gamma\\times\\{L,R\\}\\), the transio - \\(s\\in Q\\) (the start state). - \\(t\\in Q\\) (the accepting state). - \\(r\\in Q, r\\ne t\\) (the reject state). Intuitively the transition \\(\\delta(q, \\gamma) \\to (p, \\gamma', d)\\) means, if at state \\(q\\), the head reads \\(\\gamma\\) then replace \\(\\gamma\\) with \\(\\gamma'\\), moving the state \\(p\\), and the head moves in the direction \\(d\\)(left or right).</p> <p>The configuration of the stack symbol is an element of \\(Q\\times\\{y\\sqcup^\\omega|y\\in\\Gamma^*\\}\\times \\mathbb N\\) which represent, the state, stack content and the position of the pointer.</p> <p></p> <p>There are several different flavours of turing machines and other machines that may seem more or less powerful but are computationally equivalent, some of them are, also known as Turing Complete: ^71be18 - Turing Machines with multiple tapes - Two-Way Infinite Tape Turing Machine - Two Stacks - Counter Automata - Enumeration Machine</p> <p>Turing Macines can be simulated by other Turing machines, to do that we can construst can Universal Turing Machine. A Universal Turing machines is a turing machine whose language is  $$ L(U) = {M#x | x\\in L(M)} $$ Although Universal Turing Machines can simulate other Turing Machines, doing more advanced analysis on turing machines isn't easy, sometimes not even possible.  Undecidability of the Halting Problem and Undecidability of the Membership Problem are some of the examples. ^71406d</p>"},{"location":"Turing%20Machines/#related-problems","title":"Related Problems","text":""},{"location":"Turing%20Machines/#references","title":"References","text":"<p>Recursive and Recursively Eumberable Sets</p>"},{"location":"Two%20Stacks/","title":"Two Stacks","text":"<p>202211171811</p> <p>Type : #Note Tags : [[Theory of Computation]]</p>"},{"location":"Two%20Stacks/#two-stacks","title":"Two Stacks","text":"<p>A two stack machine is a machine with a two-way, read only input head, it is as powerful as a turing machine. A turing  machine can be simulated by adding the elments to the left and right of the head to diffent stacks, the closest elments to the head being at the top, and moving the head would be the same as popping and element off a stack and pushing it onto another.  can be simulated by  { width=\"700\" }</p>"},{"location":"Two%20Stacks/#related-problems","title":"Related Problems","text":"<p>Two-Way Infinite Tape Turing Machine Turing Machines with multiple tapes</p>"},{"location":"Two%20Stacks/#references","title":"References","text":"<p>Turing Machines</p>"},{"location":"Two-Way%20Infinite%20Tape%20Turing%20Machine/","title":"Two Way Infinite Tape Turing Machine","text":"<p>202211171811</p> <p>Type : #Note Tags : [[Theory of Computation]]</p>"},{"location":"Two-Way%20Infinite%20Tape%20Turing%20Machine/#two-way-infinite-tape-turing-machine","title":"Two-Way Infinite Tape Turing Machine","text":"<p>Two-way Infinte tape does not add any power. we can just break it and put it back together such that the two halfs are now two tracks of a semi-infinte tape { width=\"600\" } The bottom track will be used to simulate the machine when the head is on the left of the fold, and the top track will be used when the head is on the right.</p>"},{"location":"Two-Way%20Infinite%20Tape%20Turing%20Machine/#related-problems","title":"Related Problems","text":"<p>Turing Machines with multiple tapes</p>"},{"location":"Two-Way%20Infinite%20Tape%20Turing%20Machine/#references","title":"References","text":"<p>Turing Machines</p>"},{"location":"Tychonoff%27s%20Theorem/","title":"Tychonoff's Theorem","text":"<p>202304031304</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Tychonoff%27s%20Theorem/#tychonoffs-theorem","title":"Tychonoff's Theorem","text":""},{"location":"Tychonoff%27s%20Theorem/#references","title":"References","text":""},{"location":"Type%20Inhabitation%20in%20P-SPACE/","title":"Type Inhabitation in P SPACE","text":"<p>202309181409</p> <p>Tags : [[Lambda Calculus]], [[Type Theory]]</p>","tags":["Note","Incomplete"]},{"location":"Type%20Inhabitation%20in%20P-SPACE/#type-inhabitation-in-p-space","title":"Type Inhabitation in P-SPACE","text":"<p>For every \\(L\\subseteq\\Sigma^{*}\\) such that \\(L\\in \\text{PSPACE}\\), there is an efficient \\((\\text{PTIME})\\) \\(f:\\Sigma^{*} \\to \\text{TQBF}\\) such that such that $$ \\forall x\\in \\Sigma^{*}\\iff f(x)\\text{ is valid} $$</p> <p>for every \\(\\Phi\\), we will define \\(\\Gamma_{\\Phi}\\) and \\(\\alpha_{\\Phi}\\) such that \\(\\Phi\\) is valid iff \\(\\Gamma_{\\Phi}\\vdash_{\\text{NJ}(\\to)}\\alpha_{\\Phi}\\) </p> <p>Let \\(\\text{Voc}(\\Phi)\\) be the set of propositional letters occurring in \\(\\Phi\\). For every \\(p \\in \\text{Voc}(\\Phi)\\) let \\(\\alpha_{p},\\alpha_{\\lnot p}\\) be two propositional letters. For every \\(\\varphi\\in \\text{sf}(\\Phi)\\) let \\(\\alpha_{\\varphi}\\) be a propositional letter.</p> <pre><code>write types in $\\Gamma_{\\Phi}$ for logical operators\n</code></pre>","tags":["Note","Incomplete"]},{"location":"Type%20Inhabitation%20in%20P-SPACE/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Typed%20Combinatory%20Logic/","title":"Typed Combinatory Logic","text":"<p>202309210909</p> <p>Tags : [[Logic]], [[Lambda Calculus]], [[Type Theory]]</p>","tags":["Note"]},{"location":"Typed%20Combinatory%20Logic/#typed-combinatory-logic","title":"Typed Combinatory Logic","text":"<p>Typed Combinatory Logic corresponds to Hilbert Style Proofs as the combinatory terms can be given the following types $$ \\begin{align} K &amp;: \\alpha\\to \\beta\\to\\alpha\\ S &amp;: (\\alpha\\to\\beta\\to\\gamma)\\to(\\alpha\\to\\beta)\\to\\alpha\\to\\gamma \\end{align} $$ and application works as modus ponens.</p> <p>Since Combinatory Logic can be rewritten as Lambda Calculus. It enjoys all the properties of Typed Lambda Calculus</p>","tags":["Note"]},{"location":"Typed%20Combinatory%20Logic/#references","title":"References","text":"<p>Combinatory Logic</p>","tags":["Note"]},{"location":"Uncapacitated%20Facility%20location/","title":"Uncapacitated Facility location","text":"<p>202309211309</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note"]},{"location":"Uncapacitated%20Facility%20location/#uncapacitated-facility-location-no-max-number-of-customers-for-each-facility","title":"Uncapacitated Facility location (no max number of customers for each facility)","text":"<p>Set of \\(n\\) possible locations for opening facilities. - Cost \\(f_i \\geq 0\\) for opening a facility at \\(i^{th}\\) location - Set of customers (\\(m\\)) that need to be connected to an open facility. Multiple customers can connect to one facility. - Cost \\(c_{ij} \\geq 0\\) for connecting customer \\(j\\) to facility \\(i\\).</p> <p>Goal: Choose a set of facilities to open s.t. total cost (opening + connections) is minimum.</p>","tags":["Note"]},{"location":"Uncapacitated%20Facility%20location/#lp","title":"LP","text":"<p>Minimise \\(\\sum\\limits_{i} f_{i}x_{i} + \\sum\\limits_{i,j} c_{ij}y_{ij}\\) s.t.: - \\(\\sum\\limits_{i} y_{ij} \\geq 1\\) \\(\\forall j\\) - \\(x_i \\geq y_{ij}\\) \\(\\forall i,j\\) - \\(x_i, y_{ij} \\in [0,1]\\) \\(\\forall i,j\\).</p> <pre><code>There is a simple reduction from set cover to facility location. So this problem has no constant factor approximation either.\nSo we'll assume metric property: $c_{i',j'} \\leq c_{ij'} +c_{i'j} +c_{ij}$. (since no triangles)\n</code></pre> <p>Solve LP. Let LP OPT be \\(x^*, y^*\\).</p> <p>We can't simply round up every non integer to \\(1\\) because we might end up opening a lot of facilities which would be very expensive and unnecessary. Hence we will do rounding in two stages. Two stages of rounding: 1. \\(x^*,y^* \\to x',y'\\) (not integral solution) 2. \\(x',y' \\to\\) integral solution</p> <p>LP OPT value \\(= \\sum\\limits_{i} f_{i}x^{*}_{i} + \\sum\\limits_{i,j}c_{ij}y_{ij}^*\\) Connection cost in LP OPT for customer \\(j = \\sum\\limits_i c_{ij} y_{ij}^* = c^*_j\\)</p> <p>In LP OPT \\(\\sum_i y_{ij}^* = 1\\) \\(\\forall j\\) and \\(y^*_{ij} = Pr[\\text{connecting } j \\text{ to } i]\\) </p> <p>We will set all the connections that take more than \\(2c^*_j\\) to \\(0\\), because they are 'too expensive'. (Here \\(2\\) is chosen arbitrarily, we can do the same thing with some constant \\(\\alpha\\).)</p>","tags":["Note"]},{"location":"Uncapacitated%20Facility%20location/#rounding-i","title":"Rounding I","text":"<p>\\(\\forall j\\) \\(S_j = \\{ i | c_{ij} \\leq 2c^*_j\\}\\) \\(y'_{ij} = 0\\) if \\(i \\notin S_j\\)</p> <p>We can't simply set the non zero values in \\(S_j\\) to themselves because we'll violate the constraint that their sum should be \\(\\geq 1\\). So we scale all of them to make the sum equal to \\(1\\).</p> <p>\\(y'_{ij} = \\dfrac{y^*_{ij}}{\\sum_{i \\in S_j} y^*_{ij}}\\) \\(\\forall i \\in S_j\\).</p> <pre><code>title:\n**Lemma 1:** $\\sum\\limits_{i \\in S_j} y^*_{ij} \\geq 1/2.$\n($\\implies y'_{ij} \\leq 2y^*_{ij}$.)\n*Proof:*\n$$\n\\begin{align*}\nc_j^* &amp;= \\sum_i c_{ij} y^*_{ij}\\\\\n&amp;\\geq \\sum_{i \\notin S_j} c_{ij} y^*_{ij}\\\\\n&amp;&gt; \\sum_{i \\notin S_j} 2c^*_j y^*_{ij}\\\\\nc^*_j &amp;&gt; 2c^*_j \\sum_{i \\notin S_j} y^*_{ij}\\\\\n\\sum_{i \\notin S_j} y^*_{ij} &amp;&lt; \\frac{1}{2}.\\\\\n\\end{align*}\n$$\n</code></pre> <p>\\(y'_{ij} \\geq y^*_{ij}\\) \\(\\forall i \\in S_j\\) \\(x'_i \\geq y'_{ij}\\) \\(x^*_i \\geq y^*_{ij} \\geq \\frac12 y'_{ij}\\) We can safely round up \\(x^*_i\\) by a factor of \\(2\\) to satisfy the second constraint that \\(x_i \\geq y_i\\).</p> <p>Set \\(x'_i = \\min\\{2x^*_i, 1\\}\\) Observe: \\(x', y'\\) is a feasible solution of LP.</p> <p>cost \\((x', y') \\leq 2\\).cost \\((x^*, y^*)\\)</p>","tags":["Note"]},{"location":"Uncapacitated%20Facility%20location/#rounding-ii","title":"Rounding II","text":"<ol> <li>Pick a customer \\(j\\) st \\(c_j^*\\) is minimum.</li> <li>Pick \\(i \\in S_j\\) with minimum \\(f_i\\). Open \\(i\\). Connect \\(j\\) to \\(i\\).</li> <li>For every (unassigned) customer \\(j'\\) st \\(S_j \\cap S_{j'} \\neq \\phi\\). Connect \\(j'\\) to \\(i\\). Repeat until all customers are assigned to a facility.</li> </ol> <p>Let \\(L\\) be the set of facilities opened Let \\(C_f(L) =\\) opening cost of \\(L\\) \\(C_r(L) =\\) connection cost of \\(L\\).</p>","tags":["Note"]},{"location":"Uncapacitated%20Facility%20location/#lemma-2","title":"Lemma 2:","text":"<ol> <li>\\(C_f(L) \\leq 2\\sum_i f_i x^*_i\\)</li> <li>\\(C_r(L) \\leq 6 \\sum_{ij} c_{ij}y^*_{ij}\\) \\(\\implies\\) cost\\((L) \\leq 6 LP.OPT\\).</li> </ol> <p>Observe: If \\(j_1\\) and \\(j_2\\) are picked in Step 1 of the algorithm then \\(S_{j_1} \\cap S_{j_2} = \\phi\\).</p>","tags":["Note"]},{"location":"Uncapacitated%20Facility%20location/#references","title":"References","text":"<p>Linear Programming</p>","tags":["Note"]},{"location":"Undecidability%20of%20the%20Halting%20Problem/","title":"Undecidability of the Halting Problem","text":"<p>202211172111</p> <p>Type : #Note Tags : [[Theory of Computation]]</p>"},{"location":"Undecidability%20of%20the%20Halting%20Problem/#undecidability-of-the-halting-problem","title":"Undecidability of the Halting Problem","text":"<p>We can encode a turing machine and simulate it on another turing machine, but doing further analysis on a turing machine is not always possible, for example, there is no turing machine, that would take in a turing machine and an input for the turing machine as its inputs and return if the machine will halt on it input.</p> <p>Assuming the accepting language of all turing machines is \\(\\{0,1\\}\\)</p> <p>The constructions uses the [[Diagonalization|diagonalization]] argument. let \\(HP\\) be the set of all turing machines, and there is a turing machine \\(H\\) that takes in inputs of the form \\(M\\#x\\) that accepts if \\(M\\) halts on \\(x\\) and rejects otherwise. Therefore \\(H\\) is a total turing machine. we can label the turing macine by \\(\\Sigma^*\\), so we have \\(M_\\epsilon,M_0,M_1,M_{00},M_{01}\\dots\\) and we can make a matrix  here the i,jth elment is \\(L\\) if \\(M_i\\) halts on \\(j\\)</p> <p>Now we make a make a Turing Machine \\(N\\) that takes \\(x\\) as its input and it halts if \\(M_x\\) loops on \\(x\\) and it loops of \\(M_x\\) halts on \\(x\\)., Hence, \\(N\\) differs from every single \\(M_x\\) for atleast some input</p> <p>This is a contradiction as \\(HP\\) is supposed to be the set of all Turing Machines, which includes \\(N\\), but \\(N\\) cannot differ from itself at any point.</p>"},{"location":"Undecidability%20of%20the%20Halting%20Problem/#related-problems","title":"Related Problems","text":"<p>Undecidability of the Membership Problem</p>"},{"location":"Undecidability%20of%20the%20Halting%20Problem/#references","title":"References","text":"<p>Turing Machines</p>"},{"location":"Undecidability%20of%20the%20Membership%20Problem/","title":"Undecidability of the Membership Problem","text":"<p>202211180011</p> <p>Type : #Note Tags : [[Theory of Computation]]</p>"},{"location":"Undecidability%20of%20the%20Membership%20Problem/#undecidability-of-the-membership-problem","title":"Undecidability of the Membership Problem","text":"<p>Just like the Halting Problem, the Membership Problem is also undecidable, this can be shown by reducing the Halting Problem to the Memebership Problem, which means that if we have an instance of the Halting Problem, we can convert it to a Membership Problem, Hence if we have a general solution to the membership problem, then we have a general solution to the Halting Problem. But the Halting Problem does not have a general solution. So Membership problem doesn't either.</p> <p>Let \\(M\\) be a turing Machine and we want to figure out wether \\(M\\) halts on the input \\(x\\). We construct a new Turing Machine \\(N\\) that will accept a work if \\(M\\) accepts or rejects it, we can do this by making the accept and reject state both go to a new accept it. Now the Halting problem for \\(M\\) has become the membership problem for \\(N\\). Hence any Halting Problem can be converted to membership problem, thus there is no general solution for the Halting Problem.</p>"},{"location":"Undecidability%20of%20the%20Membership%20Problem/#related-problems","title":"Related Problems","text":""},{"location":"Undecidability%20of%20the%20Membership%20Problem/#references","title":"References","text":"<p>Turing Machines Undecidability of the Halting Problem</p>"},{"location":"Uniform%20Definition%20of%20Haskell%20Functions/","title":"Uniform Definition of Haskell Functions","text":"<p>202310291510</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note"]},{"location":"Uniform%20Definition%20of%20Haskell%20Functions/#uniform-definition-of-haskell-functions","title":"Uniform Definition of Haskell Functions","text":"<p>One of the more complicated rules for evaluating match is the mixture rule. This is also the only rule because of which it is not possible to change the order between non-identical pattern matches.</p> <p>The subclass of definitions that allows are worth studying for two reasons. - It makes it easier to reason pattern matching functions.      - One instance being the False Assumptions easy to make about Patterns - They are easier to compiler because they guarantee to avoid certain kinds of inefficiencies.</p>","tags":["Note"]},{"location":"Uniform%20Definition%20of%20Haskell%20Functions/#uniform-definitions","title":"Uniform Definitions","text":"<p>[!info] Definition A set of equations is called uniform if one of the following three conditions hold. 1. Either all equations begin with the variable pattern and applying the variable rule yields a uniform definition 2. All equations begin with the constructor pattern, and applying the constructor rule yields new sets of equations that are also uniform 3. all equations are have an empty list of patterns, so the empty rule apply and there is at most one equation in the set.</p> <p>That is, a set of rules that can be compiled without using the mixture rule and if the empty rule is only applied to sets containing 0 or 1 equation.</p>","tags":["Note"]},{"location":"Uniform%20Definition%20of%20Haskell%20Functions/#reasoning-about-uniform-definitions","title":"Reasoning about Uniform Definitions","text":"<p>There are two nice theorem's related to uniform definitions  and </p> <p>These two theorems make the definition of functions much more intuitive as it helps us evade the easy to make False Assumptions easy to make about Patterns while writing programs as termination of the argument is not always in the mind of someone programming in the language.</p>","tags":["Note"]},{"location":"Uniform%20Definition%20of%20Haskell%20Functions/#implementing-uniform-definitions","title":"Implementing Uniform Definitions","text":"<p>There 3 problems that occur with non uniform definitions - Resulting case expression may examine a variable more than once. - Compiler must used modified constructor rule to avoid duplicating right hand side of equations - The resulting expression may contain \\(\\triangleright\\) and \\(\\text{FAIL}\\). Implementing such expressions efficiently requires additional simplification rules and special ways of implementation.</p>","tags":["Note"]},{"location":"Uniform%20Definition%20of%20Haskell%20Functions/#references","title":"References","text":"<p>Match Function for Enriched Lambda Calculus Patterns</p>","tags":["Note"]},{"location":"Uniqueness%20of%20fourier%20series/","title":"Uniqueness of fourier series","text":"<p>2022-11-16 00:46 am</p> <p>Type : #Note Tags : [[Analysis]]</p>"},{"location":"Uniqueness%20of%20fourier%20series/#uniqueness-of-fourier-series","title":"Uniqueness of fourier series","text":"<pre><code>title: Theorem\nSuppose that $f$ is an integrable function on the circle with $\\hat{f}(n) = 0$ for all $n \\in \\mathbb{Z}$.\nThen $f(\\theta_0) = 0$ whenever $f$ is continuous at the point $\\theta_0$.\n</code></pre> <pre><code>title: Corollary\nIf $f$ is continuous on the circle and $\\hat{f}(n) = 0 \\ \\forall \\ n\\in \\mathbb{Z}$ then $f = 0$.\n</code></pre> <pre><code>title: Corollary \nSuppose $f$ is continuous on the circle, and the fourier series of $f$ converges absolutely i.e., $\\sum\\limits_{n=-\\infty}^{\\infty}|\\hat{f}(n)| &lt; \\infty$, then the fourier series converges uniformly to $f$, that is, $$\\lim_{N\\to\\infty}S_N(f)(\\theta) = f(\\theta) $$ for all $\\theta$.\n</code></pre>"},{"location":"Uniqueness%20of%20fourier%20series/#proof","title":"Proof:","text":"<p>The function \\(g(\\theta) = \\sum \\limits_{-\\infty}^{\\infty} \\hat{f}(n)e^{in\\theta}\\) is the uniform limit of the functions \\(g_N(\\theta) = \\sum\\limits_{-N}^{N}\\hat{f}(n)e^{in\\theta}\\). So, \\(g\\) is continuous on the circle, and note that \\(\\widehat{(f-g)}(n) = 0\\), with \\(f-g\\) continuous, this gives that \\(f = g\\).</p> <pre><code>title: Corollary\nSuppose $f$ is a $C^2$ function on the circle, then \n$$\\hat{f}(n) = \\mathcal{O}(1/|n|^2) \\ as \\ n \\to \\infty$$\n\nso that the fourier series of $f$ converges absolutely and uniformly to $f$.\n</code></pre>"},{"location":"Uniqueness%20of%20fourier%20series/#related-problems","title":"Related Problems","text":"<p>Convergence of Fourier Series Fourier series is the best approximation</p>"},{"location":"Uniqueness%20of%20fourier%20series/#references","title":"References","text":"<p>Fourier Series</p>"},{"location":"Universal%20Property/","title":"Universal Property","text":"<p>202304020304</p> <p>Type : #Note Tags : [[Category Theory]]</p>"},{"location":"Universal%20Property/#universal-property","title":"Universal Property","text":"<p>A Universal Property is an abstract quality that characterizes a given construction. This is used to define objects independently from their methods of construction.</p> <p>For example there is a Universal Property for Products and that will given the usual defintions of products in many different cases like Cartesian Product for sets, Product Topology for Topologies and Direct product in Groups</p>"},{"location":"Universal%20Property/#references","title":"References","text":""},{"location":"Universal%20Turing%20Machine/","title":"Universal Turing Machine","text":"<p>202211171911</p> <p>Type : #Note Tags : [[Theory of Computation]]</p>"},{"location":"Universal%20Turing%20Machine/#universal-turing-machine","title":"Universal Turing Machine","text":"<p>A Universal Turing machine \\(U\\) is a turing machine whose language us  $$ L(U) = {M#x | x\\in L(M)} $$ The first step is to encode \\(M\\) as an input along with \\(x\\) for example $$ 0<sup>n10</sup>m10<sup>k10</sup>s10<sup>t10</sup>r10<sup>u10</sup>v1 $$ This might represent that the machine has \\(n\\) states represeted by the numbers \\(0\\) to \\(n-1\\),  It has \\(m\\) tape symbols represented by the numbers \\(0\\) to \\(m-1\\), of which the first \\(k\\) are input symbols, \\(s\\) is the starting state while \\(t\\) and \\(r\\) are accpeting and rejecting states, \\(u\\) is the left marker and \\(v\\) is the blank.</p> <p>The remainder of the string can consist of transitions of the form  $$ 0<sup>p10</sup>a10<sup>q10</sup>b10 \\to ((p, a), (q, b, L)) $$ Once we have such an encoding we can proceed to build a universal turing machine</p> <p>We can have The description of the machine one the first tape, the input on the second tape and the information about the state the machine is in and where the header is located in the third one. On each step of \\(M\\) it reads the data in the third tape and the letter that the head is pointing to, and after reading the transition from the first tape it updates the other \\(2\\), as soon as the \\(M\\) accepts of rejects a word, so does \\(U\\)</p> <p>If the number of letters used in \\(M\\) are more than that in \\(U\\), then each letter has to be encoded using letters of \\(U\\).</p>"},{"location":"Universal%20Turing%20Machine/#related-problems","title":"Related Problems","text":""},{"location":"Universal%20Turing%20Machine/#references","title":"References","text":"<p>Turing Machines</p>"},{"location":"Universality%20is%20Decidable%20for%20One-Clock%20Timed%20Automata/","title":"Universality is Decidable for One Clock Timed Automata","text":"<p>202310182310</p> <p>Tags : Timed Automata</p>","tags":["Note","Incomplete"]},{"location":"Universality%20is%20Decidable%20for%20One-Clock%20Timed%20Automata/#universality-is-decidable-for-one-clock-timed-automata","title":"Universality is Decidable for One-Clock Timed Automata","text":"","tags":["Note","Incomplete"]},{"location":"Universality%20is%20Decidable%20for%20One-Clock%20Timed%20Automata/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"Untiming%20Timed%20Automata/","title":"Untiming Timed Automata","text":"<p>202308171108</p> <p>Type : #Note Tags : [[Theory of Computation]], Timed Automata</p>"},{"location":"Untiming%20Timed%20Automata/#untiming-timed-automata","title":"Untiming Timed Automata","text":"<p>Given \\(\\mathcal L\\) which is a timed language. We add the following operation $$ \\text{Untime}(\\mathcal L)={w\\in\\Sigma^{*}:\\exists \\text{ a time sequence }\\tau,(w, \\tau)\\in\\mathcal L} $$ which basically removes the time component from a timed language</p> <p>Claim: \\(\\text{Untime}(\\mathcal L)\\) is regular.</p> <p>Spoiler: The answer is no, and this also shows deterministic version is strictly weaker. And that timed languages are not closed under complementation. which also shows that timed FSA are strictly stronger than untimed.</p>"},{"location":"Untiming%20Timed%20Automata/#references","title":"References","text":""},{"location":"Updatable%20Timed%20Automata%20with%20only%20update%20being%20x%3Dx-1/","title":"Updatable Timed Automata with only update being x=x 1","text":"<p>202310301110</p> <p>Tags :Timed Automata</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20with%20only%20update%20being%20x%3Dx-1/#updatable-timed-automata-with-only-update-being-xx-1","title":"Updatable Timed Automata with only update being \\(x:=x-1\\)","text":"<p>[!note] Theorem Updatable Timed Automata with the only update being \\(x:=x-1\\) is more expressive than a regular timed automata</p> <p>To prove the theorem we simulate a 2-Counter Automata using the given subclass of UTA.</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20with%20only%20update%20being%20x%3Dx-1/#simulation-of-a-2-counter-automata","title":"Simulation of a 2-Counter Automata","text":"<p>The process of simulate a 2-Counter Automata is almost identical to the process for doing this in the general case discussed here. With the only problem being increments.</p> <p>We redo the increments in the following way. Given the transition $$ q\\xrightarrow[c_1++]{a} q' $$ we create the following transition on the UTA $$ q\\xrightarrow[x_{2}:= x_{2}-1]{a,\\quad x_{\\text{fix}}=1,x_{\\text{fix}}:=0} $$ Idea being, we let the time elapse exactly 1 second, and reset all the clocks that are not implemented.</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20with%20only%20update%20being%20x%3Dx-1/#references","title":"References","text":"<p>Updatable Timed Automata Emptiness for Updatable Timed Automata</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20with%20only%20x%3Dc%20and%20x%3Dy%20updates/","title":"Updatable Timed Automata with only x=c and x=y updates","text":"<p>202310301124</p> <p>tags : Timed Automata</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20with%20only%20x%3Dc%20and%20x%3Dy%20updates/#updatable-timed-automata-with-only-xc-and-xy-updates","title":"Updatable Timed Automata with only \\(x:=c\\) and \\(x:=y\\) updates","text":"<p>[!note] Theorem Updatable Timed Automata where the only updates allowed are \\(x:=c\\) and \\(x:=y\\) are just as expressive as regular Timed Automata</p> <p>To prove the above theorem we will give a construction from the given subset of Updatable Timed Automata to D-Timed Automata.</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20with%20only%20x%3Dc%20and%20x%3Dy%20updates/#converting-to-d-timed-automata","title":"Converting to D-Timed Automata","text":"","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20with%20only%20x%3Dc%20and%20x%3Dy%20updates/#dealing-with-xc","title":"Dealing with \\(x:=c\\)","text":"<p>For this step, make copies of our automata for all of the \\(c\\) that are used to update a given clock.</p> <p>Now for every transition that has the clock \\(x:=c\\), we change it such that the transition resets \\(x\\) and takes it to the copy of the automata for \\(c\\). In every guard of every transition of that automata \\(x\\) is replaced with \\(x-c\\).</p> <p>We do that for every single clock and now we have eliminated all resets of the form \\(x:=c\\).</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20with%20only%20x%3Dc%20and%20x%3Dy%20updates/#dealing-with-xy","title":"Dealing with \\(x:=y\\)","text":"<p>For a pair of clocks \\(x, y\\) we first make a copy of the automata and replace all \\(x\\) with \\(y\\) any transition which contains the update \\(x:=y\\) will be taken to that copy of the automata. Now resetting \\(x\\) to a constant will take us back to the original automata, but resetting \\(y\\) is a problem. So we make another copy of the automata, and swap all \\(x\\)s and \\(y\\)s in the guards. Whenever we are in the \\(x:=y\\) automata and we see a reset of \\(y\\) we instead reset \\(x\\) and go the new automata we created.</p> <p>Thus we have eliminated both kinds of updates and we have that the Updatable Timed Automata with just \\(x:=c\\) and \\(x:=y\\) is not more expressive than a regular timed automata.</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20with%20only%20x%3Dc%20and%20x%3Dy%20updates/#related","title":"Related","text":"<p>Updatable Timed Automata</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20without%20Diagonal%20Constraints%20and%20c%20in%20N/","title":"Updatable Timed Automata without Diagonal Constraints and c in N","text":"<p>202310301144</p> <p>tags : Timed Automata</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20without%20Diagonal%20Constraints%20and%20c%20in%20N/#updatable-timed-automata-without-diagonal-constraints-and-positive-constants-only","title":"Updatable Timed Automata without Diagonal Constraints and positive constants only","text":"<p>[!note] Theorem Updatable Timed Automata without Diagonal constraints and all constants positive are just as expressive as regular Timed Automata </p> <p>To prove the above theorem, we give a construction to regular timed automata.</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20without%20Diagonal%20Constraints%20and%20c%20in%20N/#construction-of-regular-timed-automata","title":"Construction of Regular Timed Automata.","text":"<p>Since we are not allowed to assign values of one clock to another in a regular timed automata, we simulate that process by replacing the clocks in guards with whatever other value that we want. This will create a dependency chain that needs to be dealt with while resetting clock that have other clocks dependent on them.</p> <p>We first get rid of all the transitions of the form \\(x:=c\\) for all \\(c\\ge 0\\) using the same method discussed in Updatable Timed Automata with only x=c and x=y updates</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20without%20Diagonal%20Constraints%20and%20c%20in%20N/#removing-updates-of-the-form-xyc","title":"Removing updates of the form \\(x:=y+c\\)","text":"<p>For such constraints we create a copy of of the states that we mark  with the transition \\(x:=y+c\\) and instead of using \\(x\\) in all of the guards in the outgoing edges, we use \\(y+c\\), rendering the variable \\(x\\) free to be used elsewhere. Now for all transitions that create a dependency on \\(x\\) like \\(z:=x+c_2\\) we rewrite them as \\(z:=y+c+c_2\\). Also transition that will make \\(y\\) dependent will also be handled in a similar way. For a transition with update \\(y:=w+c_{3}\\) we also do \\(x:=w+c_3+c\\) and \\(z:=w+c_3+c+c_2\\) </p> <p>Updates of the from \\(x:= x+c\\) are trivial to deal with.  We also update all clocks that are dependent on it.</p> <p>Now we have guaranteed that there will be no dependency changes longer than 2 elements long.</p> <p>This gets rid of all the transitions of the form \\(x:= y+c\\) but creates a dependency issue that should be dealt with carefully in the next step.</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20without%20Diagonal%20Constraints%20and%20c%20in%20N/#resets-of-clocks","title":"Resets of clocks","text":"<p>Resetting a clock that is dependent on another clock or is not a part of any dependency tree is trivial, it resets normally. If the clock is dependent on other clocks then it removes the dependency.</p> <p>For resetting of clocks that have other clocks dependent on them, we reset one of the free clock available and use those instead, to preserve the original value of the clock.</p> <p>The only non trivial fact left to prove is that there will be free clocks available to use.</p> <p>This is true because whenever we reset the base of a tree, we use a different with the same name as the base for its alias such that it shadows the original one, now if it becomes dependent on something else, that frees the clock again, and if something else becomes dependent on it, then that also gives a free clock. Hence whenever there is a dependency tree, there will always be free clocks.</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20without%20Diagonal%20Constraints%20and%20c%20in%20N/#final-steps","title":"Final steps","text":"<p>All constants that have value greater than the maximum value used in guards and updates of the original automata are reset to the maximum guard value. This ensures that the number of states will be finite. This also ensure the termination of the procedure as we can only produce a finite amount of copies of states and transitions</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20without%20Diagonal%20Constraints%20and%20c%20in%20N/#example","title":"Example","text":"<p> here the subscripts of states represent the actual values used instead of directly using the clocks, like for \\(p_{x,x}\\) the clock \\(x\\) is being used for \\(y\\).</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20without%20Diagonal%20Constraints%20and%20c%20in%20N/#importance-of-not-having-diagonal-constraints","title":"Importance of not having diagonal constraints","text":"<p>We could do the modification done in the section Final steps because there are no negative constants and diagonal constraints. Having negative constant means that, you can add any constant to a clock any number of times but the new value need to kept track of because its not necessary that subtracting values from the clocks would bring the value below the max constant, hence we cannot guarantee a finite amount of states. Having diagonal constraints lets us have negative constants to add for difference between clocks, causing the same problem.</p>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata%20without%20Diagonal%20Constraints%20and%20c%20in%20N/#related","title":"Related","text":"<ul> <li>Updatable Timed Automata</li> </ul>","tags":["Example"]},{"location":"Updatable%20Timed%20Automata/","title":"Updatable Timed Automata","text":"<p>202310290110</p> <p>Tags : Timed Automata</p>","tags":["Note"]},{"location":"Updatable%20Timed%20Automata/#updatable-timed-automata","title":"Updatable Timed Automata","text":"<p>[!note] Definition An  Updatable Timed Automata is a d-timed automata with clock resets replaced with more general clock updates as defined below.</p>","tags":["Note"]},{"location":"Updatable%20Timed%20Automata/#clock-updates","title":"Clock Updates","text":"<p>Let \\(X\\) be a set of clocks. An update is an expression generated by the following grammar $$ x:=c\\quad\\quad|\\quad\\quad x:=y+d $$ where \\(x, y \\in X,c\\in\\mathbb N,d\\in\\mathbb Z\\).</p> <p>[!example] Example of an updatable timed automata  ![[Updatable Timed Automata.excalidraw]] The language for this automata is \\(\\{(a^kb,\\tau)\\;:\\;k\\leq 10,\\tau_{1}\\leq \\tau_{2}\\dots \\tau_{k}\\leq \\tau_{k+1}\\}\\) </p>","tags":["Note"]},{"location":"Updatable%20Timed%20Automata/#subclasses-of-updatable-timed-automata","title":"Subclasses of Updatable Timed Automata","text":"<p>Updatable Timed Automata are in general more expressive than timed automata. One simple way to check is to check the untime their languages, It is simply the set of all recursive languages. This is because we can simulate Turing Machines in them which is discussed more in Emptiness for Updatable Timed Automata.</p> <p>So we restrict the updates and guards to reduce the power of Updatable Timed Automata in order to get properties like decidability back.</p> <p>Some of these subclasses are: 1. Updatable Timed Automata with only x=c and x=y updates 2. Updatable Timed Automata without Diagonal Constraints and c in N 3. Updatable Timed Automata with only update being x=x-1</p> <p>The expressive power of these subclasses can be summed up in the following table</p> Diagonal Free Diagonal \\(x:=c,x:=y\\) Timed Automata Timed Automata \\(x:=y+c\\) Timed Automata More expressive \\(x:=x-1\\) More expressive More expressive","tags":["Note"]},{"location":"Updatable%20Timed%20Automata/#references","title":"References","text":"<p>Emptiness for Updatable Timed Automata</p>","tags":["Note"]},{"location":"Urysohn%20Lemma/","title":"Urysohn Lemma","text":"<p>202303211003</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Urysohn%20Lemma/#urysohn-lemma","title":"Urysohn Lemma","text":"<pre><code>This lemma roughly states that given two disjoint closed subsets of a normal space, we can separate them by real valued functions.\n</code></pre> <pre><code>title: Statement of the lemma\nLet $A,B$ be disjoint closed subsets of a normal space $X$. Then there exists a continuous map $f: X \\to [0,1]$ such that $f(x) = 0$ for all $x \\in A$ and $f(x) = 1$ for all $x \\in B$.\n</code></pre>"},{"location":"Urysohn%20Lemma/#proof","title":"Proof:","text":"<p>A dyadic number is a rational number of the form \\(\\dfrac{a}{2^{n}}\\), where \\(a,n\\) are integers with \\(n \\ge 0\\). Then it is easy to check that these numbers are dense in \\(\\mathbb{R}\\).</p> <p>We shall construct an open subset \\(U_{r}\\subseteq X\\) for each dyadic number \\(r\\in[0,1]\\), with \\(A \\subset U_{r} \\subset X  \\setminus B\\). Such that for each pair \\(p&lt;q\\) of dyadics in \\([0,1]\\), we have that \\(Cl(U_{p}) \\subset U_{q}\\).</p> <p>Let \\(U_{1} = X \\setminus B\\). Then since \\(X\\) is normal, there is a nbhd around \\(A\\), call this \\(U_{0}\\), such that \\(Cl\\left( U_{0} \\right) \\subset U_{1}\\). Similarly, we get another open set \\(U_{\\frac{1}{2}}\\) such that \\(Cl(U_{0}) \\subset U_{\\frac{1}{2}} \\subset Cl\\left( U_{\\frac{1}{2}} \\right) \\subset U_{1}\\).</p> <p>Now let \\(n \\ge 2\\). Assume we have constructed the required subsets for all \\(r\\) in \\([0,1]\\) of the form \\(\\dfrac{b}{2^{n-1}} = \\dfrac{2b}{2^{n}}\\). So we need to constuct \\(U_{\\frac{2n+1}{2^{n}}}\\), again use normality, and get $$ Cl\\left(U_{\\frac{2b}{2^{n}}}\\right) \\subset U_{\\frac{2n+1}{2^{n+1}}} \\subset Cl\\left( U_{\\frac{2n+1}{2^{n}}} \\right) \\subset U_{\\frac{2n+2}{2^{n}}}  $$ Hence, by induction, we construct \\(U_{r}\\) for all dyadic \\(r\\) in \\([0,1]\\). Now let \\(U_{r} = \\phi\\) for \\(r &lt; 0\\). and \\(U_{r} = X\\) for \\(r &gt; 1\\).</p> <p>Now let \\(x \\in X\\), we define $$ D(x) := { r \\mathrm{dyadic} : x \\in U_{r} } $$ Since \\(x \\notin U_r\\) for \\(r &lt; 0\\), and \\(x \\in U_{r}\\) for \\(r&gt;1\\), \\(D(x) \\neq \\phi\\) and it is bounded below by 0. So $$ f(x) := \\inf D(x) $$ exists and lies in \\([0,1]\\). Now see that \\(f(x) = 0\\) for \\(x \\in A\\), and \\(f(x) = 1\\) for \\(x \\in B\\).</p> <p>To show that \\(f\\) is continuous, first observe that if \\(x \\in Cl(U_{r})\\) then \\(f(x)\\le r\\). And also, if \\(x \\notin U_{r}\\), then \\(f(x) \\geq r\\).</p> <p>Now let \\(x \\in X\\), pick a nbhd \\((c,d)\\) of \\(f(x)\\). Take dyadic numbers \\(p,q\\) such that \\(c &lt; p &lt;f(x)&lt; q &lt; d\\).  This means that \\(x \\notin Cl(U_{p})\\) but \\(x \\in U_{q} \\implies x \\in U := U_{q} \\setminus Cl(U_{p})\\). But then for any \\(y \\in U\\), \\(y \\notin U_{p}\\), and so \\(f(y) \\geq p\\). Also since \\(y \\in U_{q}\\), \\(f(y)\\le q\\). This gives \\(f(U) \\subset (c,d)\\). Hence \\(f\\) is continuous.</p>"},{"location":"Urysohn%20Lemma/#related-problems","title":"Related Problems","text":"<p>Urysohn Metrization Theorem</p>"},{"location":"Urysohn%20Lemma/#references","title":"References","text":"<p>Normal Spaces Closure and Interior and Limit Points Continuous Functions</p>"},{"location":"Urysohn%20Metrization%20Theorem/","title":"Urysohn Metrization Theorem","text":"<p>202303241403</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"Urysohn%20Metrization%20Theorem/#urysohn-metrization-theorem","title":"Urysohn Metrization Theorem","text":"<pre><code>title:\nEvery regular second countable space is metrizable.\n</code></pre>"},{"location":"Urysohn%20Metrization%20Theorem/#proof","title":"Proof:","text":"<p>We prove two claims and the result follows from them.</p>"},{"location":"Urysohn%20Metrization%20Theorem/#claim-1","title":"Claim 1:","text":"<p>There is a countable collection \\(\\{ f_{n} \\}_{n \\in \\mathbb{N}}\\) of maps \\(f_{n} : X \\to [0,1]\\), such that for any \\(p \\in U \\subseteq X\\) with \\(U\\) open, there is an \\(f_{n}\\) in the collection such that \\(f_{n}(p) = 1\\) and \\(f_{n}(X\\setminus U)= \\{ 0 \\}\\).</p>"},{"location":"Urysohn%20Metrization%20Theorem/#claim-2","title":"Claim 2:","text":"<p>The function \\(F : X \\to [0,1]^{\\omega}\\) given by \\(F(x) = (f_{1}(x),f_{2}(x),\\dots)\\) is an embedding.</p> <p>It is easy to see the result follows from this, since subspace of a metrizable space is metrizable.</p>"},{"location":"Urysohn%20Metrization%20Theorem/#proof-of-claim-1","title":"Proof of Claim 1:","text":"<p>For each pair \\((i,j)\\) of indices with \\(Cl(B_{i}) \\subseteq B_{j}\\), use Urysohn's lemma to get a function \\(g_{i,j}\\) which is 1 on \\(B_{i}\\) and 0 on \\(X\\setminus B_{j}\\). Now for any \\(p \\in U\\) with \\(U\\) open, pick a basis element \\(B_{i}\\) s.t. \\(p \\in B_{i} \\subseteq U\\), then by regularity, there is an open set \\(V\\) such that \\(p \\in V \\subseteq Cl(V) \\subseteq B_{i} \\subseteq U\\). Then there is a \\(B_{j}\\) s.t. \\(p \\in B_{j} \\subseteq V\\). So take \\(g_{j,i}\\) as the required function. Now just reindex the g's to get the f's.</p>"},{"location":"Urysohn%20Metrization%20Theorem/#proof-of-claim-2","title":"Proof of Claim 2:","text":"<p>\\(F\\) is cts since each of the components is cts. \\(F\\) is also injective. Let \\(F(x) = F(y)\\) then \\(f_{i}(x) = f_{i}(y)\\) for all \\(i\\). But, since \\(x,y\\) are closed as singletons, and \\(X\\) is regular, there are open nbhds \\(U,V\\) of \\(x,y\\) respectively which are disjoint. But this means that \\(f_{n}(x) = 1\\) and \\(f_{n}(y) = 0\\) for some \\(n\\).</p> <p>We need to show that given \\(U \\subset X\\) open, \\(F(U) \\subset F(X)\\) is open. Let \\(x_{0} \\in U\\) be any element. There exists \\(f_{n}\\) such that \\(f_{n}(x_{0}) = 1\\) and \\(f_{n}(X\\setminus U) \\subset \\{ 0 \\}\\). Let \\(V_{n} := \\pi_{n} ^{-1}((0,1]) \\cap F(X)\\), here \\(\\pi_{n}\\) is the nth projection map from \\([0,1]^{\\omega}\\). So, \\(V_{n}\\) is open in \\(F(X)\\), \\(x_{0} \\in F ^{-1}(V_{n}) \\subset U\\). Hence \\(F(x_{0}) \\in V_{n} \\subseteq F(U)\\), giving \\(F(U)\\) open.</p>"},{"location":"Urysohn%20Metrization%20Theorem/#note-the-construction-used-can-be-used-to-show-that-if-normal-space-x-has-a-basis-indexed-by-j-then-we-can-embed-x-into-mathbbrj","title":"NOTE: The construction used can be used to show that if normal space \\(X\\) has a basis indexed by \\(J\\) then we can embed \\(X\\) into \\(\\mathbb{R}^{J}\\)","text":""},{"location":"Urysohn%20Metrization%20Theorem/#references","title":"References","text":"<p>Urysohn Lemma Regular Spaces Second Countability Product topology Metrizable Spaces Subspace Topology Continuous Functions Homeomorphisms</p>"},{"location":"Variable%20Rule%20for%20Match%20Function/","title":"Variable Rule for Match Function","text":"<p>202310251510</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note"]},{"location":"Variable%20Rule%20for%20Match%20Function/#variable-rule-for-match-function","title":"Variable Rule for Match Function","text":"","tags":["Note"]},{"location":"Variable%20Rule%20for%20Match%20Function/#example","title":"Example","text":"<p>In the example given in the Main note <pre><code>demo =\n  \\u1. \\u2. \\u3. \n    match [u1, u2, u3]\n          [ ( [f, [],     ys],     (A f ys)  ),\n            ( [f, (x:xs), []],     (B f x xs)),\n            ( [f, (x:xs), (y:ys)], (C f x xs y ys)) ]\n          ERROR\n</code></pre> This can be reduced to the equivalent form <pre><code>demo =\n  \\u1. \\u2. \\u3. \n    match [u2, u3]\n          [ ( [[],     ys],     (A u1 ys)  ),\n            ( [(x:xs), []],     (B u1 x xs)),\n            ( [(x:xs), (y:ys)], (C u1 x xs y ys)) ]\n          ERROR\n</code></pre></p>","tags":["Note"]},{"location":"Variable%20Rule%20for%20Match%20Function/#variable-rule","title":"Variable Rule","text":"<p>The variable rule is the conversion defined as follows</p> <p><pre><code>match (u:us)\n      [ ( (v1:ps1), E1 ),\n        ...\n        ( (vm:psm), Em )]\n      E\n</code></pre> will get transformed to <pre><code>match us\n      [ ( ps1, E1[u/v1] ),\n        ...\n        ( psm, Em[u/vm] )]\n      E\n</code></pre></p> <p>This rule formalizes the statement that if the pattern to be match is a variable, then it is always accepted without any conditions.</p>","tags":["Note"]},{"location":"Variable%20Rule%20for%20Match%20Function/#references","title":"References","text":"<ul> <li>Match Function for Enriched Lambda Calculus</li> <li>Constructor Rule for Match Function</li> <li>Empty Rule for Match Function</li> <li>Mixture Rule for Match Expression</li> </ul>","tags":["Note"]},{"location":"Vertex%20Cover%20Problem/","title":"Vertex Cover Problem","text":"<p>202309281809</p> <p>Tags : [[Advanced Algorithms]]</p>","tags":["Note"]},{"location":"Vertex%20Cover%20Problem/#vertex-cover-problem","title":"Vertex Cover Problem","text":"<p>Given: Graph \\(G=(V,E)\\) Goal: If \\(S\\subset V\\) is s.t. every edge has at least one endpoint in \\(S\\), it is called a Vertex Cover. Minimise \\(|S|\\).</p> <pre><code>title: Idea: Lower Bounds\n1. Find a lower bound (efficiently computable).\n2. $A(I)\\le\\alpha.LB(I)\\le\\alpha.OPT(I)$.\n</code></pre> <p>\\(OPT\\ge\\) size of any matching Algorithm: Take a maximal matching, and pick both endpoints and add them to \\(S\\). Analysis: \\(A\\le2.\\text{(size of a matching)}\\le2.OPT\\). Thus we get a \\(2-\\)approximation.</p>","tags":["Note"]},{"location":"Vertex%20Cover%20Problem/#natural-questions","title":"Natural questions","text":"<ul> <li>Can we get a \\((&gt;2)-\\)approximation using the same algorithm and the same LB? (Can the analysis be improved?) Answer: No. Consider \\(K_{n,n}\\). Max \\(VC=n\\), max matching \\(=n\\). Our solution will have cost \\(2n\\), which means our analysis is tight.</li> <li>Can a different algorithm give a better approximation using the same LB? Answer: No. \\(K_{n}\\), \\(n\\) odd has \\(OPT=n-1\\), max matching \\(=\\frac{n-1}{2}\\).</li> <li>Can we improve the approx ratio beyond \\(2\\) using a different LB? Answer: Open</li> </ul>","tags":["Note"]},{"location":"Vertex%20Cover%20Problem/#references","title":"References","text":"<p>Approximation Algorithms</p>","tags":["Note"]},{"location":"Well-Preorder/","title":"Well Preorder","text":"<p>202311031511</p> <p>Tags :[[Order Theory]]</p>","tags":["Note"]},{"location":"Well-Preorder/#well-preorder","title":"Well-Preorder","text":"<p>[!note] Definitions An infinite sequence \\((q_{1},q_{2}\\dots)\\) in \\((\\mathcal(Q),\\sqsubseteq)\\)  is saturating if \\(\\exists\\ i\\le j:q_{i}\\sqsubseteq q_{j}\\)  A Quasi-Order is called a well-quasi-order if every infinite sequence is saturating</p>","tags":["Note"]},{"location":"Well-Preorder/#examples","title":"Examples","text":"<ul> <li>\\((\\mathbb N, \\leq)\\)</li> </ul>","tags":["Note"]},{"location":"Well-Preorder/#non-examples","title":"Non-Examples","text":"<ul> <li>\\((\\mathbb Z, \\leq)\\): \\(-1,-2\\dots\\)</li> <li>Lexicographical Order: \\(B, AB, AB B\\dots\\)</li> <li>Prefix Order: \\(B, AB, AAB\\)</li> </ul> <p>An important theorem for well-quasi-order is </p>","tags":["Note"]},{"location":"Well-Preorder/#references","title":"References","text":"<p>Preorder Monotone Domination Order Higman's Lemma [[Well Ordering]]</p>","tags":["Note"]},{"location":"What%20are%20Differential%20equations/","title":"What are Differential equations","text":"<p>202301041201</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"What%20are%20Differential%20equations/#what-are-differential-equations","title":"What are Differential equations","text":"<pre><code>title:\nIn this course we will be dealing with ordinary differential equations.\n</code></pre> <p>An ODE is a functions of the form  $$ \\phi\\left(x, y, y', y'', y^{(3)}\\dots y^{(n)}\\right) = 0 $$ where \\(y^{(n)}\\) is the \\(nth\\) derivative of \\(y\\)</p> <p>Given an ODE we would want to find out the following- 1. Existence of solution, if there exists a function that satisfies \\(\\phi\\) 2. If there exists a solution, then is it unique (given an initial condition as constants are in the kernel of the differential operator) 3. Can you solve it \\(\\longleftrightarrow\\) does there exits an exact form. Solution of a differential equation</p>"},{"location":"What%20are%20Differential%20equations/#related-problems","title":"Related Problems","text":""},{"location":"What%20are%20Differential%20equations/#references","title":"References","text":""},{"location":"What%20is%20Topology/","title":"What is Topology","text":"<p>202301031001</p> <p>Type : #Note Tags : [[Topology]]</p>"},{"location":"What%20is%20Topology/#what-is-topology","title":"What is Topology","text":"<pre><code>title:\nIt is the study of topological properties of topological spaces.\n</code></pre> <p>Topological Properties are properties that can be expressed in terms of continuity. Topological Spaces are sets which have some notion of nearness between elements. - A continuous function \\(f:X\\to Y\\) is such that if \\(x,x'\\in X\\) are near each other implies \\(f(x),f(x')\\in Y\\) are near each other</p> <p>One way to define the notion of distance is to make a function \\(d:X\\times X \\to \\mathbb R\\) which takes a pair of values and returns the \"Distance\" between them, following some nice properties. This is called a metric space.</p> <p>We care about this because - Continuous Functions on  a domain \\(X\\) will tell you about \\(X\\) itself. - There are spaces which carry a reasonable notion of nearness which are not metric spaces.     - Example: Pointwise convergence of functions       $$       \\begin{aligned}       {f_n} &amp;:\\mathbb R \\to \\mathbb R \\text{ are pointwise convergent if }\\       g \\text{ is a limit of } &amp;{f_n} \\text{ for each } t\\in\\mathbb R \\text{ and } \\lim\\limits_{n\\to\\infty} f_n(t)=g(t)        \\end{aligned}$$       Here there is a notion of nearness on the space of functions which is not a metric</p> <p>Nearness: If \\(U\\) is open in \\(X,x\\in U\\), then all \\(y\\) that are sufficiently near to \\(x\\) also satisfy \\(y\\in U\\).</p>"},{"location":"What%20is%20Topology/#related-problems","title":"Related Problems","text":""},{"location":"What%20is%20Topology/#references","title":"References","text":""},{"location":"Why%20Differential%20Equations/","title":"Why Differential Equations","text":"<p>202301210101</p> <p>Type : #Note Tags : [[Differential Equations]]</p>"},{"location":"Why%20Differential%20Equations/#why-differential-equations","title":"Why Differential Equations","text":"<p>The theory of ordinary differential equation is one of the best basic tool of mathetical science. This theory makes it possible to study all evolutionary processes that posses the properties of Determinacy, Finite-Dimentionality and Differentiability. </p> <p>A Process is called Deterministic if its entire future course and its entire past are uniquely determined by its state at the present time. The space of all states is called the Phase Space. Classical mechanics considers motions of systems which are deterministic. The motion of particles in quantum mechanics is not deterministic.The propogation of heat is semi-deterministic: The future is decided by the current state but the past is not.</p> <p>A process is called Finite Dimentional if its phase space is finite dimentional, i.e, if the number of parameter neede to describe its states is finite. Like motion of particles and rigid bodies in classical mechanics. Motions of a fluid in fluid mechanics, the vibration of a string or a membrane, and the propogation of waves in optics and acoustics are examples of systems which cannot be described using a finite dimensional phase space.</p> <p>A process is called Differentiable if is phase space has struction of a differentiable manifold, and the change of state is described by differentiable functions. So the coordinates and velocities of particles of a mechanical system change differentiably with time. The motion studied in impact theory do not change differentiably.</p> <p>Thus motion of system in classical mechanics can be described using ordinanry differential equations, while quantum mechanics, elasticity theory, optics, acoustics and imact theory require other tools. </p> <p>The process of radio active decay and the process of population growth can also be described in terms of ordinary differential equations. </p> <p>Systems like these and much more can be very elegantly modelled by ordinary differential equations. Which makes Differential equations an important field of study.</p> <p>The precise fomulation of the above principles requiers concepts like Phase Space</p>"},{"location":"Why%20Differential%20Equations/#related-problems","title":"Related Problems","text":""},{"location":"Why%20Differential%20Equations/#references","title":"References","text":""},{"location":"Winning%20Strategy%20for%20Player%201%20in%20Banach-Mazur%20games/","title":"Winning Strategy for Player 1 in Banach Mazur games","text":"<p>202311172311</p> <p>Tags : [[Topology]]</p>","tags":["Note","Incomplete"]},{"location":"Winning%20Strategy%20for%20Player%201%20in%20Banach-Mazur%20games/#winning-strategy-for-player-1-in-banach-mazur-games","title":"Winning Strategy for Player 1 in Banach-Mazur games","text":"<p>[!info] Given that there exists a winning strategy for player 2 in certain conditions, If the 'opposite' conditions are met, then player 1 can steal her strategy.</p> <p>The idea for player 1 here is to make sure that intersection of \\(W'\\) with the compliment of \\(X\\) is empty. So if \\(W'\\) is not empty itself, then it will intersect with \\(X\\). To ensure this condition we make the space \\(Y\\) a complete metric space. Then every decreasing nested sequence of sets in \\(\\mathcal W\\) will give a corresponding decreasing nested sequence of compact sets.</p> <p>[!hint] Winning Condition If \\(Y\\) is a metric space, then there exists a winning strategy for player 1 iff the set \\(X\\) is large in some open set of \\(Y\\)</p> <p>^305cd2</p> <p>If \\(X\\) is large in an open set \\(O\\). Player 1 picks \\(W_i\\) inside \\(O\\). Now she will pretend to be the second player of the game that starts after this step, i.e of the game \\(MB(X', W_1, \\mathcal W)\\) where \\(X' =X^c\\cap W_1\\)</p> <p>Here \\(X^c\\cap W_{1}\\) is meager. This means that player 1 can use Winning Strategy for Player 2 in Banach-Mazur games by slightly modifying it to produce the following sequence of sets  $$ W_{1}, W_{2}\\dots $$ and \\(W'\\cap X'=\\emptyset\\). So \\(W'\\cap X = W'\\) But we have that \\(Y\\) is a complete metric space so for each \\(W_{2i}\\) find an open set \\(O\\) in it which contains \\(W_{2i+1}\\). The modification we use above is that instead finding \\(W_{2i+1}\\) in \\(O\\), we find an open set \\(O'\\in O\\) such that \\(\\overline{O'}\\subseteq O\\). This ensure that if we take the odd terms in the above sequence, the give a corresponding decreasing nested sequence of compact sets \\(\\overline{O'}\\) for each \\(W_i\\).  Intersection of all of those compact sets is non empty because of the [[Cantor's Intersection Theorem]] Hence Intersection of all \\(W_i\\ne\\emptyset\\).</p> <p>Thus \\(W'\\cap X\\) is nonempty.</p>","tags":["Note","Incomplete"]},{"location":"Winning%20Strategy%20for%20Player%201%20in%20Banach-Mazur%20games/#references","title":"References","text":"<p>Winning Strategy for Player 2 in Banach-Mazur games Banach-Mazur Games</p>","tags":["Note","Incomplete"]},{"location":"Winning%20Strategy%20for%20Player%202%20in%20Banach-Mazur%20games/","title":"Winning Strategy for Player 2 in Banach Mazur games","text":"<p>202311172311</p> <p>Tags : [[Topology]]</p>","tags":["Note","Incomplete"]},{"location":"Winning%20Strategy%20for%20Player%202%20in%20Banach-Mazur%20games/#winning-strategy-for-player-2-in-banach-mazur-games","title":"Winning Strategy for Player 2 in Banach-Mazur games","text":"<p>[!tip] Winning Condition There exists a winning strategy for player 2 iff \\(X\\) is meager </p> <p>^3923a6</p> <p>The goal for player \\(2\\) is to make the intersection of the sets with \\(X\\) empty. Hence it is natural to ask how 'small' should a set be to ensure her victory.</p> <p>A set is called meager if it is a countable union of nowhere-dense sets Since \\(X\\) is meager. Let \\(X\\) be the union of the sequence \\(\\{ S_{i} \\}\\) of nowhere-dense sets.</p> <p>Since \\(S_i\\) is nowhere-dense. Given any open set \\(O\\), we can find an open set \\(O'\\subseteq (O\\cup S_i^c)\\). This gives an open set that does not intersect with \\(S_i\\).</p> <p>This gives us the following strategy for the \\(i^{\\text {th}}\\) move for player 2 - If \\(\\text{Int}(W_{2i-1}) \\cap S_{i}=\\emptyset\\)  then arbitrarily pick \\(W_{2i}\\). - Otherwise, let \\(O\\) be an open set in the \\(W_{2i-1}\\) that does not intersect with \\(S_i\\) and pick an \\(W_{2i}\\) inside \\(O\\). using this idea, we have made sure that \\(S_i\\) will not intersect with \\(W_{2i}\\).</p> <p>Using the above strategy on each step gives us that for all \\(i\\in\\mathbb N\\) we have \\(S_i\\cap W'=\\emptyset\\) as \\(S_i\\cap W_{2i}=\\emptyset\\) and \\(W'\\subseteq W_{2i}\\) Hence \\(X\\cap W'=\\emptyset\\) and player \\(2\\) wins.</p>","tags":["Note","Incomplete"]},{"location":"Winning%20Strategy%20for%20Player%202%20in%20Banach-Mazur%20games/#references","title":"References","text":"<p>Banach-Mazur Games Winning Strategy for Player 1 in Banach-Mazur games</p>","tags":["Note","Incomplete"]},{"location":"Zone%20Automata%20Example/","title":"Zone Automata Example","text":"<p>202310122308</p> <p>tags : Timed Automata</p>","tags":["Example"]},{"location":"Zone%20Automata%20Example/#zone-automata-example","title":"Zone Automata Example","text":"<p>Consider the following Networks of Timed Automata.</p> <p>![[Drawing 2023-10-12 23.11.22.excalidraw]]</p> <p>The Zone Automata for the above network would be.</p> <p>![[Drawing 2023-10-12 23.15.44.excalidraw|500]]</p> <p>The graphs here represent zones.</p>","tags":["Example"]},{"location":"Zone%20Automata%20Example/#related","title":"Related","text":"","tags":["Example"]},{"location":"Zone%20Automata/","title":"Zone Automata","text":"<p>202310101410</p> <p>Tags : Timed Automata</p>","tags":["Note"]},{"location":"Zone%20Automata/#zone-automata","title":"Zone Automata","text":"<p>Consider this Example. It shows that having a Reachability Axiom is Important for software verification purposes. </p> <pre><code>title: Region Automata is Too Inefficient!\nLets say we have a Network of 10 Timed Automata, and each of these have 10 states. All of them have a single clock and the maximum number present guards of all of the automata is 2.\n\nA na\u00efve way would be to first construct a [Monolithic Automata](&lt;./Monolithic Timed Automaton for a Network of Timed Automata.md&gt;) which accepts the same language.\n\nThe above construction gives $10^{10}$ states, with 10 clock and guards having the maximum number $2$.\n\nThe time bound that for Region automata construction would give time around $10!\\cdot 2^{10}\\cdot 10^{10}$. Doing a reachability check in this automata is not feasible. A better approach is required $!!$\n</code></pre> <p>Much like [[Region Automata]], Zone Automata also makes an Untimed Finite State Automaton given a Timed Automaton by combining states with a set of clock configurations to represent time.</p> <pre><code>title: Tldr: Zones\n*Zones* are sets in the **Clock Space**, which represent the set of clock configurations, these are obtained by\n- Starting with a set of points\n- Taking away the points that do not satisfy guards when the transition is taken\n- Then adding points that can be achieved by waiting. i.e the *future* operator.\n</code></pre>","tags":["Note"]},{"location":"Zone%20Automata/#zones","title":"Zones","text":"<p>A Zone is a set of clock valuations. We define \\(3\\) operations of zones to help us model the configuration of clocks for a run in a timed automaton. - Intersection of two zones \\(W\\) and \\(W'\\)  - Resets of a subset of clocks to \\(0\\) in \\(W\\) - Future of \\(W\\): \\(\\overrightarrow W=\\{v+\\delta\\; |\\; v\\in W,\\delta\\in\\mathbb{R}^{+}\\}\\) </p> <p>using these three operations, we will emulate the change in a zone caused by a transition. Given a transition \\(t=\\langle q, g, Y, q'\\rangle\\) we define $$ \\text{Post}_{t}(W) =  \\overrightarrow{     Y } $$ where \\([Y]\\) denotes the resets and \\(\\textlbrackdbl g\\textrbrackdbl\\) denotes the zone that is satisfied by \\(g\\).  <pre><code>title:idea\n$$\n(q,Z)\\xrightarrow{\\text{guards}}(q, Z\\cap\\textlbrackdbl g\\textrbrackdbl)\\xrightarrow{\\text{resets}} (q, [Y](Z\\cap\\textlbrackdbl g\\textrbrackdbl))\\xrightarrow{\\text{elapse}}(q, \\overrightarrow{[Y](Z\\cap\\textlbrackdbl g\\textrbrackdbl)})\n$$\n</code></pre></p>","tags":["Note"]},{"location":"Zone%20Automata/#zone-automata_1","title":"Zone Automata","text":"<p>Let \\((q, Z)\\) be a symbolic state where \\(q\\) is a state and \\(Z\\) is a zone. Let \\(s_{0}=(q_{0},\\overrightarrow{\\{\\textbf0\\}})\\) be the initial state. Then the set of reachable symbolic states \\(\\mathcal S\\) is given by $$ \\frac{}{s_0\\in\\mathcal{S}}\\text{Init} $$ $$ \\frac{\\begin{matrix}(q,Z)\\in \\mathcal S &amp;&amp; t=(q,g,Y,q') &amp;&amp; Z'=\\text{Post}{t}(Z)\\ne \\emptyset\\end{matrix}}{(q',Z')\\in\\mathcal S}\\text{Trans} $$ Whenever we derive a _symbolic state using the \\(\\text{Trans}\\) rule we also get a corresponding transition in the zone automata between the symbolic states.</p> <p>Now we can start building our Reachability Algorithm for Zone Automata</p> <p>But this method does not terminate. The two main approaches to that problem are extrapolation and simulation.</p> <p>Zone Automata Example</p>","tags":["Note"]},{"location":"Zone%20Automata/#references","title":"References","text":"<p>Zone Automata Example Extrapolation for Zone Automata Simulation for Zone Automata Reachability Algorithm for Zone Automata</p>","tags":["Note"]},{"location":"let-expressions%20in%20Enriched%20Lambda%20Calculus/","title":"let expressions in Enriched Lambda Calculus","text":"<p>202310201710</p> <p>Tags : [[Lambda Calculus]], [[Programming Languages]]</p>","tags":["Note"]},{"location":"let-expressions%20in%20Enriched%20Lambda%20Calculus/#let-expressions-in-enriched-lambda-calculus","title":"let-expressions in Enriched Lambda Calculus","text":"<p>\\(\\text{let-}\\)expressions are a way to bound a name to value within a given expression, written as  <pre><code>let v = B in E\n</code></pre> which means, that the variable <code>v</code> is bound to the expressions <code>B</code>. The scope of this definition is the expression <code>E</code>. </p> <p>Example 1: <code>+ 1 (let x = 3 in (* x x))</code> The evaluations of the above expression is as follows</p>","tags":["Note"]},{"location":"let-expressions%20in%20Enriched%20Lambda%20Calculus/#1-let-x-3-in-x-x-1-3-3-1-9-10","title":"<pre><code>   + 1 (let x = 3 in (* x x))\n-&gt; + 1 (* 3 3)\n-&gt; + 1 9\n-&gt; 10\n</code></pre>","text":"<p>let expressions can also be embedded within other expression, including other let expressions, like</p> <p>Example 2: <code>let x = 3 in (let y = 4 in (* x y))</code> The evaluation of the above expression would  be <pre><code>   let x = 3 in (let y = 4 in (* x y))\n-&gt; let x = 3 in (* x 4)\n-&gt; * 3 4\n-&gt; 12\n</code></pre></p> <p>as a matter of convenience, the above expression is written as <pre><code>let x = 3\n    y = 4\nin  (* x y)\n</code></pre></p> <p>the semantics can be given as <pre><code>let v = B in E &lt;=&gt; (\\v. E) B\n</code></pre></p>","tags":["Note"]},{"location":"let-expressions%20in%20Enriched%20Lambda%20Calculus/#references","title":"References","text":"<p>Enriched Lambda Calculus letrec-expressions in Enriched lambda Calculus</p>","tags":["Note"]},{"location":"letrec-expressions%20in%20Enriched%20lambda%20Calculus/","title":"letrec expressions in Enriched lambda Calculus","text":"<p>202310201710</p> <p>Tags : [[Lambda Calculus]], [[Programming Languages]]</p>","tags":["Note"]},{"location":"letrec-expressions%20in%20Enriched%20lambda%20Calculus/#letrec-expressions-in-enriched-lambda-calculus","title":"letrec-expressions in Enriched lambda Calculus","text":"<p>The syntax of \\(\\text{letrec-}\\)expressions is similar to \\(\\text{let-}\\)expressions. <pre><code>letrec v1 = E1\n       v2 = E2\n       ...\n       vn = En\nin\n       E\n</code></pre> where <code>v1</code>..<code>vn</code> are variables and <code>E</code>, <code>E1</code>..<code>En</code> are expressions in Enriched Lambda Calculus</p> <p>the term <code>letrec</code> is short for let recursively, it introduces the possibility to define variables recursive, the difference between <code>let</code> and <code>letrec</code> is that the scope of any <code>v</code> is <code>E</code> and all of the <code>E1</code>..<code>En</code>.</p> <p>Example: <pre><code>letrec fac = \\n IF (= n 0) \n                   1 \n                   (* n (fac (- n 1)))\nin     fac 4\n</code></pre></p> <p>Multi-line definitions are necessary for <code>letrec</code> for example <pre><code>letrec f = ... f ... g\n       g = ... f ...\nin ...\n</code></pre> Here nesting makes the inner definition out of scope for the outer one.</p> <p>The semantics for single definition <code>letrec</code> is given by the Y combinator. we say <pre><code>letrec v = B in E &lt;=&gt; let v = Y (\\v.B) in E\n</code></pre></p>","tags":["Note"]},{"location":"letrec-expressions%20in%20Enriched%20lambda%20Calculus/#references","title":"References","text":"<p>Enriched Lambda Calculus let-expressions in Enriched Lambda Calculus</p>","tags":["Note"]},{"location":"letrec-expressions%20to%20Irrefurtable%20let-expressions/","title":"letrec expressions to Irrefurtable let expressions","text":"<p>202311072011</p> <p>Tags : [[Programming Languages]]</p>","tags":["Note","Incomplete"]},{"location":"letrec-expressions%20to%20Irrefurtable%20let-expressions/#letrec-expressions-to-irrefurtable-let-expressions","title":"letrec-expressions to Irrefurtable let-expressions","text":"<p>A letrec with a single definition, can be converted as follows <pre><code>letrec v = B in E   ===   let v = Y(\\v.B) in E\n</code></pre></p> <p>To extend this definition for the general case, we first convert a letrec so that it will have just 1 definition as follows</p> <pre><code>letrec p1 = B1   ===   letrec (t p1 ... pn) = (t B1 ... Bn) in E\n       ...\n       pn = Bn\nin E\n</code></pre> <p>Now we use the above idea to get  <pre><code>letrec p = B in E    ===    let p = Y(\\p.B) in E\n</code></pre></p>","tags":["Note","Incomplete"]},{"location":"letrec-expressions%20to%20Irrefurtable%20let-expressions/#references","title":"References","text":"","tags":["Note","Incomplete"]},{"location":"similar%20tunes/","title":"Similar tunes","text":"<p>Kaun tujhe starting - Set fire to the rain</p>"},{"location":"ww%20where%20w%20is%20any%20word/","title":"Ww where w is any word","text":"<p>202211150118</p> <p>type : #Example tags : [[Theory of Computation]]</p>"},{"location":"ww%20where%20w%20is%20any%20word/#ww-where-w-is-any-word","title":"ww where w is any word","text":""},{"location":"ww%20where%20w%20is%20any%20word/#question-wwwin-a-b-is-a-recursive-set","title":"Question: \\(\\{ww|w\\in \\{a, b\\}^*\\}\\) is a recursive set","text":""},{"location":"ww%20where%20w%20is%20any%20word/#answer","title":"Answer:","text":"<p>The machine first scans through the word until it finds the first \\(\\sqcup\\) and keeps track of the size of the word in mod 2, rejecting immediately if the word is not of even length. If it is then it puts a \\(\\dashv\\) at the end and then repeatedly scans back and forth the input. On each passing from right to left, it marks the first unmarked \\(a\\) or \\(b\\) with a \\('\\) and on each passing from left to right, it marks the first unmarked letter with \\(`\\), it continues until all symbols are marked.</p> <p>The initial tape contents are $$ \\begin{matrix} \\vdash &amp;a&amp;a&amp;b&amp;b&amp;a&amp;a&amp;a&amp;b&amp;a \\end{matrix} $$ and the tape condition in the following few steps would be  $$ \\begin{matrix} \\vdash &amp;a&amp;a&amp;b&amp;b&amp;a&amp;a&amp;a&amp;b&amp;b&amp;\\acute a &amp;\\dashv &amp;\\sqcup&amp;\\sqcup&amp;\\sqcup&amp;\\dots\\ \\vdash &amp;\\dot a&amp;a&amp;b&amp;b&amp;a&amp;a&amp;a&amp;b&amp;b&amp;\\acute a &amp;\\dashv&amp; \\sqcup&amp;\\sqcup&amp;\\sqcup&amp;\\dots\\ \\vdash &amp;\\dot a &amp;\\dot a&amp;b&amp;b&amp;a&amp;a&amp;a&amp;b&amp;\\acute b&amp;\\acute a&amp; \\dashv&amp; \\sqcup&amp;\\sqcup&amp;\\sqcup&amp;\\dots\\ \\vdash &amp;\\dot a &amp;\\dot a&amp;b&amp;b&amp;a&amp;a&amp;a&amp;b&amp;\\acute b&amp;\\acute a &amp;\\dashv &amp;\\sqcup&amp;\\sqcup&amp;\\sqcup&amp;\\dots\\ \\vdash &amp;\\dot a&amp;\\dot a&amp;b&amp;b&amp;a&amp;a&amp;a&amp;\\acute b&amp;\\acute b&amp; \\acute a&amp; \\dashv&amp; \\sqcup&amp;\\sqcup&amp;\\sqcup&amp;\\dots\\</p> <p>\\end{matrix} $$ now the left and the right half of the word are marked differently. $$ \\begin{matrix} \\vdash &amp;\\dot{a}&amp;\\dot{a}&amp;\\dot b &amp;\\dot b&amp;\\dot{a}&amp;\\acute{a}&amp;\\acute{a}&amp;\\acute b&amp;\\acute b&amp;\\acute{a}&amp; \\dashv &amp;\\sqcup &amp;\\sqcup&amp;\\sqcup&amp; \\dots \\end{matrix} $$</p> <p>Then the machine starts from the left and scans to the right repeatedly, it sees the first unremoved symbol, removes it, but remembers it, then it keeps moving forward until it finds the first symbol marked the other way, if it corresponds to the original letter then repeat the process, it at any point it fails,  move to the reject state. Otherwise it will remove all letters and then go to the accpeting state.</p> \\[ \\begin{matrix} \\vdash &amp; \\dot a &amp; \\dot a &amp; \\dot b &amp; \\dot b &amp;\\dot a&amp; \\acute a &amp; \\acute a&amp; \\acute b &amp; \\acute b &amp;\\acute a &amp; \\dashv &amp; \\sqcup &amp; \\sqcup &amp; \\sqcup &amp; \\dots\\\\ \\vdash &amp; \\sqcup &amp; \\dot a &amp; \\dot b &amp; \\dot b &amp;\\dot a&amp; \\sqcup &amp; \\acute a&amp; \\acute b &amp; \\acute b &amp;\\acute a &amp; \\dashv &amp; \\sqcup &amp; \\sqcup &amp; \\sqcup &amp; \\dots\\\\ \\vdash &amp; \\sqcup &amp; \\sqcup &amp; \\dot b &amp; \\dot b &amp;\\dot a&amp; \\sqcup &amp; \\sqcup&amp; \\acute b &amp; \\acute b &amp;\\acute a &amp; \\dashv &amp; \\sqcup &amp; \\sqcup &amp; \\sqcup &amp; \\dots\\\\ \\vdash &amp; \\sqcup &amp; \\sqcup &amp; \\sqcup &amp; \\dot b &amp;\\dot a&amp; \\sqcup &amp; \\sqcup&amp; \\sqcup &amp; \\acute b &amp;\\acute a &amp; \\dashv &amp; \\sqcup &amp; \\sqcup &amp; \\sqcup &amp; \\dots\\\\ \\vdash &amp; \\sqcup &amp; \\sqcup &amp; \\sqcup &amp; \\sqcup &amp;\\dot a&amp; \\sqcup &amp; \\sqcup&amp; \\sqcup &amp; \\sqcup &amp;\\acute a &amp; \\dashv &amp; \\sqcup &amp; \\sqcup &amp; \\sqcup &amp; \\dots\\\\ \\vdash &amp; \\sqcup &amp; \\sqcup &amp; \\sqcup &amp; \\sqcup &amp;\\sqcup&amp; \\sqcup &amp; \\sqcup&amp; \\sqcup &amp; \\sqcup &amp;\\sqcup &amp; \\dashv &amp; \\sqcup &amp; \\sqcup &amp; \\sqcup &amp; \\dots\\\\ \\end{matrix} \\]"},{"location":"ww%20where%20w%20is%20any%20word/#related","title":"Related","text":""},{"location":"Features/LaTeX%20Math%20Support/","title":"LaTeX Math Support","text":"<p>LaTeX math is supported using MathJax.</p> <p>Inline math looks like \\(f(x) = x^2\\). The input for this is <code>$f(x) = x^2$</code>. Use <code>$...$</code>.</p> <p>For a block of math, use <code>$$...$$</code> on separate lines</p> <pre><code>$$\nF(x) = \\int^a_b \\frac{1}{2}x^4\n$$\n</code></pre> <p>gives </p> \\[ F(x) = \\int^a_b \\frac{1}{2}x^4 \\]"},{"location":"Features/Mermaid%20Diagrams/","title":"Mermaid diagrams","text":"<p>Here's the example from MkDocs Material documentation: </p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"},{"location":"Features/Text%20Formatting/","title":"Text Formatting","text":"<p>You can have lists like this</p> <ul> <li>first</li> <li>second</li> <li>third</li> </ul> <p>Or checklist lists to</p> <ul> <li> Get</li> <li> things</li> <li> done</li> </ul> <p>Also, get highlights and strikethroughs as above (similar to Obsidian).</p> <p>More formatting options for your webpage here. (but not compatible with Obsidian)</p>"},{"location":"Topic%201/Note%201/","title":"Note 1","text":"<p>Example: link to Mermaid Diagrams under <code>Features</code></p>"},{"location":"Topic%201/Note%202/","title":"Note 2","text":""}]}